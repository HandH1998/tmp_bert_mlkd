06/11/2022 11:50:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
06/11/2022 11:50:32 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/original_pretrained_model/bert-large-uncased/config.json
06/11/2022 11:50:32 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

06/11/2022 11:50:32 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/original_pretrained_model/bert-large-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/original_pretrained_model/bert-large-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
06/11/2022 11:50:32 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/bert-large-uncased/added_tokens.json. We won't load it.
06/11/2022 11:50:32 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/bert-large-uncased/special_tokens_map.json. We won't load it.
06/11/2022 11:50:32 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/bert-large-uncased/tokenizer_config.json. We won't load it.
06/11/2022 11:50:32 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/original_pretrained_model/bert-large-uncased/vocab.txt
06/11/2022 11:50:32 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 11:50:32 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 11:50:32 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 11:50:32 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/original_pretrained_model/bert-large-uncased/pytorch_model.bin
06/11/2022 11:50:39 - INFO - bert_fineturn.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
06/11/2022 11:50:39 - INFO - bert_fineturn.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
06/11/2022 11:50:42 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='../../data/glue_data/CoLA/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_predict=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_id=0, gradient_accumulation_steps=1, learning_rate=1e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=64, max_steps=-1, model_name_or_path='../../model/original_pretrained_model/bert-large-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=6.0, output_dir='../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=1000000, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', warmup_steps=0, weight_decay=0.0)
06/11/2022 11:50:42 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/CoLA/cached_train_bert-large-uncased_64_cola
06/11/2022 11:50:42 - INFO - __main__ -   ***** Running training *****
06/11/2022 11:50:42 - INFO - __main__ -     Num examples = 8551
06/11/2022 11:50:42 - INFO - __main__ -     Num Epochs = 6
06/11/2022 11:50:42 - INFO - __main__ -     Instantaneous batch size per GPU = 32
06/11/2022 11:50:42 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
06/11/2022 11:50:42 - INFO - __main__ -     Gradient Accumulation steps = 1
06/11/2022 11:50:42 - INFO - __main__ -     Total optimization steps = 1608
06/11/2022 11:50:42 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step
06/11/2022 11:50:42 - INFO - __main__ -     Continuing training from epoch 0
06/11/2022 11:50:42 - INFO - __main__ -     Continuing training from global step 0
06/11/2022 11:50:42 - INFO - __main__ -     Will skip the first 0 steps in the first epoch
06/11/2022 11:50:42 - INFO - __main__ -   epoch 0/6
/root/autodl-tmp/tiny_bert/BERT-EMD/bert_fineturn/optimization.py:155: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
06/11/2022 11:51:35 - INFO - __main__ -   epoch 1/6
06/11/2022 11:52:21 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/CoLA/cached_dev_bert-large-uncased_64_cola
06/11/2022 11:52:21 - INFO - __main__ -   ***** Running evaluation  *****
06/11/2022 11:52:21 - INFO - __main__ -     Num examples = 1043
06/11/2022 11:52:21 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/131 [00:00<?, ?it/s]Evaluating:   5%|▌         | 7/131 [00:00<00:01, 65.33it/s]Evaluating:  11%|█         | 14/131 [00:00<00:01, 65.51it/s]Evaluating:  16%|█▌        | 21/131 [00:00<00:01, 65.53it/s]Evaluating:  21%|██▏       | 28/131 [00:00<00:01, 65.67it/s]Evaluating:  27%|██▋       | 35/131 [00:00<00:01, 65.23it/s]Evaluating:  32%|███▏      | 42/131 [00:00<00:01, 64.60it/s]Evaluating:  37%|███▋      | 49/131 [00:00<00:01, 64.98it/s]Evaluating:  43%|████▎     | 56/131 [00:00<00:01, 64.99it/s]Evaluating:  48%|████▊     | 63/131 [00:00<00:01, 64.95it/s]Evaluating:  53%|█████▎    | 70/131 [00:01<00:00, 64.86it/s]Evaluating:  59%|█████▉    | 77/131 [00:01<00:00, 64.57it/s]Evaluating:  64%|██████▍   | 84/131 [00:01<00:00, 64.24it/s]Evaluating:  69%|██████▉   | 91/131 [00:01<00:00, 64.19it/s]Evaluating:  75%|███████▍  | 98/131 [00:01<00:00, 64.36it/s]Evaluating:  80%|████████  | 105/131 [00:01<00:00, 64.26it/s]Evaluating:  85%|████████▌ | 112/131 [00:01<00:00, 63.70it/s]Evaluating:  91%|█████████ | 119/131 [00:01<00:00, 63.28it/s]Evaluating:  96%|█████████▌| 126/131 [00:01<00:00, 63.69it/s]Evaluating: 100%|██████████| 131/131 [00:02<00:00, 64.44it/s]
06/11/2022 11:52:23 - INFO - __main__ -   ***** Eval results  *****
06/11/2022 11:52:23 - INFO - __main__ -     mcc = 0.5803715696880752
/root/miniconda3/envs/TinyBert/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
{"eval_mcc": 0.5803715696880752, "learning_rate": 6.8905472636815925e-06, "loss": 0.3735790603607893, "step": 500}
06/11/2022 11:52:30 - INFO - __main__ -   epoch 2/6
06/11/2022 11:53:22 - INFO - __main__ -   epoch 3/6
06/11/2022 11:54:01 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/CoLA/cached_dev_bert-large-uncased_64_cola
06/11/2022 11:54:01 - INFO - __main__ -   ***** Running evaluation  *****
06/11/2022 11:54:01 - INFO - __main__ -     Num examples = 1043
06/11/2022 11:54:01 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/131 [00:00<?, ?it/s]Evaluating:   5%|▌         | 7/131 [00:00<00:01, 65.14it/s]Evaluating:  11%|█         | 14/131 [00:00<00:01, 65.48it/s]Evaluating:  16%|█▌        | 21/131 [00:00<00:01, 65.75it/s]Evaluating:  21%|██▏       | 28/131 [00:00<00:01, 65.55it/s]Evaluating:  27%|██▋       | 35/131 [00:00<00:01, 65.33it/s]Evaluating:  32%|███▏      | 42/131 [00:00<00:01, 65.40it/s]Evaluating:  37%|███▋      | 49/131 [00:00<00:01, 65.64it/s]Evaluating:  43%|████▎     | 56/131 [00:00<00:01, 65.23it/s]Evaluating:  48%|████▊     | 63/131 [00:00<00:01, 64.93it/s]Evaluating:  53%|█████▎    | 70/131 [00:01<00:00, 64.13it/s]Evaluating:  59%|█████▉    | 77/131 [00:01<00:00, 63.96it/s]Evaluating:  64%|██████▍   | 84/131 [00:01<00:00, 63.93it/s]Evaluating:  69%|██████▉   | 91/131 [00:01<00:00, 64.43it/s]Evaluating:  75%|███████▍  | 98/131 [00:01<00:00, 64.33it/s]Evaluating:  80%|████████  | 105/131 [00:01<00:00, 64.23it/s]Evaluating:  85%|████████▌ | 112/131 [00:01<00:00, 64.58it/s]Evaluating:  91%|█████████ | 119/131 [00:01<00:00, 64.95it/s]Evaluating:  96%|█████████▌| 126/131 [00:01<00:00, 65.13it/s]Evaluating: 100%|██████████| 131/131 [00:02<00:00, 64.87it/s]
06/11/2022 11:54:03 - INFO - __main__ -   ***** Eval results  *****
06/11/2022 11:54:03 - INFO - __main__ -     mcc = 0.6056594364604692
{"eval_mcc": 0.6056594364604692, "learning_rate": 3.7810945273631843e-06, "loss": 0.16776572565175593, "step": 1000}
06/11/2022 11:54:17 - INFO - __main__ -   epoch 4/6
06/11/2022 11:55:09 - INFO - __main__ -   epoch 5/6
06/11/2022 11:55:41 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/CoLA/cached_dev_bert-large-uncased_64_cola
06/11/2022 11:55:41 - INFO - __main__ -   ***** Running evaluation  *****
06/11/2022 11:55:41 - INFO - __main__ -     Num examples = 1043
06/11/2022 11:55:41 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/131 [00:00<?, ?it/s]Evaluating:   5%|▌         | 7/131 [00:00<00:01, 65.96it/s]Evaluating:  11%|█         | 14/131 [00:00<00:01, 65.92it/s]Evaluating:  16%|█▌        | 21/131 [00:00<00:01, 66.07it/s]Evaluating:  21%|██▏       | 28/131 [00:00<00:01, 66.11it/s]Evaluating:  27%|██▋       | 35/131 [00:00<00:01, 65.29it/s]Evaluating:  32%|███▏      | 42/131 [00:00<00:01, 64.91it/s]Evaluating:  37%|███▋      | 49/131 [00:00<00:01, 64.62it/s]Evaluating:  43%|████▎     | 56/131 [00:00<00:01, 64.49it/s]Evaluating:  48%|████▊     | 63/131 [00:00<00:01, 64.39it/s]Evaluating:  53%|█████▎    | 70/131 [00:01<00:00, 64.15it/s]Evaluating:  59%|█████▉    | 77/131 [00:01<00:00, 64.61it/s]Evaluating:  64%|██████▍   | 84/131 [00:01<00:00, 64.97it/s]Evaluating:  69%|██████▉   | 91/131 [00:01<00:00, 65.17it/s]Evaluating:  75%|███████▍  | 98/131 [00:01<00:00, 65.26it/s]Evaluating:  80%|████████  | 105/131 [00:01<00:00, 65.11it/s]Evaluating:  85%|████████▌ | 112/131 [00:01<00:00, 65.29it/s]Evaluating:  91%|█████████ | 119/131 [00:01<00:00, 65.34it/s]Evaluating:  96%|█████████▌| 126/131 [00:01<00:00, 65.53it/s]Evaluating: 100%|██████████| 131/131 [00:02<00:00, 65.08it/s]
06/11/2022 11:55:43 - INFO - __main__ -   ***** Eval results  *****
06/11/2022 11:55:43 - INFO - __main__ -     mcc = 0.5930840237241851
{"eval_mcc": 0.5930840237241851, "learning_rate": 6.716417910447762e-07, "loss": 0.09304617992602289, "step": 1500}
06/11/2022 11:56:04 - INFO - __main__ -    global_step = 1608, average loss = 0.20234066574279433
06/11/2022 11:56:04 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/
06/11/2022 11:56:04 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/config.json
06/11/2022 11:56:05 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/pytorch_model.bin
06/11/2022 11:56:05 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/config.json
06/11/2022 11:56:05 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

06/11/2022 11:56:05 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/pytorch_model.bin
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/' is a path, a model identifier, or url to a directory containing tokenizer files.
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/added_tokens.json. We won't load it.
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/vocab.txt
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/special_tokens_map.json
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/tokenizer_config.json
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/' is a path, a model identifier, or url to a directory containing tokenizer files.
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/added_tokens.json. We won't load it.
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/vocab.txt
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/special_tokens_map.json
06/11/2022 11:56:11 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/tokenizer_config.json
06/11/2022 11:56:11 - INFO - __main__ -   Evaluate the following checkpoints: ['../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/']
06/11/2022 11:56:11 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/config.json
06/11/2022 11:56:11 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

06/11/2022 11:56:11 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/BERT-EMD/fine-tuned_pretrained_model/CoLA/on_original_data/pytorch_model.bin
06/11/2022 11:56:17 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/CoLA/cached_dev_bert-large-uncased_64_cola
06/11/2022 11:56:17 - INFO - __main__ -   ***** Running evaluation  *****
06/11/2022 11:56:17 - INFO - __main__ -     Num examples = 1043
06/11/2022 11:56:17 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/131 [00:00<?, ?it/s]Evaluating:   5%|▌         | 7/131 [00:00<00:01, 63.81it/s]Evaluating:  11%|█         | 14/131 [00:00<00:01, 64.39it/s]Evaluating:  16%|█▌        | 21/131 [00:00<00:01, 64.80it/s]Evaluating:  21%|██▏       | 28/131 [00:00<00:01, 64.94it/s]Evaluating:  27%|██▋       | 35/131 [00:00<00:01, 65.41it/s]Evaluating:  32%|███▏      | 42/131 [00:00<00:01, 65.61it/s]Evaluating:  37%|███▋      | 49/131 [00:00<00:01, 65.73it/s]Evaluating:  43%|████▎     | 56/131 [00:00<00:01, 65.32it/s]Evaluating:  48%|████▊     | 63/131 [00:00<00:01, 64.63it/s]Evaluating:  53%|█████▎    | 70/131 [00:01<00:00, 65.08it/s]Evaluating:  59%|█████▉    | 77/131 [00:01<00:00, 65.39it/s]Evaluating:  64%|██████▍   | 84/131 [00:01<00:00, 65.63it/s]Evaluating:  69%|██████▉   | 91/131 [00:01<00:00, 65.81it/s]Evaluating:  75%|███████▍  | 98/131 [00:01<00:00, 65.86it/s]Evaluating:  80%|████████  | 105/131 [00:01<00:00, 65.99it/s]Evaluating:  85%|████████▌ | 112/131 [00:01<00:00, 66.02it/s]Evaluating:  91%|█████████ | 119/131 [00:01<00:00, 66.02it/s]Evaluating:  96%|█████████▌| 126/131 [00:01<00:00, 65.35it/s]Evaluating: 100%|██████████| 131/131 [00:02<00:00, 65.44it/s]
06/11/2022 11:56:19 - INFO - __main__ -   ***** Eval results  *****
06/11/2022 11:56:19 - INFO - __main__ -     mcc = 0.5858661515147512
