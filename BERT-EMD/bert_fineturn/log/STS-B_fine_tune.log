06/11/2022 12:04:06 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
06/11/2022 12:04:06 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/original_pretrained_model/bert-large-uncased/config.json
06/11/2022 12:04:06 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 1,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "sts-b",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

06/11/2022 12:04:06 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/original_pretrained_model/bert-large-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/original_pretrained_model/bert-large-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
06/11/2022 12:04:06 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/bert-large-uncased/added_tokens.json. We won't load it.
06/11/2022 12:04:06 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/bert-large-uncased/special_tokens_map.json. We won't load it.
06/11/2022 12:04:06 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/bert-large-uncased/tokenizer_config.json. We won't load it.
06/11/2022 12:04:06 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/original_pretrained_model/bert-large-uncased/vocab.txt
06/11/2022 12:04:06 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 12:04:06 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 12:04:06 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 12:04:06 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/original_pretrained_model/bert-large-uncased/pytorch_model.bin
06/11/2022 12:04:12 - INFO - bert_fineturn.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
06/11/2022 12:04:12 - INFO - bert_fineturn.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
06/11/2022 12:04:15 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='../../data/glue_data/STS-B/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_predict=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gpu_id=0, gradient_accumulation_steps=1, learning_rate=3e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='../../model/original_pretrained_model/bert-large-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=6.0, output_dir='../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/', output_mode='regression', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=1000000, seed=42, server_ip='', server_port='', task_name='sts-b', tokenizer_name='', warmup_steps=0, weight_decay=0.0)
06/11/2022 12:04:15 - INFO - __main__ -   Creating features from dataset file at ../../data/glue_data/STS-B/
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   Writing example 0/5749
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   guid: train-0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 4946 2003 2635 2125 1012 102 2019 2250 4946 2003 2635 2125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   label: 5.000 (id = 5)
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   guid: train-1
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 2158 2003 2652 1037 2312 8928 1012 102 1037 2158 2003 2652 1037 8928 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   label: 3.800 (id = 3)
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   guid: train-2
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 2158 2003 9359 14021 5596 2098 8808 2006 1037 10733 1012 102 1037 2158 2003 9359 29022 8808 2006 2019 4895 3597 23461 10733 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   label: 3.800 (id = 3)
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   guid: train-3
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 2093 2273 2024 2652 7433 1012 102 2048 2273 2024 2652 7433 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   label: 2.600 (id = 2)
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   guid: train-4
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 2158 2003 2652 1996 10145 1012 102 1037 2158 8901 2003 2652 1996 10145 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:04:15 - INFO - bert_fineturn.data_processor.glue -   label: 4.250 (id = 4)
06/11/2022 12:04:18 - INFO - __main__ -   Saving features into cached file ../../data/glue_data/STS-B/cached_train_bert-large-uncased_128_sts-b
06/11/2022 12:04:19 - INFO - __main__ -   ***** Running training *****
06/11/2022 12:04:19 - INFO - __main__ -     Num examples = 5749
06/11/2022 12:04:19 - INFO - __main__ -     Num Epochs = 6
06/11/2022 12:04:19 - INFO - __main__ -     Instantaneous batch size per GPU = 32
06/11/2022 12:04:19 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
06/11/2022 12:04:19 - INFO - __main__ -     Gradient Accumulation steps = 1
06/11/2022 12:04:19 - INFO - __main__ -     Total optimization steps = 1080
06/11/2022 12:04:19 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step
06/11/2022 12:04:19 - INFO - __main__ -     Continuing training from epoch 0
06/11/2022 12:04:19 - INFO - __main__ -     Continuing training from global step 0
06/11/2022 12:04:19 - INFO - __main__ -     Will skip the first 0 steps in the first epoch
06/11/2022 12:04:19 - INFO - __main__ -   epoch 0/6
/root/autodl-tmp/tiny_bert/BERT-EMD/bert_fineturn/optimization.py:155: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
06/11/2022 12:05:21 - INFO - __main__ -   epoch 1/6
06/11/2022 12:06:23 - INFO - __main__ -   epoch 2/6
06/11/2022 12:07:12 - INFO - __main__ -   Creating features from dataset file at ../../data/glue_data/STS-B/
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   Writing example 0/1500
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   guid: dev-0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 2158 2007 1037 2524 6045 2003 5613 1012 102 1037 2158 4147 1037 2524 6045 2003 5613 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   label: 5.000 (id = 5)
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   guid: dev-1
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 2402 2775 2003 5559 1037 3586 1012 102 1037 2775 2003 5559 1037 3586 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   label: 4.750 (id = 4)
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   guid: dev-2
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 2158 2003 8521 1037 8000 2000 1037 7488 1012 102 1996 2158 2003 8521 1037 8000 2000 1996 7488 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   label: 5.000 (id = 5)
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   guid: dev-3
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 2450 2003 2652 1996 2858 1012 102 1037 2158 2003 2652 2858 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   label: 2.400 (id = 2)
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   *** Example ***
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   guid: dev-4
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   input_ids: 101 1037 2450 2003 2652 1996 8928 1012 102 1037 2158 2003 2652 1037 8928 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/11/2022 12:07:12 - INFO - bert_fineturn.data_processor.glue -   label: 2.750 (id = 2)
06/11/2022 12:07:12 - INFO - __main__ -   Saving features into cached file ../../data/glue_data/STS-B/cached_dev_bert-large-uncased_128_sts-b
06/11/2022 12:07:12 - INFO - __main__ -   ***** Running evaluation  *****
06/11/2022 12:07:12 - INFO - __main__ -     Num examples = 1500
06/11/2022 12:07:12 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/188 [00:00<?, ?it/s]Evaluating:   2%|▏         | 4/188 [00:00<00:05, 35.95it/s]Evaluating:   4%|▍         | 8/188 [00:00<00:05, 35.86it/s]Evaluating:   6%|▋         | 12/188 [00:00<00:04, 35.88it/s]Evaluating:   9%|▊         | 16/188 [00:00<00:04, 35.94it/s]Evaluating:  11%|█         | 20/188 [00:00<00:04, 35.91it/s]Evaluating:  13%|█▎        | 24/188 [00:00<00:04, 35.91it/s]Evaluating:  15%|█▍        | 28/188 [00:00<00:04, 35.92it/s]Evaluating:  17%|█▋        | 32/188 [00:00<00:04, 35.83it/s]Evaluating:  19%|█▉        | 36/188 [00:01<00:04, 35.73it/s]Evaluating:  21%|██▏       | 40/188 [00:01<00:04, 35.72it/s]Evaluating:  23%|██▎       | 44/188 [00:01<00:04, 35.67it/s]Evaluating:  26%|██▌       | 48/188 [00:01<00:03, 35.71it/s]Evaluating:  28%|██▊       | 52/188 [00:01<00:03, 35.77it/s]Evaluating:  30%|██▉       | 56/188 [00:01<00:03, 35.85it/s]Evaluating:  32%|███▏      | 60/188 [00:01<00:03, 35.86it/s]Evaluating:  34%|███▍      | 64/188 [00:01<00:03, 35.86it/s]Evaluating:  36%|███▌      | 68/188 [00:01<00:03, 35.78it/s]Evaluating:  38%|███▊      | 72/188 [00:02<00:03, 35.57it/s]Evaluating:  40%|████      | 76/188 [00:02<00:03, 35.62it/s]Evaluating:  43%|████▎     | 80/188 [00:02<00:03, 35.57it/s]Evaluating:  45%|████▍     | 84/188 [00:02<00:02, 35.59it/s]Evaluating:  47%|████▋     | 88/188 [00:02<00:02, 35.65it/s]Evaluating:  49%|████▉     | 92/188 [00:02<00:02, 35.70it/s]Evaluating:  51%|█████     | 96/188 [00:02<00:02, 35.71it/s]Evaluating:  53%|█████▎    | 100/188 [00:02<00:02, 35.70it/s]Evaluating:  55%|█████▌    | 104/188 [00:02<00:02, 35.65it/s]Evaluating:  57%|█████▋    | 108/188 [00:03<00:02, 35.66it/s]Evaluating:  60%|█████▉    | 112/188 [00:03<00:02, 35.63it/s]Evaluating:  62%|██████▏   | 116/188 [00:03<00:02, 35.67it/s]Evaluating:  64%|██████▍   | 120/188 [00:03<00:01, 35.71it/s]Evaluating:  66%|██████▌   | 124/188 [00:03<00:01, 35.69it/s]Evaluating:  68%|██████▊   | 128/188 [00:03<00:01, 35.75it/s]Evaluating:  70%|███████   | 132/188 [00:03<00:01, 35.80it/s]Evaluating:  72%|███████▏  | 136/188 [00:03<00:01, 35.81it/s]Evaluating:  74%|███████▍  | 140/188 [00:03<00:01, 35.79it/s]Evaluating:  77%|███████▋  | 144/188 [00:04<00:01, 35.72it/s]Evaluating:  79%|███████▊  | 148/188 [00:04<00:01, 35.69it/s]Evaluating:  81%|████████  | 152/188 [00:04<00:01, 35.71it/s]Evaluating:  83%|████████▎ | 156/188 [00:04<00:00, 35.75it/s]Evaluating:  85%|████████▌ | 160/188 [00:04<00:00, 35.77it/s]Evaluating:  87%|████████▋ | 164/188 [00:04<00:00, 35.79it/s]Evaluating:  89%|████████▉ | 168/188 [00:04<00:00, 35.84it/s]Evaluating:  91%|█████████▏| 172/188 [00:04<00:00, 35.85it/s]Evaluating:  94%|█████████▎| 176/188 [00:04<00:00, 35.85it/s]Evaluating:  96%|█████████▌| 180/188 [00:05<00:00, 35.78it/s]Evaluating:  98%|█████████▊| 184/188 [00:05<00:00, 35.74it/s]Evaluating: 100%|██████████| 188/188 [00:05<00:00, 36.88it/s]Evaluating: 100%|██████████| 188/188 [00:05<00:00, 35.82it/s]
06/11/2022 12:07:18 - INFO - numexpr.utils -   Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
06/11/2022 12:07:18 - INFO - numexpr.utils -   Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
06/11/2022 12:07:18 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.
06/11/2022 12:07:18 - INFO - __main__ -   ***** Eval results  *****
06/11/2022 12:07:18 - INFO - __main__ -     corr = 0.8938328108744474
06/11/2022 12:07:18 - INFO - __main__ -     pearson = 0.8953988929002245
06/11/2022 12:07:18 - INFO - __main__ -     spearmanr = 0.8922667288486704
/root/miniconda3/envs/TinyBert/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
{"eval_pearson": 0.8953988929002245, "eval_spearmanr": 0.8922667288486704, "eval_corr": 0.8938328108744474, "learning_rate": 1.6111111111111115e-05, "loss": 0.5650783601701259, "step": 500}
06/11/2022 12:07:31 - INFO - __main__ -   epoch 3/6
06/11/2022 12:08:33 - INFO - __main__ -   epoch 4/6
06/11/2022 12:09:35 - INFO - __main__ -   epoch 5/6
06/11/2022 12:10:10 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/STS-B/cached_dev_bert-large-uncased_128_sts-b
06/11/2022 12:10:10 - INFO - __main__ -   ***** Running evaluation  *****
06/11/2022 12:10:10 - INFO - __main__ -     Num examples = 1500
06/11/2022 12:10:10 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/188 [00:00<?, ?it/s]Evaluating:   2%|▏         | 4/188 [00:00<00:05, 36.37it/s]Evaluating:   4%|▍         | 8/188 [00:00<00:04, 36.16it/s]Evaluating:   6%|▋         | 12/188 [00:00<00:04, 36.00it/s]Evaluating:   9%|▊         | 16/188 [00:00<00:04, 35.98it/s]Evaluating:  11%|█         | 20/188 [00:00<00:04, 35.93it/s]Evaluating:  13%|█▎        | 24/188 [00:00<00:04, 35.89it/s]Evaluating:  15%|█▍        | 28/188 [00:00<00:04, 35.89it/s]Evaluating:  17%|█▋        | 32/188 [00:00<00:04, 35.83it/s]Evaluating:  19%|█▉        | 36/188 [00:01<00:04, 35.81it/s]Evaluating:  21%|██▏       | 40/188 [00:01<00:04, 35.78it/s]Evaluating:  23%|██▎       | 44/188 [00:01<00:04, 35.77it/s]Evaluating:  26%|██▌       | 48/188 [00:01<00:03, 35.75it/s]Evaluating:  28%|██▊       | 52/188 [00:01<00:03, 35.70it/s]Evaluating:  30%|██▉       | 56/188 [00:01<00:03, 35.72it/s]Evaluating:  32%|███▏      | 60/188 [00:01<00:03, 35.70it/s]Evaluating:  34%|███▍      | 64/188 [00:01<00:03, 35.72it/s]Evaluating:  36%|███▌      | 68/188 [00:01<00:03, 35.73it/s]Evaluating:  38%|███▊      | 72/188 [00:02<00:03, 35.72it/s]Evaluating:  40%|████      | 76/188 [00:02<00:03, 35.72it/s]Evaluating:  43%|████▎     | 80/188 [00:02<00:03, 35.72it/s]Evaluating:  45%|████▍     | 84/188 [00:02<00:02, 35.72it/s]Evaluating:  47%|████▋     | 88/188 [00:02<00:02, 35.73it/s]Evaluating:  49%|████▉     | 92/188 [00:02<00:02, 35.70it/s]Evaluating:  51%|█████     | 96/188 [00:02<00:02, 35.72it/s]Evaluating:  53%|█████▎    | 100/188 [00:02<00:02, 35.69it/s]Evaluating:  55%|█████▌    | 104/188 [00:02<00:02, 35.70it/s]Evaluating:  57%|█████▋    | 108/188 [00:03<00:02, 35.72it/s]Evaluating:  60%|█████▉    | 112/188 [00:03<00:02, 35.70it/s]Evaluating:  62%|██████▏   | 116/188 [00:03<00:02, 35.72it/s]Evaluating:  64%|██████▍   | 120/188 [00:03<00:01, 35.70it/s]Evaluating:  66%|██████▌   | 124/188 [00:03<00:01, 35.72it/s]Evaluating:  68%|██████▊   | 128/188 [00:03<00:01, 35.74it/s]Evaluating:  70%|███████   | 132/188 [00:03<00:01, 35.69it/s]Evaluating:  72%|███████▏  | 136/188 [00:03<00:01, 35.68it/s]Evaluating:  74%|███████▍  | 140/188 [00:03<00:01, 35.72it/s]Evaluating:  77%|███████▋  | 144/188 [00:04<00:01, 35.71it/s]Evaluating:  79%|███████▊  | 148/188 [00:04<00:01, 35.76it/s]Evaluating:  81%|████████  | 152/188 [00:04<00:01, 35.74it/s]Evaluating:  83%|████████▎ | 156/188 [00:04<00:00, 35.76it/s]Evaluating:  85%|████████▌ | 160/188 [00:04<00:00, 35.73it/s]Evaluating:  87%|████████▋ | 164/188 [00:04<00:00, 35.73it/s]Evaluating:  89%|████████▉ | 168/188 [00:04<00:00, 35.75it/s]Evaluating:  91%|█████████▏| 172/188 [00:04<00:00, 35.76it/s]Evaluating:  94%|█████████▎| 176/188 [00:04<00:00, 35.81it/s]Evaluating:  96%|█████████▌| 180/188 [00:05<00:00, 35.81it/s]Evaluating:  98%|█████████▊| 184/188 [00:05<00:00, 35.82it/s]Evaluating: 100%|██████████| 188/188 [00:05<00:00, 36.98it/s]Evaluating: 100%|██████████| 188/188 [00:05<00:00, 35.83it/s]
06/11/2022 12:10:15 - INFO - __main__ -   ***** Eval results  *****
06/11/2022 12:10:15 - INFO - __main__ -     corr = 0.9007202331014538
06/11/2022 12:10:15 - INFO - __main__ -     pearson = 0.9026991492619167
06/11/2022 12:10:15 - INFO - __main__ -     spearmanr = 0.8987413169409908
{"eval_pearson": 0.9026991492619167, "eval_spearmanr": 0.8987413169409908, "eval_corr": 0.9007202331014538, "learning_rate": 2.222222222222222e-06, "loss": 0.1399289167970419, "step": 1000}
06/11/2022 12:10:43 - INFO - __main__ -    global_step = 1080, average loss = 0.3329234955108
06/11/2022 12:10:43 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/
06/11/2022 12:10:43 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/config.json
06/11/2022 12:10:44 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/pytorch_model.bin
06/11/2022 12:10:44 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/config.json
06/11/2022 12:10:44 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 1,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "sts-b",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

06/11/2022 12:10:44 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/pytorch_model.bin
06/11/2022 12:10:49 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/' is a path, a model identifier, or url to a directory containing tokenizer files.
06/11/2022 12:10:49 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/added_tokens.json. We won't load it.
06/11/2022 12:10:49 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/vocab.txt
06/11/2022 12:10:49 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 12:10:49 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/special_tokens_map.json
06/11/2022 12:10:49 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/tokenizer_config.json
06/11/2022 12:10:50 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/' is a path, a model identifier, or url to a directory containing tokenizer files.
06/11/2022 12:10:50 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/added_tokens.json. We won't load it.
06/11/2022 12:10:50 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/vocab.txt
06/11/2022 12:10:50 - INFO - bert_fineturn.tokenization_utils -   loading file None
06/11/2022 12:10:50 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/special_tokens_map.json
06/11/2022 12:10:50 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/tokenizer_config.json
06/11/2022 12:10:50 - INFO - __main__ -   Evaluate the following checkpoints: ['../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/']
06/11/2022 12:10:50 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/config.json
06/11/2022 12:10:50 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 1,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "sts-b",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

06/11/2022 12:10:50 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/BERT-EMD/fine-tuned_pretrained_model/STS-B/on_original_data/pytorch_model.bin
06/11/2022 12:10:56 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/STS-B/cached_dev_bert-large-uncased_128_sts-b
06/11/2022 12:10:56 - INFO - __main__ -   ***** Running evaluation  *****
06/11/2022 12:10:56 - INFO - __main__ -     Num examples = 1500
06/11/2022 12:10:56 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/188 [00:00<?, ?it/s]Evaluating:   2%|▏         | 4/188 [00:00<00:05, 36.07it/s]Evaluating:   4%|▍         | 8/188 [00:00<00:05, 35.90it/s]Evaluating:   6%|▋         | 12/188 [00:00<00:04, 35.78it/s]Evaluating:   9%|▊         | 16/188 [00:00<00:04, 35.69it/s]Evaluating:  11%|█         | 20/188 [00:00<00:04, 35.68it/s]Evaluating:  13%|█▎        | 24/188 [00:00<00:04, 35.69it/s]Evaluating:  15%|█▍        | 28/188 [00:00<00:04, 35.74it/s]Evaluating:  17%|█▋        | 32/188 [00:00<00:04, 35.79it/s]Evaluating:  19%|█▉        | 36/188 [00:01<00:04, 35.84it/s]Evaluating:  21%|██▏       | 40/188 [00:01<00:04, 35.88it/s]Evaluating:  23%|██▎       | 44/188 [00:01<00:04, 35.83it/s]Evaluating:  26%|██▌       | 48/188 [00:01<00:03, 35.78it/s]Evaluating:  28%|██▊       | 52/188 [00:01<00:03, 35.75it/s]Evaluating:  30%|██▉       | 56/188 [00:01<00:03, 35.74it/s]Evaluating:  32%|███▏      | 60/188 [00:01<00:03, 35.72it/s]Evaluating:  34%|███▍      | 64/188 [00:01<00:03, 35.70it/s]Evaluating:  36%|███▌      | 68/188 [00:01<00:03, 35.77it/s]Evaluating:  38%|███▊      | 72/188 [00:02<00:03, 35.77it/s]Evaluating:  40%|████      | 76/188 [00:02<00:03, 35.76it/s]Evaluating:  43%|████▎     | 80/188 [00:02<00:03, 35.77it/s]Evaluating:  45%|████▍     | 84/188 [00:02<00:02, 35.76it/s]Evaluating:  47%|████▋     | 88/188 [00:02<00:02, 35.79it/s]Evaluating:  49%|████▉     | 92/188 [00:02<00:02, 35.74it/s]Evaluating:  51%|█████     | 96/188 [00:02<00:02, 35.71it/s]Evaluating:  53%|█████▎    | 100/188 [00:02<00:02, 35.73it/s]Evaluating:  55%|█████▌    | 104/188 [00:02<00:02, 35.71it/s]Evaluating:  57%|█████▋    | 108/188 [00:03<00:02, 35.74it/s]Evaluating:  60%|█████▉    | 112/188 [00:03<00:02, 35.73it/s]Evaluating:  62%|██████▏   | 116/188 [00:03<00:02, 35.74it/s]Evaluating:  64%|██████▍   | 120/188 [00:03<00:01, 35.77it/s]Evaluating:  66%|██████▌   | 124/188 [00:03<00:01, 35.74it/s]Evaluating:  68%|██████▊   | 128/188 [00:03<00:01, 35.76it/s]Evaluating:  70%|███████   | 132/188 [00:03<00:01, 35.72it/s]Evaluating:  72%|███████▏  | 136/188 [00:03<00:01, 35.71it/s]Evaluating:  74%|███████▍  | 140/188 [00:03<00:01, 35.70it/s]Evaluating:  77%|███████▋  | 144/188 [00:04<00:01, 35.63it/s]Evaluating:  79%|███████▊  | 148/188 [00:04<00:01, 35.66it/s]Evaluating:  81%|████████  | 152/188 [00:04<00:01, 35.71it/s]Evaluating:  83%|████████▎ | 156/188 [00:04<00:00, 35.72it/s]Evaluating:  85%|████████▌ | 160/188 [00:04<00:00, 35.76it/s]Evaluating:  87%|████████▋ | 164/188 [00:04<00:00, 35.72it/s]Evaluating:  89%|████████▉ | 168/188 [00:04<00:00, 35.76it/s]Evaluating:  91%|█████████▏| 172/188 [00:04<00:00, 35.74it/s]Evaluating:  94%|█████████▎| 176/188 [00:04<00:00, 35.70it/s]Evaluating:  96%|█████████▌| 180/188 [00:05<00:00, 35.71it/s]Evaluating:  98%|█████████▊| 184/188 [00:05<00:00, 35.71it/s]Evaluating: 100%|██████████| 188/188 [00:05<00:00, 36.83it/s]Evaluating: 100%|██████████| 188/188 [00:05<00:00, 35.81it/s]
06/11/2022 12:11:01 - INFO - __main__ -   ***** Eval results  *****
06/11/2022 12:11:01 - INFO - __main__ -     corr = 0.9007819285280242
06/11/2022 12:11:01 - INFO - __main__ -     pearson = 0.9027462162190373
06/11/2022 12:11:01 - INFO - __main__ -     spearmanr = 0.8988176408370111
