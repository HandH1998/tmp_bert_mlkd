D:\ProgramData\Anaconda3\envs\TinyBERT\lib\site-packages\numpy\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:
D:\ProgramData\Anaconda3\envs\TinyBERT\lib\site-packages\numpy\.libs\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll
D:\ProgramData\Anaconda3\envs\TinyBERT\lib\site-packages\numpy\.libs\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll
  stacklevel=1)
2021-11-21 18:06:24.709426: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
11/21/2021 18:06:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 18:06:26 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/original_pretrained_model/config.json
11/21/2021 18:06:26 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": null,
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

11/21/2021 18:06:26 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/original_pretrained_model/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/original_pretrained_model/' is a path, a model identifier, or url to a directory containing tokenizer files.
11/21/2021 18:06:26 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/added_tokens.json. We won't load it.
11/21/2021 18:06:26 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/special_tokens_map.json. We won't load it.
11/21/2021 18:06:26 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/original_pretrained_model/tokenizer_config.json. We won't load it.
11/21/2021 18:06:26 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/original_pretrained_model/vocab.txt
11/21/2021 18:06:26 - INFO - bert_fineturn.tokenization_utils -   loading file None
11/21/2021 18:06:26 - INFO - bert_fineturn.tokenization_utils -   loading file None
11/21/2021 18:06:26 - INFO - bert_fineturn.tokenization_utils -   loading file None
11/21/2021 18:06:26 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/original_pretrained_model/pytorch_model.bin
11/21/2021 18:06:28 - INFO - bert_fineturn.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
11/21/2021 18:06:28 - INFO - bert_fineturn.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
11/21/2021 18:06:28 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='../../data/glue_data/MRPC/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_predict=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='../../model/original_pretrained_model/', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=8.0, output_dir='../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=100, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)
11/21/2021 18:06:28 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/MRPC/cached_train_original_pretrained_model_128_mrpc
11/21/2021 18:06:28 - INFO - __main__ -   ***** Running training *****
11/21/2021 18:06:28 - INFO - __main__ -     Num examples = 3668
11/21/2021 18:06:28 - INFO - __main__ -     Num Epochs = 8
11/21/2021 18:06:28 - INFO - __main__ -     Instantaneous batch size per GPU = 32
11/21/2021 18:06:28 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
11/21/2021 18:06:28 - INFO - __main__ -     Gradient Accumulation steps = 1
11/21/2021 18:06:28 - INFO - __main__ -     Total optimization steps = 920
11/21/2021 18:06:28 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step
11/21/2021 18:06:28 - INFO - __main__ -     Continuing training from epoch 0
11/21/2021 18:06:28 - INFO - __main__ -     Continuing training from global step 0
11/21/2021 18:06:28 - INFO - __main__ -     Will skip the first 0 steps in the first epoch
11/21/2021 18:06:28 - INFO - __main__ -   epoch 0/8
D:\zy\tiny_bert\BERT-EMD\bert_fineturn\optimization.py:155: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ..\torch\csrc\utils\python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
11/21/2021 18:06:55 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-100\config.json
11/21/2021 18:06:56 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-100\pytorch_model.bin
11/21/2021 18:06:56 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-100
11/21/2021 18:06:57 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-100
11/21/2021 18:07:01 - INFO - __main__ -   epoch 1/8
11/21/2021 18:07:22 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-200\config.json
11/21/2021 18:07:23 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-200\pytorch_model.bin
11/21/2021 18:07:23 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-200
11/21/2021 18:07:24 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-200
11/21/2021 18:07:31 - INFO - __main__ -   epoch 2/8
11/21/2021 18:07:49 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-300\config.json
11/21/2021 18:07:50 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-300\pytorch_model.bin
11/21/2021 18:07:50 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-300
11/21/2021 18:07:51 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-300
11/21/2021 18:08:02 - INFO - __main__ -   epoch 3/8
11/21/2021 18:08:16 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-400\config.json
11/21/2021 18:08:17 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-400\pytorch_model.bin
11/21/2021 18:08:17 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-400
11/21/2021 18:08:18 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-400
11/21/2021 18:08:33 - INFO - __main__ -   epoch 4/8
11/21/2021 18:08:43 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/MRPC/cached_dev_original_pretrained_model_128_mrpc
11/21/2021 18:08:43 - INFO - __main__ -   ***** Running evaluation  *****
11/21/2021 18:08:43 - INFO - __main__ -     Num examples = 408
11/21/2021 18:08:43 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/51 [00:00<?, ?it/s]Evaluating:  10%|¨         | 5/51 [00:00<00:01, 45.17it/s]Evaluating:  20%|¨€¨        | 10/51 [00:00<00:00, 44.81it/s]Evaluating:  29%|¨€¨€¨       | 15/51 [00:00<00:00, 44.67it/s]Evaluating:  39%|¨€¨€¨€¨      | 20/51 [00:00<00:00, 44.70it/s]Evaluating:  49%|¨€¨€¨€¨€¨     | 25/51 [00:00<00:00, 44.72it/s]Evaluating:  59%|¨€¨€¨€¨€¨€¨    | 30/51 [00:00<00:00, 44.73it/s]Evaluating:  69%|¨€¨€¨€¨€¨€¨€¨‚   | 35/51 [00:00<00:00, 44.62it/s]Evaluating:  78%|¨€¨€¨€¨€¨€¨€¨€¨‚  | 40/51 [00:00<00:00, 44.54it/s]Evaluating:  88%|¨€¨€¨€¨€¨€¨€¨€¨€¨‚ | 45/51 [00:01<00:00, 44.49it/s]Evaluating:  98%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨‚| 50/51 [00:01<00:00, 44.45it/s]Evaluating: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 51/51 [00:01<00:00, 44.54it/s]
11/21/2021 18:08:44 - INFO - __main__ -   ***** Eval results  *****
11/21/2021 18:08:44 - INFO - __main__ -     acc = 0.8455882352941176
11/21/2021 18:08:44 - INFO - __main__ -     acc_and_f1 = 0.8693136422650554
11/21/2021 18:08:44 - INFO - __main__ -     f1 = 0.8930390492359932
D:\ProgramData\Anaconda3\envs\TinyBERT\lib\site-packages\torch\optim\lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
{"eval_acc": 0.8455882352941176, "eval_f1": 0.8930390492359932, "eval_acc_and_f1": 0.8693136422650554, "learning_rate": 9.130434782608697e-06, "loss": 0.3210840651914477, "step": 500}
11/21/2021 18:08:44 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-500\config.json
11/21/2021 18:08:45 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-500\pytorch_model.bin
11/21/2021 18:08:45 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-500
11/21/2021 18:08:46 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-500
11/21/2021 18:09:05 - INFO - __main__ -   epoch 5/8
11/21/2021 18:09:11 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-600\config.json
11/21/2021 18:09:12 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-600\pytorch_model.bin
11/21/2021 18:09:12 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-600
11/21/2021 18:09:13 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-600
11/21/2021 18:09:36 - INFO - __main__ -   epoch 6/8
11/21/2021 18:09:39 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-700\config.json
11/21/2021 18:09:39 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-700\pytorch_model.bin
11/21/2021 18:09:39 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-700
11/21/2021 18:09:40 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-700
11/21/2021 18:10:06 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-800\config.json
11/21/2021 18:10:07 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-800\pytorch_model.bin
11/21/2021 18:10:07 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-800
11/21/2021 18:10:08 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-800
11/21/2021 18:10:09 - INFO - __main__ -   epoch 7/8
11/21/2021 18:10:33 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-900\config.json
11/21/2021 18:10:34 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-900\pytorch_model.bin
11/21/2021 18:10:34 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-900
11/21/2021 18:10:35 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/checkpoint-900
11/21/2021 18:10:40 - INFO - __main__ -    global_step = 920, average loss = 0.19320872428449903
11/21/2021 18:10:40 - INFO - __main__ -   Saving model checkpoint to ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/
11/21/2021 18:10:40 - INFO - bert_fineturn.configuration_utils -   Configuration saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/config.json
11/21/2021 18:10:41 - INFO - bert_fineturn.modeling_utils -   Model weights saved in ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/pytorch_model.bin
11/21/2021 18:10:41 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/config.json
11/21/2021 18:10:41 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

11/21/2021 18:10:41 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/pytorch_model.bin
11/21/2021 18:10:42 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/' is a path, a model identifier, or url to a directory containing tokenizer files.
11/21/2021 18:10:42 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/added_tokens.json. We won't load it.
11/21/2021 18:10:42 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/vocab.txt
11/21/2021 18:10:42 - INFO - bert_fineturn.tokenization_utils -   loading file None
11/21/2021 18:10:42 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/special_tokens_map.json
11/21/2021 18:10:42 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/tokenizer_config.json
11/21/2021 18:10:43 - INFO - bert_fineturn.tokenization_utils -   Model name '../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/' is a path, a model identifier, or url to a directory containing tokenizer files.
11/21/2021 18:10:43 - INFO - bert_fineturn.tokenization_utils -   Didn't find file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/added_tokens.json. We won't load it.
11/21/2021 18:10:43 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/vocab.txt
11/21/2021 18:10:43 - INFO - bert_fineturn.tokenization_utils -   loading file None
11/21/2021 18:10:43 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/special_tokens_map.json
11/21/2021 18:10:43 - INFO - bert_fineturn.tokenization_utils -   loading file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/tokenizer_config.json
11/21/2021 18:10:43 - INFO - __main__ -   Evaluate the following checkpoints: ['../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/']
11/21/2021 18:10:43 - INFO - bert_fineturn.configuration_utils -   loading configuration file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/config.json
11/21/2021 18:10:43 - INFO - bert_fineturn.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

11/21/2021 18:10:43 - INFO - bert_fineturn.modeling_utils -   loading weights file ../../model/BERT-EMD/fine_tuned_pretrained_model/MRPC/on_original_data/pytorch_model.bin
11/21/2021 18:10:44 - INFO - __main__ -   Loading features from cached file ../../data/glue_data/MRPC/cached_dev_original_pretrained_model_128_mrpc
11/21/2021 18:10:44 - INFO - __main__ -   ***** Running evaluation  *****
11/21/2021 18:10:44 - INFO - __main__ -     Num examples = 408
11/21/2021 18:10:44 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/51 [00:00<?, ?it/s]Evaluating:   8%|¨‚         | 4/51 [00:00<00:01, 37.84it/s]Evaluating:  18%|¨€¨‚        | 9/51 [00:00<00:01, 38.85it/s]Evaluating:  27%|¨€¨€¨ƒ       | 14/51 [00:00<00:00, 40.45it/s]Evaluating:  37%|¨€¨€¨€¨ƒ      | 19/51 [00:00<00:00, 41.65it/s]Evaluating:  47%|¨€¨€¨€¨€¨ƒ     | 24/51 [00:00<00:00, 42.43it/s]Evaluating:  57%|¨€¨€¨€¨€¨€¨ƒ    | 29/51 [00:00<00:00, 43.11it/s]Evaluating:  67%|¨€¨€¨€¨€¨€¨€¨ƒ   | 34/51 [00:00<00:00, 43.48it/s]Evaluating:  76%|¨€¨€¨€¨€¨€¨€¨€¨ƒ  | 39/51 [00:00<00:00, 43.85it/s]Evaluating:  86%|¨€¨€¨€¨€¨€¨€¨€¨€¨ƒ | 44/51 [00:01<00:00, 44.01it/s]Evaluating:  96%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨„| 49/51 [00:01<00:00, 43.88it/s]Evaluating: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 51/51 [00:01<00:00, 43.52it/s]
11/21/2021 18:10:46 - INFO - __main__ -   ***** Eval results  *****
11/21/2021 18:10:46 - INFO - __main__ -     acc = 0.8406862745098039
11/21/2021 18:10:46 - INFO - __main__ -     acc_and_f1 = 0.8647875816993464
11/21/2021 18:10:46 - INFO - __main__ -     f1 = 0.888888888888889
