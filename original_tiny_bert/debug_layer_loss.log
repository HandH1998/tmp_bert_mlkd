2022-06-11 23:53:05,856 Task start! 
2022-06-11 23:53:05,893 device: cuda n_gpu: 1
2022-06-11 23:53:05,895 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=20, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/test', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='mrpc', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/test', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-11 23:53:05,948 Writing example 0 of 3668
2022-06-11 23:53:05,951 *** Example ***
2022-06-11 23:53:05,951 guid: train-1
2022-06-11 23:53:05,951 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-06-11 23:53:05,951 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:53:05,951 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:53:05,952 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:53:05,952 label: 1
2022-06-11 23:53:05,952 label_id: 1
2022-06-11 23:53:14,602 Writing example 0 of 408
2022-06-11 23:53:14,605 *** Example ***
2022-06-11 23:53:14,605 guid: dev-1
2022-06-11 23:53:14,605 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-06-11 23:53:14,605 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:53:14,605 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:53:14,605 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:53:14,605 label: 1
2022-06-11 23:53:14,606 label_id: 1
2022-06-11 23:53:38,592 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-11 23:53:43,728 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data/pytorch_model.bin
2022-06-11 23:53:44,421 loading model...
2022-06-11 23:53:59,109 done!
2022-06-11 23:54:06,231 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-11 23:54:07,439 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-11 23:54:07,617 loading model...
2022-06-11 23:54:38,169 done!
2022-06-11 23:54:38,170 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-11 23:54:38,170 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-11 23:54:40,262 ***** Running training *****
2022-06-11 23:54:40,272   Num examples = 3668
2022-06-11 23:54:40,283   Batch size = 32
2022-06-11 23:54:40,289   Num steps = 2280
2022-06-11 23:54:40,315 n: bert.embeddings.word_embeddings.weight
2022-06-11 23:54:40,321 n: bert.embeddings.position_embeddings.weight
2022-06-11 23:54:40,326 n: bert.embeddings.token_type_embeddings.weight
2022-06-11 23:54:40,342 n: bert.embeddings.LayerNorm.weight
2022-06-11 23:54:40,348 n: bert.embeddings.LayerNorm.bias
2022-06-11 23:54:40,359 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-11 23:54:40,374 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-11 23:54:40,375 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-11 23:54:40,376 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-11 23:54:40,376 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-11 23:54:40,377 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-11 23:54:40,377 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-11 23:54:40,378 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-11 23:54:40,378 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-11 23:54:40,378 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-11 23:54:40,379 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-11 23:54:40,379 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-11 23:54:40,379 n: bert.encoder.layer.0.output.dense.weight
2022-06-11 23:54:40,380 n: bert.encoder.layer.0.output.dense.bias
2022-06-11 23:54:40,380 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-11 23:54:40,380 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-11 23:54:40,381 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-11 23:54:40,381 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-11 23:54:40,381 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-11 23:54:40,381 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-11 23:54:40,382 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-11 23:54:40,382 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-11 23:54:40,382 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-11 23:54:40,382 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-11 23:54:40,383 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-11 23:54:40,383 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-11 23:54:40,383 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-11 23:54:40,383 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-11 23:54:40,384 n: bert.encoder.layer.1.output.dense.weight
2022-06-11 23:54:40,384 n: bert.encoder.layer.1.output.dense.bias
2022-06-11 23:54:40,384 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-11 23:54:40,385 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-11 23:54:40,385 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-11 23:54:40,385 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-11 23:54:40,385 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-11 23:54:40,386 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-11 23:54:40,386 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-11 23:54:40,386 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-11 23:54:40,387 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-11 23:54:40,387 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-11 23:54:40,387 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-11 23:54:40,387 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-11 23:54:40,388 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-11 23:54:40,388 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-11 23:54:40,388 n: bert.encoder.layer.2.output.dense.weight
2022-06-11 23:54:40,389 n: bert.encoder.layer.2.output.dense.bias
2022-06-11 23:54:40,389 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-11 23:54:40,389 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-11 23:54:40,389 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-11 23:54:40,390 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-11 23:54:40,390 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-11 23:54:40,390 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-11 23:54:40,390 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-11 23:54:40,391 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-11 23:54:40,391 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-11 23:54:40,391 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-11 23:54:40,391 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-11 23:54:40,391 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-11 23:54:40,391 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-11 23:54:40,391 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-11 23:54:40,392 n: bert.encoder.layer.3.output.dense.weight
2022-06-11 23:54:40,392 n: bert.encoder.layer.3.output.dense.bias
2022-06-11 23:54:40,392 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-11 23:54:40,392 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-11 23:54:40,392 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-11 23:54:40,392 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-11 23:54:40,393 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-11 23:54:40,393 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-11 23:54:40,393 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-11 23:54:40,393 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-11 23:54:40,393 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-11 23:54:40,393 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-11 23:54:40,394 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-11 23:54:40,394 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-11 23:54:40,394 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-11 23:54:40,394 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-11 23:54:40,394 n: bert.encoder.layer.4.output.dense.weight
2022-06-11 23:54:40,394 n: bert.encoder.layer.4.output.dense.bias
2022-06-11 23:54:40,394 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-11 23:54:40,395 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-11 23:54:40,395 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-11 23:54:40,395 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-11 23:54:40,395 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-11 23:54:40,395 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-11 23:54:40,395 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-11 23:54:40,396 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-11 23:54:40,396 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-11 23:54:40,396 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-11 23:54:40,396 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-11 23:54:40,396 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-11 23:54:40,396 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-11 23:54:40,397 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-11 23:54:40,397 n: bert.encoder.layer.5.output.dense.weight
2022-06-11 23:54:40,397 n: bert.encoder.layer.5.output.dense.bias
2022-06-11 23:54:40,397 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-11 23:54:40,397 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-11 23:54:40,397 n: bert.pooler.dense.weight
2022-06-11 23:54:40,398 n: bert.pooler.dense.bias
2022-06-11 23:54:40,398 n: classifier.weight
2022-06-11 23:54:40,398 n: classifier.bias
2022-06-11 23:54:40,398 n: fit_denses.0.weight
2022-06-11 23:54:40,398 n: fit_denses.0.bias
2022-06-11 23:54:40,398 n: fit_denses.1.weight
2022-06-11 23:54:40,399 n: fit_denses.1.bias
2022-06-11 23:54:40,399 n: fit_denses.2.weight
2022-06-11 23:54:40,399 n: fit_denses.2.bias
2022-06-11 23:54:40,399 n: fit_denses.3.weight
2022-06-11 23:54:40,399 n: fit_denses.3.bias
2022-06-11 23:54:40,399 n: fit_denses.4.weight
2022-06-11 23:54:40,400 n: fit_denses.4.bias
2022-06-11 23:54:40,400 n: fit_denses.5.weight
2022-06-11 23:54:40,400 n: fit_denses.5.bias
2022-06-11 23:54:40,400 n: fit_denses.6.weight
2022-06-11 23:54:40,400 n: fit_denses.6.bias
2022-06-11 23:54:40,400 Total parameters: 71090690
2022-06-11 23:54:58,827 Task start! 
2022-06-11 23:54:58,871 device: cuda n_gpu: 1
2022-06-11 23:54:58,873 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=20, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/test', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='mrpc', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/test', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-11 23:54:58,927 Writing example 0 of 3668
2022-06-11 23:54:58,930 *** Example ***
2022-06-11 23:54:58,930 guid: train-1
2022-06-11 23:54:58,930 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-06-11 23:54:58,930 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:54:58,930 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:54:58,931 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:54:58,931 label: 1
2022-06-11 23:54:58,931 label_id: 1
2022-06-11 23:55:07,571 Writing example 0 of 408
2022-06-11 23:55:07,573 *** Example ***
2022-06-11 23:55:07,573 guid: dev-1
2022-06-11 23:55:07,573 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-06-11 23:55:07,573 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:55:07,573 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:55:07,574 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:55:07,574 label: 1
2022-06-11 23:55:07,574 label_id: 1
2022-06-11 23:55:12,122 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-11 23:55:17,663 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data/pytorch_model.bin
2022-06-11 23:55:18,257 loading model...
2022-06-11 23:55:20,276 done!
2022-06-11 23:55:33,375 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-11 23:55:34,553 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-11 23:55:34,720 loading model...
2022-06-11 23:55:54,444 done!
2022-06-11 23:55:54,445 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-11 23:55:54,445 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-11 23:55:54,514 Tensorboard log directory (../tiny_bert/model/tiny_bert/distilled_intermediate_model/test) already exists and is not empty.
2022-06-11 23:55:56,492 ***** Running training *****
2022-06-11 23:55:56,498   Num examples = 3668
2022-06-11 23:55:56,504   Batch size = 32
2022-06-11 23:55:56,514   Num steps = 2280
2022-06-11 23:55:56,535 n: bert.embeddings.word_embeddings.weight
2022-06-11 23:55:56,541 n: bert.embeddings.position_embeddings.weight
2022-06-11 23:55:56,546 n: bert.embeddings.token_type_embeddings.weight
2022-06-11 23:55:56,557 n: bert.embeddings.LayerNorm.weight
2022-06-11 23:55:56,563 n: bert.embeddings.LayerNorm.bias
2022-06-11 23:55:56,568 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-11 23:55:56,574 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-11 23:55:56,579 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-11 23:55:56,585 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-11 23:55:56,590 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-11 23:55:56,596 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-11 23:55:56,602 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-11 23:55:56,605 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-11 23:55:56,606 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-11 23:55:56,606 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-11 23:55:56,607 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-11 23:55:56,607 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-11 23:55:56,608 n: bert.encoder.layer.0.output.dense.weight
2022-06-11 23:55:56,608 n: bert.encoder.layer.0.output.dense.bias
2022-06-11 23:55:56,608 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-11 23:55:56,609 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-11 23:55:56,609 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-11 23:55:56,610 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-11 23:55:56,610 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-11 23:55:56,610 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-11 23:55:56,610 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-11 23:55:56,611 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-11 23:55:56,611 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-11 23:55:56,611 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-11 23:55:56,612 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-11 23:55:56,612 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-11 23:55:56,612 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-11 23:55:56,612 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-11 23:55:56,613 n: bert.encoder.layer.1.output.dense.weight
2022-06-11 23:55:56,613 n: bert.encoder.layer.1.output.dense.bias
2022-06-11 23:55:56,613 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-11 23:55:56,613 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-11 23:55:56,614 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-11 23:55:56,614 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-11 23:55:56,614 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-11 23:55:56,614 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-11 23:55:56,615 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-11 23:55:56,615 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-11 23:55:56,615 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-11 23:55:56,616 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-11 23:55:56,616 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-11 23:55:56,616 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-11 23:55:56,616 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-11 23:55:56,617 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-11 23:55:56,617 n: bert.encoder.layer.2.output.dense.weight
2022-06-11 23:55:56,617 n: bert.encoder.layer.2.output.dense.bias
2022-06-11 23:55:56,618 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-11 23:55:56,618 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-11 23:55:56,618 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-11 23:55:56,618 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-11 23:55:56,619 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-11 23:55:56,619 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-11 23:55:56,619 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-11 23:55:56,619 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-11 23:55:56,620 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-11 23:55:56,620 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-11 23:55:56,620 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-11 23:55:56,621 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-11 23:55:56,621 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-11 23:55:56,621 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-11 23:55:56,621 n: bert.encoder.layer.3.output.dense.weight
2022-06-11 23:55:56,622 n: bert.encoder.layer.3.output.dense.bias
2022-06-11 23:55:56,622 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-11 23:55:56,622 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-11 23:55:56,622 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-11 23:55:56,622 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-11 23:55:56,623 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-11 23:55:56,623 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-11 23:55:56,623 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-11 23:55:56,623 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-11 23:55:56,623 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-11 23:55:56,623 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-11 23:55:56,624 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-11 23:55:56,624 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-11 23:55:56,624 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-11 23:55:56,624 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-11 23:55:56,624 n: bert.encoder.layer.4.output.dense.weight
2022-06-11 23:55:56,625 n: bert.encoder.layer.4.output.dense.bias
2022-06-11 23:55:56,625 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-11 23:55:56,625 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-11 23:55:56,625 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-11 23:55:56,625 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-11 23:55:56,625 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-11 23:55:56,626 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-11 23:55:56,626 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-11 23:55:56,626 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-11 23:55:56,626 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-11 23:55:56,626 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-11 23:55:56,626 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-11 23:55:56,627 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-11 23:55:56,627 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-11 23:55:56,627 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-11 23:55:56,627 n: bert.encoder.layer.5.output.dense.weight
2022-06-11 23:55:56,627 n: bert.encoder.layer.5.output.dense.bias
2022-06-11 23:55:56,628 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-11 23:55:56,628 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-11 23:55:56,628 n: bert.pooler.dense.weight
2022-06-11 23:55:56,628 n: bert.pooler.dense.bias
2022-06-11 23:55:56,628 n: classifier.weight
2022-06-11 23:55:56,628 n: classifier.bias
2022-06-11 23:55:56,629 n: fit_denses.0.weight
2022-06-11 23:55:56,629 n: fit_denses.0.bias
2022-06-11 23:55:56,629 n: fit_denses.1.weight
2022-06-11 23:55:56,629 n: fit_denses.1.bias
2022-06-11 23:55:56,629 n: fit_denses.2.weight
2022-06-11 23:55:56,629 n: fit_denses.2.bias
2022-06-11 23:55:56,630 n: fit_denses.3.weight
2022-06-11 23:55:56,630 n: fit_denses.3.bias
2022-06-11 23:55:56,630 n: fit_denses.4.weight
2022-06-11 23:55:56,630 n: fit_denses.4.bias
2022-06-11 23:55:56,630 n: fit_denses.5.weight
2022-06-11 23:55:56,630 n: fit_denses.5.bias
2022-06-11 23:55:56,631 n: fit_denses.6.weight
2022-06-11 23:55:56,631 n: fit_denses.6.bias
2022-06-11 23:55:56,631 Total parameters: 71090690
2022-06-11 23:56:41,933 Task start! 
2022-06-11 23:56:41,975 device: cuda n_gpu: 1
2022-06-11 23:56:41,977 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=20, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/test', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='mrpc', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/test', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-11 23:56:42,031 Writing example 0 of 3668
2022-06-11 23:56:42,034 *** Example ***
2022-06-11 23:56:42,034 guid: train-1
2022-06-11 23:56:42,034 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-06-11 23:56:42,035 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:56:42,035 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:56:42,035 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:56:42,035 label: 1
2022-06-11 23:56:42,035 label_id: 1
2022-06-11 23:56:50,688 Writing example 0 of 408
2022-06-11 23:56:50,690 *** Example ***
2022-06-11 23:56:50,690 guid: dev-1
2022-06-11 23:56:50,690 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-06-11 23:56:50,690 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:56:50,690 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:56:50,690 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:56:50,691 label: 1
2022-06-11 23:56:50,691 label_id: 1
2022-06-11 23:56:56,204 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-11 23:57:01,936 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data/pytorch_model.bin
2022-06-11 23:57:02,545 loading model...
2022-06-11 23:57:21,402 done!
2022-06-11 23:57:28,686 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-11 23:57:29,755 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-11 23:57:29,913 loading model...
2022-06-11 23:57:50,614 done!
2022-06-11 23:57:50,615 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-11 23:57:50,615 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-11 23:58:05,459 Task start! 
2022-06-11 23:58:05,499 device: cuda n_gpu: 1
2022-06-11 23:58:05,501 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=20, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/test', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='mrpc', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/test', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-11 23:58:05,557 Writing example 0 of 3668
2022-06-11 23:58:05,560 *** Example ***
2022-06-11 23:58:05,560 guid: train-1
2022-06-11 23:58:05,560 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-06-11 23:58:05,561 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:58:05,561 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:58:05,561 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:58:05,561 label: 1
2022-06-11 23:58:05,561 label_id: 1
2022-06-11 23:58:14,336 Writing example 0 of 408
2022-06-11 23:58:14,338 *** Example ***
2022-06-11 23:58:14,338 guid: dev-1
2022-06-11 23:58:14,338 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-06-11 23:58:14,338 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:58:14,338 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:58:14,339 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-11 23:58:14,339 label: 1
2022-06-11 23:58:14,339 label_id: 1
2022-06-11 23:58:16,359 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-11 23:58:21,811 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data/pytorch_model.bin
2022-06-11 23:58:22,430 loading model...
2022-06-11 23:58:29,297 done!
2022-06-11 23:58:35,485 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-11 23:58:36,559 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-11 23:58:36,701 loading model...
2022-06-11 23:58:47,147 done!
2022-06-11 23:58:47,148 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-11 23:58:47,148 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-11 23:58:47,195 Tensorboard log directory (../tiny_bert/model/tiny_bert/distilled_intermediate_model/test) already exists and is not empty.
2022-06-11 23:58:49,194 ***** Running training *****
2022-06-11 23:58:49,205   Num examples = 3668
2022-06-11 23:58:49,211   Batch size = 32
2022-06-11 23:58:49,216   Num steps = 2280
2022-06-11 23:58:49,224 n: bert.embeddings.word_embeddings.weight
2022-06-11 23:58:49,230 n: bert.embeddings.position_embeddings.weight
2022-06-11 23:58:49,235 n: bert.embeddings.token_type_embeddings.weight
2022-06-11 23:58:49,241 n: bert.embeddings.LayerNorm.weight
2022-06-11 23:58:49,246 n: bert.embeddings.LayerNorm.bias
2022-06-11 23:58:49,257 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-11 23:58:49,262 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-11 23:58:49,273 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-11 23:58:49,278 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-11 23:58:49,284 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-11 23:58:49,289 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-11 23:58:49,300 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-11 23:58:49,305 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-11 23:58:49,311 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-11 23:58:49,312 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-11 23:58:49,312 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-11 23:58:49,312 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-11 23:58:49,313 n: bert.encoder.layer.0.output.dense.weight
2022-06-11 23:58:49,313 n: bert.encoder.layer.0.output.dense.bias
2022-06-11 23:58:49,314 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-11 23:58:49,314 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-11 23:58:49,314 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-11 23:58:49,314 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-11 23:58:49,314 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-11 23:58:49,314 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-11 23:58:49,315 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-11 23:58:49,315 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-11 23:58:49,315 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-11 23:58:49,315 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-11 23:58:49,315 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-11 23:58:49,315 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-11 23:58:49,316 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-11 23:58:49,316 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-11 23:58:49,316 n: bert.encoder.layer.1.output.dense.weight
2022-06-11 23:58:49,316 n: bert.encoder.layer.1.output.dense.bias
2022-06-11 23:58:49,316 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-11 23:58:49,316 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-11 23:58:49,317 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-11 23:58:49,317 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-11 23:58:49,317 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-11 23:58:49,317 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-11 23:58:49,317 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-11 23:58:49,317 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-11 23:58:49,318 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-11 23:58:49,318 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-11 23:58:49,318 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-11 23:58:49,318 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-11 23:58:49,318 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-11 23:58:49,318 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-11 23:58:49,319 n: bert.encoder.layer.2.output.dense.weight
2022-06-11 23:58:49,319 n: bert.encoder.layer.2.output.dense.bias
2022-06-11 23:58:49,319 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-11 23:58:49,319 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-11 23:58:49,319 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-11 23:58:49,319 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-11 23:58:49,320 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-11 23:58:49,320 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-11 23:58:49,320 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-11 23:58:49,320 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-11 23:58:49,320 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-11 23:58:49,320 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-11 23:58:49,321 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-11 23:58:49,321 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-11 23:58:49,321 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-11 23:58:49,321 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-11 23:58:49,321 n: bert.encoder.layer.3.output.dense.weight
2022-06-11 23:58:49,321 n: bert.encoder.layer.3.output.dense.bias
2022-06-11 23:58:49,322 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-11 23:58:49,322 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-11 23:58:49,322 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-11 23:58:49,322 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-11 23:58:49,322 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-11 23:58:49,323 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-11 23:58:49,323 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-11 23:58:49,323 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-11 23:58:49,323 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-11 23:58:49,323 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-11 23:58:49,323 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-11 23:58:49,324 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-11 23:58:49,324 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-11 23:58:49,324 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-11 23:58:49,324 n: bert.encoder.layer.4.output.dense.weight
2022-06-11 23:58:49,324 n: bert.encoder.layer.4.output.dense.bias
2022-06-11 23:58:49,324 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-11 23:58:49,325 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-11 23:58:49,325 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-11 23:58:49,325 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-11 23:58:49,325 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-11 23:58:49,325 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-11 23:58:49,325 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-11 23:58:49,326 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-11 23:58:49,326 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-11 23:58:49,326 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-11 23:58:49,326 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-11 23:58:49,326 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-11 23:58:49,326 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-11 23:58:49,327 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-11 23:58:49,327 n: bert.encoder.layer.5.output.dense.weight
2022-06-11 23:58:49,327 n: bert.encoder.layer.5.output.dense.bias
2022-06-11 23:58:49,327 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-11 23:58:49,327 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-11 23:58:49,327 n: bert.pooler.dense.weight
2022-06-11 23:58:49,328 n: bert.pooler.dense.bias
2022-06-11 23:58:49,328 n: classifier.weight
2022-06-11 23:58:49,328 n: classifier.bias
2022-06-11 23:58:49,328 n: fit_denses.0.weight
2022-06-11 23:58:49,328 n: fit_denses.0.bias
2022-06-11 23:58:49,328 n: fit_denses.1.weight
2022-06-11 23:58:49,329 n: fit_denses.1.bias
2022-06-11 23:58:49,329 n: fit_denses.2.weight
2022-06-11 23:58:49,329 n: fit_denses.2.bias
2022-06-11 23:58:49,329 n: fit_denses.3.weight
2022-06-11 23:58:49,329 n: fit_denses.3.bias
2022-06-11 23:58:49,329 n: fit_denses.4.weight
2022-06-11 23:58:49,329 n: fit_denses.4.bias
2022-06-11 23:58:49,330 n: fit_denses.5.weight
2022-06-11 23:58:49,330 n: fit_denses.5.bias
2022-06-11 23:58:49,330 n: fit_denses.6.weight
2022-06-11 23:58:49,330 n: fit_denses.6.bias
2022-06-11 23:58:49,330 Total parameters: 72468738
2022-06-12 00:02:46,651 ***** Running evaluation *****
2022-06-12 00:02:46,652   Epoch = 0 iter 19 step
2022-06-12 00:02:46,652   Num examples = 408
2022-06-12 00:02:46,653   Batch size = 32
2022-06-12 00:02:46,656 ***** Eval results *****
2022-06-12 00:02:46,657   att_loss = 10.637492230063991
2022-06-12 00:02:46,657   global_step = 19
2022-06-12 00:02:46,657   loss = 13.302694621839022
2022-06-12 00:02:46,658   rep_loss = 2.6652024545167623
2022-06-12 00:02:46,658 ***** Save model *****
2022-06-12 00:02:52,378 ***** Running evaluation *****
2022-06-12 00:02:52,379   Epoch = 0 iter 39 step
2022-06-12 00:02:52,379   Num examples = 408
2022-06-12 00:02:52,379   Batch size = 32
2022-06-12 00:02:52,382 ***** Eval results *****
2022-06-12 00:02:52,382   att_loss = 8.99273188908895
2022-06-12 00:02:52,382   global_step = 39
2022-06-12 00:02:52,382   loss = 11.364451481745792
2022-06-12 00:02:52,382   rep_loss = 2.371719620166681
2022-06-12 00:02:52,383 ***** Save model *****
2022-06-12 00:02:58,182 ***** Running evaluation *****
2022-06-12 00:02:58,182   Epoch = 0 iter 59 step
2022-06-12 00:02:58,182   Num examples = 408
2022-06-12 00:02:58,183   Batch size = 32
2022-06-12 00:02:58,185 ***** Eval results *****
2022-06-12 00:02:58,186   att_loss = 8.253140942525055
2022-06-12 00:02:58,186   global_step = 59
2022-06-12 00:02:58,186   loss = 10.465350538997328
2022-06-12 00:02:58,186   rep_loss = 2.2122096207182285
2022-06-12 00:02:58,187 ***** Save model *****
2022-06-12 00:03:03,957 ***** Running evaluation *****
2022-06-12 00:03:03,958   Epoch = 0 iter 79 step
2022-06-12 00:03:03,958   Num examples = 408
2022-06-12 00:03:03,958   Batch size = 32
2022-06-12 00:03:03,961 ***** Eval results *****
2022-06-12 00:03:03,961   att_loss = 7.803253535982929
2022-06-12 00:03:03,961   global_step = 79
2022-06-12 00:03:03,961   loss = 9.914287482635885
2022-06-12 00:03:03,961   rep_loss = 2.1110339421260207
2022-06-12 00:03:03,961 ***** Save model *****
2022-06-12 00:06:10,261 Task start! 
2022-06-12 00:06:10,308 device: cuda n_gpu: 1
2022-06-12 00:06:10,310 The args: Namespace(aug_train=False, cache_dir='', data_dir='data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=15, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/test', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/test', task_name='mrpc', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/test', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 00:06:46,233 Task start! 
2022-06-12 00:06:46,274 device: cuda n_gpu: 1
2022-06-12 00:06:46,276 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=15, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/test', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/test', task_name='mrpc', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/test', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 00:06:46,329 Writing example 0 of 3668
2022-06-12 00:06:46,332 *** Example ***
2022-06-12 00:06:46,332 guid: train-1
2022-06-12 00:06:46,332 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-06-12 00:06:46,333 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:06:46,333 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:06:46,333 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:06:46,333 label: 1
2022-06-12 00:06:46,333 label_id: 1
2022-06-12 00:06:55,139 Writing example 0 of 408
2022-06-12 00:06:55,141 *** Example ***
2022-06-12 00:06:55,142 guid: dev-1
2022-06-12 00:06:55,142 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-06-12 00:06:55,142 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:06:55,142 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:06:55,142 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:06:55,142 label: 1
2022-06-12 00:06:55,142 label_id: 1
2022-06-12 00:07:02,454 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 00:07:07,790 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data/pytorch_model.bin
2022-06-12 00:07:08,433 loading model...
2022-06-12 00:07:10,023 done!
2022-06-12 00:07:40,991 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 00:07:42,130 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/test/pytorch_model.bin
2022-06-12 00:07:42,325 loading model...
2022-06-12 00:07:56,319 done!
2022-06-12 00:07:58,603 ***** Running training *****
2022-06-12 00:07:58,622   Num examples = 3668
2022-06-12 00:07:58,628   Batch size = 32
2022-06-12 00:07:58,633   Num steps = 1710
2022-06-12 00:07:58,641 n: bert.embeddings.word_embeddings.weight
2022-06-12 00:07:58,646 n: bert.embeddings.position_embeddings.weight
2022-06-12 00:07:58,651 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 00:07:58,657 n: bert.embeddings.LayerNorm.weight
2022-06-12 00:07:58,662 n: bert.embeddings.LayerNorm.bias
2022-06-12 00:07:58,668 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 00:07:58,678 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 00:07:58,683 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 00:07:58,689 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 00:07:58,694 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 00:07:58,699 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 00:07:58,705 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 00:07:58,708 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 00:07:58,708 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 00:07:58,708 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 00:07:58,709 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 00:07:58,709 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 00:07:58,709 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 00:07:58,709 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 00:07:58,710 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 00:07:58,710 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 00:07:58,710 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 00:07:58,710 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 00:07:58,710 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 00:07:58,711 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 00:07:58,711 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 00:07:58,711 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 00:07:58,711 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 00:07:58,711 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 00:07:58,711 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 00:07:58,712 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 00:07:58,712 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 00:07:58,712 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 00:07:58,712 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 00:07:58,712 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 00:07:58,712 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 00:07:58,713 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 00:07:58,713 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 00:07:58,713 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 00:07:58,713 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 00:07:58,713 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 00:07:58,713 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 00:07:58,714 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 00:07:58,714 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 00:07:58,714 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 00:07:58,714 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 00:07:58,714 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 00:07:58,714 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 00:07:58,715 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 00:07:58,715 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 00:07:58,715 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 00:07:58,715 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 00:07:58,715 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 00:07:58,715 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 00:07:58,716 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 00:07:58,716 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 00:07:58,716 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 00:07:58,716 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 00:07:58,716 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 00:07:58,716 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 00:07:58,716 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 00:07:58,717 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 00:07:58,717 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 00:07:58,717 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 00:07:58,717 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 00:07:58,717 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 00:07:58,718 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 00:07:58,718 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 00:07:58,718 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 00:07:58,718 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 00:07:58,718 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 00:07:58,718 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 00:07:58,718 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 00:07:58,719 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 00:07:58,719 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 00:07:58,719 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 00:07:58,719 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 00:07:58,719 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 00:07:58,719 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 00:07:58,720 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 00:07:58,720 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 00:07:58,720 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 00:07:58,720 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 00:07:58,720 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 00:07:58,720 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 00:07:58,721 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 00:07:58,721 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 00:07:58,721 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 00:07:58,721 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 00:07:58,721 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 00:07:58,721 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 00:07:58,722 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 00:07:58,722 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 00:07:58,722 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 00:07:58,722 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 00:07:58,722 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 00:07:58,722 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 00:07:58,723 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 00:07:58,723 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 00:07:58,723 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 00:07:58,723 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 00:07:58,723 n: bert.pooler.dense.weight
2022-06-12 00:07:58,723 n: bert.pooler.dense.bias
2022-06-12 00:07:58,724 n: classifier.weight
2022-06-12 00:07:58,724 n: classifier.bias
2022-06-12 00:07:58,724 n: fit_denses.0.weight
2022-06-12 00:07:58,724 n: fit_denses.0.bias
2022-06-12 00:07:58,724 n: fit_denses.1.weight
2022-06-12 00:07:58,724 n: fit_denses.1.bias
2022-06-12 00:07:58,724 n: fit_denses.2.weight
2022-06-12 00:07:58,725 n: fit_denses.2.bias
2022-06-12 00:07:58,725 n: fit_denses.3.weight
2022-06-12 00:07:58,725 n: fit_denses.3.bias
2022-06-12 00:07:58,725 n: fit_denses.4.weight
2022-06-12 00:07:58,725 n: fit_denses.4.bias
2022-06-12 00:07:58,725 n: fit_denses.5.weight
2022-06-12 00:07:58,726 n: fit_denses.5.bias
2022-06-12 00:07:58,726 n: fit_denses.6.weight
2022-06-12 00:07:58,726 n: fit_denses.6.bias
2022-06-12 00:07:58,726 Total parameters: 72468738
2022-06-12 00:08:15,211 ***** Running evaluation *****
2022-06-12 00:08:15,211   Epoch = 0 iter 19 step
2022-06-12 00:08:15,211   Num examples = 408
2022-06-12 00:08:15,212   Batch size = 32
2022-06-12 00:08:15,571 ***** Eval results *****
2022-06-12 00:08:15,571   acc = 0.6838235294117647
2022-06-12 00:08:15,572   acc_and_f1 = 0.7480253018237863
2022-06-12 00:08:15,572   cls_loss = 0.3357099827967192
2022-06-12 00:08:15,572   eval_loss = 0.6306752470823435
2022-06-12 00:08:15,573   f1 = 0.8122270742358079
2022-06-12 00:08:15,573   global_step = 19
2022-06-12 00:08:15,573   loss = 0.3357099827967192
2022-06-12 00:08:15,574 ***** Save model *****
2022-06-12 00:08:20,970 ***** Running evaluation *****
2022-06-12 00:08:20,971   Epoch = 0 iter 39 step
2022-06-12 00:08:20,971   Num examples = 408
2022-06-12 00:08:20,971   Batch size = 32
2022-06-12 00:08:21,326 ***** Eval results *****
2022-06-12 00:08:21,327   acc = 0.6838235294117647
2022-06-12 00:08:21,327   acc_and_f1 = 0.7480253018237863
2022-06-12 00:08:21,327   cls_loss = 0.3279446439865308
2022-06-12 00:08:21,327   eval_loss = 0.6224502187508804
2022-06-12 00:08:21,327   f1 = 0.8122270742358079
2022-06-12 00:08:21,328   global_step = 39
2022-06-12 00:08:21,328   loss = 0.3279446439865308
2022-06-12 00:08:26,302 ***** Running evaluation *****
2022-06-12 00:08:26,302   Epoch = 0 iter 59 step
2022-06-12 00:08:26,302   Num examples = 408
2022-06-12 00:08:26,302   Batch size = 32
2022-06-12 00:08:26,659 ***** Eval results *****
2022-06-12 00:08:26,660   acc = 0.6838235294117647
2022-06-12 00:08:26,660   acc_and_f1 = 0.7480253018237863
2022-06-12 00:08:26,660   cls_loss = 0.32384918857428985
2022-06-12 00:08:26,660   eval_loss = 0.6229572296142578
2022-06-12 00:08:26,661   f1 = 0.8122270742358079
2022-06-12 00:08:26,661   global_step = 59
2022-06-12 00:08:26,661   loss = 0.32384918857428985
2022-06-12 00:49:54,777 Task start! 
2022-06-12 00:49:54,798 device: cuda n_gpu: 1
2022-06-12 00:49:54,799 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=20, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/mrpc/on_original_data', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='mrpc', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/mrpc/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 00:49:54,840 Writing example 0 of 3668
2022-06-12 00:49:54,841 *** Example ***
2022-06-12 00:49:54,841 guid: train-1
2022-06-12 00:49:54,841 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-06-12 00:49:54,841 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:49:54,841 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:49:54,841 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:49:54,841 label: 1
2022-06-12 00:49:54,841 label_id: 1
2022-06-12 00:49:57,743 Writing example 0 of 408
2022-06-12 00:49:57,744 *** Example ***
2022-06-12 00:49:57,745 guid: dev-1
2022-06-12 00:49:57,745 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-06-12 00:49:57,745 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:49:57,745 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:49:57,745 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:49:57,745 label: 1
2022-06-12 00:49:57,745 label_id: 1
2022-06-12 00:49:58,071 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 00:50:03,576 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data/pytorch_model.bin
2022-06-12 00:50:04,144 loading model...
2022-06-12 00:50:04,434 done!
2022-06-12 00:50:08,325 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 00:50:09,418 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-12 00:50:09,629 loading model...
2022-06-12 00:50:09,658 done!
2022-06-12 00:50:09,658 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-12 00:50:09,658 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-12 00:50:11,034 Task start! 
2022-06-12 00:50:11,057 device: cuda n_gpu: 1
2022-06-12 00:50:11,057 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/QNLI', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=500, gpu_id=3, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=10, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/qnli/on_original_data', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='qnli', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/qnli/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/qnli/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 00:50:11,467 ***** Running training *****
2022-06-12 00:50:11,483   Num examples = 3668
2022-06-12 00:50:11,483   Batch size = 32
2022-06-12 00:50:11,483   Num steps = 2280
2022-06-12 00:50:11,484 n: bert.embeddings.word_embeddings.weight
2022-06-12 00:50:11,489 n: bert.embeddings.position_embeddings.weight
2022-06-12 00:50:11,504 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 00:50:11,510 n: bert.embeddings.LayerNorm.weight
2022-06-12 00:50:11,525 n: bert.embeddings.LayerNorm.bias
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 00:50:11,532 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 00:50:11,533 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 00:50:11,533 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 00:50:11,534 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 00:50:11,535 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 00:50:11,535 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 00:50:11,536 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 00:50:11,536 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 00:50:11,537 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 00:50:11,537 n: bert.pooler.dense.weight
2022-06-12 00:50:11,537 n: bert.pooler.dense.bias
2022-06-12 00:50:11,537 n: classifier.weight
2022-06-12 00:50:11,537 n: classifier.bias
2022-06-12 00:50:11,537 n: fit_denses.0.weight
2022-06-12 00:50:11,538 n: fit_denses.0.bias
2022-06-12 00:50:11,538 n: fit_denses.1.weight
2022-06-12 00:50:11,538 n: fit_denses.1.bias
2022-06-12 00:50:11,538 n: fit_denses.2.weight
2022-06-12 00:50:11,538 n: fit_denses.2.bias
2022-06-12 00:50:11,538 n: fit_denses.3.weight
2022-06-12 00:50:11,538 n: fit_denses.3.bias
2022-06-12 00:50:11,538 n: fit_denses.4.weight
2022-06-12 00:50:11,538 n: fit_denses.4.bias
2022-06-12 00:50:11,538 n: fit_denses.5.weight
2022-06-12 00:50:11,538 n: fit_denses.5.bias
2022-06-12 00:50:11,538 n: fit_denses.6.weight
2022-06-12 00:50:11,538 n: fit_denses.6.bias
2022-06-12 00:50:11,538 Total parameters: 72468738
2022-06-12 00:50:11,727 Writing example 0 of 104743
2022-06-12 00:50:11,728 *** Example ***
2022-06-12 00:50:11,728 guid: train-0
2022-06-12 00:50:11,728 tokens: [CLS] when did the third dig ##imo ##n series begin ? [SEP] unlike the two seasons before it and most of the seasons that followed , dig ##imo ##n tame ##rs takes a darker and more realistic approach to its story featuring dig ##imo ##n who do not rein ##car ##nate after their deaths and more complex character development in the original japanese . [SEP]
2022-06-12 00:50:11,728 input_ids: 101 2043 2106 1996 2353 10667 16339 2078 2186 4088 1029 102 4406 1996 2048 3692 2077 2009 1998 2087 1997 1996 3692 2008 2628 1010 10667 16339 2078 24763 2869 3138 1037 9904 1998 2062 12689 3921 2000 2049 2466 3794 10667 16339 2078 2040 2079 2025 27788 10010 12556 2044 2037 6677 1998 2062 3375 2839 2458 1999 1996 2434 2887 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:50:11,728 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:50:11,728 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:50:11,729 label: not_entailment
2022-06-12 00:50:11,729 label_id: 1
2022-06-12 00:50:16,554 ***** Running evaluation *****
2022-06-12 00:50:16,555   Epoch = 0 iter 19 step
2022-06-12 00:50:16,555   Num examples = 408
2022-06-12 00:50:16,555   Batch size = 32
2022-06-12 00:50:16,556 ***** Eval results *****
2022-06-12 00:50:16,557   att_loss = 10.637492230063991
2022-06-12 00:50:16,557   global_step = 19
2022-06-12 00:50:16,557   loss = 13.302694621839022
2022-06-12 00:50:16,557   rep_loss = 2.6652024545167623
2022-06-12 00:50:16,557 ***** Save model *****
2022-06-12 00:50:19,111 Writing example 10000 of 104743
2022-06-12 00:50:22,100 ***** Running evaluation *****
2022-06-12 00:50:22,101   Epoch = 0 iter 39 step
2022-06-12 00:50:22,101   Num examples = 408
2022-06-12 00:50:22,101   Batch size = 32
2022-06-12 00:50:22,103 ***** Eval results *****
2022-06-12 00:50:22,104   att_loss = 8.99273188908895
2022-06-12 00:50:22,104   global_step = 39
2022-06-12 00:50:22,104   loss = 11.364451481745792
2022-06-12 00:50:22,104   rep_loss = 2.371719620166681
2022-06-12 00:50:22,105 ***** Save model *****
2022-06-12 00:50:26,697 Writing example 20000 of 104743
2022-06-12 00:50:27,830 ***** Running evaluation *****
2022-06-12 00:50:27,831   Epoch = 0 iter 59 step
2022-06-12 00:50:27,831   Num examples = 408
2022-06-12 00:50:27,831   Batch size = 32
2022-06-12 00:50:27,832 ***** Eval results *****
2022-06-12 00:50:27,832   att_loss = 8.253140942525055
2022-06-12 00:50:27,832   global_step = 59
2022-06-12 00:50:27,832   loss = 10.465350538997328
2022-06-12 00:50:27,832   rep_loss = 2.2122096207182285
2022-06-12 00:50:27,832 ***** Save model *****
2022-06-12 00:50:33,499 ***** Running evaluation *****
2022-06-12 00:50:33,499   Epoch = 0 iter 79 step
2022-06-12 00:50:33,499   Num examples = 408
2022-06-12 00:50:33,499   Batch size = 32
2022-06-12 00:50:33,501 ***** Eval results *****
2022-06-12 00:50:33,501   att_loss = 7.803253535982929
2022-06-12 00:50:33,501   global_step = 79
2022-06-12 00:50:33,501   loss = 9.914287482635885
2022-06-12 00:50:33,501   rep_loss = 2.1110339421260207
2022-06-12 00:50:33,501 ***** Save model *****
2022-06-12 00:50:34,331 Writing example 30000 of 104743
2022-06-12 00:50:39,242 ***** Running evaluation *****
2022-06-12 00:50:39,243   Epoch = 0 iter 99 step
2022-06-12 00:50:39,243   Num examples = 408
2022-06-12 00:50:39,243   Batch size = 32
2022-06-12 00:50:39,244 ***** Eval results *****
2022-06-12 00:50:39,244   att_loss = 7.563640782327363
2022-06-12 00:50:39,244   global_step = 99
2022-06-12 00:50:39,244   loss = 9.608715221135304
2022-06-12 00:50:39,244   rep_loss = 2.0450744279707322
2022-06-12 00:50:39,244 ***** Save model *****
2022-06-12 00:50:41,926 Writing example 40000 of 104743
2022-06-12 00:50:44,967 ***** Running evaluation *****
2022-06-12 00:50:44,968   Epoch = 1 iter 119 step
2022-06-12 00:50:44,968   Num examples = 408
2022-06-12 00:50:44,968   Batch size = 32
2022-06-12 00:50:44,969 ***** Eval results *****
2022-06-12 00:50:44,969   att_loss = 6.350759029388428
2022-06-12 00:50:44,969   global_step = 119
2022-06-12 00:50:44,969   loss = 8.076134586334229
2022-06-12 00:50:44,969   rep_loss = 1.7253753423690796
2022-06-12 00:50:44,969 ***** Save model *****
2022-06-12 00:50:49,345 Writing example 50000 of 104743
2022-06-12 00:50:50,664 ***** Running evaluation *****
2022-06-12 00:50:50,664   Epoch = 1 iter 139 step
2022-06-12 00:50:50,665   Num examples = 408
2022-06-12 00:50:50,665   Batch size = 32
2022-06-12 00:50:50,666 ***** Eval results *****
2022-06-12 00:50:50,666   att_loss = 6.0806911849975585
2022-06-12 00:50:50,667   global_step = 139
2022-06-12 00:50:50,667   loss = 7.793775844573974
2022-06-12 00:50:50,667   rep_loss = 1.7130846691131592
2022-06-12 00:50:50,667 ***** Save model *****
2022-06-12 00:50:56,373 ***** Running evaluation *****
2022-06-12 00:50:56,373   Epoch = 1 iter 159 step
2022-06-12 00:50:56,373   Num examples = 408
2022-06-12 00:50:56,373   Batch size = 32
2022-06-12 00:50:56,374 ***** Eval results *****
2022-06-12 00:50:56,374   att_loss = 6.070165867275662
2022-06-12 00:50:56,374   global_step = 159
2022-06-12 00:50:56,374   loss = 7.773506206936307
2022-06-12 00:50:56,374   rep_loss = 1.7033403052224054
2022-06-12 00:50:56,374 ***** Save model *****
2022-06-12 00:50:57,047 Writing example 60000 of 104743
2022-06-12 00:51:02,103 ***** Running evaluation *****
2022-06-12 00:51:02,104   Epoch = 1 iter 179 step
2022-06-12 00:51:02,104   Num examples = 408
2022-06-12 00:51:02,104   Batch size = 32
2022-06-12 00:51:02,105 ***** Eval results *****
2022-06-12 00:51:02,105   att_loss = 6.016860536428598
2022-06-12 00:51:02,106   global_step = 179
2022-06-12 00:51:02,106   loss = 7.709604710798997
2022-06-12 00:51:02,106   rep_loss = 1.692744156030508
2022-06-12 00:51:02,106 ***** Save model *****
2022-06-12 00:51:04,440 Writing example 70000 of 104743
2022-06-12 00:51:07,848 ***** Running evaluation *****
2022-06-12 00:51:07,849   Epoch = 1 iter 199 step
2022-06-12 00:51:07,849   Num examples = 408
2022-06-12 00:51:07,849   Batch size = 32
2022-06-12 00:51:07,850 ***** Eval results *****
2022-06-12 00:51:07,850   att_loss = 5.997993676802691
2022-06-12 00:51:07,850   global_step = 199
2022-06-12 00:51:07,850   loss = 7.683733850366929
2022-06-12 00:51:07,850   rep_loss = 1.6857401651494643
2022-06-12 00:51:07,850 ***** Save model *****
2022-06-12 00:51:12,206 Writing example 80000 of 104743
2022-06-12 00:51:13,614 ***** Running evaluation *****
2022-06-12 00:51:13,614   Epoch = 1 iter 219 step
2022-06-12 00:51:13,614   Num examples = 408
2022-06-12 00:51:13,614   Batch size = 32
2022-06-12 00:51:13,615 ***** Eval results *****
2022-06-12 00:51:13,615   att_loss = 5.9289678164890836
2022-06-12 00:51:13,615   global_step = 219
2022-06-12 00:51:13,615   loss = 7.6041384833199634
2022-06-12 00:51:13,616   rep_loss = 1.6751706588835944
2022-06-12 00:51:13,616 ***** Save model *****
2022-06-12 00:51:19,328 ***** Running evaluation *****
2022-06-12 00:51:19,329   Epoch = 2 iter 239 step
2022-06-12 00:51:19,329   Num examples = 408
2022-06-12 00:51:19,329   Batch size = 32
2022-06-12 00:51:19,330 ***** Eval results *****
2022-06-12 00:51:19,330   att_loss = 5.705594669688832
2022-06-12 00:51:19,330   global_step = 239
2022-06-12 00:51:19,330   loss = 7.333580320531672
2022-06-12 00:51:19,330   rep_loss = 1.627985661680048
2022-06-12 00:51:19,330 ***** Save model *****
2022-06-12 00:51:19,699 Writing example 90000 of 104743
2022-06-12 00:51:25,058 ***** Running evaluation *****
2022-06-12 00:51:25,059   Epoch = 2 iter 259 step
2022-06-12 00:51:25,059   Num examples = 408
2022-06-12 00:51:25,059   Batch size = 32
2022-06-12 00:51:25,060 ***** Eval results *****
2022-06-12 00:51:25,060   att_loss = 5.642841631366361
2022-06-12 00:51:25,060   global_step = 259
2022-06-12 00:51:25,060   loss = 7.259671580406927
2022-06-12 00:51:25,060   rep_loss = 1.6168299721133323
2022-06-12 00:51:25,060 ***** Save model *****
2022-06-12 00:51:27,189 Writing example 100000 of 104743
2022-06-12 00:51:30,812 ***** Running evaluation *****
2022-06-12 00:51:30,813   Epoch = 2 iter 279 step
2022-06-12 00:51:30,813   Num examples = 408
2022-06-12 00:51:30,813   Batch size = 32
2022-06-12 00:51:30,814 ***** Eval results *****
2022-06-12 00:51:30,814   att_loss = 5.626680336746515
2022-06-12 00:51:30,814   global_step = 279
2022-06-12 00:51:30,814   loss = 7.237209666009043
2022-06-12 00:51:30,814   rep_loss = 1.6105293549743354
2022-06-12 00:51:30,815 ***** Save model *****
2022-06-12 00:51:31,924 Writing example 0 of 5463
2022-06-12 00:51:31,925 *** Example ***
2022-06-12 00:51:31,925 guid: dev_matched-0
2022-06-12 00:51:31,925 tokens: [CLS] what came into force after the new constitution was herald ? [SEP] as of that day , the new constitution herald ##ing the second republic came into force . [SEP]
2022-06-12 00:51:31,925 input_ids: 101 2054 2234 2046 2486 2044 1996 2047 4552 2001 9536 1029 102 2004 1997 2008 2154 1010 1996 2047 4552 9536 2075 1996 2117 3072 2234 2046 2486 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:51:31,925 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:51:31,925 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 00:51:31,925 label: entailment
2022-06-12 00:51:31,925 label_id: 0
2022-06-12 00:51:36,161 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 00:51:36,552 ***** Running evaluation *****
2022-06-12 00:51:36,552   Epoch = 2 iter 299 step
2022-06-12 00:51:36,552   Num examples = 408
2022-06-12 00:51:36,552   Batch size = 32
2022-06-12 00:51:36,554 ***** Eval results *****
2022-06-12 00:51:36,554   att_loss = 5.614507312506017
2022-06-12 00:51:36,554   global_step = 299
2022-06-12 00:51:36,554   loss = 7.221072183528417
2022-06-12 00:51:36,554   rep_loss = 1.606564894528456
2022-06-12 00:51:36,554 ***** Save model *****
2022-06-12 00:51:41,554 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/qnli/on_original_data/pytorch_model.bin
2022-06-12 00:51:42,257 loading model...
2022-06-12 00:51:42,263 ***** Running evaluation *****
2022-06-12 00:51:42,263   Epoch = 2 iter 319 step
2022-06-12 00:51:42,263   Num examples = 408
2022-06-12 00:51:42,263   Batch size = 32
2022-06-12 00:51:42,265 ***** Eval results *****
2022-06-12 00:51:42,265   att_loss = 5.571586184449249
2022-06-12 00:51:42,265   global_step = 319
2022-06-12 00:51:42,265   loss = 7.172214660015735
2022-06-12 00:51:42,265   rep_loss = 1.6006284965263617
2022-06-12 00:51:42,265 ***** Save model *****
2022-06-12 00:51:42,418 done!
2022-06-12 00:51:42,418 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.dense_fit.weight', 'bert.dense_fit.bias']
2022-06-12 00:51:46,061 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 00:51:47,170 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-12 00:51:47,317 loading model...
2022-06-12 00:51:47,345 done!
2022-06-12 00:51:47,345 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-12 00:51:47,345 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-12 00:51:48,095 ***** Running evaluation *****
2022-06-12 00:51:48,095   Epoch = 2 iter 339 step
2022-06-12 00:51:48,095   Num examples = 408
2022-06-12 00:51:48,095   Batch size = 32
2022-06-12 00:51:48,097 ***** Eval results *****
2022-06-12 00:51:48,097   att_loss = 5.54659754950721
2022-06-12 00:51:48,097   global_step = 339
2022-06-12 00:51:48,097   loss = 7.141998166436547
2022-06-12 00:51:48,097   rep_loss = 1.5954006373345315
2022-06-12 00:51:48,097 ***** Save model *****
2022-06-12 00:51:48,765 ***** Running training *****
2022-06-12 00:51:48,776   Num examples = 104743
2022-06-12 00:51:48,776   Batch size = 32
2022-06-12 00:51:48,776   Num steps = 32730
2022-06-12 00:51:48,777 n: bert.embeddings.word_embeddings.weight
2022-06-12 00:51:48,787 n: bert.embeddings.position_embeddings.weight
2022-06-12 00:51:48,798 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 00:51:48,813 n: bert.embeddings.LayerNorm.weight
2022-06-12 00:51:48,829 n: bert.embeddings.LayerNorm.bias
2022-06-12 00:51:48,839 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 00:51:48,839 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 00:51:48,839 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 00:51:48,839 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 00:51:48,839 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 00:51:48,839 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 00:51:48,839 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 00:51:48,840 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 00:51:48,841 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 00:51:48,841 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 00:51:48,841 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 00:51:48,841 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 00:51:48,841 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 00:51:48,841 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 00:51:48,842 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 00:51:48,843 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 00:51:48,844 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 00:51:48,845 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 00:51:48,845 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 00:51:48,845 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 00:51:48,845 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 00:51:48,846 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 00:51:48,846 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 00:51:48,846 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 00:51:48,846 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 00:51:48,846 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 00:51:48,846 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 00:51:48,846 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 00:51:48,846 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 00:51:48,846 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 00:51:48,847 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 00:51:48,847 n: bert.pooler.dense.weight
2022-06-12 00:51:48,847 n: bert.pooler.dense.bias
2022-06-12 00:51:48,847 n: classifier.weight
2022-06-12 00:51:48,847 n: classifier.bias
2022-06-12 00:51:48,847 n: fit_denses.0.weight
2022-06-12 00:51:48,848 n: fit_denses.0.bias
2022-06-12 00:51:48,848 n: fit_denses.1.weight
2022-06-12 00:51:48,848 n: fit_denses.1.bias
2022-06-12 00:51:48,848 n: fit_denses.2.weight
2022-06-12 00:51:48,848 n: fit_denses.2.bias
2022-06-12 00:51:48,848 n: fit_denses.3.weight
2022-06-12 00:51:48,848 n: fit_denses.3.bias
2022-06-12 00:51:48,848 n: fit_denses.4.weight
2022-06-12 00:51:48,848 n: fit_denses.4.bias
2022-06-12 00:51:48,848 n: fit_denses.5.weight
2022-06-12 00:51:48,848 n: fit_denses.5.bias
2022-06-12 00:51:48,849 n: fit_denses.6.weight
2022-06-12 00:51:48,849 n: fit_denses.6.bias
2022-06-12 00:51:48,849 Total parameters: 72468738
2022-06-12 00:51:53,811 ***** Running evaluation *****
2022-06-12 00:51:53,812   Epoch = 3 iter 359 step
2022-06-12 00:51:53,812   Num examples = 408
2022-06-12 00:51:53,812   Batch size = 32
2022-06-12 00:51:53,813 ***** Eval results *****
2022-06-12 00:51:53,813   att_loss = 5.2991430899676155
2022-06-12 00:51:53,813   global_step = 359
2022-06-12 00:51:53,813   loss = 6.859161881839528
2022-06-12 00:51:53,813   rep_loss = 1.5600187357734232
2022-06-12 00:51:53,813 ***** Save model *****
2022-06-12 00:51:59,578 ***** Running evaluation *****
2022-06-12 00:51:59,579   Epoch = 3 iter 379 step
2022-06-12 00:51:59,579   Num examples = 408
2022-06-12 00:51:59,579   Batch size = 32
2022-06-12 00:51:59,580 ***** Eval results *****
2022-06-12 00:51:59,580   att_loss = 5.3347225318083895
2022-06-12 00:51:59,580   global_step = 379
2022-06-12 00:51:59,580   loss = 6.89401642051903
2022-06-12 00:51:59,580   rep_loss = 1.5592938468262956
2022-06-12 00:51:59,580 ***** Save model *****
2022-06-12 00:52:05,349 ***** Running evaluation *****
2022-06-12 00:52:05,350   Epoch = 3 iter 399 step
2022-06-12 00:52:05,350   Num examples = 408
2022-06-12 00:52:05,350   Batch size = 32
2022-06-12 00:52:05,351 ***** Eval results *****
2022-06-12 00:52:05,351   att_loss = 5.300364201528984
2022-06-12 00:52:05,351   global_step = 399
2022-06-12 00:52:05,351   loss = 6.854972069723564
2022-06-12 00:52:05,352   rep_loss = 1.5546078514634518
2022-06-12 00:52:05,352 ***** Save model *****
2022-06-12 00:52:11,093 ***** Running evaluation *****
2022-06-12 00:52:11,093   Epoch = 3 iter 419 step
2022-06-12 00:52:11,093   Num examples = 408
2022-06-12 00:52:11,093   Batch size = 32
2022-06-12 00:52:11,094 ***** Eval results *****
2022-06-12 00:52:11,094   att_loss = 5.283823075232568
2022-06-12 00:52:11,094   global_step = 419
2022-06-12 00:52:11,094   loss = 6.835170299975903
2022-06-12 00:52:11,095   rep_loss = 1.551347213906127
2022-06-12 00:52:11,095 ***** Save model *****
2022-06-12 00:52:16,815 ***** Running evaluation *****
2022-06-12 00:52:16,815   Epoch = 3 iter 439 step
2022-06-12 00:52:16,815   Num examples = 408
2022-06-12 00:52:16,815   Batch size = 32
2022-06-12 00:52:16,816 ***** Eval results *****
2022-06-12 00:52:16,816   att_loss = 5.2881525570584325
2022-06-12 00:52:16,817   global_step = 439
2022-06-12 00:52:16,817   loss = 6.836660847221453
2022-06-12 00:52:16,817   rep_loss = 1.5485082643548238
2022-06-12 00:52:16,817 ***** Save model *****
2022-06-12 00:52:22,571 ***** Running evaluation *****
2022-06-12 00:52:22,572   Epoch = 4 iter 459 step
2022-06-12 00:52:22,572   Num examples = 408
2022-06-12 00:52:22,572   Batch size = 32
2022-06-12 00:52:22,573 ***** Eval results *****
2022-06-12 00:52:22,573   att_loss = 5.319143136342366
2022-06-12 00:52:22,573   global_step = 459
2022-06-12 00:52:22,573   loss = 6.8585812250773115
2022-06-12 00:52:22,574   rep_loss = 1.5394381682078044
2022-06-12 00:52:22,574 ***** Save model *****
2022-06-12 00:52:28,362 ***** Running evaluation *****
2022-06-12 00:52:28,362   Epoch = 4 iter 479 step
2022-06-12 00:52:28,362   Num examples = 408
2022-06-12 00:52:28,362   Batch size = 32
2022-06-12 00:52:28,364 ***** Eval results *****
2022-06-12 00:52:28,364   att_loss = 5.133306213047193
2022-06-12 00:52:28,364   global_step = 479
2022-06-12 00:52:28,364   loss = 6.651251191678255
2022-06-12 00:52:28,364   rep_loss = 1.5179449993631113
2022-06-12 00:52:28,364 ***** Save model *****
2022-06-12 00:52:34,118 ***** Running evaluation *****
2022-06-12 00:52:34,119   Epoch = 4 iter 499 step
2022-06-12 00:52:34,119   Num examples = 408
2022-06-12 00:52:34,119   Batch size = 32
2022-06-12 00:52:34,121 ***** Eval results *****
2022-06-12 00:52:34,121   att_loss = 5.119882228762605
2022-06-12 00:52:34,121   global_step = 499
2022-06-12 00:52:34,121   loss = 6.638859382895536
2022-06-12 00:52:34,121   rep_loss = 1.5189771569052408
2022-06-12 00:52:34,121 ***** Save model *****
2022-06-12 00:52:39,962 ***** Running evaluation *****
2022-06-12 00:52:39,963   Epoch = 4 iter 519 step
2022-06-12 00:52:39,963   Num examples = 408
2022-06-12 00:52:39,963   Batch size = 32
2022-06-12 00:52:39,964 ***** Eval results *****
2022-06-12 00:52:39,964   att_loss = 5.1149972885374035
2022-06-12 00:52:39,965   global_step = 519
2022-06-12 00:52:39,965   loss = 6.631822381700788
2022-06-12 00:52:39,965   rep_loss = 1.5168250950555953
2022-06-12 00:52:39,965 ***** Save model *****
2022-06-12 00:52:45,720 ***** Running evaluation *****
2022-06-12 00:52:45,721   Epoch = 4 iter 539 step
2022-06-12 00:52:45,721   Num examples = 408
2022-06-12 00:52:45,721   Batch size = 32
2022-06-12 00:52:45,723 ***** Eval results *****
2022-06-12 00:52:45,723   att_loss = 5.0712648184902696
2022-06-12 00:52:45,723   global_step = 539
2022-06-12 00:52:45,723   loss = 6.583174079297537
2022-06-12 00:52:45,723   rep_loss = 1.5119092579347542
2022-06-12 00:52:45,723 ***** Save model *****
2022-06-12 00:52:51,466 ***** Running evaluation *****
2022-06-12 00:52:51,467   Epoch = 4 iter 559 step
2022-06-12 00:52:51,467   Num examples = 408
2022-06-12 00:52:51,467   Batch size = 32
2022-06-12 00:52:51,468 ***** Eval results *****
2022-06-12 00:52:51,468   att_loss = 5.048331348641405
2022-06-12 00:52:51,468   global_step = 559
2022-06-12 00:52:51,468   loss = 6.557047126362625
2022-06-12 00:52:51,468   rep_loss = 1.5087157684622459
2022-06-12 00:52:51,468 ***** Save model *****
2022-06-12 00:52:57,238 ***** Running evaluation *****
2022-06-12 00:52:57,238   Epoch = 5 iter 579 step
2022-06-12 00:52:57,238   Num examples = 408
2022-06-12 00:52:57,238   Batch size = 32
2022-06-12 00:52:57,239 ***** Eval results *****
2022-06-12 00:52:57,239   att_loss = 4.943167739444309
2022-06-12 00:52:57,239   global_step = 579
2022-06-12 00:52:57,239   loss = 6.437912411159939
2022-06-12 00:52:57,239   rep_loss = 1.494744724697537
2022-06-12 00:52:57,240 ***** Save model *****
2022-06-12 00:53:02,934 ***** Running evaluation *****
2022-06-12 00:53:02,935   Epoch = 5 iter 599 step
2022-06-12 00:53:02,935   Num examples = 408
2022-06-12 00:53:02,935   Batch size = 32
2022-06-12 00:53:02,936 ***** Eval results *****
2022-06-12 00:53:02,936   att_loss = 4.920734323304275
2022-06-12 00:53:02,936   global_step = 599
2022-06-12 00:53:02,936   loss = 6.409638059550319
2022-06-12 00:53:02,936   rep_loss = 1.4889037732420296
2022-06-12 00:53:02,936 ***** Save model *****
2022-06-12 00:53:08,618 ***** Running evaluation *****
2022-06-12 00:53:08,618   Epoch = 5 iter 619 step
2022-06-12 00:53:08,619   Num examples = 408
2022-06-12 00:53:08,619   Batch size = 32
2022-06-12 00:53:08,620 ***** Eval results *****
2022-06-12 00:53:08,620   att_loss = 4.908900620986004
2022-06-12 00:53:08,620   global_step = 619
2022-06-12 00:53:08,620   loss = 6.395285509070572
2022-06-12 00:53:08,620   rep_loss = 1.4863849416071055
2022-06-12 00:53:08,620 ***** Save model *****
2022-06-12 00:53:14,330 ***** Running evaluation *****
2022-06-12 00:53:14,330   Epoch = 5 iter 639 step
2022-06-12 00:53:14,330   Num examples = 408
2022-06-12 00:53:14,330   Batch size = 32
2022-06-12 00:53:14,331 ***** Eval results *****
2022-06-12 00:53:14,331   att_loss = 4.891761489536451
2022-06-12 00:53:14,331   global_step = 639
2022-06-12 00:53:14,331   loss = 6.3737106530562695
2022-06-12 00:53:14,332   rep_loss = 1.4819492049839185
2022-06-12 00:53:14,332 ***** Save model *****
2022-06-12 00:53:20,042 ***** Running evaluation *****
2022-06-12 00:53:20,042   Epoch = 5 iter 659 step
2022-06-12 00:53:20,042   Num examples = 408
2022-06-12 00:53:20,042   Batch size = 32
2022-06-12 00:53:20,043 ***** Eval results *****
2022-06-12 00:53:20,043   att_loss = 4.872856900933083
2022-06-12 00:53:20,043   global_step = 659
2022-06-12 00:53:20,043   loss = 6.352093980553445
2022-06-12 00:53:20,043   rep_loss = 1.479237114445547
2022-06-12 00:53:20,043 ***** Save model *****
2022-06-12 00:53:25,742 ***** Running evaluation *****
2022-06-12 00:53:25,743   Epoch = 5 iter 679 step
2022-06-12 00:53:25,743   Num examples = 408
2022-06-12 00:53:25,743   Batch size = 32
2022-06-12 00:53:25,744 ***** Eval results *****
2022-06-12 00:53:25,744   att_loss = 4.853454003640271
2022-06-12 00:53:25,744   global_step = 679
2022-06-12 00:53:25,744   loss = 6.329979612192976
2022-06-12 00:53:25,744   rep_loss = 1.476525639175275
2022-06-12 00:53:25,744 ***** Save model *****
2022-06-12 00:53:31,457 ***** Running evaluation *****
2022-06-12 00:53:31,457   Epoch = 6 iter 699 step
2022-06-12 00:53:31,457   Num examples = 408
2022-06-12 00:53:31,457   Batch size = 32
2022-06-12 00:53:31,458 ***** Eval results *****
2022-06-12 00:53:31,459   att_loss = 4.808022658030192
2022-06-12 00:53:31,459   global_step = 699
2022-06-12 00:53:31,459   loss = 6.270092074076334
2022-06-12 00:53:31,459   rep_loss = 1.4620694398880005
2022-06-12 00:53:31,459 ***** Save model *****
2022-06-12 00:53:37,168 ***** Running evaluation *****
2022-06-12 00:53:37,168   Epoch = 6 iter 719 step
2022-06-12 00:53:37,168   Num examples = 408
2022-06-12 00:53:37,168   Batch size = 32
2022-06-12 00:53:37,169 ***** Eval results *****
2022-06-12 00:53:37,169   att_loss = 4.817317349570138
2022-06-12 00:53:37,169   global_step = 719
2022-06-12 00:53:37,169   loss = 6.280618245261056
2022-06-12 00:53:37,170   rep_loss = 1.4633009161267962
2022-06-12 00:53:37,170 ***** Save model *****
2022-06-12 00:53:42,882 ***** Running evaluation *****
2022-06-12 00:53:42,882   Epoch = 6 iter 739 step
2022-06-12 00:53:42,882   Num examples = 408
2022-06-12 00:53:42,882   Batch size = 32
2022-06-12 00:53:42,884 ***** Eval results *****
2022-06-12 00:53:42,884   att_loss = 4.777657760273327
2022-06-12 00:53:42,884   global_step = 739
2022-06-12 00:53:42,884   loss = 6.23593919060447
2022-06-12 00:53:42,884   rep_loss = 1.4582814411683516
2022-06-12 00:53:42,884 ***** Save model *****
2022-06-12 00:53:48,597 ***** Running evaluation *****
2022-06-12 00:53:48,598   Epoch = 6 iter 759 step
2022-06-12 00:53:48,598   Num examples = 408
2022-06-12 00:53:48,598   Batch size = 32
2022-06-12 00:53:48,599 ***** Eval results *****
2022-06-12 00:53:48,599   att_loss = 4.7660600980122885
2022-06-12 00:53:48,599   global_step = 759
2022-06-12 00:53:48,599   loss = 6.2223327891031905
2022-06-12 00:53:48,599   rep_loss = 1.4562726895014444
2022-06-12 00:53:48,599 ***** Save model *****
2022-06-12 00:53:54,333 ***** Running evaluation *****
2022-06-12 00:53:54,333   Epoch = 6 iter 779 step
2022-06-12 00:53:54,333   Num examples = 408
2022-06-12 00:53:54,333   Batch size = 32
2022-06-12 00:53:54,335 ***** Eval results *****
2022-06-12 00:53:54,335   att_loss = 4.738442390843441
2022-06-12 00:53:54,335   global_step = 779
2022-06-12 00:53:54,335   loss = 6.192421110052812
2022-06-12 00:53:54,335   rep_loss = 1.4539787104255275
2022-06-12 00:53:54,335 ***** Save model *****
2022-06-12 00:53:55,918 ***** Running evaluation *****
2022-06-12 00:53:55,918   Epoch = 0 iter 499 step
2022-06-12 00:53:55,918   Num examples = 5463
2022-06-12 00:53:55,918   Batch size = 32
2022-06-12 00:53:55,919 ***** Eval results *****
2022-06-12 00:53:55,919   att_loss = 7.311712701717216
2022-06-12 00:53:55,919   global_step = 499
2022-06-12 00:53:55,920   loss = 8.756626913686075
2022-06-12 00:53:55,920   rep_loss = 1.4449141847346731
2022-06-12 00:53:55,920 ***** Save model *****
2022-06-12 00:54:00,073 ***** Running evaluation *****
2022-06-12 00:54:00,074   Epoch = 7 iter 799 step
2022-06-12 00:54:00,074   Num examples = 408
2022-06-12 00:54:00,074   Batch size = 32
2022-06-12 00:54:00,075 ***** Eval results *****
2022-06-12 00:54:00,075   att_loss = 4.7475385665893555
2022-06-12 00:54:00,075   global_step = 799
2022-06-12 00:54:00,075   loss = 6.200934410095215
2022-06-12 00:54:00,075   rep_loss = 1.4533958435058594
2022-06-12 00:54:00,076 ***** Save model *****
2022-06-12 00:54:05,817 ***** Running evaluation *****
2022-06-12 00:54:05,818   Epoch = 7 iter 819 step
2022-06-12 00:54:05,818   Num examples = 408
2022-06-12 00:54:05,818   Batch size = 32
2022-06-12 00:54:05,819 ***** Eval results *****
2022-06-12 00:54:05,819   att_loss = 4.511256672087169
2022-06-12 00:54:05,819   global_step = 819
2022-06-12 00:54:05,819   loss = 5.943328017280216
2022-06-12 00:54:05,819   rep_loss = 1.4320713508696783
2022-06-12 00:54:05,819 ***** Save model *****
2022-06-12 00:54:11,548 ***** Running evaluation *****
2022-06-12 00:54:11,548   Epoch = 7 iter 839 step
2022-06-12 00:54:11,548   Num examples = 408
2022-06-12 00:54:11,548   Batch size = 32
2022-06-12 00:54:11,550 ***** Eval results *****
2022-06-12 00:54:11,550   att_loss = 4.563407374591362
2022-06-12 00:54:11,550   global_step = 839
2022-06-12 00:54:11,550   loss = 5.99537594725446
2022-06-12 00:54:11,550   rep_loss = 1.4319685610329234
2022-06-12 00:54:11,550 ***** Save model *****
2022-06-12 00:54:17,266 ***** Running evaluation *****
2022-06-12 00:54:17,266   Epoch = 7 iter 859 step
2022-06-12 00:54:17,267   Num examples = 408
2022-06-12 00:54:17,267   Batch size = 32
2022-06-12 00:54:17,268 ***** Eval results *****
2022-06-12 00:54:17,268   att_loss = 4.538959499265327
2022-06-12 00:54:17,268   global_step = 859
2022-06-12 00:54:17,268   loss = 5.97033652321237
2022-06-12 00:54:17,268   rep_loss = 1.4313770317640462
2022-06-12 00:54:17,268 ***** Save model *****
2022-06-12 00:54:23,011 ***** Running evaluation *****
2022-06-12 00:54:23,012   Epoch = 7 iter 879 step
2022-06-12 00:54:23,012   Num examples = 408
2022-06-12 00:54:23,012   Batch size = 32
2022-06-12 00:54:23,014 ***** Eval results *****
2022-06-12 00:54:23,014   att_loss = 4.578907628118256
2022-06-12 00:54:23,014   global_step = 879
2022-06-12 00:54:23,014   loss = 6.010758594230369
2022-06-12 00:54:23,014   rep_loss = 1.431850967583833
2022-06-12 00:54:23,014 ***** Save model *****
2022-06-12 00:54:28,758 ***** Running evaluation *****
2022-06-12 00:54:28,758   Epoch = 7 iter 899 step
2022-06-12 00:54:28,759   Num examples = 408
2022-06-12 00:54:28,759   Batch size = 32
2022-06-12 00:54:28,760 ***** Eval results *****
2022-06-12 00:54:28,760   att_loss = 4.57345989198968
2022-06-12 00:54:28,760   global_step = 899
2022-06-12 00:54:28,760   loss = 6.003062130200981
2022-06-12 00:54:28,760   rep_loss = 1.4296022452930413
2022-06-12 00:54:28,760 ***** Save model *****
2022-06-12 00:54:34,473 ***** Running evaluation *****
2022-06-12 00:54:34,474   Epoch = 8 iter 919 step
2022-06-12 00:54:34,474   Num examples = 408
2022-06-12 00:54:34,474   Batch size = 32
2022-06-12 00:54:34,475 ***** Eval results *****
2022-06-12 00:54:34,475   att_loss = 4.679750442504883
2022-06-12 00:54:34,475   global_step = 919
2022-06-12 00:54:34,475   loss = 6.1028000967843195
2022-06-12 00:54:34,475   rep_loss = 1.4230496031897408
2022-06-12 00:54:34,475 ***** Save model *****
2022-06-12 00:54:40,211 ***** Running evaluation *****
2022-06-12 00:54:40,212   Epoch = 8 iter 939 step
2022-06-12 00:54:40,212   Num examples = 408
2022-06-12 00:54:40,212   Batch size = 32
2022-06-12 00:54:40,213 ***** Eval results *****
2022-06-12 00:54:40,213   att_loss = 4.608613402755172
2022-06-12 00:54:40,213   global_step = 939
2022-06-12 00:54:40,213   loss = 6.032401455773248
2022-06-12 00:54:40,213   rep_loss = 1.4237880486029166
2022-06-12 00:54:40,214 ***** Save model *****
2022-06-12 00:54:45,907 ***** Running evaluation *****
2022-06-12 00:54:45,907   Epoch = 8 iter 959 step
2022-06-12 00:54:45,907   Num examples = 408
2022-06-12 00:54:45,908   Batch size = 32
2022-06-12 00:54:45,908 ***** Eval results *****
2022-06-12 00:54:45,909   att_loss = 4.543252731891388
2022-06-12 00:54:45,909   global_step = 959
2022-06-12 00:54:45,909   loss = 5.961675684502784
2022-06-12 00:54:45,909   rep_loss = 1.4184229576841314
2022-06-12 00:54:45,909 ***** Save model *****
2022-06-12 00:54:51,593 ***** Running evaluation *****
2022-06-12 00:54:51,594   Epoch = 8 iter 979 step
2022-06-12 00:54:51,594   Num examples = 408
2022-06-12 00:54:51,594   Batch size = 32
2022-06-12 00:54:51,595 ***** Eval results *****
2022-06-12 00:54:51,595   att_loss = 4.512559830252804
2022-06-12 00:54:51,595   global_step = 979
2022-06-12 00:54:51,595   loss = 5.9272239670824645
2022-06-12 00:54:51,595   rep_loss = 1.414664151063606
2022-06-12 00:54:51,595 ***** Save model *****
2022-06-12 00:54:57,311 ***** Running evaluation *****
2022-06-12 00:54:57,311   Epoch = 8 iter 999 step
2022-06-12 00:54:57,311   Num examples = 408
2022-06-12 00:54:57,311   Batch size = 32
2022-06-12 00:54:57,313 ***** Eval results *****
2022-06-12 00:54:57,313   att_loss = 4.499348199230501
2022-06-12 00:54:57,313   global_step = 999
2022-06-12 00:54:57,313   loss = 5.912665421935333
2022-06-12 00:54:57,313   rep_loss = 1.4133172322963845
2022-06-12 00:54:57,313 ***** Save model *****
2022-06-12 00:55:03,059 ***** Running evaluation *****
2022-06-12 00:55:03,059   Epoch = 8 iter 1019 step
2022-06-12 00:55:03,060   Num examples = 408
2022-06-12 00:55:03,060   Batch size = 32
2022-06-12 00:55:03,061 ***** Eval results *****
2022-06-12 00:55:03,061   att_loss = 4.476848352735288
2022-06-12 00:55:03,061   global_step = 1019
2022-06-12 00:55:03,061   loss = 5.886990551636598
2022-06-12 00:55:03,061   rep_loss = 1.4101422078141541
2022-06-12 00:55:03,061 ***** Save model *****
2022-06-12 00:55:08,758 ***** Running evaluation *****
2022-06-12 00:55:08,758   Epoch = 9 iter 1039 step
2022-06-12 00:55:08,758   Num examples = 408
2022-06-12 00:55:08,759   Batch size = 32
2022-06-12 00:55:08,760 ***** Eval results *****
2022-06-12 00:55:08,760   att_loss = 4.345270156860352
2022-06-12 00:55:08,760   global_step = 1039
2022-06-12 00:55:08,760   loss = 5.744947580190805
2022-06-12 00:55:08,760   rep_loss = 1.399677349970891
2022-06-12 00:55:08,760 ***** Save model *****
2022-06-12 00:55:14,432 ***** Running evaluation *****
2022-06-12 00:55:14,432   Epoch = 9 iter 1059 step
2022-06-12 00:55:14,432   Num examples = 408
2022-06-12 00:55:14,432   Batch size = 32
2022-06-12 00:55:14,433 ***** Eval results *****
2022-06-12 00:55:14,433   att_loss = 4.38981294631958
2022-06-12 00:55:14,433   global_step = 1059
2022-06-12 00:55:14,433   loss = 5.788954575856526
2022-06-12 00:55:14,433   rep_loss = 1.399141625924544
2022-06-12 00:55:14,434 ***** Save model *****
2022-06-12 00:55:20,150 ***** Running evaluation *****
2022-06-12 00:55:20,151   Epoch = 9 iter 1079 step
2022-06-12 00:55:20,151   Num examples = 408
2022-06-12 00:55:20,151   Batch size = 32
2022-06-12 00:55:20,153 ***** Eval results *****
2022-06-12 00:55:20,153   att_loss = 4.395357401865833
2022-06-12 00:55:20,153   global_step = 1079
2022-06-12 00:55:20,153   loss = 5.793103200084758
2022-06-12 00:55:20,153   rep_loss = 1.3977457892219975
2022-06-12 00:55:20,153 ***** Save model *****
2022-06-12 00:55:25,930 ***** Running evaluation *****
2022-06-12 00:55:25,930   Epoch = 9 iter 1099 step
2022-06-12 00:55:25,930   Num examples = 408
2022-06-12 00:55:25,930   Batch size = 32
2022-06-12 00:55:25,931 ***** Eval results *****
2022-06-12 00:55:25,931   att_loss = 4.395717183204546
2022-06-12 00:55:25,931   global_step = 1099
2022-06-12 00:55:25,931   loss = 5.791680205358218
2022-06-12 00:55:25,931   rep_loss = 1.3959630221536714
2022-06-12 00:55:25,932 ***** Save model *****
2022-06-12 00:55:31,623 ***** Running evaluation *****
2022-06-12 00:55:31,623   Epoch = 9 iter 1119 step
2022-06-12 00:55:31,623   Num examples = 408
2022-06-12 00:55:31,624   Batch size = 32
2022-06-12 00:55:31,625 ***** Eval results *****
2022-06-12 00:55:31,625   att_loss = 4.386617245212678
2022-06-12 00:55:31,625   global_step = 1119
2022-06-12 00:55:31,625   loss = 5.780839679061725
2022-06-12 00:55:31,625   rep_loss = 1.3942224300035866
2022-06-12 00:55:31,625 ***** Save model *****
2022-06-12 00:55:37,351 ***** Running evaluation *****
2022-06-12 00:55:37,351   Epoch = 9 iter 1139 step
2022-06-12 00:55:37,351   Num examples = 408
2022-06-12 00:55:37,351   Batch size = 32
2022-06-12 00:55:37,352 ***** Eval results *****
2022-06-12 00:55:37,352   att_loss = 4.3750743380690045
2022-06-12 00:55:37,352   global_step = 1139
2022-06-12 00:55:37,352   loss = 5.767411063202714
2022-06-12 00:55:37,352   rep_loss = 1.3923367240787607
2022-06-12 00:55:37,352 ***** Save model *****
2022-06-12 00:55:43,062 ***** Running evaluation *****
2022-06-12 00:55:43,063   Epoch = 10 iter 1159 step
2022-06-12 00:55:43,063   Num examples = 408
2022-06-12 00:55:43,063   Batch size = 32
2022-06-12 00:55:43,064 ***** Eval results *****
2022-06-12 00:55:43,064   att_loss = 4.425291400206716
2022-06-12 00:55:43,064   global_step = 1159
2022-06-12 00:55:43,064   loss = 5.817645273710552
2022-06-12 00:55:43,064   rep_loss = 1.3923538672296625
2022-06-12 00:55:43,064 ***** Save model *****
2022-06-12 00:55:48,804 ***** Running evaluation *****
2022-06-12 00:55:48,805   Epoch = 10 iter 1179 step
2022-06-12 00:55:48,805   Num examples = 408
2022-06-12 00:55:48,805   Batch size = 32
2022-06-12 00:55:48,807 ***** Eval results *****
2022-06-12 00:55:48,807   att_loss = 4.347332031298906
2022-06-12 00:55:48,807   global_step = 1179
2022-06-12 00:55:48,807   loss = 5.7296444574991865
2022-06-12 00:55:48,807   rep_loss = 1.3823124170303345
2022-06-12 00:55:48,807 ***** Save model *****
2022-06-12 00:55:54,553 ***** Running evaluation *****
2022-06-12 00:55:54,554   Epoch = 10 iter 1199 step
2022-06-12 00:55:54,554   Num examples = 408
2022-06-12 00:55:54,554   Batch size = 32
2022-06-12 00:55:54,555 ***** Eval results *****
2022-06-12 00:55:54,555   att_loss = 4.301063699237371
2022-06-12 00:55:54,555   global_step = 1199
2022-06-12 00:55:54,555   loss = 5.679681769872116
2022-06-12 00:55:54,555   rep_loss = 1.3786180888192128
2022-06-12 00:55:54,555 ***** Save model *****
2022-06-12 00:56:00,255 ***** Running evaluation *****
2022-06-12 00:56:00,256   Epoch = 10 iter 1219 step
2022-06-12 00:56:00,256   Num examples = 408
2022-06-12 00:56:00,256   Batch size = 32
2022-06-12 00:56:00,257 ***** Eval results *****
2022-06-12 00:56:00,257   att_loss = 4.301420960245253
2022-06-12 00:56:00,257   global_step = 1219
2022-06-12 00:56:00,257   loss = 5.679734024820449
2022-06-12 00:56:00,257   rep_loss = 1.3783130781560005
2022-06-12 00:56:00,257 ***** Save model *****
2022-06-12 00:56:03,791 ***** Running evaluation *****
2022-06-12 00:56:03,792   Epoch = 0 iter 999 step
2022-06-12 00:56:03,792   Num examples = 5463
2022-06-12 00:56:03,792   Batch size = 32
2022-06-12 00:56:03,793 ***** Eval results *****
2022-06-12 00:56:03,793   att_loss = 6.684091410956703
2022-06-12 00:56:03,793   global_step = 999
2022-06-12 00:56:03,794   loss = 8.021174176915869
2022-06-12 00:56:03,794   rep_loss = 1.3370827553389188
2022-06-12 00:56:03,794 ***** Save model *****
2022-06-12 00:56:05,982 ***** Running evaluation *****
2022-06-12 00:56:05,983   Epoch = 10 iter 1239 step
2022-06-12 00:56:05,983   Num examples = 408
2022-06-12 00:56:05,983   Batch size = 32
2022-06-12 00:56:05,985 ***** Eval results *****
2022-06-12 00:56:05,986   att_loss = 4.310663295514656
2022-06-12 00:56:05,986   global_step = 1239
2022-06-12 00:56:05,986   loss = 5.689266421578147
2022-06-12 00:56:05,986   rep_loss = 1.3786031332882969
2022-06-12 00:56:05,986 ***** Save model *****
2022-06-12 00:56:11,734 ***** Running evaluation *****
2022-06-12 00:56:11,734   Epoch = 11 iter 1259 step
2022-06-12 00:56:11,734   Num examples = 408
2022-06-12 00:56:11,734   Batch size = 32
2022-06-12 00:56:11,735 ***** Eval results *****
2022-06-12 00:56:11,735   att_loss = 4.315512466430664
2022-06-12 00:56:11,735   global_step = 1259
2022-06-12 00:56:11,735   loss = 5.6889488220214846
2022-06-12 00:56:11,735   rep_loss = 1.3734363079071046
2022-06-12 00:56:11,736 ***** Save model *****
2022-06-12 00:56:17,436 ***** Running evaluation *****
2022-06-12 00:56:17,437   Epoch = 11 iter 1279 step
2022-06-12 00:56:17,437   Num examples = 408
2022-06-12 00:56:17,437   Batch size = 32
2022-06-12 00:56:17,438 ***** Eval results *****
2022-06-12 00:56:17,438   att_loss = 4.178675565719605
2022-06-12 00:56:17,438   global_step = 1279
2022-06-12 00:56:17,438   loss = 5.544436702728271
2022-06-12 00:56:17,438   rep_loss = 1.365761137008667
2022-06-12 00:56:17,438 ***** Save model *****
2022-06-12 00:56:23,171 ***** Running evaluation *****
2022-06-12 00:56:23,172   Epoch = 11 iter 1299 step
2022-06-12 00:56:23,172   Num examples = 408
2022-06-12 00:56:23,172   Batch size = 32
2022-06-12 00:56:23,173 ***** Eval results *****
2022-06-12 00:56:23,174   att_loss = 4.263308058844673
2022-06-12 00:56:23,174   global_step = 1299
2022-06-12 00:56:23,174   loss = 5.631597900390625
2022-06-12 00:56:23,174   rep_loss = 1.3682898388968574
2022-06-12 00:56:23,174 ***** Save model *****
2022-06-12 00:56:28,913 ***** Running evaluation *****
2022-06-12 00:56:28,914   Epoch = 11 iter 1319 step
2022-06-12 00:56:28,914   Num examples = 408
2022-06-12 00:56:28,914   Batch size = 32
2022-06-12 00:56:28,915 ***** Eval results *****
2022-06-12 00:56:28,915   att_loss = 4.269349160561195
2022-06-12 00:56:28,915   global_step = 1319
2022-06-12 00:56:28,915   loss = 5.63697667488685
2022-06-12 00:56:28,915   rep_loss = 1.367627506989699
2022-06-12 00:56:28,915 ***** Save model *****
2022-06-12 00:56:34,607 ***** Running evaluation *****
2022-06-12 00:56:34,607   Epoch = 11 iter 1339 step
2022-06-12 00:56:34,607   Num examples = 408
2022-06-12 00:56:34,607   Batch size = 32
2022-06-12 00:56:34,609 ***** Eval results *****
2022-06-12 00:56:34,609   att_loss = 4.282650260364308
2022-06-12 00:56:34,609   global_step = 1339
2022-06-12 00:56:34,609   loss = 5.651035942750819
2022-06-12 00:56:34,609   rep_loss = 1.3683856767766616
2022-06-12 00:56:34,609 ***** Save model *****
2022-06-12 00:56:40,345 ***** Running evaluation *****
2022-06-12 00:56:40,345   Epoch = 11 iter 1359 step
2022-06-12 00:56:40,345   Num examples = 408
2022-06-12 00:56:40,345   Batch size = 32
2022-06-12 00:56:40,347 ***** Eval results *****
2022-06-12 00:56:40,347   att_loss = 4.257802658989316
2022-06-12 00:56:40,347   global_step = 1359
2022-06-12 00:56:40,347   loss = 5.622535832722982
2022-06-12 00:56:40,347   rep_loss = 1.364733168057033
2022-06-12 00:56:40,347 ***** Save model *****
2022-06-12 00:56:46,025 ***** Running evaluation *****
2022-06-12 00:56:46,026   Epoch = 12 iter 1379 step
2022-06-12 00:56:46,026   Num examples = 408
2022-06-12 00:56:46,026   Batch size = 32
2022-06-12 00:56:46,027 ***** Eval results *****
2022-06-12 00:56:46,027   att_loss = 4.208249373869463
2022-06-12 00:56:46,027   global_step = 1379
2022-06-12 00:56:46,027   loss = 5.567652442238548
2022-06-12 00:56:46,027   rep_loss = 1.359403046694669
2022-06-12 00:56:46,027 ***** Save model *****
2022-06-12 00:56:51,794 ***** Running evaluation *****
2022-06-12 00:56:51,794   Epoch = 12 iter 1399 step
2022-06-12 00:56:51,794   Num examples = 408
2022-06-12 00:56:51,794   Batch size = 32
2022-06-12 00:56:51,796 ***** Eval results *****
2022-06-12 00:56:51,796   att_loss = 4.208210983584004
2022-06-12 00:56:51,796   global_step = 1399
2022-06-12 00:56:51,796   loss = 5.562997525738131
2022-06-12 00:56:51,796   rep_loss = 1.3547865344632057
2022-06-12 00:56:51,796 ***** Save model *****
2022-06-12 00:56:57,525 ***** Running evaluation *****
2022-06-12 00:56:57,525   Epoch = 12 iter 1419 step
2022-06-12 00:56:57,525   Num examples = 408
2022-06-12 00:56:57,525   Batch size = 32
2022-06-12 00:56:57,526 ***** Eval results *****
2022-06-12 00:56:57,527   att_loss = 4.204242000392839
2022-06-12 00:56:57,527   global_step = 1419
2022-06-12 00:56:57,527   loss = 5.559086042291978
2022-06-12 00:56:57,527   rep_loss = 1.3548440465740128
2022-06-12 00:56:57,527 ***** Save model *****
2022-06-12 00:57:03,252 ***** Running evaluation *****
2022-06-12 00:57:03,253   Epoch = 12 iter 1439 step
2022-06-12 00:57:03,253   Num examples = 408
2022-06-12 00:57:03,253   Batch size = 32
2022-06-12 00:57:03,254 ***** Eval results *****
2022-06-12 00:57:03,254   att_loss = 4.210763706287867
2022-06-12 00:57:03,254   global_step = 1439
2022-06-12 00:57:03,254   loss = 5.564444958324164
2022-06-12 00:57:03,254   rep_loss = 1.3536812553943043
2022-06-12 00:57:03,255 ***** Save model *****
2022-06-12 00:57:08,995 ***** Running evaluation *****
2022-06-12 00:57:08,995   Epoch = 12 iter 1459 step
2022-06-12 00:57:08,995   Num examples = 408
2022-06-12 00:57:08,995   Batch size = 32
2022-06-12 00:57:08,996 ***** Eval results *****
2022-06-12 00:57:08,996   att_loss = 4.196274209808517
2022-06-12 00:57:08,996   global_step = 1459
2022-06-12 00:57:08,996   loss = 5.548433864509667
2022-06-12 00:57:08,996   rep_loss = 1.3521596520811647
2022-06-12 00:57:08,997 ***** Save model *****
2022-06-12 00:57:14,734 ***** Running evaluation *****
2022-06-12 00:57:14,734   Epoch = 12 iter 1479 step
2022-06-12 00:57:14,734   Num examples = 408
2022-06-12 00:57:14,734   Batch size = 32
2022-06-12 00:57:14,736 ***** Eval results *****
2022-06-12 00:57:14,736   att_loss = 4.188329986623816
2022-06-12 00:57:14,736   global_step = 1479
2022-06-12 00:57:14,736   loss = 5.538260554408168
2022-06-12 00:57:14,736   rep_loss = 1.3499305731541402
2022-06-12 00:57:14,736 ***** Save model *****
2022-06-12 00:57:20,466 ***** Running evaluation *****
2022-06-12 00:57:20,467   Epoch = 13 iter 1499 step
2022-06-12 00:57:20,467   Num examples = 408
2022-06-12 00:57:20,467   Batch size = 32
2022-06-12 00:57:20,468 ***** Eval results *****
2022-06-12 00:57:20,468   att_loss = 4.148918137830846
2022-06-12 00:57:20,469   global_step = 1499
2022-06-12 00:57:20,469   loss = 5.490809216218836
2022-06-12 00:57:20,469   rep_loss = 1.341891043326434
2022-06-12 00:57:20,469 ***** Save model *****
2022-06-12 00:57:26,218 ***** Running evaluation *****
2022-06-12 00:57:26,218   Epoch = 13 iter 1519 step
2022-06-12 00:57:26,218   Num examples = 408
2022-06-12 00:57:26,219   Batch size = 32
2022-06-12 00:57:26,220 ***** Eval results *****
2022-06-12 00:57:26,220   att_loss = 4.122654773093559
2022-06-12 00:57:26,220   global_step = 1519
2022-06-12 00:57:26,220   loss = 5.4613261093964445
2022-06-12 00:57:26,220   rep_loss = 1.3386713427466315
2022-06-12 00:57:26,220 ***** Save model *****
2022-06-12 00:57:31,979 ***** Running evaluation *****
2022-06-12 00:57:31,980   Epoch = 13 iter 1539 step
2022-06-12 00:57:31,980   Num examples = 408
2022-06-12 00:57:31,980   Batch size = 32
2022-06-12 00:57:31,981 ***** Eval results *****
2022-06-12 00:57:31,981   att_loss = 4.118464641403734
2022-06-12 00:57:31,981   global_step = 1539
2022-06-12 00:57:31,981   loss = 5.457298462851005
2022-06-12 00:57:31,981   rep_loss = 1.33883381726449
2022-06-12 00:57:31,981 ***** Save model *****
2022-06-12 00:57:37,702 ***** Running evaluation *****
2022-06-12 00:57:37,702   Epoch = 13 iter 1559 step
2022-06-12 00:57:37,702   Num examples = 408
2022-06-12 00:57:37,702   Batch size = 32
2022-06-12 00:57:37,704 ***** Eval results *****
2022-06-12 00:57:37,704   att_loss = 4.142207749478229
2022-06-12 00:57:37,704   global_step = 1559
2022-06-12 00:57:37,704   loss = 5.482882877448937
2022-06-12 00:57:37,704   rep_loss = 1.3406751372597434
2022-06-12 00:57:37,704 ***** Save model *****
2022-06-12 00:57:43,360 ***** Running evaluation *****
2022-06-12 00:57:43,361   Epoch = 13 iter 1579 step
2022-06-12 00:57:43,361   Num examples = 408
2022-06-12 00:57:43,361   Batch size = 32
2022-06-12 00:57:43,362 ***** Eval results *****
2022-06-12 00:57:43,362   att_loss = 4.140174251241782
2022-06-12 00:57:43,362   global_step = 1579
2022-06-12 00:57:43,362   loss = 5.479008310848904
2022-06-12 00:57:43,362   rep_loss = 1.3388340706677782
2022-06-12 00:57:43,362 ***** Save model *****
2022-06-12 00:57:49,064 ***** Running evaluation *****
2022-06-12 00:57:49,065   Epoch = 14 iter 1599 step
2022-06-12 00:57:49,065   Num examples = 408
2022-06-12 00:57:49,065   Batch size = 32
2022-06-12 00:57:49,066 ***** Eval results *****
2022-06-12 00:57:49,066   att_loss = 4.153934637705485
2022-06-12 00:57:49,066   global_step = 1599
2022-06-12 00:57:49,066   loss = 5.497312227884929
2022-06-12 00:57:49,066   rep_loss = 1.3433774312337239
2022-06-12 00:57:49,066 ***** Save model *****
2022-06-12 00:57:54,782 ***** Running evaluation *****
2022-06-12 00:57:54,782   Epoch = 14 iter 1619 step
2022-06-12 00:57:54,782   Num examples = 408
2022-06-12 00:57:54,782   Batch size = 32
2022-06-12 00:57:54,783 ***** Eval results *****
2022-06-12 00:57:54,783   att_loss = 4.10853678247203
2022-06-12 00:57:54,783   global_step = 1619
2022-06-12 00:57:54,784   loss = 5.442298308662746
2022-06-12 00:57:54,784   rep_loss = 1.3337615261907163
2022-06-12 00:57:54,784 ***** Save model *****
2022-06-12 00:58:00,512 ***** Running evaluation *****
2022-06-12 00:58:00,513   Epoch = 14 iter 1639 step
2022-06-12 00:58:00,513   Num examples = 408
2022-06-12 00:58:00,513   Batch size = 32
2022-06-12 00:58:00,515 ***** Eval results *****
2022-06-12 00:58:00,515   att_loss = 4.084116835926855
2022-06-12 00:58:00,515   global_step = 1639
2022-06-12 00:58:00,515   loss = 5.414199330085932
2022-06-12 00:58:00,515   rep_loss = 1.3300824830698412
2022-06-12 00:58:00,515 ***** Save model *****
2022-06-12 00:58:06,314 ***** Running evaluation *****
2022-06-12 00:58:06,314   Epoch = 14 iter 1659 step
2022-06-12 00:58:06,314   Num examples = 408
2022-06-12 00:58:06,314   Batch size = 32
2022-06-12 00:58:06,315 ***** Eval results *****
2022-06-12 00:58:06,315   att_loss = 4.068539290201096
2022-06-12 00:58:06,315   global_step = 1659
2022-06-12 00:58:06,316   loss = 5.397346292223249
2022-06-12 00:58:06,316   rep_loss = 1.3288069755311995
2022-06-12 00:58:06,316 ***** Save model *****
2022-06-12 00:58:12,000 ***** Running evaluation *****
2022-06-12 00:58:12,001   Epoch = 14 iter 1679 step
2022-06-12 00:58:12,001   Num examples = 408
2022-06-12 00:58:12,001   Batch size = 32
2022-06-12 00:58:12,002 ***** Eval results *****
2022-06-12 00:58:12,002   att_loss = 4.062413603426462
2022-06-12 00:58:12,002   global_step = 1679
2022-06-12 00:58:12,002   loss = 5.389346651284091
2022-06-12 00:58:12,002   rep_loss = 1.3269330378038338
2022-06-12 00:58:12,002 ***** Save model *****
2022-06-12 00:58:12,097 ***** Running evaluation *****
2022-06-12 00:58:12,097   Epoch = 0 iter 1499 step
2022-06-12 00:58:12,097   Num examples = 5463
2022-06-12 00:58:12,097   Batch size = 32
2022-06-12 00:58:12,099 ***** Eval results *****
2022-06-12 00:58:12,099   att_loss = 6.3319931583773545
2022-06-12 00:58:12,099   global_step = 1499
2022-06-12 00:58:12,099   loss = 7.6122932411814785
2022-06-12 00:58:12,099   rep_loss = 1.2803000765215802
2022-06-12 00:58:12,099 ***** Save model *****
2022-06-12 00:58:17,734 ***** Running evaluation *****
2022-06-12 00:58:17,735   Epoch = 14 iter 1699 step
2022-06-12 00:58:17,735   Num examples = 408
2022-06-12 00:58:17,735   Batch size = 32
2022-06-12 00:58:17,736 ***** Eval results *****
2022-06-12 00:58:17,736   att_loss = 4.0631872751180405
2022-06-12 00:58:17,736   global_step = 1699
2022-06-12 00:58:17,736   loss = 5.389643854307897
2022-06-12 00:58:17,736   rep_loss = 1.3264565699308821
2022-06-12 00:58:17,736 ***** Save model *****
2022-06-12 00:58:23,417 ***** Running evaluation *****
2022-06-12 00:58:23,417   Epoch = 15 iter 1719 step
2022-06-12 00:58:23,417   Num examples = 408
2022-06-12 00:58:23,417   Batch size = 32
2022-06-12 00:58:23,418 ***** Eval results *****
2022-06-12 00:58:23,418   att_loss = 4.09376843770345
2022-06-12 00:58:23,418   global_step = 1719
2022-06-12 00:58:23,418   loss = 5.421205520629883
2022-06-12 00:58:23,419   rep_loss = 1.3274370034535725
2022-06-12 00:58:23,419 ***** Save model *****
2022-06-12 00:58:29,103 ***** Running evaluation *****
2022-06-12 00:58:29,104   Epoch = 15 iter 1739 step
2022-06-12 00:58:29,104   Num examples = 408
2022-06-12 00:58:29,104   Batch size = 32
2022-06-12 00:58:29,105 ***** Eval results *****
2022-06-12 00:58:29,105   att_loss = 4.071877989275702
2022-06-12 00:58:29,105   global_step = 1739
2022-06-12 00:58:29,105   loss = 5.398218138464566
2022-06-12 00:58:29,105   rep_loss = 1.3263401450781986
2022-06-12 00:58:29,105 ***** Save model *****
2022-06-12 00:58:34,857 ***** Running evaluation *****
2022-06-12 00:58:34,858   Epoch = 15 iter 1759 step
2022-06-12 00:58:34,858   Num examples = 408
2022-06-12 00:58:34,858   Batch size = 32
2022-06-12 00:58:34,859 ***** Eval results *****
2022-06-12 00:58:34,859   att_loss = 4.019497730294052
2022-06-12 00:58:34,859   global_step = 1759
2022-06-12 00:58:34,860   loss = 5.340029035295759
2022-06-12 00:58:34,860   rep_loss = 1.32053128067328
2022-06-12 00:58:34,860 ***** Save model *****
2022-06-12 00:58:40,594 ***** Running evaluation *****
2022-06-12 00:58:40,594   Epoch = 15 iter 1779 step
2022-06-12 00:58:40,595   Num examples = 408
2022-06-12 00:58:40,595   Batch size = 32
2022-06-12 00:58:40,596 ***** Eval results *****
2022-06-12 00:58:40,596   att_loss = 3.99968445473823
2022-06-12 00:58:40,596   global_step = 1779
2022-06-12 00:58:40,596   loss = 5.317393848861474
2022-06-12 00:58:40,596   rep_loss = 1.3177093785742056
2022-06-12 00:58:40,596 ***** Save model *****
2022-06-12 00:58:46,345 ***** Running evaluation *****
2022-06-12 00:58:46,345   Epoch = 15 iter 1799 step
2022-06-12 00:58:46,345   Num examples = 408
2022-06-12 00:58:46,345   Batch size = 32
2022-06-12 00:58:46,346 ***** Eval results *****
2022-06-12 00:58:46,346   att_loss = 3.9900122117460444
2022-06-12 00:58:46,346   global_step = 1799
2022-06-12 00:58:46,346   loss = 5.3056030702055175
2022-06-12 00:58:46,346   rep_loss = 1.3155908584594727
2022-06-12 00:58:46,347 ***** Save model *****
2022-06-12 00:58:52,071 ***** Running evaluation *****
2022-06-12 00:58:52,072   Epoch = 15 iter 1819 step
2022-06-12 00:58:52,072   Num examples = 408
2022-06-12 00:58:52,072   Batch size = 32
2022-06-12 00:58:52,073 ***** Eval results *****
2022-06-12 00:58:52,073   att_loss = 3.9846803332687517
2022-06-12 00:58:52,073   global_step = 1819
2022-06-12 00:58:52,073   loss = 5.299555726007584
2022-06-12 00:58:52,073   rep_loss = 1.3148753894578427
2022-06-12 00:58:52,074 ***** Save model *****
2022-06-12 00:58:57,793 ***** Running evaluation *****
2022-06-12 00:58:57,794   Epoch = 16 iter 1839 step
2022-06-12 00:58:57,794   Num examples = 408
2022-06-12 00:58:57,794   Batch size = 32
2022-06-12 00:58:57,795 ***** Eval results *****
2022-06-12 00:58:57,796   att_loss = 3.990256595611572
2022-06-12 00:58:57,796   global_step = 1839
2022-06-12 00:58:57,796   loss = 5.300548998514811
2022-06-12 00:58:57,796   rep_loss = 1.310292371114095
2022-06-12 00:58:57,796 ***** Save model *****
2022-06-12 00:59:03,459 ***** Running evaluation *****
2022-06-12 00:59:03,460   Epoch = 16 iter 1859 step
2022-06-12 00:59:03,460   Num examples = 408
2022-06-12 00:59:03,460   Batch size = 32
2022-06-12 00:59:03,461 ***** Eval results *****
2022-06-12 00:59:03,461   att_loss = 4.032992594582694
2022-06-12 00:59:03,461   global_step = 1859
2022-06-12 00:59:03,461   loss = 5.347198758806501
2022-06-12 00:59:03,461   rep_loss = 1.3142061574118478
2022-06-12 00:59:03,461 ***** Save model *****
2022-06-12 00:59:09,180 ***** Running evaluation *****
2022-06-12 00:59:09,180   Epoch = 16 iter 1879 step
2022-06-12 00:59:09,180   Num examples = 408
2022-06-12 00:59:09,180   Batch size = 32
2022-06-12 00:59:09,181 ***** Eval results *****
2022-06-12 00:59:09,181   att_loss = 3.9990130337801846
2022-06-12 00:59:09,181   global_step = 1879
2022-06-12 00:59:09,182   loss = 5.309831844676625
2022-06-12 00:59:09,182   rep_loss = 1.3108188087289983
2022-06-12 00:59:09,182 ***** Save model *****
2022-06-12 00:59:14,907 ***** Running evaluation *****
2022-06-12 00:59:14,908   Epoch = 16 iter 1899 step
2022-06-12 00:59:14,908   Num examples = 408
2022-06-12 00:59:14,908   Batch size = 32
2022-06-12 00:59:14,909 ***** Eval results *****
2022-06-12 00:59:14,909   att_loss = 3.9815014680226644
2022-06-12 00:59:14,909   global_step = 1899
2022-06-12 00:59:14,909   loss = 5.2909643300374345
2022-06-12 00:59:14,909   rep_loss = 1.3094628651936848
2022-06-12 00:59:14,909 ***** Save model *****
2022-06-12 00:59:20,678 ***** Running evaluation *****
2022-06-12 00:59:20,678   Epoch = 16 iter 1919 step
2022-06-12 00:59:20,678   Num examples = 408
2022-06-12 00:59:20,678   Batch size = 32
2022-06-12 00:59:20,679 ***** Eval results *****
2022-06-12 00:59:20,679   att_loss = 3.959272773642289
2022-06-12 00:59:20,680   global_step = 1919
2022-06-12 00:59:20,680   loss = 5.266364940844084
2022-06-12 00:59:20,680   rep_loss = 1.3070921584179527
2022-06-12 00:59:20,680 ***** Save model *****
2022-06-12 00:59:26,390 ***** Running evaluation *****
2022-06-12 00:59:26,391   Epoch = 17 iter 1939 step
2022-06-12 00:59:26,391   Num examples = 408
2022-06-12 00:59:26,391   Batch size = 32
2022-06-12 00:59:26,392 ***** Eval results *****
2022-06-12 00:59:26,392   att_loss = 3.8399574756622314
2022-06-12 00:59:26,392   global_step = 1939
2022-06-12 00:59:26,392   loss = 5.131036758422852
2022-06-12 00:59:26,392   rep_loss = 1.2910795211791992
2022-06-12 00:59:26,392 ***** Save model *****
2022-06-12 00:59:32,105 ***** Running evaluation *****
2022-06-12 00:59:32,106   Epoch = 17 iter 1959 step
2022-06-12 00:59:32,106   Num examples = 408
2022-06-12 00:59:32,106   Batch size = 32
2022-06-12 00:59:32,107 ***** Eval results *****
2022-06-12 00:59:32,107   att_loss = 3.852477959224156
2022-06-12 00:59:32,107   global_step = 1959
2022-06-12 00:59:32,107   loss = 5.14780314763387
2022-06-12 00:59:32,107   rep_loss = 1.2953252338227772
2022-06-12 00:59:32,107 ***** Save model *****
2022-06-12 00:59:37,846 ***** Running evaluation *****
2022-06-12 00:59:37,847   Epoch = 17 iter 1979 step
2022-06-12 00:59:37,847   Num examples = 408
2022-06-12 00:59:37,847   Batch size = 32
2022-06-12 00:59:37,848 ***** Eval results *****
2022-06-12 00:59:37,849   att_loss = 3.841495862821253
2022-06-12 00:59:37,849   global_step = 1979
2022-06-12 00:59:37,849   loss = 5.136174004252364
2022-06-12 00:59:37,849   rep_loss = 1.2946781530612852
2022-06-12 00:59:37,849 ***** Save model *****
2022-06-12 00:59:43,578 ***** Running evaluation *****
2022-06-12 00:59:43,578   Epoch = 17 iter 1999 step
2022-06-12 00:59:43,579   Num examples = 408
2022-06-12 00:59:43,579   Batch size = 32
2022-06-12 00:59:43,580 ***** Eval results *****
2022-06-12 00:59:43,580   att_loss = 3.8627364752722566
2022-06-12 00:59:43,580   global_step = 1999
2022-06-12 00:59:43,580   loss = 5.1577650836256685
2022-06-12 00:59:43,580   rep_loss = 1.2950286122619128
2022-06-12 00:59:43,580 ***** Save model *****
2022-06-12 00:59:49,298 ***** Running evaluation *****
2022-06-12 00:59:49,298   Epoch = 17 iter 2019 step
2022-06-12 00:59:49,298   Num examples = 408
2022-06-12 00:59:49,298   Batch size = 32
2022-06-12 00:59:49,299 ***** Eval results *****
2022-06-12 00:59:49,300   att_loss = 3.869360617649408
2022-06-12 00:59:49,300   global_step = 2019
2022-06-12 00:59:49,300   loss = 5.164988176322278
2022-06-12 00:59:49,300   rep_loss = 1.2956275586728696
2022-06-12 00:59:49,300 ***** Save model *****
2022-06-12 00:59:55,034 ***** Running evaluation *****
2022-06-12 00:59:55,034   Epoch = 17 iter 2039 step
2022-06-12 00:59:55,034   Num examples = 408
2022-06-12 00:59:55,035   Batch size = 32
2022-06-12 00:59:55,036 ***** Eval results *****
2022-06-12 00:59:55,036   att_loss = 3.882550371755468
2022-06-12 00:59:55,036   global_step = 2039
2022-06-12 00:59:55,036   loss = 5.178589953054296
2022-06-12 00:59:55,036   rep_loss = 1.2960395812988281
2022-06-12 00:59:55,036 ***** Save model *****
2022-06-12 01:00:00,789 ***** Running evaluation *****
2022-06-12 01:00:00,789   Epoch = 18 iter 2059 step
2022-06-12 01:00:00,789   Num examples = 408
2022-06-12 01:00:00,789   Batch size = 32
2022-06-12 01:00:00,791 ***** Eval results *****
2022-06-12 01:00:00,791   att_loss = 3.857560430254255
2022-06-12 01:00:00,791   global_step = 2059
2022-06-12 01:00:00,791   loss = 5.153288296290806
2022-06-12 01:00:00,791   rep_loss = 1.2957278490066528
2022-06-12 01:00:00,791 ***** Save model *****
2022-06-12 01:00:06,544 ***** Running evaluation *****
2022-06-12 01:00:06,545   Epoch = 18 iter 2079 step
2022-06-12 01:00:06,545   Num examples = 408
2022-06-12 01:00:06,545   Batch size = 32
2022-06-12 01:00:06,546 ***** Eval results *****
2022-06-12 01:00:06,546   att_loss = 3.865198497419004
2022-06-12 01:00:06,546   global_step = 2079
2022-06-12 01:00:06,546   loss = 5.156658896693477
2022-06-12 01:00:06,546   rep_loss = 1.2914603507077251
2022-06-12 01:00:06,546 ***** Save model *****
2022-06-12 01:00:12,321 ***** Running evaluation *****
2022-06-12 01:00:12,322   Epoch = 18 iter 2099 step
2022-06-12 01:00:12,322   Num examples = 408
2022-06-12 01:00:12,322   Batch size = 32
2022-06-12 01:00:12,323 ***** Eval results *****
2022-06-12 01:00:12,323   att_loss = 3.8304561097571193
2022-06-12 01:00:12,323   global_step = 2099
2022-06-12 01:00:12,323   loss = 5.117877706568292
2022-06-12 01:00:12,323   rep_loss = 1.2874215714474941
2022-06-12 01:00:12,323 ***** Save model *****
2022-06-12 01:00:18,071 ***** Running evaluation *****
2022-06-12 01:00:18,071   Epoch = 18 iter 2119 step
2022-06-12 01:00:18,071   Num examples = 408
2022-06-12 01:00:18,071   Batch size = 32
2022-06-12 01:00:18,072 ***** Eval results *****
2022-06-12 01:00:18,072   att_loss = 3.8340805181816444
2022-06-12 01:00:18,072   global_step = 2119
2022-06-12 01:00:18,072   loss = 5.121321407716666
2022-06-12 01:00:18,073   rep_loss = 1.2872408735218333
2022-06-12 01:00:18,073 ***** Save model *****
2022-06-12 01:00:19,950 ***** Running evaluation *****
2022-06-12 01:00:19,950   Epoch = 0 iter 1999 step
2022-06-12 01:00:19,950   Num examples = 5463
2022-06-12 01:00:19,950   Batch size = 32
2022-06-12 01:00:19,951 ***** Eval results *****
2022-06-12 01:00:19,951   att_loss = 6.097926303229968
2022-06-12 01:00:19,951   global_step = 1999
2022-06-12 01:00:19,952   loss = 7.340630223597211
2022-06-12 01:00:19,952   rep_loss = 1.2427039157753887
2022-06-12 01:00:19,952 ***** Save model *****
2022-06-12 01:00:23,782 ***** Running evaluation *****
2022-06-12 01:00:23,782   Epoch = 18 iter 2139 step
2022-06-12 01:00:23,782   Num examples = 408
2022-06-12 01:00:23,782   Batch size = 32
2022-06-12 01:00:23,783 ***** Eval results *****
2022-06-12 01:00:23,784   att_loss = 3.837886807562291
2022-06-12 01:00:23,784   global_step = 2139
2022-06-12 01:00:23,784   loss = 5.125077258581403
2022-06-12 01:00:23,784   rep_loss = 1.2871904359466728
2022-06-12 01:00:23,784 ***** Save model *****
2022-06-12 01:00:29,499 ***** Running evaluation *****
2022-06-12 01:00:29,500   Epoch = 18 iter 2159 step
2022-06-12 01:00:29,500   Num examples = 408
2022-06-12 01:00:29,500   Batch size = 32
2022-06-12 01:00:29,501 ***** Eval results *****
2022-06-12 01:00:29,501   att_loss = 3.8324958832464486
2022-06-12 01:00:29,501   global_step = 2159
2022-06-12 01:00:29,501   loss = 5.118294247957033
2022-06-12 01:00:29,501   rep_loss = 1.285798351341319
2022-06-12 01:00:29,501 ***** Save model *****
2022-06-12 01:00:35,288 ***** Running evaluation *****
2022-06-12 01:00:35,289   Epoch = 19 iter 2179 step
2022-06-12 01:00:35,289   Num examples = 408
2022-06-12 01:00:35,289   Batch size = 32
2022-06-12 01:00:35,290 ***** Eval results *****
2022-06-12 01:00:35,290   att_loss = 3.7738612431746263
2022-06-12 01:00:35,290   global_step = 2179
2022-06-12 01:00:35,290   loss = 5.053202702448918
2022-06-12 01:00:35,290   rep_loss = 1.279341431764456
2022-06-12 01:00:35,291 ***** Save model *****
2022-06-12 01:00:41,053 ***** Running evaluation *****
2022-06-12 01:00:41,053   Epoch = 19 iter 2199 step
2022-06-12 01:00:41,053   Num examples = 408
2022-06-12 01:00:41,053   Batch size = 32
2022-06-12 01:00:41,054 ***** Eval results *****
2022-06-12 01:00:41,055   att_loss = 3.8159171306725703
2022-06-12 01:00:41,055   global_step = 2199
2022-06-12 01:00:41,055   loss = 5.092722155831077
2022-06-12 01:00:41,055   rep_loss = 1.2768049962592847
2022-06-12 01:00:41,055 ***** Save model *****
2022-06-12 01:00:46,779 ***** Running evaluation *****
2022-06-12 01:00:46,780   Epoch = 19 iter 2219 step
2022-06-12 01:00:46,780   Num examples = 408
2022-06-12 01:00:46,780   Batch size = 32
2022-06-12 01:00:46,781 ***** Eval results *****
2022-06-12 01:00:46,781   att_loss = 3.8023630178199626
2022-06-12 01:00:46,781   global_step = 2219
2022-06-12 01:00:46,781   loss = 5.078773219630404
2022-06-12 01:00:46,781   rep_loss = 1.276410183816586
2022-06-12 01:00:46,782 ***** Save model *****
2022-06-12 01:00:52,528 ***** Running evaluation *****
2022-06-12 01:00:52,528   Epoch = 19 iter 2239 step
2022-06-12 01:00:52,528   Num examples = 408
2022-06-12 01:00:52,528   Batch size = 32
2022-06-12 01:00:52,529 ***** Eval results *****
2022-06-12 01:00:52,529   att_loss = 3.7946032040739714
2022-06-12 01:00:52,529   global_step = 2239
2022-06-12 01:00:52,530   loss = 5.070784666766859
2022-06-12 01:00:52,530   rep_loss = 1.2761814496288562
2022-06-12 01:00:52,530 ***** Save model *****
2022-06-12 01:00:58,239 ***** Running evaluation *****
2022-06-12 01:00:58,240   Epoch = 19 iter 2259 step
2022-06-12 01:00:58,240   Num examples = 408
2022-06-12 01:00:58,240   Batch size = 32
2022-06-12 01:00:58,241 ***** Eval results *****
2022-06-12 01:00:58,242   att_loss = 3.7937954215593237
2022-06-12 01:00:58,242   global_step = 2259
2022-06-12 01:00:58,242   loss = 5.070049865271455
2022-06-12 01:00:58,242   rep_loss = 1.2762544257666475
2022-06-12 01:00:58,242 ***** Save model *****
2022-06-12 01:01:03,949 ***** Running evaluation *****
2022-06-12 01:01:03,949   Epoch = 19 iter 2279 step
2022-06-12 01:01:03,949   Num examples = 408
2022-06-12 01:01:03,949   Batch size = 32
2022-06-12 01:01:03,950 ***** Eval results *****
2022-06-12 01:01:03,951   att_loss = 3.8014784935301384
2022-06-12 01:01:03,951   global_step = 2279
2022-06-12 01:01:03,951   loss = 5.079270388172791
2022-06-12 01:01:03,951   rep_loss = 1.277791884093158
2022-06-12 01:01:03,951 ***** Save model *****
2022-06-12 01:01:04,672 Task finish! 
2022-06-12 01:01:04,673 Task cost 11.164919283333335 minutes, i.e. 0.1860819913888889 hours. 
2022-06-12 01:01:06,797 Task start! 
2022-06-12 01:01:06,817 device: cuda n_gpu: 1
2022-06-12 01:01:06,817 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=15, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/mrpc/on_original_data', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/mrpc/on_original_data', task_name='mrpc', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/mrpc/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 01:01:06,860 Writing example 0 of 3668
2022-06-12 01:01:06,861 *** Example ***
2022-06-12 01:01:06,861 guid: train-1
2022-06-12 01:01:06,861 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-06-12 01:01:06,861 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:01:06,861 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:01:06,861 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:01:06,861 label: 1
2022-06-12 01:01:06,861 label_id: 1
2022-06-12 01:01:09,816 Writing example 0 of 408
2022-06-12 01:01:09,817 *** Example ***
2022-06-12 01:01:09,817 guid: dev-1
2022-06-12 01:01:09,817 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-06-12 01:01:09,817 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:01:09,817 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:01:09,817 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:01:09,817 label: 1
2022-06-12 01:01:09,817 label_id: 1
2022-06-12 01:01:10,154 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 01:01:15,892 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mrpc/on_original_data/pytorch_model.bin
2022-06-12 01:01:16,660 loading model...
2022-06-12 01:01:16,841 done!
2022-06-12 01:01:21,182 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 01:01:22,294 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/mrpc/on_original_data/pytorch_model.bin
2022-06-12 01:01:22,477 loading model...
2022-06-12 01:01:22,509 done!
2022-06-12 01:01:24,049 ***** Running training *****
2022-06-12 01:01:24,065   Num examples = 3668
2022-06-12 01:01:24,070   Batch size = 32
2022-06-12 01:01:24,070   Num steps = 1710
2022-06-12 01:01:24,071 n: bert.embeddings.word_embeddings.weight
2022-06-12 01:01:24,086 n: bert.embeddings.position_embeddings.weight
2022-06-12 01:01:24,102 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 01:01:24,118 n: bert.embeddings.LayerNorm.weight
2022-06-12 01:01:24,118 n: bert.embeddings.LayerNorm.bias
2022-06-12 01:01:24,118 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 01:01:24,119 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 01:01:24,119 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 01:01:24,120 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 01:01:24,121 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 01:01:24,122 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 01:01:24,122 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 01:01:24,123 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 01:01:24,123 n: bert.pooler.dense.weight
2022-06-12 01:01:24,123 n: bert.pooler.dense.bias
2022-06-12 01:01:24,123 n: classifier.weight
2022-06-12 01:01:24,123 n: classifier.bias
2022-06-12 01:01:24,123 n: fit_denses.0.weight
2022-06-12 01:01:24,123 n: fit_denses.0.bias
2022-06-12 01:01:24,123 n: fit_denses.1.weight
2022-06-12 01:01:24,123 n: fit_denses.1.bias
2022-06-12 01:01:24,123 n: fit_denses.2.weight
2022-06-12 01:01:24,123 n: fit_denses.2.bias
2022-06-12 01:01:24,123 n: fit_denses.3.weight
2022-06-12 01:01:24,123 n: fit_denses.3.bias
2022-06-12 01:01:24,124 n: fit_denses.4.weight
2022-06-12 01:01:24,124 n: fit_denses.4.bias
2022-06-12 01:01:24,124 n: fit_denses.5.weight
2022-06-12 01:01:24,124 n: fit_denses.5.bias
2022-06-12 01:01:24,124 n: fit_denses.6.weight
2022-06-12 01:01:24,124 n: fit_denses.6.bias
2022-06-12 01:01:24,124 Total parameters: 72468738
2022-06-12 01:01:28,852 ***** Running evaluation *****
2022-06-12 01:01:28,852   Epoch = 0 iter 19 step
2022-06-12 01:01:28,852   Num examples = 408
2022-06-12 01:01:28,852   Batch size = 32
2022-06-12 01:01:29,206 ***** Eval results *****
2022-06-12 01:01:29,206   acc = 0.7647058823529411
2022-06-12 01:01:29,207   acc_and_f1 = 0.8082788671023964
2022-06-12 01:01:29,207   cls_loss = 0.3153329444558997
2022-06-12 01:01:29,207   eval_loss = 0.5640781636421497
2022-06-12 01:01:29,207   f1 = 0.8518518518518517
2022-06-12 01:01:29,207   global_step = 19
2022-06-12 01:01:29,207   loss = 0.3153329444558997
2022-06-12 01:01:29,207 ***** Save model *****
2022-06-12 01:01:34,568 ***** Running evaluation *****
2022-06-12 01:01:34,569   Epoch = 0 iter 39 step
2022-06-12 01:01:34,569   Num examples = 408
2022-06-12 01:01:34,569   Batch size = 32
2022-06-12 01:01:34,924 ***** Eval results *****
2022-06-12 01:01:34,924   acc = 0.7867647058823529
2022-06-12 01:01:34,924   acc_and_f1 = 0.8240043625105544
2022-06-12 01:01:34,924   cls_loss = 0.25841902081783
2022-06-12 01:01:34,924   eval_loss = 0.5586392948260674
2022-06-12 01:01:34,924   f1 = 0.861244019138756
2022-06-12 01:01:34,925   global_step = 39
2022-06-12 01:01:34,925   loss = 0.25841902081783
2022-06-12 01:01:34,925 ***** Save model *****
2022-06-12 01:01:40,394 ***** Running evaluation *****
2022-06-12 01:01:40,395   Epoch = 0 iter 59 step
2022-06-12 01:01:40,395   Num examples = 408
2022-06-12 01:01:40,395   Batch size = 32
2022-06-12 01:01:40,748 ***** Eval results *****
2022-06-12 01:01:40,749   acc = 0.7696078431372549
2022-06-12 01:01:40,749   acc_and_f1 = 0.8069893520322036
2022-06-12 01:01:40,749   cls_loss = 0.24802513117507352
2022-06-12 01:01:40,749   eval_loss = 0.5123857190975776
2022-06-12 01:01:40,749   f1 = 0.8443708609271523
2022-06-12 01:01:40,749   global_step = 59
2022-06-12 01:01:40,749   loss = 0.24802513117507352
2022-06-12 01:01:45,706 ***** Running evaluation *****
2022-06-12 01:01:45,706   Epoch = 0 iter 79 step
2022-06-12 01:01:45,706   Num examples = 408
2022-06-12 01:01:45,706   Batch size = 32
2022-06-12 01:01:46,060 ***** Eval results *****
2022-06-12 01:01:46,060   acc = 0.6862745098039216
2022-06-12 01:01:46,060   acc_and_f1 = 0.7498427942605613
2022-06-12 01:01:46,060   cls_loss = 0.238632460749602
2022-06-12 01:01:46,060   eval_loss = 0.8313940029877883
2022-06-12 01:01:46,060   f1 = 0.8134110787172011
2022-06-12 01:01:46,060   global_step = 79
2022-06-12 01:01:46,060   loss = 0.238632460749602
2022-06-12 01:01:51,001 ***** Running evaluation *****
2022-06-12 01:01:51,001   Epoch = 0 iter 99 step
2022-06-12 01:01:51,001   Num examples = 408
2022-06-12 01:01:51,002   Batch size = 32
2022-06-12 01:01:51,353 ***** Eval results *****
2022-06-12 01:01:51,353   acc = 0.803921568627451
2022-06-12 01:01:51,353   acc_and_f1 = 0.8292335115864528
2022-06-12 01:01:51,353   cls_loss = 0.2424907956761543
2022-06-12 01:01:51,353   eval_loss = 0.46248621436265797
2022-06-12 01:01:51,353   f1 = 0.8545454545454545
2022-06-12 01:01:51,353   global_step = 99
2022-06-12 01:01:51,354   loss = 0.2424907956761543
2022-06-12 01:01:56,289 ***** Running evaluation *****
2022-06-12 01:01:56,290   Epoch = 1 iter 119 step
2022-06-12 01:01:56,290   Num examples = 408
2022-06-12 01:01:56,290   Batch size = 32
2022-06-12 01:01:56,641 ***** Eval results *****
2022-06-12 01:01:56,641   acc = 0.7843137254901961
2022-06-12 01:01:56,641   acc_and_f1 = 0.8234068627450981
2022-06-12 01:01:56,641   cls_loss = 0.16842926442623138
2022-06-12 01:01:56,641   eval_loss = 0.563939807506708
2022-06-12 01:01:56,641   f1 = 0.8625
2022-06-12 01:01:56,641   global_step = 119
2022-06-12 01:01:56,641   loss = 0.16842926442623138
2022-06-12 01:01:56,641 ***** Save model *****
2022-06-12 01:02:02,038 ***** Running evaluation *****
2022-06-12 01:02:02,038   Epoch = 1 iter 139 step
2022-06-12 01:02:02,038   Num examples = 408
2022-06-12 01:02:02,038   Batch size = 32
2022-06-12 01:02:02,390 ***** Eval results *****
2022-06-12 01:02:02,391   acc = 0.8137254901960784
2022-06-12 01:02:02,391   acc_and_f1 = 0.8394868585732165
2022-06-12 01:02:02,391   cls_loss = 0.16264549046754836
2022-06-12 01:02:02,391   eval_loss = 0.4322377145290375
2022-06-12 01:02:02,391   f1 = 0.8652482269503546
2022-06-12 01:02:02,391   global_step = 139
2022-06-12 01:02:02,391   loss = 0.16264549046754836
2022-06-12 01:02:02,391 ***** Save model *****
2022-06-12 01:02:07,811 ***** Running evaluation *****
2022-06-12 01:02:07,812   Epoch = 1 iter 159 step
2022-06-12 01:02:07,812   Num examples = 408
2022-06-12 01:02:07,812   Batch size = 32
2022-06-12 01:02:08,164 ***** Eval results *****
2022-06-12 01:02:08,164   acc = 0.7867647058823529
2022-06-12 01:02:08,164   acc_and_f1 = 0.8163912024986986
2022-06-12 01:02:08,164   cls_loss = 0.1552783994211091
2022-06-12 01:02:08,164   eval_loss = 0.46486541628837585
2022-06-12 01:02:08,164   f1 = 0.8460176991150442
2022-06-12 01:02:08,164   global_step = 159
2022-06-12 01:02:08,164   loss = 0.1552783994211091
2022-06-12 01:02:13,081 ***** Running evaluation *****
2022-06-12 01:02:13,081   Epoch = 1 iter 179 step
2022-06-12 01:02:13,081   Num examples = 408
2022-06-12 01:02:13,082   Batch size = 32
2022-06-12 01:02:13,434 ***** Eval results *****
2022-06-12 01:02:13,434   acc = 0.7867647058823529
2022-06-12 01:02:13,434   acc_and_f1 = 0.8253072355702845
2022-06-12 01:02:13,434   cls_loss = 0.1557508688706618
2022-06-12 01:02:13,434   eval_loss = 0.5144029431618177
2022-06-12 01:02:13,435   f1 = 0.8638497652582161
2022-06-12 01:02:13,435   global_step = 179
2022-06-12 01:02:13,435   loss = 0.1557508688706618
2022-06-12 01:02:18,333 ***** Running evaluation *****
2022-06-12 01:02:18,334   Epoch = 1 iter 199 step
2022-06-12 01:02:18,334   Num examples = 408
2022-06-12 01:02:18,334   Batch size = 32
2022-06-12 01:02:18,687 ***** Eval results *****
2022-06-12 01:02:18,687   acc = 0.7867647058823529
2022-06-12 01:02:18,687   acc_and_f1 = 0.8169324408146388
2022-06-12 01:02:18,687   cls_loss = 0.1623713896555059
2022-06-12 01:02:18,687   eval_loss = 0.45334081237132734
2022-06-12 01:02:18,687   f1 = 0.8471001757469245
2022-06-12 01:02:18,687   global_step = 199
2022-06-12 01:02:18,687   loss = 0.1623713896555059
2022-06-12 01:02:23,590 ***** Running evaluation *****
2022-06-12 01:02:23,590   Epoch = 1 iter 219 step
2022-06-12 01:02:23,590   Num examples = 408
2022-06-12 01:02:23,591   Batch size = 32
2022-06-12 01:02:23,943 ***** Eval results *****
2022-06-12 01:02:23,943   acc = 0.7524509803921569
2022-06-12 01:02:23,943   acc_and_f1 = 0.7995942307120116
2022-06-12 01:02:23,943   cls_loss = 0.16769933218047733
2022-06-12 01:02:23,943   eval_loss = 0.6365659489081457
2022-06-12 01:02:23,943   f1 = 0.8467374810318664
2022-06-12 01:02:23,943   global_step = 219
2022-06-12 01:02:23,943   loss = 0.16769933218047733
2022-06-12 01:02:27,952 ***** Running evaluation *****
2022-06-12 01:02:27,953   Epoch = 0 iter 2499 step
2022-06-12 01:02:27,953   Num examples = 5463
2022-06-12 01:02:27,953   Batch size = 32
2022-06-12 01:02:27,954 ***** Eval results *****
2022-06-12 01:02:27,954   att_loss = 5.908821333690184
2022-06-12 01:02:27,955   global_step = 2499
2022-06-12 01:02:27,955   loss = 7.1223257291121405
2022-06-12 01:02:27,955   rep_loss = 1.213504392941411
2022-06-12 01:02:27,955 ***** Save model *****
2022-06-12 01:02:28,860 ***** Running evaluation *****
2022-06-12 01:02:28,860   Epoch = 2 iter 239 step
2022-06-12 01:02:28,860   Num examples = 408
2022-06-12 01:02:28,860   Batch size = 32
2022-06-12 01:02:29,212 ***** Eval results *****
2022-06-12 01:02:29,212   acc = 0.7916666666666666
2022-06-12 01:02:29,212   acc_and_f1 = 0.8236771363893605
2022-06-12 01:02:29,212   cls_loss = 0.12003191763704474
2022-06-12 01:02:29,212   eval_loss = 0.5887736964684266
2022-06-12 01:02:29,212   f1 = 0.8556876061120543
2022-06-12 01:02:29,213   global_step = 239
2022-06-12 01:02:29,213   loss = 0.12003191763704474
2022-06-12 01:02:34,175 ***** Running evaluation *****
2022-06-12 01:02:34,175   Epoch = 2 iter 259 step
2022-06-12 01:02:34,175   Num examples = 408
2022-06-12 01:02:34,175   Batch size = 32
2022-06-12 01:02:34,527 ***** Eval results *****
2022-06-12 01:02:34,527   acc = 0.7794117647058824
2022-06-12 01:02:34,527   acc_and_f1 = 0.8193933823529412
2022-06-12 01:02:34,527   cls_loss = 0.1260386791921431
2022-06-12 01:02:34,527   eval_loss = 0.627958228954902
2022-06-12 01:02:34,527   f1 = 0.8593750000000001
2022-06-12 01:02:34,527   global_step = 259
2022-06-12 01:02:34,528   loss = 0.1260386791921431
2022-06-12 01:02:39,423 ***** Running evaluation *****
2022-06-12 01:02:39,423   Epoch = 2 iter 279 step
2022-06-12 01:02:39,423   Num examples = 408
2022-06-12 01:02:39,423   Batch size = 32
2022-06-12 01:02:39,776 ***** Eval results *****
2022-06-12 01:02:39,776   acc = 0.7720588235294118
2022-06-12 01:02:39,776   acc_and_f1 = 0.8121025437201908
2022-06-12 01:02:39,776   cls_loss = 0.11791576138314079
2022-06-12 01:02:39,776   eval_loss = 0.6470031061997781
2022-06-12 01:02:39,776   f1 = 0.8521462639109697
2022-06-12 01:02:39,776   global_step = 279
2022-06-12 01:02:39,776   loss = 0.11791576138314079
2022-06-12 01:02:44,662 ***** Running evaluation *****
2022-06-12 01:02:44,663   Epoch = 2 iter 299 step
2022-06-12 01:02:44,663   Num examples = 408
2022-06-12 01:02:44,663   Batch size = 32
2022-06-12 01:02:45,016 ***** Eval results *****
2022-06-12 01:02:45,016   acc = 0.7818627450980392
2022-06-12 01:02:45,016   acc_and_f1 = 0.8197313725490196
2022-06-12 01:02:45,016   cls_loss = 0.12106506316594674
2022-06-12 01:02:45,016   eval_loss = 0.5517632411076472
2022-06-12 01:02:45,016   f1 = 0.8576
2022-06-12 01:02:45,016   global_step = 299
2022-06-12 01:02:45,016   loss = 0.12106506316594674
2022-06-12 01:02:49,904 ***** Running evaluation *****
2022-06-12 01:02:49,904   Epoch = 2 iter 319 step
2022-06-12 01:02:49,904   Num examples = 408
2022-06-12 01:02:49,904   Batch size = 32
2022-06-12 01:02:50,258 ***** Eval results *****
2022-06-12 01:02:50,258   acc = 0.7818627450980392
2022-06-12 01:02:50,258   acc_and_f1 = 0.8183375715702268
2022-06-12 01:02:50,258   cls_loss = 0.11777195187060388
2022-06-12 01:02:50,258   eval_loss = 0.6556388139724731
2022-06-12 01:02:50,258   f1 = 0.8548123980424144
2022-06-12 01:02:50,258   global_step = 319
2022-06-12 01:02:50,258   loss = 0.11777195187060388
2022-06-12 01:02:55,180 ***** Running evaluation *****
2022-06-12 01:02:55,181   Epoch = 2 iter 339 step
2022-06-12 01:02:55,181   Num examples = 408
2022-06-12 01:02:55,181   Batch size = 32
2022-06-12 01:02:55,534 ***** Eval results *****
2022-06-12 01:02:55,534   acc = 0.7720588235294118
2022-06-12 01:02:55,534   acc_and_f1 = 0.8089149839040093
2022-06-12 01:02:55,534   cls_loss = 0.11972126935247902
2022-06-12 01:02:55,534   eval_loss = 0.5453841479925009
2022-06-12 01:02:55,534   f1 = 0.8457711442786069
2022-06-12 01:02:55,534   global_step = 339
2022-06-12 01:02:55,534   loss = 0.11972126935247902
2022-06-12 01:03:00,465 ***** Running evaluation *****
2022-06-12 01:03:00,465   Epoch = 3 iter 359 step
2022-06-12 01:03:00,466   Num examples = 408
2022-06-12 01:03:00,466   Batch size = 32
2022-06-12 01:03:00,818 ***** Eval results *****
2022-06-12 01:03:00,819   acc = 0.7794117647058824
2022-06-12 01:03:00,819   acc_and_f1 = 0.8187279643718686
2022-06-12 01:03:00,819   cls_loss = 0.10426829930614023
2022-06-12 01:03:00,819   eval_loss = 0.6787884785578802
2022-06-12 01:03:00,819   f1 = 0.8580441640378549
2022-06-12 01:03:00,819   global_step = 359
2022-06-12 01:03:00,819   loss = 0.10426829930614023
2022-06-12 01:03:05,721 ***** Running evaluation *****
2022-06-12 01:03:05,722   Epoch = 3 iter 379 step
2022-06-12 01:03:05,722   Num examples = 408
2022-06-12 01:03:05,722   Batch size = 32
2022-06-12 01:03:06,074 ***** Eval results *****
2022-06-12 01:03:06,075   acc = 0.7867647058823529
2022-06-12 01:03:06,075   acc_and_f1 = 0.8202731092436975
2022-06-12 01:03:06,075   cls_loss = 0.1039743189876144
2022-06-12 01:03:06,075   eval_loss = 0.5599609911441803
2022-06-12 01:03:06,075   f1 = 0.8537815126050421
2022-06-12 01:03:06,075   global_step = 379
2022-06-12 01:03:06,075   loss = 0.1039743189876144
2022-06-12 01:03:11,002 ***** Running evaluation *****
2022-06-12 01:03:11,002   Epoch = 3 iter 399 step
2022-06-12 01:03:11,002   Num examples = 408
2022-06-12 01:03:11,002   Batch size = 32
2022-06-12 01:03:11,355 ***** Eval results *****
2022-06-12 01:03:11,355   acc = 0.7965686274509803
2022-06-12 01:03:11,355   acc_and_f1 = 0.8290021768306655
2022-06-12 01:03:11,355   cls_loss = 0.10096832783075801
2022-06-12 01:03:11,355   eval_loss = 0.5868021754118112
2022-06-12 01:03:11,355   f1 = 0.8614357262103506
2022-06-12 01:03:11,355   global_step = 399
2022-06-12 01:03:11,355   loss = 0.10096832783075801
2022-06-12 01:03:16,292 ***** Running evaluation *****
2022-06-12 01:03:16,292   Epoch = 3 iter 419 step
2022-06-12 01:03:16,292   Num examples = 408
2022-06-12 01:03:16,292   Batch size = 32
2022-06-12 01:03:16,645 ***** Eval results *****
2022-06-12 01:03:16,645   acc = 0.7867647058823529
2022-06-12 01:03:16,646   acc_and_f1 = 0.8163912024986986
2022-06-12 01:03:16,646   cls_loss = 0.10235081642092049
2022-06-12 01:03:16,646   eval_loss = 0.5305125782122979
2022-06-12 01:03:16,646   f1 = 0.8460176991150442
2022-06-12 01:03:16,646   global_step = 419
2022-06-12 01:03:16,646   loss = 0.10235081642092049
2022-06-12 01:03:21,580 ***** Running evaluation *****
2022-06-12 01:03:21,580   Epoch = 3 iter 439 step
2022-06-12 01:03:21,580   Num examples = 408
2022-06-12 01:03:21,580   Batch size = 32
2022-06-12 01:03:21,933 ***** Eval results *****
2022-06-12 01:03:21,933   acc = 0.7965686274509803
2022-06-12 01:03:21,933   acc_and_f1 = 0.8278259096507873
2022-06-12 01:03:21,933   cls_loss = 0.10330308566695635
2022-06-12 01:03:21,933   eval_loss = 0.54690021276474
2022-06-12 01:03:21,933   f1 = 0.8590831918505942
2022-06-12 01:03:21,934   global_step = 439
2022-06-12 01:03:21,934   loss = 0.10330308566695635
2022-06-12 01:03:26,889 ***** Running evaluation *****
2022-06-12 01:03:26,889   Epoch = 4 iter 459 step
2022-06-12 01:03:26,890   Num examples = 408
2022-06-12 01:03:26,890   Batch size = 32
2022-06-12 01:03:27,243 ***** Eval results *****
2022-06-12 01:03:27,243   acc = 0.821078431372549
2022-06-12 01:03:27,243   acc_and_f1 = 0.8513820033686084
2022-06-12 01:03:27,243   cls_loss = 0.09959179659684499
2022-06-12 01:03:27,243   eval_loss = 0.5704532701235551
2022-06-12 01:03:27,243   f1 = 0.8816855753646677
2022-06-12 01:03:27,243   global_step = 459
2022-06-12 01:03:27,243   loss = 0.09959179659684499
2022-06-12 01:03:27,243 ***** Save model *****
2022-06-12 01:03:32,653 ***** Running evaluation *****
2022-06-12 01:03:32,654   Epoch = 4 iter 479 step
2022-06-12 01:03:32,654   Num examples = 408
2022-06-12 01:03:32,654   Batch size = 32
2022-06-12 01:03:33,007 ***** Eval results *****
2022-06-12 01:03:33,007   acc = 0.7990196078431373
2022-06-12 01:03:33,008   acc_and_f1 = 0.8338046757164405
2022-06-12 01:03:33,008   cls_loss = 0.09121178641267445
2022-06-12 01:03:33,008   eval_loss = 0.6012872216793207
2022-06-12 01:03:33,008   f1 = 0.8685897435897436
2022-06-12 01:03:33,008   global_step = 479
2022-06-12 01:03:33,008   loss = 0.09121178641267445
2022-06-12 01:03:37,941 ***** Running evaluation *****
2022-06-12 01:03:37,941   Epoch = 4 iter 499 step
2022-06-12 01:03:37,941   Num examples = 408
2022-06-12 01:03:37,941   Batch size = 32
2022-06-12 01:03:38,293 ***** Eval results *****
2022-06-12 01:03:38,293   acc = 0.7941176470588235
2022-06-12 01:03:38,294   acc_and_f1 = 0.8270588235294117
2022-06-12 01:03:38,294   cls_loss = 0.09114978878304016
2022-06-12 01:03:38,294   eval_loss = 0.5819993362976954
2022-06-12 01:03:38,294   f1 = 0.86
2022-06-12 01:03:38,294   global_step = 499
2022-06-12 01:03:38,294   loss = 0.09114978878304016
2022-06-12 01:03:43,208 ***** Running evaluation *****
2022-06-12 01:03:43,209   Epoch = 4 iter 519 step
2022-06-12 01:03:43,209   Num examples = 408
2022-06-12 01:03:43,209   Batch size = 32
2022-06-12 01:03:43,561 ***** Eval results *****
2022-06-12 01:03:43,561   acc = 0.7965686274509803
2022-06-12 01:03:43,561   acc_and_f1 = 0.8318843137254901
2022-06-12 01:03:43,561   cls_loss = 0.09020610744990999
2022-06-12 01:03:43,562   eval_loss = 0.6540558624726075
2022-06-12 01:03:43,562   f1 = 0.8672
2022-06-12 01:03:43,562   global_step = 519
2022-06-12 01:03:43,562   loss = 0.09020610744990999
2022-06-12 01:03:48,476 ***** Running evaluation *****
2022-06-12 01:03:48,476   Epoch = 4 iter 539 step
2022-06-12 01:03:48,476   Num examples = 408
2022-06-12 01:03:48,476   Batch size = 32
2022-06-12 01:03:48,830 ***** Eval results *****
2022-06-12 01:03:48,830   acc = 0.7769607843137255
2022-06-12 01:03:48,830   acc_and_f1 = 0.816600455348016
2022-06-12 01:03:48,830   cls_loss = 0.09051505736557834
2022-06-12 01:03:48,830   eval_loss = 0.7297279926446768
2022-06-12 01:03:48,830   f1 = 0.8562401263823065
2022-06-12 01:03:48,830   global_step = 539
2022-06-12 01:03:48,831   loss = 0.09051505736557834
2022-06-12 01:03:53,779 ***** Running evaluation *****
2022-06-12 01:03:53,780   Epoch = 4 iter 559 step
2022-06-12 01:03:53,780   Num examples = 408
2022-06-12 01:03:53,780   Batch size = 32
2022-06-12 01:03:54,134 ***** Eval results *****
2022-06-12 01:03:54,134   acc = 0.7843137254901961
2022-06-12 01:03:54,134   acc_and_f1 = 0.8190671617484203
2022-06-12 01:03:54,134   cls_loss = 0.09032526793121134
2022-06-12 01:03:54,134   eval_loss = 0.6482862555063688
2022-06-12 01:03:54,134   f1 = 0.8538205980066446
2022-06-12 01:03:54,134   global_step = 559
2022-06-12 01:03:54,134   loss = 0.09032526793121134
2022-06-12 01:03:59,096 ***** Running evaluation *****
2022-06-12 01:03:59,097   Epoch = 5 iter 579 step
2022-06-12 01:03:59,097   Num examples = 408
2022-06-12 01:03:59,097   Batch size = 32
2022-06-12 01:03:59,449 ***** Eval results *****
2022-06-12 01:03:59,449   acc = 0.7843137254901961
2022-06-12 01:03:59,449   acc_and_f1 = 0.8183313593893933
2022-06-12 01:03:59,450   cls_loss = 0.08298564122782813
2022-06-12 01:03:59,450   eval_loss = 0.631142829473202
2022-06-12 01:03:59,450   f1 = 0.8523489932885906
2022-06-12 01:03:59,450   global_step = 579
2022-06-12 01:03:59,450   loss = 0.08298564122782813
2022-06-12 01:04:04,384 ***** Running evaluation *****
2022-06-12 01:04:04,384   Epoch = 5 iter 599 step
2022-06-12 01:04:04,384   Num examples = 408
2022-06-12 01:04:04,384   Batch size = 32
2022-06-12 01:04:04,738 ***** Eval results *****
2022-06-12 01:04:04,738   acc = 0.7818627450980392
2022-06-12 01:04:04,738   acc_and_f1 = 0.8188081958877554
2022-06-12 01:04:04,738   cls_loss = 0.09268005295046444
2022-06-12 01:04:04,738   eval_loss = 0.6378819610063846
2022-06-12 01:04:04,738   f1 = 0.8557536466774717
2022-06-12 01:04:04,738   global_step = 599
2022-06-12 01:04:04,738   loss = 0.09268005295046444
2022-06-12 01:04:09,723 ***** Running evaluation *****
2022-06-12 01:04:09,723   Epoch = 5 iter 619 step
2022-06-12 01:04:09,723   Num examples = 408
2022-06-12 01:04:09,723   Batch size = 32
2022-06-12 01:04:10,076 ***** Eval results *****
2022-06-12 01:04:10,076   acc = 0.7818627450980392
2022-06-12 01:04:10,076   acc_and_f1 = 0.8204083931512383
2022-06-12 01:04:10,076   cls_loss = 0.09249331269945417
2022-06-12 01:04:10,076   eval_loss = 0.6602090104268148
2022-06-12 01:04:10,076   f1 = 0.8589540412044374
2022-06-12 01:04:10,076   global_step = 619
2022-06-12 01:04:10,076   loss = 0.09249331269945417
2022-06-12 01:04:15,036 ***** Running evaluation *****
2022-06-12 01:04:15,036   Epoch = 5 iter 639 step
2022-06-12 01:04:15,036   Num examples = 408
2022-06-12 01:04:15,036   Batch size = 32
2022-06-12 01:04:15,389 ***** Eval results *****
2022-06-12 01:04:15,390   acc = 0.7867647058823529
2022-06-12 01:04:15,390   acc_and_f1 = 0.8250934989380367
2022-06-12 01:04:15,390   cls_loss = 0.09106901903515277
2022-06-12 01:04:15,390   eval_loss = 0.7191243870900228
2022-06-12 01:04:15,390   f1 = 0.8634222919937206
2022-06-12 01:04:15,390   global_step = 639
2022-06-12 01:04:15,390   loss = 0.09106901903515277
2022-06-12 01:04:20,367 ***** Running evaluation *****
2022-06-12 01:04:20,368   Epoch = 5 iter 659 step
2022-06-12 01:04:20,368   Num examples = 408
2022-06-12 01:04:20,368   Batch size = 32
2022-06-12 01:04:20,722 ***** Eval results *****
2022-06-12 01:04:20,722   acc = 0.7941176470588235
2022-06-12 01:04:20,723   acc_and_f1 = 0.8297511312217195
2022-06-12 01:04:20,723   cls_loss = 0.09235495940018236
2022-06-12 01:04:20,723   eval_loss = 0.619330848638828
2022-06-12 01:04:20,723   f1 = 0.8653846153846155
2022-06-12 01:04:20,723   global_step = 659
2022-06-12 01:04:20,723   loss = 0.09235495940018236
2022-06-12 01:04:25,709 ***** Running evaluation *****
2022-06-12 01:04:25,709   Epoch = 5 iter 679 step
2022-06-12 01:04:25,709   Num examples = 408
2022-06-12 01:04:25,709   Batch size = 32
2022-06-12 01:04:26,064 ***** Eval results *****
2022-06-12 01:04:26,065   acc = 0.7843137254901961
2022-06-12 01:04:26,065   acc_and_f1 = 0.8225366095805411
2022-06-12 01:04:26,065   cls_loss = 0.09171353765857329
2022-06-12 01:04:26,065   eval_loss = 0.6867991445156244
2022-06-12 01:04:26,065   f1 = 0.860759493670886
2022-06-12 01:04:26,065   global_step = 679
2022-06-12 01:04:26,065   loss = 0.09171353765857329
2022-06-12 01:04:31,035 ***** Running evaluation *****
2022-06-12 01:04:31,035   Epoch = 6 iter 699 step
2022-06-12 01:04:31,036   Num examples = 408
2022-06-12 01:04:31,036   Batch size = 32
2022-06-12 01:04:31,390 ***** Eval results *****
2022-06-12 01:04:31,390   acc = 0.7916666666666666
2022-06-12 01:04:31,390   acc_and_f1 = 0.82464405360134
2022-06-12 01:04:31,390   cls_loss = 0.08930111825466155
2022-06-12 01:04:31,390   eval_loss = 0.6125563153853784
2022-06-12 01:04:31,390   f1 = 0.8576214405360134
2022-06-12 01:04:31,390   global_step = 699
2022-06-12 01:04:31,390   loss = 0.08930111825466155
2022-06-12 01:04:36,343 ***** Running evaluation *****
2022-06-12 01:04:36,344   Epoch = 6 iter 719 step
2022-06-12 01:04:36,344   Num examples = 408
2022-06-12 01:04:36,344   Batch size = 32
2022-06-12 01:04:36,352 ***** Running evaluation *****
2022-06-12 01:04:36,353   Epoch = 0 iter 2999 step
2022-06-12 01:04:36,353   Num examples = 5463
2022-06-12 01:04:36,353   Batch size = 32
2022-06-12 01:04:36,354 ***** Eval results *****
2022-06-12 01:04:36,354   att_loss = 5.754907200121013
2022-06-12 01:04:36,354   global_step = 2999
2022-06-12 01:04:36,354   loss = 6.944677550540681
2022-06-12 01:04:36,354   rep_loss = 1.189770344973962
2022-06-12 01:04:36,355 ***** Save model *****
2022-06-12 01:04:36,698 ***** Eval results *****
2022-06-12 01:04:36,699   acc = 0.7867647058823529
2022-06-12 01:04:36,699   acc_and_f1 = 0.8235589179492021
2022-06-12 01:04:36,699   cls_loss = 0.08961881697177887
2022-06-12 01:04:36,699   eval_loss = 0.6750211371825292
2022-06-12 01:04:36,699   f1 = 0.8603531300160513
2022-06-12 01:04:36,699   global_step = 719
2022-06-12 01:04:36,699   loss = 0.08961881697177887
2022-06-12 01:04:41,673 ***** Running evaluation *****
2022-06-12 01:04:41,673   Epoch = 6 iter 739 step
2022-06-12 01:04:41,673   Num examples = 408
2022-06-12 01:04:41,674   Batch size = 32
2022-06-12 01:04:42,027 ***** Eval results *****
2022-06-12 01:04:42,028   acc = 0.7818627450980392
2022-06-12 01:04:42,028   acc_and_f1 = 0.8183375715702268
2022-06-12 01:04:42,028   cls_loss = 0.08878145962953568
2022-06-12 01:04:42,028   eval_loss = 0.6964158255320329
2022-06-12 01:04:42,028   f1 = 0.8548123980424144
2022-06-12 01:04:42,028   global_step = 739
2022-06-12 01:04:42,028   loss = 0.08878145962953568
2022-06-12 01:04:46,991 ***** Running evaluation *****
2022-06-12 01:04:46,991   Epoch = 6 iter 759 step
2022-06-12 01:04:46,991   Num examples = 408
2022-06-12 01:04:46,991   Batch size = 32
2022-06-12 01:04:47,345 ***** Eval results *****
2022-06-12 01:04:47,345   acc = 0.7843137254901961
2022-06-12 01:04:47,345   acc_and_f1 = 0.8175805915586574
2022-06-12 01:04:47,345   cls_loss = 0.0887979656457901
2022-06-12 01:04:47,345   eval_loss = 0.665480226278305
2022-06-12 01:04:47,345   f1 = 0.8508474576271187
2022-06-12 01:04:47,346   global_step = 759
2022-06-12 01:04:47,346   loss = 0.0887979656457901
2022-06-12 01:04:52,297 ***** Running evaluation *****
2022-06-12 01:04:52,298   Epoch = 6 iter 779 step
2022-06-12 01:04:52,298   Num examples = 408
2022-06-12 01:04:52,298   Batch size = 32
2022-06-12 01:04:52,651 ***** Eval results *****
2022-06-12 01:04:52,651   acc = 0.7794117647058824
2022-06-12 01:04:52,651   acc_and_f1 = 0.8168903483723586
2022-06-12 01:04:52,651   cls_loss = 0.08843709065725928
2022-06-12 01:04:52,651   eval_loss = 0.6750839329682864
2022-06-12 01:04:52,652   f1 = 0.854368932038835
2022-06-12 01:04:52,652   global_step = 779
2022-06-12 01:04:52,652   loss = 0.08843709065725928
2022-06-12 01:04:57,613 ***** Running evaluation *****
2022-06-12 01:04:57,613   Epoch = 7 iter 799 step
2022-06-12 01:04:57,613   Num examples = 408
2022-06-12 01:04:57,613   Batch size = 32
2022-06-12 01:04:57,969 ***** Eval results *****
2022-06-12 01:04:57,969   acc = 0.7843137254901961
2022-06-12 01:04:57,969   acc_and_f1 = 0.8225366095805411
2022-06-12 01:04:57,969   cls_loss = 0.09622315317392349
2022-06-12 01:04:57,969   eval_loss = 0.7159030999128635
2022-06-12 01:04:57,969   f1 = 0.860759493670886
2022-06-12 01:04:57,969   global_step = 799
2022-06-12 01:04:57,969   loss = 0.09622315317392349
2022-06-12 01:05:02,956 ***** Running evaluation *****
2022-06-12 01:05:02,957   Epoch = 7 iter 819 step
2022-06-12 01:05:02,957   Num examples = 408
2022-06-12 01:05:02,957   Batch size = 32
2022-06-12 01:05:03,310 ***** Eval results *****
2022-06-12 01:05:03,311   acc = 0.7843137254901961
2022-06-12 01:05:03,311   acc_and_f1 = 0.8209594517418618
2022-06-12 01:05:03,311   cls_loss = 0.08608198839993704
2022-06-12 01:05:03,311   eval_loss = 0.6710186417286212
2022-06-12 01:05:03,311   f1 = 0.8576051779935275
2022-06-12 01:05:03,311   global_step = 819
2022-06-12 01:05:03,311   loss = 0.08608198839993704
2022-06-12 01:05:08,289 ***** Running evaluation *****
2022-06-12 01:05:08,290   Epoch = 7 iter 839 step
2022-06-12 01:05:08,290   Num examples = 408
2022-06-12 01:05:08,290   Batch size = 32
2022-06-12 01:05:08,643 ***** Eval results *****
2022-06-12 01:05:08,643   acc = 0.7892156862745098
2022-06-12 01:05:08,643   acc_and_f1 = 0.8248026483320601
2022-06-12 01:05:08,643   cls_loss = 0.0873721074767229
2022-06-12 01:05:08,643   eval_loss = 0.6449474119223081
2022-06-12 01:05:08,643   f1 = 0.8603896103896104
2022-06-12 01:05:08,643   global_step = 839
2022-06-12 01:05:08,643   loss = 0.0873721074767229
2022-06-12 01:05:13,587 ***** Running evaluation *****
2022-06-12 01:05:13,587   Epoch = 7 iter 859 step
2022-06-12 01:05:13,587   Num examples = 408
2022-06-12 01:05:13,587   Batch size = 32
2022-06-12 01:05:13,942 ***** Eval results *****
2022-06-12 01:05:13,942   acc = 0.7892156862745098
2022-06-12 01:05:13,942   acc_and_f1 = 0.8259177472906095
2022-06-12 01:05:13,942   cls_loss = 0.08726577057701643
2022-06-12 01:05:13,942   eval_loss = 0.6686621193702404
2022-06-12 01:05:13,942   f1 = 0.8626198083067093
2022-06-12 01:05:13,943   global_step = 859
2022-06-12 01:05:13,943   loss = 0.08726577057701643
2022-06-12 01:05:18,911 ***** Running evaluation *****
2022-06-12 01:05:18,911   Epoch = 7 iter 879 step
2022-06-12 01:05:18,911   Num examples = 408
2022-06-12 01:05:18,911   Batch size = 32
2022-06-12 01:05:19,264 ***** Eval results *****
2022-06-12 01:05:19,264   acc = 0.7941176470588235
2022-06-12 01:05:19,264   acc_and_f1 = 0.8272913816689467
2022-06-12 01:05:19,264   cls_loss = 0.08879839077040001
2022-06-12 01:05:19,264   eval_loss = 0.6262239355307359
2022-06-12 01:05:19,264   f1 = 0.8604651162790697
2022-06-12 01:05:19,264   global_step = 879
2022-06-12 01:05:19,264   loss = 0.08879839077040001
2022-06-12 01:05:24,232 ***** Running evaluation *****
2022-06-12 01:05:24,233   Epoch = 7 iter 899 step
2022-06-12 01:05:24,233   Num examples = 408
2022-06-12 01:05:24,233   Batch size = 32
2022-06-12 01:05:24,587 ***** Eval results *****
2022-06-12 01:05:24,587   acc = 0.7941176470588235
2022-06-12 01:05:24,587   acc_and_f1 = 0.8306031273268801
2022-06-12 01:05:24,587   cls_loss = 0.09012216735299271
2022-06-12 01:05:24,587   eval_loss = 0.6668552698997351
2022-06-12 01:05:24,587   f1 = 0.8670886075949368
2022-06-12 01:05:24,587   global_step = 899
2022-06-12 01:05:24,587   loss = 0.09012216735299271
2022-06-12 01:05:29,580 ***** Running evaluation *****
2022-06-12 01:05:29,580   Epoch = 8 iter 919 step
2022-06-12 01:05:29,580   Num examples = 408
2022-06-12 01:05:29,581   Batch size = 32
2022-06-12 01:05:29,934 ***** Eval results *****
2022-06-12 01:05:29,935   acc = 0.7843137254901961
2022-06-12 01:05:29,935   acc_and_f1 = 0.8195496020190254
2022-06-12 01:05:29,935   cls_loss = 0.08658893299954278
2022-06-12 01:05:29,935   eval_loss = 0.6385390758514404
2022-06-12 01:05:29,935   f1 = 0.8547854785478548
2022-06-12 01:05:29,935   global_step = 919
2022-06-12 01:05:29,935   loss = 0.08658893299954278
2022-06-12 01:05:34,885 ***** Running evaluation *****
2022-06-12 01:05:34,885   Epoch = 8 iter 939 step
2022-06-12 01:05:34,885   Num examples = 408
2022-06-12 01:05:34,885   Batch size = 32
2022-06-12 01:05:35,238 ***** Eval results *****
2022-06-12 01:05:35,239   acc = 0.7965686274509803
2022-06-12 01:05:35,239   acc_and_f1 = 0.8308046389287422
2022-06-12 01:05:35,239   cls_loss = 0.08791590537186023
2022-06-12 01:05:35,239   eval_loss = 0.6206756245631438
2022-06-12 01:05:35,239   f1 = 0.865040650406504
2022-06-12 01:05:35,239   global_step = 939
2022-06-12 01:05:35,239   loss = 0.08791590537186023
2022-06-12 01:05:40,201 ***** Running evaluation *****
2022-06-12 01:05:40,201   Epoch = 8 iter 959 step
2022-06-12 01:05:40,201   Num examples = 408
2022-06-12 01:05:40,202   Batch size = 32
2022-06-12 01:05:40,554 ***** Eval results *****
2022-06-12 01:05:40,554   acc = 0.7843137254901961
2022-06-12 01:05:40,554   acc_and_f1 = 0.8225366095805411
2022-06-12 01:05:40,554   cls_loss = 0.08937053857965672
2022-06-12 01:05:40,554   eval_loss = 0.7121358754543158
2022-06-12 01:05:40,554   f1 = 0.860759493670886
2022-06-12 01:05:40,554   global_step = 959
2022-06-12 01:05:40,554   loss = 0.08937053857965672
2022-06-12 01:05:45,526 ***** Running evaluation *****
2022-06-12 01:05:45,527   Epoch = 8 iter 979 step
2022-06-12 01:05:45,527   Num examples = 408
2022-06-12 01:05:45,527   Batch size = 32
2022-06-12 01:05:45,883 ***** Eval results *****
2022-06-12 01:05:45,883   acc = 0.7818627450980392
2022-06-12 01:05:45,883   acc_and_f1 = 0.8163920090649326
2022-06-12 01:05:45,883   cls_loss = 0.08968822168770121
2022-06-12 01:05:45,883   eval_loss = 0.6394017866024604
2022-06-12 01:05:45,883   f1 = 0.8509212730318259
2022-06-12 01:05:45,883   global_step = 979
2022-06-12 01:05:45,883   loss = 0.08968822168770121
2022-06-12 01:05:50,868 ***** Running evaluation *****
2022-06-12 01:05:50,868   Epoch = 8 iter 999 step
2022-06-12 01:05:50,868   Num examples = 408
2022-06-12 01:05:50,869   Batch size = 32
2022-06-12 01:05:51,221 ***** Eval results *****
2022-06-12 01:05:51,221   acc = 0.7843137254901961
2022-06-12 01:05:51,221   acc_and_f1 = 0.8197884416924665
2022-06-12 01:05:51,221   cls_loss = 0.09035360419202125
2022-06-12 01:05:51,221   eval_loss = 0.6437405324899234
2022-06-12 01:05:51,221   f1 = 0.8552631578947368
2022-06-12 01:05:51,221   global_step = 999
2022-06-12 01:05:51,221   loss = 0.09035360419202125
2022-06-12 01:05:56,149 ***** Running evaluation *****
2022-06-12 01:05:56,150   Epoch = 8 iter 1019 step
2022-06-12 01:05:56,150   Num examples = 408
2022-06-12 01:05:56,150   Batch size = 32
2022-06-12 01:05:56,504 ***** Eval results *****
2022-06-12 01:05:56,504   acc = 0.7745098039215687
2022-06-12 01:05:56,504   acc_and_f1 = 0.8113473112017084
2022-06-12 01:05:56,504   cls_loss = 0.08875674336591613
2022-06-12 01:05:56,504   eval_loss = 0.6504333408979269
2022-06-12 01:05:56,504   f1 = 0.8481848184818481
2022-06-12 01:05:56,504   global_step = 1019
2022-06-12 01:05:56,504   loss = 0.08875674336591613
2022-06-12 01:06:01,453 ***** Running evaluation *****
2022-06-12 01:06:01,453   Epoch = 9 iter 1039 step
2022-06-12 01:06:01,453   Num examples = 408
2022-06-12 01:06:01,454   Batch size = 32
2022-06-12 01:06:01,807 ***** Eval results *****
2022-06-12 01:06:01,808   acc = 0.7720588235294118
2022-06-12 01:06:01,808   acc_and_f1 = 0.8076145719670669
2022-06-12 01:06:01,808   cls_loss = 0.08526528053558789
2022-06-12 01:06:01,808   eval_loss = 0.6477383558566754
2022-06-12 01:06:01,808   f1 = 0.8431703204047218
2022-06-12 01:06:01,808   global_step = 1039
2022-06-12 01:06:01,808   loss = 0.08526528053558789
2022-06-12 01:06:06,759 ***** Running evaluation *****
2022-06-12 01:06:06,760   Epoch = 9 iter 1059 step
2022-06-12 01:06:06,760   Num examples = 408
2022-06-12 01:06:06,760   Batch size = 32
2022-06-12 01:06:07,116 ***** Eval results *****
2022-06-12 01:06:07,116   acc = 0.7843137254901961
2022-06-12 01:06:07,116   acc_and_f1 = 0.8223155929038282
2022-06-12 01:06:07,116   cls_loss = 0.0888276504296245
2022-06-12 01:06:07,116   eval_loss = 0.7083905568489661
2022-06-12 01:06:07,116   f1 = 0.8603174603174604
2022-06-12 01:06:07,116   global_step = 1059
2022-06-12 01:06:07,117   loss = 0.0888276504296245
2022-06-12 01:06:12,058 ***** Running evaluation *****
2022-06-12 01:06:12,059   Epoch = 9 iter 1079 step
2022-06-12 01:06:12,059   Num examples = 408
2022-06-12 01:06:12,059   Batch size = 32
2022-06-12 01:06:12,410 ***** Eval results *****
2022-06-12 01:06:12,411   acc = 0.7941176470588235
2022-06-12 01:06:12,411   acc_and_f1 = 0.8270588235294117
2022-06-12 01:06:12,411   cls_loss = 0.09027358397560299
2022-06-12 01:06:12,411   eval_loss = 0.5975361466407776
2022-06-12 01:06:12,411   f1 = 0.86
2022-06-12 01:06:12,411   global_step = 1079
2022-06-12 01:06:12,411   loss = 0.09027358397560299
2022-06-12 01:06:17,338 ***** Running evaluation *****
2022-06-12 01:06:17,339   Epoch = 9 iter 1099 step
2022-06-12 01:06:17,339   Num examples = 408
2022-06-12 01:06:17,339   Batch size = 32
2022-06-12 01:06:17,691 ***** Eval results *****
2022-06-12 01:06:17,691   acc = 0.7941176470588235
2022-06-12 01:06:17,691   acc_and_f1 = 0.8297511312217195
2022-06-12 01:06:17,691   cls_loss = 0.0917956437150093
2022-06-12 01:06:17,692   eval_loss = 0.6451339423656464
2022-06-12 01:06:17,692   f1 = 0.8653846153846155
2022-06-12 01:06:17,692   global_step = 1099
2022-06-12 01:06:17,692   loss = 0.0917956437150093
2022-06-12 01:06:22,625 ***** Running evaluation *****
2022-06-12 01:06:22,625   Epoch = 9 iter 1119 step
2022-06-12 01:06:22,625   Num examples = 408
2022-06-12 01:06:22,625   Batch size = 32
2022-06-12 01:06:22,978 ***** Eval results *****
2022-06-12 01:06:22,978   acc = 0.7965686274509803
2022-06-12 01:06:22,978   acc_and_f1 = 0.8312406949855871
2022-06-12 01:06:22,978   cls_loss = 0.08992656760959215
2022-06-12 01:06:22,978   eval_loss = 0.6059859349177434
2022-06-12 01:06:22,979   f1 = 0.8659127625201937
2022-06-12 01:06:22,979   global_step = 1119
2022-06-12 01:06:22,979   loss = 0.08992656760959215
2022-06-12 01:06:27,909 ***** Running evaluation *****
2022-06-12 01:06:27,910   Epoch = 9 iter 1139 step
2022-06-12 01:06:27,910   Num examples = 408
2022-06-12 01:06:27,910   Batch size = 32
2022-06-12 01:06:28,264 ***** Eval results *****
2022-06-12 01:06:28,264   acc = 0.7794117647058824
2022-06-12 01:06:28,264   acc_and_f1 = 0.8159353905496625
2022-06-12 01:06:28,264   cls_loss = 0.08896019639430848
2022-06-12 01:06:28,264   eval_loss = 0.6360918512711158
2022-06-12 01:06:28,265   f1 = 0.8524590163934427
2022-06-12 01:06:28,265   global_step = 1139
2022-06-12 01:06:28,265   loss = 0.08896019639430848
2022-06-12 01:06:33,212 ***** Running evaluation *****
2022-06-12 01:06:33,212   Epoch = 10 iter 1159 step
2022-06-12 01:06:33,213   Num examples = 408
2022-06-12 01:06:33,213   Batch size = 32
2022-06-12 01:06:33,565 ***** Eval results *****
2022-06-12 01:06:33,565   acc = 0.7818627450980392
2022-06-12 01:06:33,565   acc_and_f1 = 0.8201841547429783
2022-06-12 01:06:33,565   cls_loss = 0.09351997273532968
2022-06-12 01:06:33,565   eval_loss = 0.6780006541655614
2022-06-12 01:06:33,566   f1 = 0.8585055643879175
2022-06-12 01:06:33,566   global_step = 1159
2022-06-12 01:06:33,566   loss = 0.09351997273532968
2022-06-12 01:06:38,483 ***** Running evaluation *****
2022-06-12 01:06:38,483   Epoch = 10 iter 1179 step
2022-06-12 01:06:38,483   Num examples = 408
2022-06-12 01:06:38,483   Batch size = 32
2022-06-12 01:06:38,836 ***** Eval results *****
2022-06-12 01:06:38,836   acc = 0.7794117647058824
2022-06-12 01:06:38,836   acc_and_f1 = 0.8161764705882353
2022-06-12 01:06:38,836   cls_loss = 0.08967925111452739
2022-06-12 01:06:38,836   eval_loss = 0.6316814147509061
2022-06-12 01:06:38,836   f1 = 0.8529411764705881
2022-06-12 01:06:38,836   global_step = 1179
2022-06-12 01:06:38,837   loss = 0.08967925111452739
2022-06-12 01:06:43,767 ***** Running evaluation *****
2022-06-12 01:06:43,768   Epoch = 10 iter 1199 step
2022-06-12 01:06:43,768   Num examples = 408
2022-06-12 01:06:43,768   Batch size = 32
2022-06-12 01:06:44,121 ***** Eval results *****
2022-06-12 01:06:44,121   acc = 0.7818627450980392
2022-06-12 01:06:44,122   acc_and_f1 = 0.8183375715702268
2022-06-12 01:06:44,122   cls_loss = 0.08966102867813433
2022-06-12 01:06:44,122   eval_loss = 0.6485946017962235
2022-06-12 01:06:44,122   f1 = 0.8548123980424144
2022-06-12 01:06:44,122   global_step = 1199
2022-06-12 01:06:44,122   loss = 0.08966102867813433
2022-06-12 01:06:44,201 ***** Running evaluation *****
2022-06-12 01:06:44,202   Epoch = 1 iter 3499 step
2022-06-12 01:06:44,202   Num examples = 5463
2022-06-12 01:06:44,202   Batch size = 32
2022-06-12 01:06:44,203 ***** Eval results *****
2022-06-12 01:06:44,203   att_loss = 4.690130989108465
2022-06-12 01:06:44,203   global_step = 3499
2022-06-12 01:06:44,203   loss = 5.7314559358411135
2022-06-12 01:06:44,203   rep_loss = 1.041324959919516
2022-06-12 01:06:44,204 ***** Save model *****
2022-06-12 01:06:49,066 ***** Running evaluation *****
2022-06-12 01:06:49,066   Epoch = 10 iter 1219 step
2022-06-12 01:06:49,067   Num examples = 408
2022-06-12 01:06:49,067   Batch size = 32
2022-06-12 01:06:49,419 ***** Eval results *****
2022-06-12 01:06:49,420   acc = 0.7843137254901961
2022-06-12 01:06:49,420   acc_and_f1 = 0.8204956249600818
2022-06-12 01:06:49,420   cls_loss = 0.08986786177641229
2022-06-12 01:06:49,420   eval_loss = 0.6350838771233192
2022-06-12 01:06:49,420   f1 = 0.8566775244299675
2022-06-12 01:06:49,420   global_step = 1219
2022-06-12 01:06:49,420   loss = 0.08986786177641229
2022-06-12 01:06:54,358 ***** Running evaluation *****
2022-06-12 01:06:54,359   Epoch = 10 iter 1239 step
2022-06-12 01:06:54,359   Num examples = 408
2022-06-12 01:06:54,359   Batch size = 32
2022-06-12 01:06:54,711 ***** Eval results *****
2022-06-12 01:06:54,711   acc = 0.7941176470588235
2022-06-12 01:06:54,711   acc_and_f1 = 0.8297511312217195
2022-06-12 01:06:54,712   cls_loss = 0.08980732945480732
2022-06-12 01:06:54,712   eval_loss = 0.6384228330392104
2022-06-12 01:06:54,712   f1 = 0.8653846153846155
2022-06-12 01:06:54,712   global_step = 1239
2022-06-12 01:06:54,712   loss = 0.08980732945480732
2022-06-12 01:06:59,648 ***** Running evaluation *****
2022-06-12 01:06:59,648   Epoch = 11 iter 1259 step
2022-06-12 01:06:59,648   Num examples = 408
2022-06-12 01:06:59,648   Batch size = 32
2022-06-12 01:07:00,001 ***** Eval results *****
2022-06-12 01:07:00,001   acc = 0.7941176470588235
2022-06-12 01:07:00,001   acc_and_f1 = 0.8295347077737847
2022-06-12 01:07:00,001   cls_loss = 0.08739367574453354
2022-06-12 01:07:00,002   eval_loss = 0.6460394515441015
2022-06-12 01:07:00,002   f1 = 0.8649517684887459
2022-06-12 01:07:00,002   global_step = 1259
2022-06-12 01:07:00,002   loss = 0.08739367574453354
2022-06-12 01:07:04,950 ***** Running evaluation *****
2022-06-12 01:07:04,950   Epoch = 11 iter 1279 step
2022-06-12 01:07:04,950   Num examples = 408
2022-06-12 01:07:04,950   Batch size = 32
2022-06-12 01:07:05,303 ***** Eval results *****
2022-06-12 01:07:05,303   acc = 0.7916666666666666
2022-06-12 01:07:05,303   acc_and_f1 = 0.8271742057081315
2022-06-12 01:07:05,303   cls_loss = 0.08650935292243958
2022-06-12 01:07:05,303   eval_loss = 0.6279288026002737
2022-06-12 01:07:05,303   f1 = 0.8626817447495962
2022-06-12 01:07:05,304   global_step = 1279
2022-06-12 01:07:05,304   loss = 0.08650935292243958
2022-06-12 01:07:10,264 ***** Running evaluation *****
2022-06-12 01:07:10,264   Epoch = 11 iter 1299 step
2022-06-12 01:07:10,264   Num examples = 408
2022-06-12 01:07:10,265   Batch size = 32
2022-06-12 01:07:10,617 ***** Eval results *****
2022-06-12 01:07:10,617   acc = 0.7843137254901961
2022-06-12 01:07:10,617   acc_and_f1 = 0.8227562318302716
2022-06-12 01:07:10,617   cls_loss = 0.088337375720342
2022-06-12 01:07:10,618   eval_loss = 0.6952940340225513
2022-06-12 01:07:10,618   f1 = 0.8611987381703471
2022-06-12 01:07:10,618   global_step = 1299
2022-06-12 01:07:10,618   loss = 0.088337375720342
2022-06-12 01:07:15,557 ***** Running evaluation *****
2022-06-12 01:07:15,558   Epoch = 11 iter 1319 step
2022-06-12 01:07:15,558   Num examples = 408
2022-06-12 01:07:15,558   Batch size = 32
2022-06-12 01:07:15,910 ***** Eval results *****
2022-06-12 01:07:15,910   acc = 0.7867647058823529
2022-06-12 01:07:15,910   acc_and_f1 = 0.8240043625105544
2022-06-12 01:07:15,910   cls_loss = 0.08686509258472003
2022-06-12 01:07:15,910   eval_loss = 0.6616762257539309
2022-06-12 01:07:15,910   f1 = 0.861244019138756
2022-06-12 01:07:15,910   global_step = 1319
2022-06-12 01:07:15,910   loss = 0.08686509258472003
2022-06-12 01:07:20,874 ***** Running evaluation *****
2022-06-12 01:07:20,874   Epoch = 11 iter 1339 step
2022-06-12 01:07:20,874   Num examples = 408
2022-06-12 01:07:20,874   Batch size = 32
2022-06-12 01:07:21,227 ***** Eval results *****
2022-06-12 01:07:21,227   acc = 0.7867647058823529
2022-06-12 01:07:21,227   acc_and_f1 = 0.8226506456241034
2022-06-12 01:07:21,227   cls_loss = 0.08659817392335219
2022-06-12 01:07:21,227   eval_loss = 0.6196308594483596
2022-06-12 01:07:21,227   f1 = 0.8585365853658538
2022-06-12 01:07:21,228   global_step = 1339
2022-06-12 01:07:21,228   loss = 0.08659817392335219
2022-06-12 01:07:26,196 ***** Running evaluation *****
2022-06-12 01:07:26,196   Epoch = 11 iter 1359 step
2022-06-12 01:07:26,196   Num examples = 408
2022-06-12 01:07:26,196   Batch size = 32
2022-06-12 01:07:26,554 ***** Eval results *****
2022-06-12 01:07:26,554   acc = 0.7941176470588235
2022-06-12 01:07:26,554   acc_and_f1 = 0.8295347077737847
2022-06-12 01:07:26,554   cls_loss = 0.08596187432607015
2022-06-12 01:07:26,554   eval_loss = 0.6470199089783889
2022-06-12 01:07:26,554   f1 = 0.8649517684887459
2022-06-12 01:07:26,554   global_step = 1359
2022-06-12 01:07:26,554   loss = 0.08596187432607015
2022-06-12 01:07:31,497 ***** Running evaluation *****
2022-06-12 01:07:31,497   Epoch = 12 iter 1379 step
2022-06-12 01:07:31,498   Num examples = 408
2022-06-12 01:07:31,498   Batch size = 32
2022-06-12 01:07:31,850 ***** Eval results *****
2022-06-12 01:07:31,850   acc = 0.7916666666666666
2022-06-12 01:07:31,850   acc_and_f1 = 0.8271742057081315
2022-06-12 01:07:31,850   cls_loss = 0.08624640784480354
2022-06-12 01:07:31,850   eval_loss = 0.6281159084576827
2022-06-12 01:07:31,850   f1 = 0.8626817447495962
2022-06-12 01:07:31,850   global_step = 1379
2022-06-12 01:07:31,850   loss = 0.08624640784480354
2022-06-12 01:07:36,766 ***** Running evaluation *****
2022-06-12 01:07:36,767   Epoch = 12 iter 1399 step
2022-06-12 01:07:36,767   Num examples = 408
2022-06-12 01:07:36,767   Batch size = 32
2022-06-12 01:07:37,120 ***** Eval results *****
2022-06-12 01:07:37,120   acc = 0.7916666666666666
2022-06-12 01:07:37,120   acc_and_f1 = 0.8269516477579686
2022-06-12 01:07:37,120   cls_loss = 0.08774423479072509
2022-06-12 01:07:37,120   eval_loss = 0.6159364993755634
2022-06-12 01:07:37,120   f1 = 0.8622366288492707
2022-06-12 01:07:37,120   global_step = 1399
2022-06-12 01:07:37,120   loss = 0.08774423479072509
2022-06-12 01:07:42,076 ***** Running evaluation *****
2022-06-12 01:07:42,077   Epoch = 12 iter 1419 step
2022-06-12 01:07:42,077   Num examples = 408
2022-06-12 01:07:42,077   Batch size = 32
2022-06-12 01:07:42,430 ***** Eval results *****
2022-06-12 01:07:42,430   acc = 0.7916666666666666
2022-06-12 01:07:42,430   acc_and_f1 = 0.8280502392344498
2022-06-12 01:07:42,430   cls_loss = 0.08624776598869585
2022-06-12 01:07:42,430   eval_loss = 0.6628392659700834
2022-06-12 01:07:42,430   f1 = 0.8644338118022328
2022-06-12 01:07:42,431   global_step = 1419
2022-06-12 01:07:42,431   loss = 0.08624776598869585
2022-06-12 01:07:47,372 ***** Running evaluation *****
2022-06-12 01:07:47,373   Epoch = 12 iter 1439 step
2022-06-12 01:07:47,373   Num examples = 408
2022-06-12 01:07:47,373   Batch size = 32
2022-06-12 01:07:47,726 ***** Eval results *****
2022-06-12 01:07:47,726   acc = 0.7892156862745098
2022-06-12 01:07:47,726   acc_and_f1 = 0.8263538748832866
2022-06-12 01:07:47,726   cls_loss = 0.08606514114309365
2022-06-12 01:07:47,726   eval_loss = 0.6863049177023081
2022-06-12 01:07:47,726   f1 = 0.8634920634920634
2022-06-12 01:07:47,727   global_step = 1439
2022-06-12 01:07:47,727   loss = 0.08606514114309365
2022-06-12 01:07:52,634 ***** Running evaluation *****
2022-06-12 01:07:52,634   Epoch = 12 iter 1459 step
2022-06-12 01:07:52,634   Num examples = 408
2022-06-12 01:07:52,634   Batch size = 32
2022-06-12 01:07:52,987 ***** Eval results *****
2022-06-12 01:07:52,987   acc = 0.7892156862745098
2022-06-12 01:07:52,987   acc_and_f1 = 0.8261365055576371
2022-06-12 01:07:52,987   cls_loss = 0.08628721658017609
2022-06-12 01:07:52,987   eval_loss = 0.6613927185535431
2022-06-12 01:07:52,987   f1 = 0.8630573248407644
2022-06-12 01:07:52,987   global_step = 1459
2022-06-12 01:07:52,987   loss = 0.08628721658017609
2022-06-12 01:07:57,926 ***** Running evaluation *****
2022-06-12 01:07:57,926   Epoch = 12 iter 1479 step
2022-06-12 01:07:57,926   Num examples = 408
2022-06-12 01:07:57,926   Batch size = 32
2022-06-12 01:07:58,278 ***** Eval results *****
2022-06-12 01:07:58,278   acc = 0.7843137254901961
2022-06-12 01:07:58,278   acc_and_f1 = 0.8218693228089958
2022-06-12 01:07:58,279   cls_loss = 0.08555836572840407
2022-06-12 01:07:58,279   eval_loss = 0.6615798129485204
2022-06-12 01:07:58,279   f1 = 0.8594249201277956
2022-06-12 01:07:58,279   global_step = 1479
2022-06-12 01:07:58,279   loss = 0.08555836572840407
2022-06-12 01:08:03,202 ***** Running evaluation *****
2022-06-12 01:08:03,203   Epoch = 13 iter 1499 step
2022-06-12 01:08:03,203   Num examples = 408
2022-06-12 01:08:03,203   Batch size = 32
2022-06-12 01:08:03,559 ***** Eval results *****
2022-06-12 01:08:03,559   acc = 0.7916666666666666
2022-06-12 01:08:03,559   acc_and_f1 = 0.8262752318603382
2022-06-12 01:08:03,559   cls_loss = 0.08749381847241346
2022-06-12 01:08:03,559   eval_loss = 0.6286238294381362
2022-06-12 01:08:03,559   f1 = 0.8608837970540099
2022-06-12 01:08:03,559   global_step = 1499
2022-06-12 01:08:03,559   loss = 0.08749381847241346
2022-06-12 01:08:08,483 ***** Running evaluation *****
2022-06-12 01:08:08,484   Epoch = 13 iter 1519 step
2022-06-12 01:08:08,484   Num examples = 408
2022-06-12 01:08:08,484   Batch size = 32
2022-06-12 01:08:08,836 ***** Eval results *****
2022-06-12 01:08:08,836   acc = 0.7867647058823529
2022-06-12 01:08:08,836   acc_and_f1 = 0.8221875902570521
2022-06-12 01:08:08,836   cls_loss = 0.08822329845782872
2022-06-12 01:08:08,836   eval_loss = 0.6292205819716821
2022-06-12 01:08:08,837   f1 = 0.8576104746317512
2022-06-12 01:08:08,837   global_step = 1519
2022-06-12 01:08:08,837   loss = 0.08822329845782872
2022-06-12 01:08:13,760 ***** Running evaluation *****
2022-06-12 01:08:13,760   Epoch = 13 iter 1539 step
2022-06-12 01:08:13,760   Num examples = 408
2022-06-12 01:08:13,760   Batch size = 32
2022-06-12 01:08:14,113 ***** Eval results *****
2022-06-12 01:08:14,113   acc = 0.7892156862745098
2022-06-12 01:08:14,113   acc_and_f1 = 0.8256975867269984
2022-06-12 01:08:14,113   cls_loss = 0.08585225908379805
2022-06-12 01:08:14,113   eval_loss = 0.6601111567937411
2022-06-12 01:08:14,113   f1 = 0.8621794871794871
2022-06-12 01:08:14,113   global_step = 1539
2022-06-12 01:08:14,113   loss = 0.08585225908379805
2022-06-12 01:08:19,035 ***** Running evaluation *****
2022-06-12 01:08:19,035   Epoch = 13 iter 1559 step
2022-06-12 01:08:19,035   Num examples = 408
2022-06-12 01:08:19,036   Batch size = 32
2022-06-12 01:08:19,388 ***** Eval results *****
2022-06-12 01:08:19,388   acc = 0.7867647058823529
2022-06-12 01:08:19,388   acc_and_f1 = 0.8240043625105544
2022-06-12 01:08:19,389   cls_loss = 0.08653343555989204
2022-06-12 01:08:19,389   eval_loss = 0.6664440104594598
2022-06-12 01:08:19,389   f1 = 0.861244019138756
2022-06-12 01:08:19,389   global_step = 1559
2022-06-12 01:08:19,389   loss = 0.08653343555989204
2022-06-12 01:08:24,307 ***** Running evaluation *****
2022-06-12 01:08:24,308   Epoch = 13 iter 1579 step
2022-06-12 01:08:24,308   Num examples = 408
2022-06-12 01:08:24,308   Batch size = 32
2022-06-12 01:08:24,661 ***** Eval results *****
2022-06-12 01:08:24,662   acc = 0.7892156862745098
2022-06-12 01:08:24,662   acc_and_f1 = 0.8261365055576371
2022-06-12 01:08:24,662   cls_loss = 0.08645005239961073
2022-06-12 01:08:24,662   eval_loss = 0.6646650800338159
2022-06-12 01:08:24,662   f1 = 0.8630573248407644
2022-06-12 01:08:24,662   global_step = 1579
2022-06-12 01:08:24,662   loss = 0.08645005239961073
2022-06-12 01:08:29,587 ***** Running evaluation *****
2022-06-12 01:08:29,588   Epoch = 14 iter 1599 step
2022-06-12 01:08:29,588   Num examples = 408
2022-06-12 01:08:29,588   Batch size = 32
2022-06-12 01:08:29,940 ***** Eval results *****
2022-06-12 01:08:29,940   acc = 0.7892156862745098
2022-06-12 01:08:29,941   acc_and_f1 = 0.8261365055576371
2022-06-12 01:08:29,941   cls_loss = 0.08533832430839539
2022-06-12 01:08:29,941   eval_loss = 0.6616423405133761
2022-06-12 01:08:29,941   f1 = 0.8630573248407644
2022-06-12 01:08:29,941   global_step = 1599
2022-06-12 01:08:29,941   loss = 0.08533832430839539
2022-06-12 01:08:34,846 ***** Running evaluation *****
2022-06-12 01:08:34,846   Epoch = 14 iter 1619 step
2022-06-12 01:08:34,846   Num examples = 408
2022-06-12 01:08:34,846   Batch size = 32
2022-06-12 01:08:35,198 ***** Eval results *****
2022-06-12 01:08:35,199   acc = 0.7867647058823529
2022-06-12 01:08:35,199   acc_and_f1 = 0.8240043625105544
2022-06-12 01:08:35,199   cls_loss = 0.08580059752516124
2022-06-12 01:08:35,199   eval_loss = 0.6533825855988723
2022-06-12 01:08:35,199   f1 = 0.861244019138756
2022-06-12 01:08:35,199   global_step = 1619
2022-06-12 01:08:35,199   loss = 0.08580059752516124
2022-06-12 01:08:40,118 ***** Running evaluation *****
2022-06-12 01:08:40,119   Epoch = 14 iter 1639 step
2022-06-12 01:08:40,119   Num examples = 408
2022-06-12 01:08:40,119   Batch size = 32
2022-06-12 01:08:40,473 ***** Eval results *****
2022-06-12 01:08:40,473   acc = 0.7892156862745098
2022-06-12 01:08:40,473   acc_and_f1 = 0.8256975867269984
2022-06-12 01:08:40,473   cls_loss = 0.08481067661629167
2022-06-12 01:08:40,473   eval_loss = 0.6452768376240363
2022-06-12 01:08:40,473   f1 = 0.8621794871794871
2022-06-12 01:08:40,473   global_step = 1639
2022-06-12 01:08:40,473   loss = 0.08481067661629167
2022-06-12 01:08:45,402 ***** Running evaluation *****
2022-06-12 01:08:45,402   Epoch = 14 iter 1659 step
2022-06-12 01:08:45,402   Num examples = 408
2022-06-12 01:08:45,402   Batch size = 32
2022-06-12 01:08:45,755 ***** Eval results *****
2022-06-12 01:08:45,755   acc = 0.7892156862745098
2022-06-12 01:08:45,755   acc_and_f1 = 0.8254760103398272
2022-06-12 01:08:45,755   cls_loss = 0.08475924756318803
2022-06-12 01:08:45,756   eval_loss = 0.644703752719439
2022-06-12 01:08:45,756   f1 = 0.8617363344051446
2022-06-12 01:08:45,756   global_step = 1659
2022-06-12 01:08:45,756   loss = 0.08475924756318803
2022-06-12 01:08:50,689 ***** Running evaluation *****
2022-06-12 01:08:50,689   Epoch = 14 iter 1679 step
2022-06-12 01:08:50,689   Num examples = 408
2022-06-12 01:08:50,689   Batch size = 32
2022-06-12 01:08:51,041 ***** Eval results *****
2022-06-12 01:08:51,042   acc = 0.7867647058823529
2022-06-12 01:08:51,042   acc_and_f1 = 0.8237823529411764
2022-06-12 01:08:51,042   cls_loss = 0.08514764366379704
2022-06-12 01:08:51,042   eval_loss = 0.6507504536555364
2022-06-12 01:08:51,042   f1 = 0.8607999999999999
2022-06-12 01:08:51,042   global_step = 1679
2022-06-12 01:08:51,042   loss = 0.08514764366379704
2022-06-12 01:08:51,987 ***** Running evaluation *****
2022-06-12 01:08:51,988   Epoch = 1 iter 3999 step
2022-06-12 01:08:51,988   Num examples = 5463
2022-06-12 01:08:51,988   Batch size = 32
2022-06-12 01:08:51,989 ***** Eval results *****
2022-06-12 01:08:51,989   att_loss = 4.654340319397035
2022-06-12 01:08:51,989   global_step = 3999
2022-06-12 01:08:51,989   loss = 5.69131688638167
2022-06-12 01:08:51,990   rep_loss = 1.0369765630438308
2022-06-12 01:08:51,990 ***** Save model *****
2022-06-12 01:08:55,969 ***** Running evaluation *****
2022-06-12 01:08:55,970   Epoch = 14 iter 1699 step
2022-06-12 01:08:55,970   Num examples = 408
2022-06-12 01:08:55,970   Batch size = 32
2022-06-12 01:08:56,322 ***** Eval results *****
2022-06-12 01:08:56,323   acc = 0.7867647058823529
2022-06-12 01:08:56,323   acc_and_f1 = 0.8237823529411764
2022-06-12 01:08:56,323   cls_loss = 0.08513864874839783
2022-06-12 01:08:56,323   eval_loss = 0.6513682397512289
2022-06-12 01:08:56,323   f1 = 0.8607999999999999
2022-06-12 01:08:56,323   global_step = 1699
2022-06-12 01:08:56,323   loss = 0.08513864874839783
2022-06-12 01:08:59,038 **************S*************
task_name = mrpc
best_metirc = 0.8816855753646677
**************E*************

2022-06-12 01:08:59,056 Task finish! 
2022-06-12 01:08:59,056 Task cost 7.870984383333334 minutes, i.e. 0.13118307666666668 hours. 
2022-06-12 01:09:01,407 Task start! 
2022-06-12 01:09:01,427 device: cuda n_gpu: 1
2022-06-12 01:09:01,427 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/RTE', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=10, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=20, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/rte/on_original_data', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='rte', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/rte/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/rte/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 01:09:01,464 Writing example 0 of 2490
2022-06-12 01:09:01,465 *** Example ***
2022-06-12 01:09:01,465 guid: train-0
2022-06-12 01:09:01,465 tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
2022-06-12 01:09:01,465 input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:09:01,465 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:09:01,465 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:09:01,465 label: not_entailment
2022-06-12 01:09:01,465 label_id: 1
2022-06-12 01:09:04,191 Writing example 0 of 277
2022-06-12 01:09:04,192 *** Example ***
2022-06-12 01:09:04,192 guid: dev-0
2022-06-12 01:09:04,192 tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
2022-06-12 01:09:04,192 input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:09:04,192 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:09:04,192 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:09:04,192 label: not_entailment
2022-06-12 01:09:04,192 label_id: 1
2022-06-12 01:09:04,488 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 01:09:09,862 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/rte/on_original_data/pytorch_model.bin
2022-06-12 01:09:10,760 loading model...
2022-06-12 01:09:11,063 done!
2022-06-12 01:09:14,780 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 01:09:15,873 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-12 01:09:16,008 loading model...
2022-06-12 01:09:16,039 done!
2022-06-12 01:09:16,040 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-12 01:09:16,040 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-12 01:09:17,650 ***** Running training *****
2022-06-12 01:09:17,660   Num examples = 2490
2022-06-12 01:09:17,661   Batch size = 32
2022-06-12 01:09:17,661   Num steps = 1540
2022-06-12 01:09:17,661 n: bert.embeddings.word_embeddings.weight
2022-06-12 01:09:17,661 n: bert.embeddings.position_embeddings.weight
2022-06-12 01:09:17,661 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 01:09:17,667 n: bert.embeddings.LayerNorm.weight
2022-06-12 01:09:17,672 n: bert.embeddings.LayerNorm.bias
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 01:09:17,682 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 01:09:17,693 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 01:09:17,698 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 01:09:17,698 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 01:09:17,698 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 01:09:17,698 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 01:09:17,698 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 01:09:17,698 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 01:09:17,698 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 01:09:17,698 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 01:09:17,698 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 01:09:17,698 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 01:09:17,698 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 01:09:17,698 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 01:09:17,699 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 01:09:17,700 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 01:09:17,701 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 01:09:17,701 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 01:09:17,701 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 01:09:17,701 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 01:09:17,701 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 01:09:17,701 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 01:09:17,711 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 01:09:17,724 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 01:09:17,724 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 01:09:17,724 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 01:09:17,724 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 01:09:17,724 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 01:09:17,724 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 01:09:17,724 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 01:09:17,725 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 01:09:17,725 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 01:09:17,725 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 01:09:17,725 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 01:09:17,726 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 01:09:17,726 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 01:09:17,726 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 01:09:17,726 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 01:09:17,726 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 01:09:17,726 n: bert.pooler.dense.weight
2022-06-12 01:09:17,726 n: bert.pooler.dense.bias
2022-06-12 01:09:17,726 n: classifier.weight
2022-06-12 01:09:17,726 n: classifier.bias
2022-06-12 01:09:17,726 n: fit_denses.0.weight
2022-06-12 01:09:17,726 n: fit_denses.0.bias
2022-06-12 01:09:17,726 n: fit_denses.1.weight
2022-06-12 01:09:17,726 n: fit_denses.1.bias
2022-06-12 01:09:17,726 n: fit_denses.2.weight
2022-06-12 01:09:17,726 n: fit_denses.2.bias
2022-06-12 01:09:17,726 n: fit_denses.3.weight
2022-06-12 01:09:17,726 n: fit_denses.3.bias
2022-06-12 01:09:17,726 n: fit_denses.4.weight
2022-06-12 01:09:17,726 n: fit_denses.4.bias
2022-06-12 01:09:17,726 n: fit_denses.5.weight
2022-06-12 01:09:17,726 n: fit_denses.5.bias
2022-06-12 01:09:17,727 n: fit_denses.6.weight
2022-06-12 01:09:17,727 n: fit_denses.6.bias
2022-06-12 01:09:17,727 Total parameters: 72468738
2022-06-12 01:09:20,254 ***** Running evaluation *****
2022-06-12 01:09:20,254   Epoch = 0 iter 9 step
2022-06-12 01:09:20,254   Num examples = 277
2022-06-12 01:09:20,254   Batch size = 32
2022-06-12 01:09:20,255 ***** Eval results *****
2022-06-12 01:09:20,255   att_loss = 19.2010309431288
2022-06-12 01:09:20,255   global_step = 9
2022-06-12 01:09:20,255   loss = 22.23834164937337
2022-06-12 01:09:20,255   rep_loss = 3.037310308880276
2022-06-12 01:09:20,256 ***** Save model *****
2022-06-12 01:09:23,219 ***** Running evaluation *****
2022-06-12 01:09:23,219   Epoch = 0 iter 19 step
2022-06-12 01:09:23,219   Num examples = 277
2022-06-12 01:09:23,220   Batch size = 32
2022-06-12 01:09:23,221 ***** Eval results *****
2022-06-12 01:09:23,221   att_loss = 15.734569700140701
2022-06-12 01:09:23,221   global_step = 19
2022-06-12 01:09:23,221   loss = 18.460950148733037
2022-06-12 01:09:23,221   rep_loss = 2.7263802603671423
2022-06-12 01:09:23,221 ***** Save model *****
2022-06-12 01:09:26,285 ***** Running evaluation *****
2022-06-12 01:09:26,285   Epoch = 0 iter 29 step
2022-06-12 01:09:26,285   Num examples = 277
2022-06-12 01:09:26,285   Batch size = 32
2022-06-12 01:09:26,286 ***** Eval results *****
2022-06-12 01:09:26,286   att_loss = 13.905321154101141
2022-06-12 01:09:26,286   global_step = 29
2022-06-12 01:09:26,286   loss = 16.456137821592133
2022-06-12 01:09:26,287   rep_loss = 2.550816511285716
2022-06-12 01:09:26,287 ***** Save model *****
2022-06-12 01:09:29,337 ***** Running evaluation *****
2022-06-12 01:09:29,338   Epoch = 0 iter 39 step
2022-06-12 01:09:29,338   Num examples = 277
2022-06-12 01:09:29,338   Batch size = 32
2022-06-12 01:09:29,339 ***** Eval results *****
2022-06-12 01:09:29,339   att_loss = 12.844148843716352
2022-06-12 01:09:29,339   global_step = 39
2022-06-12 01:09:29,339   loss = 15.273173919090858
2022-06-12 01:09:29,339   rep_loss = 2.42902496839181
2022-06-12 01:09:29,340 ***** Save model *****
2022-06-12 01:09:32,395 ***** Running evaluation *****
2022-06-12 01:09:32,395   Epoch = 0 iter 49 step
2022-06-12 01:09:32,395   Num examples = 277
2022-06-12 01:09:32,395   Batch size = 32
2022-06-12 01:09:32,396 ***** Eval results *****
2022-06-12 01:09:32,396   att_loss = 12.074000144491391
2022-06-12 01:09:32,396   global_step = 49
2022-06-12 01:09:32,397   loss = 14.410782814025879
2022-06-12 01:09:32,397   rep_loss = 2.3367825868178387
2022-06-12 01:09:32,397 ***** Save model *****
2022-06-12 01:09:35,442 ***** Running evaluation *****
2022-06-12 01:09:35,443   Epoch = 0 iter 59 step
2022-06-12 01:09:35,443   Num examples = 277
2022-06-12 01:09:35,443   Batch size = 32
2022-06-12 01:09:35,444 ***** Eval results *****
2022-06-12 01:09:35,444   att_loss = 11.575305065866244
2022-06-12 01:09:35,444   global_step = 59
2022-06-12 01:09:35,444   loss = 13.839401519904703
2022-06-12 01:09:35,444   rep_loss = 2.264096387362076
2022-06-12 01:09:35,444 ***** Save model *****
2022-06-12 01:09:38,500 ***** Running evaluation *****
2022-06-12 01:09:38,500   Epoch = 0 iter 69 step
2022-06-12 01:09:38,500   Num examples = 277
2022-06-12 01:09:38,500   Batch size = 32
2022-06-12 01:09:38,501 ***** Eval results *****
2022-06-12 01:09:38,501   att_loss = 11.315093538035518
2022-06-12 01:09:38,501   global_step = 69
2022-06-12 01:09:38,501   loss = 13.527423084646031
2022-06-12 01:09:38,502   rep_loss = 2.212329491325047
2022-06-12 01:09:38,502 ***** Save model *****
2022-06-12 01:09:41,548 ***** Running evaluation *****
2022-06-12 01:09:41,549   Epoch = 1 iter 79 step
2022-06-12 01:09:41,549   Num examples = 277
2022-06-12 01:09:41,549   Batch size = 32
2022-06-12 01:09:41,550 ***** Eval results *****
2022-06-12 01:09:41,550   att_loss = 8.71098279953003
2022-06-12 01:09:41,550   global_step = 79
2022-06-12 01:09:41,550   loss = 10.514127731323242
2022-06-12 01:09:41,550   rep_loss = 1.8031448721885681
2022-06-12 01:09:41,550 ***** Save model *****
2022-06-12 01:09:44,622 ***** Running evaluation *****
2022-06-12 01:09:44,623   Epoch = 1 iter 89 step
2022-06-12 01:09:44,623   Num examples = 277
2022-06-12 01:09:44,623   Batch size = 32
2022-06-12 01:09:44,624 ***** Eval results *****
2022-06-12 01:09:44,624   att_loss = 8.649473071098328
2022-06-12 01:09:44,624   global_step = 89
2022-06-12 01:09:44,624   loss = 10.456892490386963
2022-06-12 01:09:44,624   rep_loss = 1.8074193000793457
2022-06-12 01:09:44,624 ***** Save model *****
2022-06-12 01:09:47,656 ***** Running evaluation *****
2022-06-12 01:09:47,657   Epoch = 1 iter 99 step
2022-06-12 01:09:47,657   Num examples = 277
2022-06-12 01:09:47,657   Batch size = 32
2022-06-12 01:09:47,658 ***** Eval results *****
2022-06-12 01:09:47,658   att_loss = 8.823459993709218
2022-06-12 01:09:47,658   global_step = 99
2022-06-12 01:09:47,658   loss = 10.628402623263272
2022-06-12 01:09:47,658   rep_loss = 1.8049426729028875
2022-06-12 01:09:47,658 ***** Save model *****
2022-06-12 01:09:50,696 ***** Running evaluation *****
2022-06-12 01:09:50,696   Epoch = 1 iter 109 step
2022-06-12 01:09:50,697   Num examples = 277
2022-06-12 01:09:50,697   Batch size = 32
2022-06-12 01:09:50,698 ***** Eval results *****
2022-06-12 01:09:50,698   att_loss = 8.745184630155563
2022-06-12 01:09:50,698   global_step = 109
2022-06-12 01:09:50,698   loss = 10.537897229194641
2022-06-12 01:09:50,698   rep_loss = 1.7927126102149487
2022-06-12 01:09:50,698 ***** Save model *****
2022-06-12 01:09:53,716 ***** Running evaluation *****
2022-06-12 01:09:53,716   Epoch = 1 iter 119 step
2022-06-12 01:09:53,716   Num examples = 277
2022-06-12 01:09:53,716   Batch size = 32
2022-06-12 01:09:53,717 ***** Eval results *****
2022-06-12 01:09:53,718   att_loss = 8.730624664397467
2022-06-12 01:09:53,718   global_step = 119
2022-06-12 01:09:53,718   loss = 10.513215564546131
2022-06-12 01:09:53,718   rep_loss = 1.7825909001486642
2022-06-12 01:09:53,718 ***** Save model *****
2022-06-12 01:09:56,763 ***** Running evaluation *****
2022-06-12 01:09:56,763   Epoch = 1 iter 129 step
2022-06-12 01:09:56,763   Num examples = 277
2022-06-12 01:09:56,763   Batch size = 32
2022-06-12 01:09:56,764 ***** Eval results *****
2022-06-12 01:09:56,764   att_loss = 8.63390854688791
2022-06-12 01:09:56,764   global_step = 129
2022-06-12 01:09:56,764   loss = 10.404755904124332
2022-06-12 01:09:56,764   rep_loss = 1.7708473595289083
2022-06-12 01:09:56,765 ***** Save model *****
2022-06-12 01:09:59,826 ***** Running evaluation *****
2022-06-12 01:09:59,826   Epoch = 1 iter 139 step
2022-06-12 01:09:59,826   Num examples = 277
2022-06-12 01:09:59,826   Batch size = 32
2022-06-12 01:09:59,828 ***** Eval results *****
2022-06-12 01:09:59,828   att_loss = 8.626553658516176
2022-06-12 01:09:59,828   global_step = 139
2022-06-12 01:09:59,828   loss = 10.390020185901273
2022-06-12 01:09:59,828   rep_loss = 1.7634665312305573
2022-06-12 01:09:59,828 ***** Save model *****
2022-06-12 01:10:02,971 ***** Running evaluation *****
2022-06-12 01:10:02,971   Epoch = 1 iter 149 step
2022-06-12 01:10:02,971   Num examples = 277
2022-06-12 01:10:02,971   Batch size = 32
2022-06-12 01:10:02,972 ***** Eval results *****
2022-06-12 01:10:02,972   att_loss = 8.542701217863295
2022-06-12 01:10:02,973   global_step = 149
2022-06-12 01:10:02,973   loss = 10.295897550053066
2022-06-12 01:10:02,973   rep_loss = 1.7531963305340872
2022-06-12 01:10:02,973 ***** Save model *****
2022-06-12 01:10:06,015 ***** Running evaluation *****
2022-06-12 01:10:06,016   Epoch = 2 iter 159 step
2022-06-12 01:10:06,016   Num examples = 277
2022-06-12 01:10:06,016   Batch size = 32
2022-06-12 01:10:06,017 ***** Eval results *****
2022-06-12 01:10:06,017   att_loss = 8.316986274719238
2022-06-12 01:10:06,017   global_step = 159
2022-06-12 01:10:06,017   loss = 10.030104446411134
2022-06-12 01:10:06,017   rep_loss = 1.7131182193756103
2022-06-12 01:10:06,017 ***** Save model *****
2022-06-12 01:10:09,058 ***** Running evaluation *****
2022-06-12 01:10:09,058   Epoch = 2 iter 169 step
2022-06-12 01:10:09,058   Num examples = 277
2022-06-12 01:10:09,058   Batch size = 32
2022-06-12 01:10:09,059 ***** Eval results *****
2022-06-12 01:10:09,059   att_loss = 8.229369195302327
2022-06-12 01:10:09,059   global_step = 169
2022-06-12 01:10:09,059   loss = 9.932909520467122
2022-06-12 01:10:09,060   rep_loss = 1.7035403490066527
2022-06-12 01:10:09,060 ***** Save model *****
2022-06-12 01:10:12,142 ***** Running evaluation *****
2022-06-12 01:10:12,142   Epoch = 2 iter 179 step
2022-06-12 01:10:12,142   Num examples = 277
2022-06-12 01:10:12,143   Batch size = 32
2022-06-12 01:10:12,143 ***** Eval results *****
2022-06-12 01:10:12,144   att_loss = 8.07229398727417
2022-06-12 01:10:12,144   global_step = 179
2022-06-12 01:10:12,144   loss = 9.763546562194824
2022-06-12 01:10:12,144   rep_loss = 1.6912525033950805
2022-06-12 01:10:12,144 ***** Save model *****
2022-06-12 01:10:15,218 ***** Running evaluation *****
2022-06-12 01:10:15,218   Epoch = 2 iter 189 step
2022-06-12 01:10:15,218   Num examples = 277
2022-06-12 01:10:15,219   Batch size = 32
2022-06-12 01:10:15,220 ***** Eval results *****
2022-06-12 01:10:15,220   att_loss = 8.153195244925362
2022-06-12 01:10:15,220   global_step = 189
2022-06-12 01:10:15,220   loss = 9.843986947195871
2022-06-12 01:10:15,220   rep_loss = 1.6907916375568934
2022-06-12 01:10:15,220 ***** Save model *****
2022-06-12 01:10:18,249 ***** Running evaluation *****
2022-06-12 01:10:18,249   Epoch = 2 iter 199 step
2022-06-12 01:10:18,249   Num examples = 277
2022-06-12 01:10:18,249   Batch size = 32
2022-06-12 01:10:18,250 ***** Eval results *****
2022-06-12 01:10:18,250   att_loss = 7.9717270109388565
2022-06-12 01:10:18,250   global_step = 199
2022-06-12 01:10:18,250   loss = 9.6510516166687
2022-06-12 01:10:18,250   rep_loss = 1.6793245474497478
2022-06-12 01:10:18,250 ***** Save model *****
2022-06-12 01:10:21,302 ***** Running evaluation *****
2022-06-12 01:10:21,302   Epoch = 2 iter 209 step
2022-06-12 01:10:21,302   Num examples = 277
2022-06-12 01:10:21,302   Batch size = 32
2022-06-12 01:10:21,303 ***** Eval results *****
2022-06-12 01:10:21,303   att_loss = 7.959569965709339
2022-06-12 01:10:21,303   global_step = 209
2022-06-12 01:10:21,303   loss = 9.635271618582985
2022-06-12 01:10:21,303   rep_loss = 1.6757016290317883
2022-06-12 01:10:21,304 ***** Save model *****
2022-06-12 01:10:24,274 ***** Running evaluation *****
2022-06-12 01:10:24,274   Epoch = 2 iter 219 step
2022-06-12 01:10:24,274   Num examples = 277
2022-06-12 01:10:24,274   Batch size = 32
2022-06-12 01:10:24,275 ***** Eval results *****
2022-06-12 01:10:24,275   att_loss = 7.9536138534545895
2022-06-12 01:10:24,275   global_step = 219
2022-06-12 01:10:24,275   loss = 9.626682582268348
2022-06-12 01:10:24,275   rep_loss = 1.6730687049719004
2022-06-12 01:10:24,275 ***** Save model *****
2022-06-12 01:10:27,349 ***** Running evaluation *****
2022-06-12 01:10:27,350   Epoch = 2 iter 229 step
2022-06-12 01:10:27,350   Num examples = 277
2022-06-12 01:10:27,350   Batch size = 32
2022-06-12 01:10:27,351 ***** Eval results *****
2022-06-12 01:10:27,351   att_loss = 7.953737564086914
2022-06-12 01:10:27,351   global_step = 229
2022-06-12 01:10:27,351   loss = 9.624397481282552
2022-06-12 01:10:27,351   rep_loss = 1.6706598933537802
2022-06-12 01:10:27,352 ***** Save model *****
2022-06-12 01:10:30,398 ***** Running evaluation *****
2022-06-12 01:10:30,399   Epoch = 3 iter 239 step
2022-06-12 01:10:30,399   Num examples = 277
2022-06-12 01:10:30,399   Batch size = 32
2022-06-12 01:10:30,400 ***** Eval results *****
2022-06-12 01:10:30,400   att_loss = 7.445109605789185
2022-06-12 01:10:30,400   global_step = 239
2022-06-12 01:10:30,400   loss = 9.073513388633728
2022-06-12 01:10:30,400   rep_loss = 1.6284037828445435
2022-06-12 01:10:30,400 ***** Save model *****
2022-06-12 01:10:33,418 ***** Running evaluation *****
2022-06-12 01:10:33,418   Epoch = 3 iter 249 step
2022-06-12 01:10:33,418   Num examples = 277
2022-06-12 01:10:33,418   Batch size = 32
2022-06-12 01:10:33,419 ***** Eval results *****
2022-06-12 01:10:33,419   att_loss = 7.506333695517646
2022-06-12 01:10:33,419   global_step = 249
2022-06-12 01:10:33,419   loss = 9.13821103837755
2022-06-12 01:10:33,420   rep_loss = 1.6318773759735956
2022-06-12 01:10:33,420 ***** Save model *****
2022-06-12 01:10:36,473 ***** Running evaluation *****
2022-06-12 01:10:36,473   Epoch = 3 iter 259 step
2022-06-12 01:10:36,474   Num examples = 277
2022-06-12 01:10:36,474   Batch size = 32
2022-06-12 01:10:36,475 ***** Eval results *****
2022-06-12 01:10:36,475   att_loss = 7.475888064929417
2022-06-12 01:10:36,475   global_step = 259
2022-06-12 01:10:36,475   loss = 9.105862191745214
2022-06-12 01:10:36,475   rep_loss = 1.6299741608755929
2022-06-12 01:10:36,475 ***** Save model *****
2022-06-12 01:10:39,510 ***** Running evaluation *****
2022-06-12 01:10:39,511   Epoch = 3 iter 269 step
2022-06-12 01:10:39,511   Num examples = 277
2022-06-12 01:10:39,511   Batch size = 32
2022-06-12 01:10:39,512 ***** Eval results *****
2022-06-12 01:10:39,512   att_loss = 7.524257672460456
2022-06-12 01:10:39,512   global_step = 269
2022-06-12 01:10:39,512   loss = 9.154618664791709
2022-06-12 01:10:39,512   rep_loss = 1.6303610331133793
2022-06-12 01:10:39,512 ***** Save model *****
2022-06-12 01:10:42,562 ***** Running evaluation *****
2022-06-12 01:10:42,562   Epoch = 3 iter 279 step
2022-06-12 01:10:42,563   Num examples = 277
2022-06-12 01:10:42,563   Batch size = 32
2022-06-12 01:10:42,564 ***** Eval results *****
2022-06-12 01:10:42,564   att_loss = 7.615368853012721
2022-06-12 01:10:42,564   global_step = 279
2022-06-12 01:10:42,564   loss = 9.247967302799225
2022-06-12 01:10:42,564   rep_loss = 1.6325985093911488
2022-06-12 01:10:42,564 ***** Save model *****
2022-06-12 01:10:45,603 ***** Running evaluation *****
2022-06-12 01:10:45,603   Epoch = 3 iter 289 step
2022-06-12 01:10:45,603   Num examples = 277
2022-06-12 01:10:45,603   Batch size = 32
2022-06-12 01:10:45,604 ***** Eval results *****
2022-06-12 01:10:45,604   att_loss = 7.5674375418959
2022-06-12 01:10:45,604   global_step = 289
2022-06-12 01:10:45,604   loss = 9.195328400052826
2022-06-12 01:10:45,605   rep_loss = 1.6278909362595657
2022-06-12 01:10:45,605 ***** Save model *****
2022-06-12 01:10:48,658 ***** Running evaluation *****
2022-06-12 01:10:48,658   Epoch = 3 iter 299 step
2022-06-12 01:10:48,658   Num examples = 277
2022-06-12 01:10:48,658   Batch size = 32
2022-06-12 01:10:48,659 ***** Eval results *****
2022-06-12 01:10:48,659   att_loss = 7.572506876552806
2022-06-12 01:10:48,659   global_step = 299
2022-06-12 01:10:48,659   loss = 9.199407198849846
2022-06-12 01:10:48,660   rep_loss = 1.6269003941732294
2022-06-12 01:10:48,660 ***** Save model *****
2022-06-12 01:10:51,679 ***** Running evaluation *****
2022-06-12 01:10:51,679   Epoch = 4 iter 309 step
2022-06-12 01:10:51,679   Num examples = 277
2022-06-12 01:10:51,680   Batch size = 32
2022-06-12 01:10:51,681 ***** Eval results *****
2022-06-12 01:10:51,681   att_loss = 7.023764610290527
2022-06-12 01:10:51,681   global_step = 309
2022-06-12 01:10:51,681   loss = 8.610464096069336
2022-06-12 01:10:51,681   rep_loss = 1.5866992473602295
2022-06-12 01:10:51,681 ***** Save model *****
2022-06-12 01:10:54,738 ***** Running evaluation *****
2022-06-12 01:10:54,739   Epoch = 4 iter 319 step
2022-06-12 01:10:54,739   Num examples = 277
2022-06-12 01:10:54,739   Batch size = 32
2022-06-12 01:10:54,740 ***** Eval results *****
2022-06-12 01:10:54,740   att_loss = 7.121734575791792
2022-06-12 01:10:54,740   global_step = 319
2022-06-12 01:10:54,740   loss = 8.714252125133168
2022-06-12 01:10:54,740   rep_loss = 1.5925175926902078
2022-06-12 01:10:54,740 ***** Save model *****
2022-06-12 01:10:57,776 ***** Running evaluation *****
2022-06-12 01:10:57,776   Epoch = 4 iter 329 step
2022-06-12 01:10:57,776   Num examples = 277
2022-06-12 01:10:57,776   Batch size = 32
2022-06-12 01:10:57,778 ***** Eval results *****
2022-06-12 01:10:57,778   att_loss = 7.076592604319255
2022-06-12 01:10:57,778   global_step = 329
2022-06-12 01:10:57,778   loss = 8.664781933739071
2022-06-12 01:10:57,778   rep_loss = 1.588189340773083
2022-06-12 01:10:57,778 ***** Save model *****
2022-06-12 01:11:00,075 ***** Running evaluation *****
2022-06-12 01:11:00,075   Epoch = 1 iter 4499 step
2022-06-12 01:11:00,075   Num examples = 5463
2022-06-12 01:11:00,075   Batch size = 32
2022-06-12 01:11:00,076 ***** Eval results *****
2022-06-12 01:11:00,076   att_loss = 4.601946745572238
2022-06-12 01:11:00,076   global_step = 4499
2022-06-12 01:11:00,077   loss = 5.631821322402208
2022-06-12 01:11:00,077   rep_loss = 1.029874578482953
2022-06-12 01:11:00,077 ***** Save model *****
2022-06-12 01:11:00,814 ***** Running evaluation *****
2022-06-12 01:11:00,815   Epoch = 4 iter 339 step
2022-06-12 01:11:00,815   Num examples = 277
2022-06-12 01:11:00,815   Batch size = 32
2022-06-12 01:11:00,816 ***** Eval results *****
2022-06-12 01:11:00,816   att_loss = 7.181047285756757
2022-06-12 01:11:00,816   global_step = 339
2022-06-12 01:11:00,816   loss = 8.770570924205165
2022-06-12 01:11:00,816   rep_loss = 1.5895236346029467
2022-06-12 01:11:00,816 ***** Save model *****
2022-06-12 01:11:03,871 ***** Running evaluation *****
2022-06-12 01:11:03,872   Epoch = 4 iter 349 step
2022-06-12 01:11:03,872   Num examples = 277
2022-06-12 01:11:03,872   Batch size = 32
2022-06-12 01:11:03,873 ***** Eval results *****
2022-06-12 01:11:03,873   att_loss = 7.24416079172274
2022-06-12 01:11:03,873   global_step = 349
2022-06-12 01:11:03,873   loss = 8.836455519606428
2022-06-12 01:11:03,873   rep_loss = 1.592294710438426
2022-06-12 01:11:03,873 ***** Save model *****
2022-06-12 01:11:06,940 ***** Running evaluation *****
2022-06-12 01:11:06,940   Epoch = 4 iter 359 step
2022-06-12 01:11:06,940   Num examples = 277
2022-06-12 01:11:06,940   Batch size = 32
2022-06-12 01:11:06,942 ***** Eval results *****
2022-06-12 01:11:06,942   att_loss = 7.254723109450995
2022-06-12 01:11:06,942   global_step = 359
2022-06-12 01:11:06,942   loss = 8.846921219545251
2022-06-12 01:11:06,942   rep_loss = 1.5921980960696351
2022-06-12 01:11:06,943 ***** Save model *****
2022-06-12 01:11:10,002 ***** Running evaluation *****
2022-06-12 01:11:10,002   Epoch = 4 iter 369 step
2022-06-12 01:11:10,002   Num examples = 277
2022-06-12 01:11:10,002   Batch size = 32
2022-06-12 01:11:10,003 ***** Eval results *****
2022-06-12 01:11:10,003   att_loss = 7.234848108447966
2022-06-12 01:11:10,003   global_step = 369
2022-06-12 01:11:10,003   loss = 8.824168424137303
2022-06-12 01:11:10,003   rep_loss = 1.5893203059180838
2022-06-12 01:11:10,003 ***** Save model *****
2022-06-12 01:11:13,076 ***** Running evaluation *****
2022-06-12 01:11:13,076   Epoch = 4 iter 379 step
2022-06-12 01:11:13,076   Num examples = 277
2022-06-12 01:11:13,076   Batch size = 32
2022-06-12 01:11:13,077 ***** Eval results *****
2022-06-12 01:11:13,078   att_loss = 7.264199941930636
2022-06-12 01:11:13,078   global_step = 379
2022-06-12 01:11:13,078   loss = 8.85182547233474
2022-06-12 01:11:13,078   rep_loss = 1.5876255186510757
2022-06-12 01:11:13,078 ***** Save model *****
2022-06-12 01:11:16,110 ***** Running evaluation *****
2022-06-12 01:11:16,110   Epoch = 5 iter 389 step
2022-06-12 01:11:16,110   Num examples = 277
2022-06-12 01:11:16,110   Batch size = 32
2022-06-12 01:11:16,112 ***** Eval results *****
2022-06-12 01:11:16,112   att_loss = 7.500404953956604
2022-06-12 01:11:16,112   global_step = 389
2022-06-12 01:11:16,112   loss = 9.083265542984009
2022-06-12 01:11:16,112   rep_loss = 1.5828606188297272
2022-06-12 01:11:16,113 ***** Save model *****
2022-06-12 01:11:19,144 ***** Running evaluation *****
2022-06-12 01:11:19,144   Epoch = 5 iter 399 step
2022-06-12 01:11:19,145   Num examples = 277
2022-06-12 01:11:19,145   Batch size = 32
2022-06-12 01:11:19,146 ***** Eval results *****
2022-06-12 01:11:19,146   att_loss = 7.351343257086618
2022-06-12 01:11:19,146   global_step = 399
2022-06-12 01:11:19,146   loss = 8.928575004850115
2022-06-12 01:11:19,146   rep_loss = 1.5772317051887512
2022-06-12 01:11:19,147 ***** Save model *****
2022-06-12 01:11:22,195 ***** Running evaluation *****
2022-06-12 01:11:22,195   Epoch = 5 iter 409 step
2022-06-12 01:11:22,195   Num examples = 277
2022-06-12 01:11:22,195   Batch size = 32
2022-06-12 01:11:22,196 ***** Eval results *****
2022-06-12 01:11:22,196   att_loss = 7.350522557894389
2022-06-12 01:11:22,196   global_step = 409
2022-06-12 01:11:22,196   loss = 8.927368382612864
2022-06-12 01:11:22,196   rep_loss = 1.5768457502126694
2022-06-12 01:11:22,196 ***** Save model *****
2022-06-12 01:11:25,235 ***** Running evaluation *****
2022-06-12 01:11:25,235   Epoch = 5 iter 419 step
2022-06-12 01:11:25,235   Num examples = 277
2022-06-12 01:11:25,235   Batch size = 32
2022-06-12 01:11:25,236 ***** Eval results *****
2022-06-12 01:11:25,236   att_loss = 7.2268455028533936
2022-06-12 01:11:25,236   global_step = 419
2022-06-12 01:11:25,236   loss = 8.797180764815387
2022-06-12 01:11:25,236   rep_loss = 1.5703352163819706
2022-06-12 01:11:25,236 ***** Save model *****
2022-06-12 01:11:28,295 ***** Running evaluation *****
2022-06-12 01:11:28,296   Epoch = 5 iter 429 step
2022-06-12 01:11:28,296   Num examples = 277
2022-06-12 01:11:28,296   Batch size = 32
2022-06-12 01:11:28,297 ***** Eval results *****
2022-06-12 01:11:28,297   att_loss = 7.221510182727467
2022-06-12 01:11:28,297   global_step = 429
2022-06-12 01:11:28,297   loss = 8.792202732779764
2022-06-12 01:11:28,297   rep_loss = 1.570692558180202
2022-06-12 01:11:28,297 ***** Save model *****
2022-06-12 01:11:31,359 ***** Running evaluation *****
2022-06-12 01:11:31,360   Epoch = 5 iter 439 step
2022-06-12 01:11:31,360   Num examples = 277
2022-06-12 01:11:31,360   Batch size = 32
2022-06-12 01:11:31,361 ***** Eval results *****
2022-06-12 01:11:31,361   att_loss = 7.235789272520277
2022-06-12 01:11:31,361   global_step = 439
2022-06-12 01:11:31,361   loss = 8.805610568435103
2022-06-12 01:11:31,361   rep_loss = 1.5698213290285181
2022-06-12 01:11:31,362 ***** Save model *****
2022-06-12 01:11:34,411 ***** Running evaluation *****
2022-06-12 01:11:34,411   Epoch = 5 iter 449 step
2022-06-12 01:11:34,411   Num examples = 277
2022-06-12 01:11:34,411   Batch size = 32
2022-06-12 01:11:34,412 ***** Eval results *****
2022-06-12 01:11:34,412   att_loss = 7.148494407534599
2022-06-12 01:11:34,412   global_step = 449
2022-06-12 01:11:34,412   loss = 8.712585993111134
2022-06-12 01:11:34,412   rep_loss = 1.5640916153788567
2022-06-12 01:11:34,412 ***** Save model *****
2022-06-12 01:11:37,480 ***** Running evaluation *****
2022-06-12 01:11:37,480   Epoch = 5 iter 459 step
2022-06-12 01:11:37,480   Num examples = 277
2022-06-12 01:11:37,480   Batch size = 32
2022-06-12 01:11:37,481 ***** Eval results *****
2022-06-12 01:11:37,481   att_loss = 7.097228849256361
2022-06-12 01:11:37,481   global_step = 459
2022-06-12 01:11:37,481   loss = 8.657031671420947
2022-06-12 01:11:37,482   rep_loss = 1.559802859216123
2022-06-12 01:11:37,482 ***** Save model *****
2022-06-12 01:11:40,521 ***** Running evaluation *****
2022-06-12 01:11:40,521   Epoch = 6 iter 469 step
2022-06-12 01:11:40,521   Num examples = 277
2022-06-12 01:11:40,522   Batch size = 32
2022-06-12 01:11:40,522 ***** Eval results *****
2022-06-12 01:11:40,523   att_loss = 7.198629856109619
2022-06-12 01:11:40,523   global_step = 469
2022-06-12 01:11:40,523   loss = 8.754799979073661
2022-06-12 01:11:40,523   rep_loss = 1.5561700889042445
2022-06-12 01:11:40,523 ***** Save model *****
2022-06-12 01:11:43,592 ***** Running evaluation *****
2022-06-12 01:11:43,592   Epoch = 6 iter 479 step
2022-06-12 01:11:43,592   Num examples = 277
2022-06-12 01:11:43,592   Batch size = 32
2022-06-12 01:11:43,594 ***** Eval results *****
2022-06-12 01:11:43,594   att_loss = 7.0892057138330795
2022-06-12 01:11:43,594   global_step = 479
2022-06-12 01:11:43,594   loss = 8.637280183679918
2022-06-12 01:11:43,594   rep_loss = 1.5480744768591488
2022-06-12 01:11:43,594 ***** Save model *****
2022-06-12 01:11:46,666 ***** Running evaluation *****
2022-06-12 01:11:46,667   Epoch = 6 iter 489 step
2022-06-12 01:11:46,667   Num examples = 277
2022-06-12 01:11:46,667   Batch size = 32
2022-06-12 01:11:46,668 ***** Eval results *****
2022-06-12 01:11:46,668   att_loss = 7.022748841179742
2022-06-12 01:11:46,668   global_step = 489
2022-06-12 01:11:46,668   loss = 8.566183072549325
2022-06-12 01:11:46,668   rep_loss = 1.5434342357847426
2022-06-12 01:11:46,668 ***** Save model *****
2022-06-12 01:11:49,740 ***** Running evaluation *****
2022-06-12 01:11:49,740   Epoch = 6 iter 499 step
2022-06-12 01:11:49,740   Num examples = 277
2022-06-12 01:11:49,740   Batch size = 32
2022-06-12 01:11:49,741 ***** Eval results *****
2022-06-12 01:11:49,741   att_loss = 6.937290062775483
2022-06-12 01:11:49,742   global_step = 499
2022-06-12 01:11:49,742   loss = 8.4751103504284
2022-06-12 01:11:49,742   rep_loss = 1.5378202844310451
2022-06-12 01:11:49,742 ***** Save model *****
2022-06-12 01:11:52,789 ***** Running evaluation *****
2022-06-12 01:11:52,789   Epoch = 6 iter 509 step
2022-06-12 01:11:52,789   Num examples = 277
2022-06-12 01:11:52,789   Batch size = 32
2022-06-12 01:11:52,790 ***** Eval results *****
2022-06-12 01:11:52,790   att_loss = 6.859650429258955
2022-06-12 01:11:52,790   global_step = 509
2022-06-12 01:11:52,790   loss = 8.39239243243603
2022-06-12 01:11:52,790   rep_loss = 1.5327420082498104
2022-06-12 01:11:52,790 ***** Save model *****
2022-06-12 01:11:55,831 ***** Running evaluation *****
2022-06-12 01:11:55,832   Epoch = 6 iter 519 step
2022-06-12 01:11:55,832   Num examples = 277
2022-06-12 01:11:55,832   Batch size = 32
2022-06-12 01:11:55,833 ***** Eval results *****
2022-06-12 01:11:55,833   att_loss = 6.869985270918462
2022-06-12 01:11:55,833   global_step = 519
2022-06-12 01:11:55,833   loss = 8.403593556922779
2022-06-12 01:11:55,833   rep_loss = 1.5336083006440548
2022-06-12 01:11:55,833 ***** Save model *****
2022-06-12 01:11:58,869 ***** Running evaluation *****
2022-06-12 01:11:58,869   Epoch = 6 iter 529 step
2022-06-12 01:11:58,869   Num examples = 277
2022-06-12 01:11:58,869   Batch size = 32
2022-06-12 01:11:58,870 ***** Eval results *****
2022-06-12 01:11:58,870   att_loss = 6.891308983760093
2022-06-12 01:11:58,871   global_step = 529
2022-06-12 01:11:58,871   loss = 8.425104667891317
2022-06-12 01:11:58,871   rep_loss = 1.5337956965859256
2022-06-12 01:11:58,871 ***** Save model *****
2022-06-12 01:12:01,949 ***** Running evaluation *****
2022-06-12 01:12:01,950   Epoch = 6 iter 539 step
2022-06-12 01:12:01,950   Num examples = 277
2022-06-12 01:12:01,950   Batch size = 32
2022-06-12 01:12:01,951 ***** Eval results *****
2022-06-12 01:12:01,951   att_loss = 6.946640355246408
2022-06-12 01:12:01,951   global_step = 539
2022-06-12 01:12:01,951   loss = 8.48199275252107
2022-06-12 01:12:01,951   rep_loss = 1.5353524034673518
2022-06-12 01:12:01,951 ***** Save model *****
2022-06-12 01:12:05,024 ***** Running evaluation *****
2022-06-12 01:12:05,024   Epoch = 7 iter 549 step
2022-06-12 01:12:05,024   Num examples = 277
2022-06-12 01:12:05,024   Batch size = 32
2022-06-12 01:12:05,025 ***** Eval results *****
2022-06-12 01:12:05,025   att_loss = 6.301939344406128
2022-06-12 01:12:05,026   global_step = 549
2022-06-12 01:12:05,026   loss = 7.7973534107208256
2022-06-12 01:12:05,026   rep_loss = 1.4954139709472656
2022-06-12 01:12:05,026 ***** Save model *****
2022-06-12 01:12:08,090 ***** Running evaluation *****
2022-06-12 01:12:08,091   Epoch = 7 iter 559 step
2022-06-12 01:12:08,091   Num examples = 277
2022-06-12 01:12:08,091   Batch size = 32
2022-06-12 01:12:08,092 ***** Eval results *****
2022-06-12 01:12:08,092   att_loss = 6.484490275382996
2022-06-12 01:12:08,092   global_step = 559
2022-06-12 01:12:08,092   loss = 7.988790845870971
2022-06-12 01:12:08,092   rep_loss = 1.5043004155158997
2022-06-12 01:12:08,092 ***** Save model *****
2022-06-12 01:12:11,210 ***** Running evaluation *****
2022-06-12 01:12:11,211   Epoch = 7 iter 569 step
2022-06-12 01:12:11,211   Num examples = 277
2022-06-12 01:12:11,211   Batch size = 32
2022-06-12 01:12:11,212 ***** Eval results *****
2022-06-12 01:12:11,212   att_loss = 6.620578893025717
2022-06-12 01:12:11,212   global_step = 569
2022-06-12 01:12:11,212   loss = 8.131996409098308
2022-06-12 01:12:11,212   rep_loss = 1.5114174127578734
2022-06-12 01:12:11,212 ***** Save model *****
2022-06-12 01:12:14,295 ***** Running evaluation *****
2022-06-12 01:12:14,295   Epoch = 7 iter 579 step
2022-06-12 01:12:14,295   Num examples = 277
2022-06-12 01:12:14,295   Batch size = 32
2022-06-12 01:12:14,296 ***** Eval results *****
2022-06-12 01:12:14,296   att_loss = 6.6622254133224486
2022-06-12 01:12:14,296   global_step = 579
2022-06-12 01:12:14,296   loss = 8.17533391714096
2022-06-12 01:12:14,296   rep_loss = 1.513108417391777
2022-06-12 01:12:14,296 ***** Save model *****
2022-06-12 01:12:17,361 ***** Running evaluation *****
2022-06-12 01:12:17,362   Epoch = 7 iter 589 step
2022-06-12 01:12:17,362   Num examples = 277
2022-06-12 01:12:17,362   Batch size = 32
2022-06-12 01:12:17,363 ***** Eval results *****
2022-06-12 01:12:17,363   att_loss = 6.726494846343994
2022-06-12 01:12:17,363   global_step = 589
2022-06-12 01:12:17,363   loss = 8.241692752838135
2022-06-12 01:12:17,363   rep_loss = 1.5151978540420532
2022-06-12 01:12:17,363 ***** Save model *****
2022-06-12 01:12:20,448 ***** Running evaluation *****
2022-06-12 01:12:20,448   Epoch = 7 iter 599 step
2022-06-12 01:12:20,448   Num examples = 277
2022-06-12 01:12:20,448   Batch size = 32
2022-06-12 01:12:20,449 ***** Eval results *****
2022-06-12 01:12:20,450   att_loss = 6.6939526478449505
2022-06-12 01:12:20,450   global_step = 599
2022-06-12 01:12:20,450   loss = 8.205872813860575
2022-06-12 01:12:20,450   rep_loss = 1.5119201103846232
2022-06-12 01:12:20,450 ***** Save model *****
2022-06-12 01:12:23,526 ***** Running evaluation *****
2022-06-12 01:12:23,526   Epoch = 7 iter 609 step
2022-06-12 01:12:23,526   Num examples = 277
2022-06-12 01:12:23,526   Batch size = 32
2022-06-12 01:12:23,527 ***** Eval results *****
2022-06-12 01:12:23,527   att_loss = 6.772889941079276
2022-06-12 01:12:23,528   global_step = 609
2022-06-12 01:12:23,528   loss = 8.286781638009208
2022-06-12 01:12:23,528   rep_loss = 1.5138916781970433
2022-06-12 01:12:23,528 ***** Save model *****
2022-06-12 01:12:26,609 ***** Running evaluation *****
2022-06-12 01:12:26,610   Epoch = 8 iter 619 step
2022-06-12 01:12:26,610   Num examples = 277
2022-06-12 01:12:26,610   Batch size = 32
2022-06-12 01:12:26,611 ***** Eval results *****
2022-06-12 01:12:26,611   att_loss = 6.414330959320068
2022-06-12 01:12:26,611   global_step = 619
2022-06-12 01:12:26,611   loss = 7.910618146260579
2022-06-12 01:12:26,611   rep_loss = 1.4962869087855022
2022-06-12 01:12:26,611 ***** Save model *****
2022-06-12 01:12:29,689 ***** Running evaluation *****
2022-06-12 01:12:29,690   Epoch = 8 iter 629 step
2022-06-12 01:12:29,690   Num examples = 277
2022-06-12 01:12:29,690   Batch size = 32
2022-06-12 01:12:29,691 ***** Eval results *****
2022-06-12 01:12:29,691   att_loss = 6.8208208450904255
2022-06-12 01:12:29,691   global_step = 629
2022-06-12 01:12:29,691   loss = 8.3286803318904
2022-06-12 01:12:29,691   rep_loss = 1.5078593400808482
2022-06-12 01:12:29,691 ***** Save model *****
2022-06-12 01:12:32,734 ***** Running evaluation *****
2022-06-12 01:12:32,734   Epoch = 8 iter 639 step
2022-06-12 01:12:32,734   Num examples = 277
2022-06-12 01:12:32,734   Batch size = 32
2022-06-12 01:12:32,735 ***** Eval results *****
2022-06-12 01:12:32,735   att_loss = 6.769885415616243
2022-06-12 01:12:32,735   global_step = 639
2022-06-12 01:12:32,735   loss = 8.275575637817383
2022-06-12 01:12:32,735   rep_loss = 1.5056901289069133
2022-06-12 01:12:32,736 ***** Save model *****
2022-06-12 01:12:35,780 ***** Running evaluation *****
2022-06-12 01:12:35,781   Epoch = 8 iter 649 step
2022-06-12 01:12:35,781   Num examples = 277
2022-06-12 01:12:35,781   Batch size = 32
2022-06-12 01:12:35,782 ***** Eval results *****
2022-06-12 01:12:35,782   att_loss = 6.8235766670920635
2022-06-12 01:12:35,782   global_step = 649
2022-06-12 01:12:35,782   loss = 8.331747922030361
2022-06-12 01:12:35,782   rep_loss = 1.508171179077842
2022-06-12 01:12:35,782 ***** Save model *****
2022-06-12 01:12:38,852 ***** Running evaluation *****
2022-06-12 01:12:38,852   Epoch = 8 iter 659 step
2022-06-12 01:12:38,852   Num examples = 277
2022-06-12 01:12:38,853   Batch size = 32
2022-06-12 01:12:38,854 ***** Eval results *****
2022-06-12 01:12:38,854   att_loss = 6.8100693724876225
2022-06-12 01:12:38,854   global_step = 659
2022-06-12 01:12:38,854   loss = 8.315824941147206
2022-06-12 01:12:38,854   rep_loss = 1.505755532619565
2022-06-12 01:12:38,854 ***** Save model *****
2022-06-12 01:12:41,902 ***** Running evaluation *****
2022-06-12 01:12:41,902   Epoch = 8 iter 669 step
2022-06-12 01:12:41,903   Num examples = 277
2022-06-12 01:12:41,903   Batch size = 32
2022-06-12 01:12:41,904 ***** Eval results *****
2022-06-12 01:12:41,904   att_loss = 6.73386598982901
2022-06-12 01:12:41,904   global_step = 669
2022-06-12 01:12:41,904   loss = 8.235308215303242
2022-06-12 01:12:41,904   rep_loss = 1.5014421894865215
2022-06-12 01:12:41,904 ***** Save model *****
2022-06-12 01:12:44,985 ***** Running evaluation *****
2022-06-12 01:12:44,986   Epoch = 8 iter 679 step
2022-06-12 01:12:44,986   Num examples = 277
2022-06-12 01:12:44,986   Batch size = 32
2022-06-12 01:12:44,987 ***** Eval results *****
2022-06-12 01:12:44,987   att_loss = 6.643773139469207
2022-06-12 01:12:44,987   global_step = 679
2022-06-12 01:12:44,987   loss = 8.1404221776932
2022-06-12 01:12:44,987   rep_loss = 1.496649006056407
2022-06-12 01:12:44,987 ***** Save model *****
2022-06-12 01:12:48,075 ***** Running evaluation *****
2022-06-12 01:12:48,075   Epoch = 8 iter 689 step
2022-06-12 01:12:48,075   Num examples = 277
2022-06-12 01:12:48,075   Batch size = 32
2022-06-12 01:12:48,077 ***** Eval results *****
2022-06-12 01:12:48,077   att_loss = 6.617408020855629
2022-06-12 01:12:48,077   global_step = 689
2022-06-12 01:12:48,077   loss = 8.111358257189188
2022-06-12 01:12:48,077   rep_loss = 1.4939502085724923
2022-06-12 01:12:48,077 ***** Save model *****
2022-06-12 01:12:51,151 ***** Running evaluation *****
2022-06-12 01:12:51,152   Epoch = 9 iter 699 step
2022-06-12 01:12:51,152   Num examples = 277
2022-06-12 01:12:51,152   Batch size = 32
2022-06-12 01:12:51,153 ***** Eval results *****
2022-06-12 01:12:51,153   att_loss = 6.7078537940979
2022-06-12 01:12:51,153   global_step = 699
2022-06-12 01:12:51,153   loss = 8.202127774556478
2022-06-12 01:12:51,153   rep_loss = 1.494273841381073
2022-06-12 01:12:51,153 ***** Save model *****
2022-06-12 01:12:54,257 ***** Running evaluation *****
2022-06-12 01:12:54,258   Epoch = 9 iter 709 step
2022-06-12 01:12:54,258   Num examples = 277
2022-06-12 01:12:54,258   Batch size = 32
2022-06-12 01:12:54,259 ***** Eval results *****
2022-06-12 01:12:54,259   att_loss = 6.518744140863419
2022-06-12 01:12:54,259   global_step = 709
2022-06-12 01:12:54,259   loss = 8.002401173114777
2022-06-12 01:12:54,259   rep_loss = 1.4836569726467133
2022-06-12 01:12:54,259 ***** Save model *****
2022-06-12 01:12:57,368 ***** Running evaluation *****
2022-06-12 01:12:57,368   Epoch = 9 iter 719 step
2022-06-12 01:12:57,369   Num examples = 277
2022-06-12 01:12:57,369   Batch size = 32
2022-06-12 01:12:57,370 ***** Eval results *****
2022-06-12 01:12:57,370   att_loss = 6.403507251005906
2022-06-12 01:12:57,370   global_step = 719
2022-06-12 01:12:57,370   loss = 7.8810993157900295
2022-06-12 01:12:57,370   rep_loss = 1.4775920464442327
2022-06-12 01:12:57,370 ***** Save model *****
2022-06-12 01:13:00,456 ***** Running evaluation *****
2022-06-12 01:13:00,456   Epoch = 9 iter 729 step
2022-06-12 01:13:00,456   Num examples = 277
2022-06-12 01:13:00,457   Batch size = 32
2022-06-12 01:13:00,457 ***** Eval results *****
2022-06-12 01:13:00,458   att_loss = 6.529848138491313
2022-06-12 01:13:00,458   global_step = 729
2022-06-12 01:13:00,458   loss = 8.01269092824724
2022-06-12 01:13:00,458   rep_loss = 1.4828427930672963
2022-06-12 01:13:00,458 ***** Save model *****
2022-06-12 01:13:03,505 ***** Running evaluation *****
2022-06-12 01:13:03,505   Epoch = 9 iter 739 step
2022-06-12 01:13:03,505   Num examples = 277
2022-06-12 01:13:03,505   Batch size = 32
2022-06-12 01:13:03,507 ***** Eval results *****
2022-06-12 01:13:03,507   att_loss = 6.494710507600204
2022-06-12 01:13:03,507   global_step = 739
2022-06-12 01:13:03,507   loss = 7.974518858868143
2022-06-12 01:13:03,507   rep_loss = 1.4798083409019138
2022-06-12 01:13:03,507 ***** Save model *****
2022-06-12 01:13:06,525 ***** Running evaluation *****
2022-06-12 01:13:06,526   Epoch = 9 iter 749 step
2022-06-12 01:13:06,526   Num examples = 277
2022-06-12 01:13:06,526   Batch size = 32
2022-06-12 01:13:06,527 ***** Eval results *****
2022-06-12 01:13:06,527   att_loss = 6.506112464836666
2022-06-12 01:13:06,527   global_step = 749
2022-06-12 01:13:06,527   loss = 7.986440028463091
2022-06-12 01:13:06,527   rep_loss = 1.480327531695366
2022-06-12 01:13:06,527 ***** Save model *****
2022-06-12 01:13:07,809 ***** Running evaluation *****
2022-06-12 01:13:07,809   Epoch = 1 iter 4999 step
2022-06-12 01:13:07,809   Num examples = 5463
2022-06-12 01:13:07,809   Batch size = 32
2022-06-12 01:13:07,811 ***** Eval results *****
2022-06-12 01:13:07,811   att_loss = 4.5831540898956264
2022-06-12 01:13:07,811   global_step = 4999
2022-06-12 01:13:07,811   loss = 5.607396863426836
2022-06-12 01:13:07,811   rep_loss = 1.02424277363481
2022-06-12 01:13:07,811 ***** Save model *****
2022-06-12 01:13:09,582 ***** Running evaluation *****
2022-06-12 01:13:09,582   Epoch = 9 iter 759 step
2022-06-12 01:13:09,582   Num examples = 277
2022-06-12 01:13:09,582   Batch size = 32
2022-06-12 01:13:09,583 ***** Eval results *****
2022-06-12 01:13:09,583   att_loss = 6.501258380485304
2022-06-12 01:13:09,583   global_step = 759
2022-06-12 01:13:09,583   loss = 7.980856490857674
2022-06-12 01:13:09,583   rep_loss = 1.4795980886979536
2022-06-12 01:13:09,583 ***** Save model *****
2022-06-12 01:13:12,647 ***** Running evaluation *****
2022-06-12 01:13:12,647   Epoch = 9 iter 769 step
2022-06-12 01:13:12,647   Num examples = 277
2022-06-12 01:13:12,647   Batch size = 32
2022-06-12 01:13:12,648 ***** Eval results *****
2022-06-12 01:13:12,648   att_loss = 6.445795404283624
2022-06-12 01:13:12,648   global_step = 769
2022-06-12 01:13:12,649   loss = 7.921047204419186
2022-06-12 01:13:12,649   rep_loss = 1.4752517969984758
2022-06-12 01:13:12,649 ***** Save model *****
2022-06-12 01:13:15,685 ***** Running evaluation *****
2022-06-12 01:13:15,685   Epoch = 10 iter 779 step
2022-06-12 01:13:15,685   Num examples = 277
2022-06-12 01:13:15,685   Batch size = 32
2022-06-12 01:13:15,686 ***** Eval results *****
2022-06-12 01:13:15,687   att_loss = 5.879099316067165
2022-06-12 01:13:15,687   global_step = 779
2022-06-12 01:13:15,687   loss = 7.322805245717366
2022-06-12 01:13:15,687   rep_loss = 1.443705889913771
2022-06-12 01:13:15,687 ***** Save model *****
2022-06-12 01:13:18,757 ***** Running evaluation *****
2022-06-12 01:13:18,758   Epoch = 10 iter 789 step
2022-06-12 01:13:18,758   Num examples = 277
2022-06-12 01:13:18,758   Batch size = 32
2022-06-12 01:13:18,759 ***** Eval results *****
2022-06-12 01:13:18,759   att_loss = 6.306015943226061
2022-06-12 01:13:18,759   global_step = 789
2022-06-12 01:13:18,759   loss = 7.768812505822432
2022-06-12 01:13:18,759   rep_loss = 1.462796493580467
2022-06-12 01:13:18,759 ***** Save model *****
2022-06-12 01:13:21,810 ***** Running evaluation *****
2022-06-12 01:13:21,810   Epoch = 10 iter 799 step
2022-06-12 01:13:21,810   Num examples = 277
2022-06-12 01:13:21,810   Batch size = 32
2022-06-12 01:13:21,811 ***** Eval results *****
2022-06-12 01:13:21,811   att_loss = 6.297381433947333
2022-06-12 01:13:21,811   global_step = 799
2022-06-12 01:13:21,811   loss = 7.757949927757526
2022-06-12 01:13:21,811   rep_loss = 1.4605684157075554
2022-06-12 01:13:21,811 ***** Save model *****
2022-06-12 01:13:24,885 ***** Running evaluation *****
2022-06-12 01:13:24,885   Epoch = 10 iter 809 step
2022-06-12 01:13:24,885   Num examples = 277
2022-06-12 01:13:24,885   Batch size = 32
2022-06-12 01:13:24,887 ***** Eval results *****
2022-06-12 01:13:24,887   att_loss = 6.223853844862718
2022-06-12 01:13:24,887   global_step = 809
2022-06-12 01:13:24,887   loss = 7.6812531520158815
2022-06-12 01:13:24,887   rep_loss = 1.4573992521334918
2022-06-12 01:13:24,887 ***** Save model *****
2022-06-12 01:13:27,941 ***** Running evaluation *****
2022-06-12 01:13:27,941   Epoch = 10 iter 819 step
2022-06-12 01:13:27,941   Num examples = 277
2022-06-12 01:13:27,941   Batch size = 32
2022-06-12 01:13:27,942 ***** Eval results *****
2022-06-12 01:13:27,942   att_loss = 6.260105707207504
2022-06-12 01:13:27,942   global_step = 819
2022-06-12 01:13:27,942   loss = 7.719098898829246
2022-06-12 01:13:27,942   rep_loss = 1.4589931356663606
2022-06-12 01:13:27,942 ***** Save model *****
2022-06-12 01:13:31,028 ***** Running evaluation *****
2022-06-12 01:13:31,028   Epoch = 10 iter 829 step
2022-06-12 01:13:31,028   Num examples = 277
2022-06-12 01:13:31,028   Batch size = 32
2022-06-12 01:13:31,030 ***** Eval results *****
2022-06-12 01:13:31,030   att_loss = 6.227318036354195
2022-06-12 01:13:31,030   global_step = 829
2022-06-12 01:13:31,030   loss = 7.682923963514425
2022-06-12 01:13:31,030   rep_loss = 1.4556058806888128
2022-06-12 01:13:31,030 ***** Save model *****
2022-06-12 01:13:34,062 ***** Running evaluation *****
2022-06-12 01:13:34,062   Epoch = 10 iter 839 step
2022-06-12 01:13:34,062   Num examples = 277
2022-06-12 01:13:34,062   Batch size = 32
2022-06-12 01:13:34,064 ***** Eval results *****
2022-06-12 01:13:34,064   att_loss = 6.292489666869675
2022-06-12 01:13:34,064   global_step = 839
2022-06-12 01:13:34,064   loss = 7.750590939452683
2022-06-12 01:13:34,064   rep_loss = 1.4581012259358945
2022-06-12 01:13:34,064 ***** Save model *****
2022-06-12 01:13:37,090 ***** Running evaluation *****
2022-06-12 01:13:37,090   Epoch = 11 iter 849 step
2022-06-12 01:13:37,090   Num examples = 277
2022-06-12 01:13:37,090   Batch size = 32
2022-06-12 01:13:37,091 ***** Eval results *****
2022-06-12 01:13:37,092   att_loss = 6.60430383682251
2022-06-12 01:13:37,092   global_step = 849
2022-06-12 01:13:37,092   loss = 8.07979440689087
2022-06-12 01:13:37,092   rep_loss = 1.4754905700683594
2022-06-12 01:13:37,092 ***** Save model *****
2022-06-12 01:13:40,134 ***** Running evaluation *****
2022-06-12 01:13:40,134   Epoch = 11 iter 859 step
2022-06-12 01:13:40,134   Num examples = 277
2022-06-12 01:13:40,134   Batch size = 32
2022-06-12 01:13:40,135 ***** Eval results *****
2022-06-12 01:13:40,135   att_loss = 6.6619038581848145
2022-06-12 01:13:40,135   global_step = 859
2022-06-12 01:13:40,135   loss = 8.130977233250936
2022-06-12 01:13:40,135   rep_loss = 1.469073345263799
2022-06-12 01:13:40,135 ***** Save model *****
2022-06-12 01:13:43,174 ***** Running evaluation *****
2022-06-12 01:13:43,174   Epoch = 11 iter 869 step
2022-06-12 01:13:43,174   Num examples = 277
2022-06-12 01:13:43,174   Batch size = 32
2022-06-12 01:13:43,175 ***** Eval results *****
2022-06-12 01:13:43,175   att_loss = 6.522656115618619
2022-06-12 01:13:43,175   global_step = 869
2022-06-12 01:13:43,175   loss = 7.982503262433139
2022-06-12 01:13:43,175   rep_loss = 1.459847163070332
2022-06-12 01:13:43,176 ***** Save model *****
2022-06-12 01:13:46,230 ***** Running evaluation *****
2022-06-12 01:13:46,231   Epoch = 11 iter 879 step
2022-06-12 01:13:46,231   Num examples = 277
2022-06-12 01:13:46,231   Batch size = 32
2022-06-12 01:13:46,232 ***** Eval results *****
2022-06-12 01:13:46,232   att_loss = 6.357681438326836
2022-06-12 01:13:46,232   global_step = 879
2022-06-12 01:13:46,232   loss = 7.812030270695686
2022-06-12 01:13:46,232   rep_loss = 1.4543488398194313
2022-06-12 01:13:46,232 ***** Save model *****
2022-06-12 01:13:49,270 ***** Running evaluation *****
2022-06-12 01:13:49,270   Epoch = 11 iter 889 step
2022-06-12 01:13:49,270   Num examples = 277
2022-06-12 01:13:49,270   Batch size = 32
2022-06-12 01:13:49,271 ***** Eval results *****
2022-06-12 01:13:49,271   att_loss = 6.371300288609096
2022-06-12 01:13:49,271   global_step = 889
2022-06-12 01:13:49,271   loss = 7.82404054914202
2022-06-12 01:13:49,271   rep_loss = 1.4527402804011391
2022-06-12 01:13:49,271 ***** Save model *****
2022-06-12 01:13:52,318 ***** Running evaluation *****
2022-06-12 01:13:52,318   Epoch = 11 iter 899 step
2022-06-12 01:13:52,318   Num examples = 277
2022-06-12 01:13:52,318   Batch size = 32
2022-06-12 01:13:52,319 ***** Eval results *****
2022-06-12 01:13:52,319   att_loss = 6.327878466019263
2022-06-12 01:13:52,319   global_step = 899
2022-06-12 01:13:52,319   loss = 7.777713344647334
2022-06-12 01:13:52,319   rep_loss = 1.4498348946754749
2022-06-12 01:13:52,320 ***** Save model *****
2022-06-12 01:13:55,363 ***** Running evaluation *****
2022-06-12 01:13:55,364   Epoch = 11 iter 909 step
2022-06-12 01:13:55,364   Num examples = 277
2022-06-12 01:13:55,364   Batch size = 32
2022-06-12 01:13:55,365 ***** Eval results *****
2022-06-12 01:13:55,365   att_loss = 6.293162130540417
2022-06-12 01:13:55,365   global_step = 909
2022-06-12 01:13:55,366   loss = 7.741141619220857
2022-06-12 01:13:55,366   rep_loss = 1.447979494448631
2022-06-12 01:13:55,366 ***** Save model *****
2022-06-12 01:13:58,408 ***** Running evaluation *****
2022-06-12 01:13:58,408   Epoch = 11 iter 919 step
2022-06-12 01:13:58,409   Num examples = 277
2022-06-12 01:13:58,409   Batch size = 32
2022-06-12 01:13:58,410 ***** Eval results *****
2022-06-12 01:13:58,410   att_loss = 6.224394612842136
2022-06-12 01:13:58,410   global_step = 919
2022-06-12 01:13:58,410   loss = 7.6688635216818914
2022-06-12 01:13:58,410   rep_loss = 1.444468918773863
2022-06-12 01:13:58,410 ***** Save model *****
2022-06-12 01:14:01,488 ***** Running evaluation *****
2022-06-12 01:14:01,488   Epoch = 12 iter 929 step
2022-06-12 01:14:01,489   Num examples = 277
2022-06-12 01:14:01,489   Batch size = 32
2022-06-12 01:14:01,489 ***** Eval results *****
2022-06-12 01:14:01,490   att_loss = 6.688884258270264
2022-06-12 01:14:01,490   global_step = 929
2022-06-12 01:14:01,490   loss = 8.152858638763428
2022-06-12 01:14:01,490   rep_loss = 1.4639744758605957
2022-06-12 01:14:01,490 ***** Save model *****
2022-06-12 01:14:04,465 ***** Running evaluation *****
2022-06-12 01:14:04,466   Epoch = 12 iter 939 step
2022-06-12 01:14:04,466   Num examples = 277
2022-06-12 01:14:04,466   Batch size = 32
2022-06-12 01:14:04,467 ***** Eval results *****
2022-06-12 01:14:04,467   att_loss = 6.409566815694173
2022-06-12 01:14:04,467   global_step = 939
2022-06-12 01:14:04,467   loss = 7.859953943888346
2022-06-12 01:14:04,467   rep_loss = 1.4503871997197468
2022-06-12 01:14:04,467 ***** Save model *****
2022-06-12 01:14:07,526 ***** Running evaluation *****
2022-06-12 01:14:07,527   Epoch = 12 iter 949 step
2022-06-12 01:14:07,527   Num examples = 277
2022-06-12 01:14:07,527   Batch size = 32
2022-06-12 01:14:07,528 ***** Eval results *****
2022-06-12 01:14:07,528   att_loss = 6.302713394165039
2022-06-12 01:14:07,528   global_step = 949
2022-06-12 01:14:07,528   loss = 7.746143093109131
2022-06-12 01:14:07,528   rep_loss = 1.4434297370910645
2022-06-12 01:14:07,528 ***** Save model *****
2022-06-12 01:14:10,567 ***** Running evaluation *****
2022-06-12 01:14:10,568   Epoch = 12 iter 959 step
2022-06-12 01:14:10,568   Num examples = 277
2022-06-12 01:14:10,568   Batch size = 32
2022-06-12 01:14:10,569 ***** Eval results *****
2022-06-12 01:14:10,569   att_loss = 6.2541373934064595
2022-06-12 01:14:10,569   global_step = 959
2022-06-12 01:14:10,569   loss = 7.694430582863944
2022-06-12 01:14:10,569   rep_loss = 1.440293230329241
2022-06-12 01:14:10,569 ***** Save model *****
2022-06-12 01:14:13,641 ***** Running evaluation *****
2022-06-12 01:14:13,641   Epoch = 12 iter 969 step
2022-06-12 01:14:13,641   Num examples = 277
2022-06-12 01:14:13,641   Batch size = 32
2022-06-12 01:14:13,642 ***** Eval results *****
2022-06-12 01:14:13,642   att_loss = 6.163383695814344
2022-06-12 01:14:13,642   global_step = 969
2022-06-12 01:14:13,642   loss = 7.599526924557156
2022-06-12 01:14:13,642   rep_loss = 1.4361432631810507
2022-06-12 01:14:13,643 ***** Save model *****
2022-06-12 01:14:16,693 ***** Running evaluation *****
2022-06-12 01:14:16,693   Epoch = 12 iter 979 step
2022-06-12 01:14:16,693   Num examples = 277
2022-06-12 01:14:16,693   Batch size = 32
2022-06-12 01:14:16,694 ***** Eval results *****
2022-06-12 01:14:16,695   att_loss = 6.127697649869051
2022-06-12 01:14:16,695   global_step = 979
2022-06-12 01:14:16,695   loss = 7.561357151378285
2022-06-12 01:14:16,695   rep_loss = 1.433659553527832
2022-06-12 01:14:16,695 ***** Save model *****
2022-06-12 01:14:19,789 ***** Running evaluation *****
2022-06-12 01:14:19,789   Epoch = 12 iter 989 step
2022-06-12 01:14:19,789   Num examples = 277
2022-06-12 01:14:19,789   Batch size = 32
2022-06-12 01:14:19,790 ***** Eval results *****
2022-06-12 01:14:19,790   att_loss = 6.1129969303424545
2022-06-12 01:14:19,790   global_step = 989
2022-06-12 01:14:19,790   loss = 7.544346163823055
2022-06-12 01:14:19,790   rep_loss = 1.4313492866662831
2022-06-12 01:14:19,790 ***** Save model *****
2022-06-12 01:14:22,834 ***** Running evaluation *****
2022-06-12 01:14:22,834   Epoch = 12 iter 999 step
2022-06-12 01:14:22,834   Num examples = 277
2022-06-12 01:14:22,834   Batch size = 32
2022-06-12 01:14:22,835 ***** Eval results *****
2022-06-12 01:14:22,835   att_loss = 6.113498916625977
2022-06-12 01:14:22,835   global_step = 999
2022-06-12 01:14:22,836   loss = 7.544549872080485
2022-06-12 01:14:22,836   rep_loss = 1.4310510126749674
2022-06-12 01:14:22,836 ***** Save model *****
2022-06-12 01:14:25,887 ***** Running evaluation *****
2022-06-12 01:14:25,887   Epoch = 13 iter 1009 step
2022-06-12 01:14:25,887   Num examples = 277
2022-06-12 01:14:25,887   Batch size = 32
2022-06-12 01:14:25,888 ***** Eval results *****
2022-06-12 01:14:25,888   att_loss = 5.567625403404236
2022-06-12 01:14:25,888   global_step = 1009
2022-06-12 01:14:25,888   loss = 6.9702635407447815
2022-06-12 01:14:25,888   rep_loss = 1.4026382565498352
2022-06-12 01:14:25,888 ***** Save model *****
2022-06-12 01:14:28,922 ***** Running evaluation *****
2022-06-12 01:14:28,923   Epoch = 13 iter 1019 step
2022-06-12 01:14:28,923   Num examples = 277
2022-06-12 01:14:28,923   Batch size = 32
2022-06-12 01:14:28,924 ***** Eval results *****
2022-06-12 01:14:28,924   att_loss = 5.76543582810296
2022-06-12 01:14:28,924   global_step = 1019
2022-06-12 01:14:28,924   loss = 7.177397171656291
2022-06-12 01:14:28,924   rep_loss = 1.4119614097807143
2022-06-12 01:14:28,924 ***** Save model *****
2022-06-12 01:14:31,980 ***** Running evaluation *****
2022-06-12 01:14:31,981   Epoch = 13 iter 1029 step
2022-06-12 01:14:31,981   Num examples = 277
2022-06-12 01:14:31,981   Batch size = 32
2022-06-12 01:14:31,982 ***** Eval results *****
2022-06-12 01:14:31,982   att_loss = 5.762644273894174
2022-06-12 01:14:31,982   global_step = 1029
2022-06-12 01:14:31,982   loss = 7.17390171119145
2022-06-12 01:14:31,982   rep_loss = 1.411257518189294
2022-06-12 01:14:31,983 ***** Save model *****
2022-06-12 01:14:34,970 ***** Running evaluation *****
2022-06-12 01:14:34,970   Epoch = 13 iter 1039 step
2022-06-12 01:14:34,970   Num examples = 277
2022-06-12 01:14:34,970   Batch size = 32
2022-06-12 01:14:34,971 ***** Eval results *****
2022-06-12 01:14:34,972   att_loss = 5.893597050717003
2022-06-12 01:14:34,972   global_step = 1039
2022-06-12 01:14:34,972   loss = 7.310258576744481
2022-06-12 01:14:34,972   rep_loss = 1.4166615856321234
2022-06-12 01:14:34,972 ***** Save model *****
2022-06-12 01:14:38,063 ***** Running evaluation *****
2022-06-12 01:14:38,063   Epoch = 13 iter 1049 step
2022-06-12 01:14:38,063   Num examples = 277
2022-06-12 01:14:38,063   Batch size = 32
2022-06-12 01:14:38,064 ***** Eval results *****
2022-06-12 01:14:38,064   att_loss = 5.981640428304672
2022-06-12 01:14:38,064   global_step = 1049
2022-06-12 01:14:38,064   loss = 7.402057508627574
2022-06-12 01:14:38,064   rep_loss = 1.420417125026385
2022-06-12 01:14:38,064 ***** Save model *****
2022-06-12 01:14:41,114 ***** Running evaluation *****
2022-06-12 01:14:41,114   Epoch = 13 iter 1059 step
2022-06-12 01:14:41,114   Num examples = 277
2022-06-12 01:14:41,114   Batch size = 32
2022-06-12 01:14:41,115 ***** Eval results *****
2022-06-12 01:14:41,115   att_loss = 5.972376206825519
2022-06-12 01:14:41,116   global_step = 1059
2022-06-12 01:14:41,116   loss = 7.390006081811313
2022-06-12 01:14:41,116   rep_loss = 1.41762991198178
2022-06-12 01:14:41,116 ***** Save model *****
2022-06-12 01:14:44,158 ***** Running evaluation *****
2022-06-12 01:14:44,158   Epoch = 13 iter 1069 step
2022-06-12 01:14:44,159   Num examples = 277
2022-06-12 01:14:44,159   Batch size = 32
2022-06-12 01:14:44,160 ***** Eval results *****
2022-06-12 01:14:44,160   att_loss = 5.978365477393655
2022-06-12 01:14:44,160   global_step = 1069
2022-06-12 01:14:44,160   loss = 7.395507223465863
2022-06-12 01:14:44,160   rep_loss = 1.4171417811337639
2022-06-12 01:14:44,160 ***** Save model *****
2022-06-12 01:14:47,210 ***** Running evaluation *****
2022-06-12 01:14:47,210   Epoch = 14 iter 1079 step
2022-06-12 01:14:47,210   Num examples = 277
2022-06-12 01:14:47,210   Batch size = 32
2022-06-12 01:14:47,211 ***** Eval results *****
2022-06-12 01:14:47,211   att_loss = 5.32028865814209
2022-06-12 01:14:47,211   global_step = 1079
2022-06-12 01:14:47,211   loss = 6.716841697692871
2022-06-12 01:14:47,212   rep_loss = 1.3965532779693604
2022-06-12 01:14:47,212 ***** Save model *****
2022-06-12 01:14:50,254 ***** Running evaluation *****
2022-06-12 01:14:50,255   Epoch = 14 iter 1089 step
2022-06-12 01:14:50,255   Num examples = 277
2022-06-12 01:14:50,255   Batch size = 32
2022-06-12 01:14:50,256 ***** Eval results *****
2022-06-12 01:14:50,256   att_loss = 5.932987906716087
2022-06-12 01:14:50,256   global_step = 1089
2022-06-12 01:14:50,256   loss = 7.344497854059393
2022-06-12 01:14:50,256   rep_loss = 1.411509936506098
2022-06-12 01:14:50,256 ***** Save model *****
2022-06-12 01:14:53,320 ***** Running evaluation *****
2022-06-12 01:14:53,321   Epoch = 14 iter 1099 step
2022-06-12 01:14:53,321   Num examples = 277
2022-06-12 01:14:53,321   Batch size = 32
2022-06-12 01:14:53,322 ***** Eval results *****
2022-06-12 01:14:53,322   att_loss = 5.94512262798491
2022-06-12 01:14:53,322   global_step = 1099
2022-06-12 01:14:53,322   loss = 7.358133702051072
2022-06-12 01:14:53,322   rep_loss = 1.4130110570362635
2022-06-12 01:14:53,322 ***** Save model *****
2022-06-12 01:14:56,424 ***** Running evaluation *****
2022-06-12 01:14:56,424   Epoch = 14 iter 1109 step
2022-06-12 01:14:56,424   Num examples = 277
2022-06-12 01:14:56,424   Batch size = 32
2022-06-12 01:14:56,425 ***** Eval results *****
2022-06-12 01:14:56,425   att_loss = 5.967089253087198
2022-06-12 01:14:56,426   global_step = 1109
2022-06-12 01:14:56,426   loss = 7.379110013284991
2022-06-12 01:14:56,426   rep_loss = 1.4120207486614105
2022-06-12 01:14:56,426 ***** Save model *****
2022-06-12 01:14:59,465 ***** Running evaluation *****
2022-06-12 01:14:59,466   Epoch = 14 iter 1119 step
2022-06-12 01:14:59,466   Num examples = 277
2022-06-12 01:14:59,466   Batch size = 32
2022-06-12 01:14:59,467 ***** Eval results *****
2022-06-12 01:14:59,467   att_loss = 5.966798759088284
2022-06-12 01:14:59,467   global_step = 1119
2022-06-12 01:14:59,467   loss = 7.378962830799382
2022-06-12 01:14:59,467   rep_loss = 1.4121640717110984
2022-06-12 01:14:59,467 ***** Save model *****
2022-06-12 01:15:02,518 ***** Running evaluation *****
2022-06-12 01:15:02,519   Epoch = 14 iter 1129 step
2022-06-12 01:15:02,519   Num examples = 277
2022-06-12 01:15:02,519   Batch size = 32
2022-06-12 01:15:02,520 ***** Eval results *****
2022-06-12 01:15:02,520   att_loss = 5.942542721243465
2022-06-12 01:15:02,520   global_step = 1129
2022-06-12 01:15:02,520   loss = 7.351936723671708
2022-06-12 01:15:02,520   rep_loss = 1.4093939930784936
2022-06-12 01:15:02,520 ***** Save model *****
2022-06-12 01:15:05,572 ***** Running evaluation *****
2022-06-12 01:15:05,572   Epoch = 14 iter 1139 step
2022-06-12 01:15:05,572   Num examples = 277
2022-06-12 01:15:05,572   Batch size = 32
2022-06-12 01:15:05,574 ***** Eval results *****
2022-06-12 01:15:05,574   att_loss = 5.981722620667004
2022-06-12 01:15:05,574   global_step = 1139
2022-06-12 01:15:05,574   loss = 7.393169934632348
2022-06-12 01:15:05,574   rep_loss = 1.4114472944228376
2022-06-12 01:15:05,574 ***** Save model *****
2022-06-12 01:15:08,624 ***** Running evaluation *****
2022-06-12 01:15:08,624   Epoch = 14 iter 1149 step
2022-06-12 01:15:08,624   Num examples = 277
2022-06-12 01:15:08,624   Batch size = 32
2022-06-12 01:15:08,626 ***** Eval results *****
2022-06-12 01:15:08,626   att_loss = 5.913612264982412
2022-06-12 01:15:08,626   global_step = 1149
2022-06-12 01:15:08,626   loss = 7.320457673408616
2022-06-12 01:15:08,626   rep_loss = 1.4068453865991513
2022-06-12 01:15:08,626 ***** Save model *****
2022-06-12 01:15:11,728 ***** Running evaluation *****
2022-06-12 01:15:11,728   Epoch = 15 iter 1159 step
2022-06-12 01:15:11,728   Num examples = 277
2022-06-12 01:15:11,728   Batch size = 32
2022-06-12 01:15:11,729 ***** Eval results *****
2022-06-12 01:15:11,729   att_loss = 5.45960807800293
2022-06-12 01:15:11,729   global_step = 1159
2022-06-12 01:15:11,729   loss = 6.8404072523117065
2022-06-12 01:15:11,729   rep_loss = 1.380799263715744
2022-06-12 01:15:11,729 ***** Save model *****
2022-06-12 01:15:14,805 ***** Running evaluation *****
2022-06-12 01:15:14,805   Epoch = 15 iter 1169 step
2022-06-12 01:15:14,805   Num examples = 277
2022-06-12 01:15:14,806   Batch size = 32
2022-06-12 01:15:14,807 ***** Eval results *****
2022-06-12 01:15:14,807   att_loss = 5.723033360072544
2022-06-12 01:15:14,807   global_step = 1169
2022-06-12 01:15:14,807   loss = 7.110174485615322
2022-06-12 01:15:14,807   rep_loss = 1.3871411510876246
2022-06-12 01:15:14,807 ***** Save model *****
2022-06-12 01:15:15,708 ***** Running evaluation *****
2022-06-12 01:15:15,708   Epoch = 1 iter 5499 step
2022-06-12 01:15:15,708   Num examples = 5463
2022-06-12 01:15:15,708   Batch size = 32
2022-06-12 01:15:15,710 ***** Eval results *****
2022-06-12 01:15:15,710   att_loss = 4.550771157696562
2022-06-12 01:15:15,710   global_step = 5499
2022-06-12 01:15:15,710   loss = 5.569508771904931
2022-06-12 01:15:15,710   rep_loss = 1.018737614395805
2022-06-12 01:15:15,710 ***** Save model *****
2022-06-12 01:15:17,878 ***** Running evaluation *****
2022-06-12 01:15:17,879   Epoch = 15 iter 1179 step
2022-06-12 01:15:17,879   Num examples = 277
2022-06-12 01:15:17,879   Batch size = 32
2022-06-12 01:15:17,880 ***** Eval results *****
2022-06-12 01:15:17,880   att_loss = 5.781491498152415
2022-06-12 01:15:17,880   global_step = 1179
2022-06-12 01:15:17,880   loss = 7.174725850423177
2022-06-12 01:15:17,880   rep_loss = 1.3932343771060307
2022-06-12 01:15:17,880 ***** Save model *****
2022-06-12 01:15:20,947 ***** Running evaluation *****
2022-06-12 01:15:20,948   Epoch = 15 iter 1189 step
2022-06-12 01:15:20,948   Num examples = 277
2022-06-12 01:15:20,948   Batch size = 32
2022-06-12 01:15:20,949 ***** Eval results *****
2022-06-12 01:15:20,949   att_loss = 5.8524310168098
2022-06-12 01:15:20,949   global_step = 1189
2022-06-12 01:15:20,949   loss = 7.248255855896893
2022-06-12 01:15:20,949   rep_loss = 1.3958248566178715
2022-06-12 01:15:20,949 ***** Save model *****
2022-06-12 01:15:24,030 ***** Running evaluation *****
2022-06-12 01:15:24,030   Epoch = 15 iter 1199 step
2022-06-12 01:15:24,030   Num examples = 277
2022-06-12 01:15:24,030   Batch size = 32
2022-06-12 01:15:24,031 ***** Eval results *****
2022-06-12 01:15:24,031   att_loss = 5.889462492682717
2022-06-12 01:15:24,031   global_step = 1199
2022-06-12 01:15:24,031   loss = 7.287198175083507
2022-06-12 01:15:24,031   rep_loss = 1.3977356905286962
2022-06-12 01:15:24,032 ***** Save model *****
2022-06-12 01:15:27,076 ***** Running evaluation *****
2022-06-12 01:15:27,077   Epoch = 15 iter 1209 step
2022-06-12 01:15:27,077   Num examples = 277
2022-06-12 01:15:27,077   Batch size = 32
2022-06-12 01:15:27,078 ***** Eval results *****
2022-06-12 01:15:27,078   att_loss = 5.861826861346209
2022-06-12 01:15:27,078   global_step = 1209
2022-06-12 01:15:27,078   loss = 7.258158030333342
2022-06-12 01:15:27,078   rep_loss = 1.3963311888553478
2022-06-12 01:15:27,079 ***** Save model *****
2022-06-12 01:15:30,104 ***** Running evaluation *****
2022-06-12 01:15:30,104   Epoch = 15 iter 1219 step
2022-06-12 01:15:30,104   Num examples = 277
2022-06-12 01:15:30,104   Batch size = 32
2022-06-12 01:15:30,105 ***** Eval results *****
2022-06-12 01:15:30,106   att_loss = 5.833159565925598
2022-06-12 01:15:30,106   global_step = 1219
2022-06-12 01:15:30,106   loss = 7.228214867413044
2022-06-12 01:15:30,106   rep_loss = 1.3950553201138973
2022-06-12 01:15:30,106 ***** Save model *****
2022-06-12 01:15:33,160 ***** Running evaluation *****
2022-06-12 01:15:33,160   Epoch = 15 iter 1229 step
2022-06-12 01:15:33,160   Num examples = 277
2022-06-12 01:15:33,160   Batch size = 32
2022-06-12 01:15:33,161 ***** Eval results *****
2022-06-12 01:15:33,161   att_loss = 5.811383827312572
2022-06-12 01:15:33,161   global_step = 1229
2022-06-12 01:15:33,161   loss = 7.204446734608831
2022-06-12 01:15:33,161   rep_loss = 1.393062921794685
2022-06-12 01:15:33,161 ***** Save model *****
2022-06-12 01:15:36,272 ***** Running evaluation *****
2022-06-12 01:15:36,272   Epoch = 16 iter 1239 step
2022-06-12 01:15:36,272   Num examples = 277
2022-06-12 01:15:36,272   Batch size = 32
2022-06-12 01:15:36,273 ***** Eval results *****
2022-06-12 01:15:36,273   att_loss = 5.830040659223284
2022-06-12 01:15:36,273   global_step = 1239
2022-06-12 01:15:36,273   loss = 7.225688253130231
2022-06-12 01:15:36,273   rep_loss = 1.395647610936846
2022-06-12 01:15:36,273 ***** Save model *****
2022-06-12 01:15:39,361 ***** Running evaluation *****
2022-06-12 01:15:39,362   Epoch = 16 iter 1249 step
2022-06-12 01:15:39,362   Num examples = 277
2022-06-12 01:15:39,362   Batch size = 32
2022-06-12 01:15:39,363 ***** Eval results *****
2022-06-12 01:15:39,363   att_loss = 5.7818457940045525
2022-06-12 01:15:39,363   global_step = 1249
2022-06-12 01:15:39,363   loss = 7.169049683739157
2022-06-12 01:15:39,363   rep_loss = 1.387203896746916
2022-06-12 01:15:39,363 ***** Save model *****
2022-06-12 01:15:42,399 ***** Running evaluation *****
2022-06-12 01:15:42,400   Epoch = 16 iter 1259 step
2022-06-12 01:15:42,400   Num examples = 277
2022-06-12 01:15:42,400   Batch size = 32
2022-06-12 01:15:42,401 ***** Eval results *****
2022-06-12 01:15:42,401   att_loss = 5.772123266149451
2022-06-12 01:15:42,401   global_step = 1259
2022-06-12 01:15:42,401   loss = 7.157445625022605
2022-06-12 01:15:42,401   rep_loss = 1.3853223588731554
2022-06-12 01:15:42,401 ***** Save model *****
2022-06-12 01:15:45,470 ***** Running evaluation *****
2022-06-12 01:15:45,470   Epoch = 16 iter 1269 step
2022-06-12 01:15:45,470   Num examples = 277
2022-06-12 01:15:45,470   Batch size = 32
2022-06-12 01:15:45,471 ***** Eval results *****
2022-06-12 01:15:45,471   att_loss = 5.768912031843856
2022-06-12 01:15:45,471   global_step = 1269
2022-06-12 01:15:45,471   loss = 7.154039357159589
2022-06-12 01:15:45,472   rep_loss = 1.3851273317594786
2022-06-12 01:15:45,472 ***** Save model *****
2022-06-12 01:15:48,507 ***** Running evaluation *****
2022-06-12 01:15:48,508   Epoch = 16 iter 1279 step
2022-06-12 01:15:48,508   Num examples = 277
2022-06-12 01:15:48,508   Batch size = 32
2022-06-12 01:15:48,509 ***** Eval results *****
2022-06-12 01:15:48,510   att_loss = 5.731735523710859
2022-06-12 01:15:48,510   global_step = 1279
2022-06-12 01:15:48,510   loss = 7.114708514923745
2022-06-12 01:15:48,510   rep_loss = 1.382972998821989
2022-06-12 01:15:48,510 ***** Save model *****
2022-06-12 01:15:51,561 ***** Running evaluation *****
2022-06-12 01:15:51,562   Epoch = 16 iter 1289 step
2022-06-12 01:15:51,562   Num examples = 277
2022-06-12 01:15:51,562   Batch size = 32
2022-06-12 01:15:51,563 ***** Eval results *****
2022-06-12 01:15:51,563   att_loss = 5.698244078117504
2022-06-12 01:15:51,563   global_step = 1289
2022-06-12 01:15:51,563   loss = 7.080579464895683
2022-06-12 01:15:51,563   rep_loss = 1.3823353972351342
2022-06-12 01:15:51,563 ***** Save model *****
2022-06-12 01:15:54,611 ***** Running evaluation *****
2022-06-12 01:15:54,612   Epoch = 16 iter 1299 step
2022-06-12 01:15:54,612   Num examples = 277
2022-06-12 01:15:54,612   Batch size = 32
2022-06-12 01:15:54,613 ***** Eval results *****
2022-06-12 01:15:54,613   att_loss = 5.753193962040232
2022-06-12 01:15:54,613   global_step = 1299
2022-06-12 01:15:54,613   loss = 7.137869486168249
2022-06-12 01:15:54,613   rep_loss = 1.3846755187902877
2022-06-12 01:15:54,613 ***** Save model *****
2022-06-12 01:15:57,700 ***** Running evaluation *****
2022-06-12 01:15:57,700   Epoch = 16 iter 1309 step
2022-06-12 01:15:57,701   Num examples = 277
2022-06-12 01:15:57,701   Batch size = 32
2022-06-12 01:15:57,702 ***** Eval results *****
2022-06-12 01:15:57,702   att_loss = 5.771726664010581
2022-06-12 01:15:57,702   global_step = 1309
2022-06-12 01:15:57,703   loss = 7.156073743646795
2022-06-12 01:15:57,703   rep_loss = 1.3843470610581434
2022-06-12 01:15:57,703 ***** Save model *****
2022-06-12 01:16:00,755 ***** Running evaluation *****
2022-06-12 01:16:00,756   Epoch = 17 iter 1319 step
2022-06-12 01:16:00,756   Num examples = 277
2022-06-12 01:16:00,756   Batch size = 32
2022-06-12 01:16:00,756 ***** Eval results *****
2022-06-12 01:16:00,757   att_loss = 5.578154611587524
2022-06-12 01:16:00,757   global_step = 1319
2022-06-12 01:16:00,757   loss = 6.953579521179199
2022-06-12 01:16:00,757   rep_loss = 1.375424897670746
2022-06-12 01:16:00,757 ***** Save model *****
2022-06-12 01:16:03,782 ***** Running evaluation *****
2022-06-12 01:16:03,782   Epoch = 17 iter 1329 step
2022-06-12 01:16:03,782   Num examples = 277
2022-06-12 01:16:03,782   Batch size = 32
2022-06-12 01:16:03,783 ***** Eval results *****
2022-06-12 01:16:03,783   att_loss = 5.753534984588623
2022-06-12 01:16:03,783   global_step = 1329
2022-06-12 01:16:03,783   loss = 7.135608077049255
2022-06-12 01:16:03,783   rep_loss = 1.3820731341838837
2022-06-12 01:16:03,784 ***** Save model *****
2022-06-12 01:16:06,834 ***** Running evaluation *****
2022-06-12 01:16:06,834   Epoch = 17 iter 1339 step
2022-06-12 01:16:06,834   Num examples = 277
2022-06-12 01:16:06,834   Batch size = 32
2022-06-12 01:16:06,835 ***** Eval results *****
2022-06-12 01:16:06,835   att_loss = 5.729000600179036
2022-06-12 01:16:06,835   global_step = 1339
2022-06-12 01:16:06,836   loss = 7.11201826731364
2022-06-12 01:16:06,836   rep_loss = 1.3830177028973898
2022-06-12 01:16:06,836 ***** Save model *****
2022-06-12 01:16:09,867 ***** Running evaluation *****
2022-06-12 01:16:09,868   Epoch = 17 iter 1349 step
2022-06-12 01:16:09,868   Num examples = 277
2022-06-12 01:16:09,868   Batch size = 32
2022-06-12 01:16:09,869 ***** Eval results *****
2022-06-12 01:16:09,869   att_loss = 5.701796126365662
2022-06-12 01:16:09,869   global_step = 1349
2022-06-12 01:16:09,869   loss = 7.081494653224945
2022-06-12 01:16:09,869   rep_loss = 1.3796985417604446
2022-06-12 01:16:09,869 ***** Save model *****
2022-06-12 01:16:12,942 ***** Running evaluation *****
2022-06-12 01:16:12,942   Epoch = 17 iter 1359 step
2022-06-12 01:16:12,942   Num examples = 277
2022-06-12 01:16:12,942   Batch size = 32
2022-06-12 01:16:12,943 ***** Eval results *****
2022-06-12 01:16:12,943   att_loss = 5.72821078300476
2022-06-12 01:16:12,943   global_step = 1359
2022-06-12 01:16:12,943   loss = 7.107775774002075
2022-06-12 01:16:12,943   rep_loss = 1.3795649933815002
2022-06-12 01:16:12,944 ***** Save model *****
2022-06-12 01:16:16,029 ***** Running evaluation *****
2022-06-12 01:16:16,030   Epoch = 17 iter 1369 step
2022-06-12 01:16:16,030   Num examples = 277
2022-06-12 01:16:16,030   Batch size = 32
2022-06-12 01:16:16,031 ***** Eval results *****
2022-06-12 01:16:16,031   att_loss = 5.70620436668396
2022-06-12 01:16:16,031   global_step = 1369
2022-06-12 01:16:16,031   loss = 7.084291736284892
2022-06-12 01:16:16,031   rep_loss = 1.3780873596668244
2022-06-12 01:16:16,031 ***** Save model *****
2022-06-12 01:16:19,137 ***** Running evaluation *****
2022-06-12 01:16:19,137   Epoch = 17 iter 1379 step
2022-06-12 01:16:19,137   Num examples = 277
2022-06-12 01:16:19,137   Batch size = 32
2022-06-12 01:16:19,138 ***** Eval results *****
2022-06-12 01:16:19,138   att_loss = 5.6892548765454976
2022-06-12 01:16:19,139   global_step = 1379
2022-06-12 01:16:19,139   loss = 7.065419387817383
2022-06-12 01:16:19,139   rep_loss = 1.3761644959449768
2022-06-12 01:16:19,139 ***** Save model *****
2022-06-12 01:16:22,183 ***** Running evaluation *****
2022-06-12 01:16:22,184   Epoch = 18 iter 1389 step
2022-06-12 01:16:22,184   Num examples = 277
2022-06-12 01:16:22,184   Batch size = 32
2022-06-12 01:16:22,185 ***** Eval results *****
2022-06-12 01:16:22,185   att_loss = 5.304981072743733
2022-06-12 01:16:22,186   global_step = 1389
2022-06-12 01:16:22,186   loss = 6.66189972559611
2022-06-12 01:16:22,186   rep_loss = 1.3569185733795166
2022-06-12 01:16:22,186 ***** Save model *****
2022-06-12 01:16:25,255 ***** Running evaluation *****
2022-06-12 01:16:25,255   Epoch = 18 iter 1399 step
2022-06-12 01:16:25,255   Num examples = 277
2022-06-12 01:16:25,255   Batch size = 32
2022-06-12 01:16:25,256 ***** Eval results *****
2022-06-12 01:16:25,256   att_loss = 5.318116995004507
2022-06-12 01:16:25,256   global_step = 1399
2022-06-12 01:16:25,256   loss = 6.670262740208552
2022-06-12 01:16:25,257   rep_loss = 1.3521457452040453
2022-06-12 01:16:25,257 ***** Save model *****
2022-06-12 01:16:28,319 ***** Running evaluation *****
2022-06-12 01:16:28,320   Epoch = 18 iter 1409 step
2022-06-12 01:16:28,320   Num examples = 277
2022-06-12 01:16:28,320   Batch size = 32
2022-06-12 01:16:28,321 ***** Eval results *****
2022-06-12 01:16:28,322   att_loss = 5.421781726505445
2022-06-12 01:16:28,322   global_step = 1409
2022-06-12 01:16:28,322   loss = 6.779831347258194
2022-06-12 01:16:28,322   rep_loss = 1.3580495844716611
2022-06-12 01:16:28,322 ***** Save model *****
2022-06-12 01:16:31,403 ***** Running evaluation *****
2022-06-12 01:16:31,403   Epoch = 18 iter 1419 step
2022-06-12 01:16:31,403   Num examples = 277
2022-06-12 01:16:31,403   Batch size = 32
2022-06-12 01:16:31,404 ***** Eval results *****
2022-06-12 01:16:31,404   att_loss = 5.397073528983376
2022-06-12 01:16:31,404   global_step = 1419
2022-06-12 01:16:31,404   loss = 6.754031860467159
2022-06-12 01:16:31,404   rep_loss = 1.3569583170341724
2022-06-12 01:16:31,405 ***** Save model *****
2022-06-12 01:16:34,463 ***** Running evaluation *****
2022-06-12 01:16:34,463   Epoch = 18 iter 1429 step
2022-06-12 01:16:34,463   Num examples = 277
2022-06-12 01:16:34,463   Batch size = 32
2022-06-12 01:16:34,464 ***** Eval results *****
2022-06-12 01:16:34,464   att_loss = 5.487150414045467
2022-06-12 01:16:34,464   global_step = 1429
2022-06-12 01:16:34,464   loss = 6.848125812619231
2022-06-12 01:16:34,464   rep_loss = 1.3609753847122192
2022-06-12 01:16:34,465 ***** Save model *****
2022-06-12 01:16:37,534 ***** Running evaluation *****
2022-06-12 01:16:37,535   Epoch = 18 iter 1439 step
2022-06-12 01:16:37,535   Num examples = 277
2022-06-12 01:16:37,535   Batch size = 32
2022-06-12 01:16:37,537 ***** Eval results *****
2022-06-12 01:16:37,537   att_loss = 5.458607205804789
2022-06-12 01:16:37,537   global_step = 1439
2022-06-12 01:16:37,537   loss = 6.8163446930219544
2022-06-12 01:16:37,537   rep_loss = 1.3577374714725423
2022-06-12 01:16:37,537 ***** Save model *****
2022-06-12 01:16:40,584 ***** Running evaluation *****
2022-06-12 01:16:40,584   Epoch = 18 iter 1449 step
2022-06-12 01:16:40,584   Num examples = 277
2022-06-12 01:16:40,584   Batch size = 32
2022-06-12 01:16:40,585 ***** Eval results *****
2022-06-12 01:16:40,585   att_loss = 5.556560327136327
2022-06-12 01:16:40,586   global_step = 1449
2022-06-12 01:16:40,586   loss = 6.9188904232449
2022-06-12 01:16:40,586   rep_loss = 1.3623300752942524
2022-06-12 01:16:40,586 ***** Save model *****
2022-06-12 01:16:43,625 ***** Running evaluation *****
2022-06-12 01:16:43,626   Epoch = 18 iter 1459 step
2022-06-12 01:16:43,626   Num examples = 277
2022-06-12 01:16:43,626   Batch size = 32
2022-06-12 01:16:43,627 ***** Eval results *****
2022-06-12 01:16:43,627   att_loss = 5.6084673698634315
2022-06-12 01:16:43,627   global_step = 1459
2022-06-12 01:16:43,627   loss = 6.973151513974961
2022-06-12 01:16:43,627   rep_loss = 1.3646841196164692
2022-06-12 01:16:43,627 ***** Save model *****
2022-06-12 01:16:46,666 ***** Running evaluation *****
2022-06-12 01:16:46,667   Epoch = 19 iter 1469 step
2022-06-12 01:16:46,667   Num examples = 277
2022-06-12 01:16:46,667   Batch size = 32
2022-06-12 01:16:46,668 ***** Eval results *****
2022-06-12 01:16:46,668   att_loss = 5.8192667961120605
2022-06-12 01:16:46,668   global_step = 1469
2022-06-12 01:16:46,668   loss = 7.190429131189982
2022-06-12 01:16:46,668   rep_loss = 1.3711622556050618
2022-06-12 01:16:46,668 ***** Save model *****
2022-06-12 01:16:49,715 ***** Running evaluation *****
2022-06-12 01:16:49,716   Epoch = 19 iter 1479 step
2022-06-12 01:16:49,716   Num examples = 277
2022-06-12 01:16:49,716   Batch size = 32
2022-06-12 01:16:49,717 ***** Eval results *****
2022-06-12 01:16:49,717   att_loss = 5.580869436264038
2022-06-12 01:16:49,717   global_step = 1479
2022-06-12 01:16:49,717   loss = 6.939902514219284
2022-06-12 01:16:49,717   rep_loss = 1.3590331375598907
2022-06-12 01:16:49,717 ***** Save model *****
2022-06-12 01:16:52,758 ***** Running evaluation *****
2022-06-12 01:16:52,759   Epoch = 19 iter 1489 step
2022-06-12 01:16:52,759   Num examples = 277
2022-06-12 01:16:52,759   Batch size = 32
2022-06-12 01:16:52,760 ***** Eval results *****
2022-06-12 01:16:52,760   att_loss = 5.606399150995108
2022-06-12 01:16:52,760   global_step = 1489
2022-06-12 01:16:52,760   loss = 6.966663158856905
2022-06-12 01:16:52,760   rep_loss = 1.3602640628814697
2022-06-12 01:16:52,760 ***** Save model *****
2022-06-12 01:16:55,819 ***** Running evaluation *****
2022-06-12 01:16:55,819   Epoch = 19 iter 1499 step
2022-06-12 01:16:55,819   Num examples = 277
2022-06-12 01:16:55,819   Batch size = 32
2022-06-12 01:16:55,820 ***** Eval results *****
2022-06-12 01:16:55,820   att_loss = 5.573949191305372
2022-06-12 01:16:55,820   global_step = 1499
2022-06-12 01:16:55,821   loss = 6.9327398671044245
2022-06-12 01:16:55,821   rep_loss = 1.3587907320923276
2022-06-12 01:16:55,821 ***** Save model *****
2022-06-12 01:16:58,869 ***** Running evaluation *****
2022-06-12 01:16:58,869   Epoch = 19 iter 1509 step
2022-06-12 01:16:58,869   Num examples = 277
2022-06-12 01:16:58,869   Batch size = 32
2022-06-12 01:16:58,870 ***** Eval results *****
2022-06-12 01:16:58,870   att_loss = 5.590685896251513
2022-06-12 01:16:58,870   global_step = 1509
2022-06-12 01:16:58,870   loss = 6.949784745340762
2022-06-12 01:16:58,870   rep_loss = 1.3590988672297935
2022-06-12 01:16:58,870 ***** Save model *****
2022-06-12 01:17:01,918 ***** Running evaluation *****
2022-06-12 01:17:01,919   Epoch = 19 iter 1519 step
2022-06-12 01:17:01,919   Num examples = 277
2022-06-12 01:17:01,919   Batch size = 32
2022-06-12 01:17:01,920 ***** Eval results *****
2022-06-12 01:17:01,920   att_loss = 5.504038052899497
2022-06-12 01:17:01,920   global_step = 1519
2022-06-12 01:17:01,920   loss = 6.858749611037118
2022-06-12 01:17:01,920   rep_loss = 1.354711577296257
2022-06-12 01:17:01,920 ***** Save model *****
2022-06-12 01:17:04,980 ***** Running evaluation *****
2022-06-12 01:17:04,980   Epoch = 19 iter 1529 step
2022-06-12 01:17:04,981   Num examples = 277
2022-06-12 01:17:04,981   Batch size = 32
2022-06-12 01:17:04,982 ***** Eval results *****
2022-06-12 01:17:04,982   att_loss = 5.535549366112911
2022-06-12 01:17:04,982   global_step = 1529
2022-06-12 01:17:04,982   loss = 6.8923829107573535
2022-06-12 01:17:04,982   rep_loss = 1.3568335609002546
2022-06-12 01:17:04,982 ***** Save model *****
2022-06-12 01:17:08,081 ***** Running evaluation *****
2022-06-12 01:17:08,082   Epoch = 19 iter 1539 step
2022-06-12 01:17:08,082   Num examples = 277
2022-06-12 01:17:08,082   Batch size = 32
2022-06-12 01:17:08,083 ***** Eval results *****
2022-06-12 01:17:08,083   att_loss = 5.540714565076326
2022-06-12 01:17:08,083   global_step = 1539
2022-06-12 01:17:08,083   loss = 6.897190959830033
2022-06-12 01:17:08,083   rep_loss = 1.356476416713313
2022-06-12 01:17:08,083 ***** Save model *****
2022-06-12 01:17:08,813 Task finish! 
2022-06-12 01:17:08,814 Task cost 8.1234364 minutes, i.e. 0.13539060972222222 hours. 
2022-06-12 01:17:10,957 Task start! 
2022-06-12 01:17:10,975 device: cuda n_gpu: 1
2022-06-12 01:17:10,976 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/RTE', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=10, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=15, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/rte/on_original_data', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/rte/on_original_data', task_name='rte', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/rte/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/rte/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 01:17:11,015 Writing example 0 of 2490
2022-06-12 01:17:11,015 *** Example ***
2022-06-12 01:17:11,015 guid: train-0
2022-06-12 01:17:11,015 tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
2022-06-12 01:17:11,015 input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:17:11,015 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:17:11,015 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:17:11,016 label: not_entailment
2022-06-12 01:17:11,016 label_id: 1
2022-06-12 01:17:13,684 Writing example 0 of 277
2022-06-12 01:17:13,685 *** Example ***
2022-06-12 01:17:13,685 guid: dev-0
2022-06-12 01:17:13,685 tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
2022-06-12 01:17:13,686 input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:17:13,686 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:17:13,686 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:17:13,686 label: not_entailment
2022-06-12 01:17:13,686 label_id: 1
2022-06-12 01:17:13,974 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 01:17:19,537 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/rte/on_original_data/pytorch_model.bin
2022-06-12 01:17:20,278 loading model...
2022-06-12 01:17:20,788 done!
2022-06-12 01:17:23,873 ***** Running evaluation *****
2022-06-12 01:17:23,873   Epoch = 1 iter 5999 step
2022-06-12 01:17:23,873   Num examples = 5463
2022-06-12 01:17:23,873   Batch size = 32
2022-06-12 01:17:23,874 ***** Eval results *****
2022-06-12 01:17:23,875   att_loss = 4.523665269857282
2022-06-12 01:17:23,875   global_step = 5999
2022-06-12 01:17:23,875   loss = 5.537156710271352
2022-06-12 01:17:23,875   rep_loss = 1.0134914401516877
2022-06-12 01:17:23,875 ***** Save model *****
2022-06-12 01:17:24,296 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 01:17:25,412 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/rte/on_original_data/pytorch_model.bin
2022-06-12 01:17:25,581 loading model...
2022-06-12 01:17:25,668 done!
2022-06-12 01:17:27,379 ***** Running training *****
2022-06-12 01:17:27,395   Num examples = 2490
2022-06-12 01:17:27,400   Batch size = 32
2022-06-12 01:17:27,406   Num steps = 1155
2022-06-12 01:17:27,406 n: bert.embeddings.word_embeddings.weight
2022-06-12 01:17:27,411 n: bert.embeddings.position_embeddings.weight
2022-06-12 01:17:27,422 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 01:17:27,432 n: bert.embeddings.LayerNorm.weight
2022-06-12 01:17:27,447 n: bert.embeddings.LayerNorm.bias
2022-06-12 01:17:27,455 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 01:17:27,455 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 01:17:27,455 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 01:17:27,455 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 01:17:27,455 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 01:17:27,456 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 01:17:27,456 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 01:17:27,457 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 01:17:27,457 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 01:17:27,457 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 01:17:27,457 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 01:17:27,457 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 01:17:27,458 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 01:17:27,458 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 01:17:27,458 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 01:17:27,458 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 01:17:27,458 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 01:17:27,458 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 01:17:27,458 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 01:17:27,458 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 01:17:27,458 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 01:17:27,458 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 01:17:27,458 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 01:17:27,459 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 01:17:27,459 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 01:17:27,460 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 01:17:27,461 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 01:17:27,461 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 01:17:27,462 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 01:17:27,462 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 01:17:27,462 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 01:17:27,462 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 01:17:27,462 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 01:17:27,462 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 01:17:27,462 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 01:17:27,462 n: bert.pooler.dense.weight
2022-06-12 01:17:27,462 n: bert.pooler.dense.bias
2022-06-12 01:17:27,462 n: classifier.weight
2022-06-12 01:17:27,462 n: classifier.bias
2022-06-12 01:17:27,463 n: fit_denses.0.weight
2022-06-12 01:17:27,463 n: fit_denses.0.bias
2022-06-12 01:17:27,463 n: fit_denses.1.weight
2022-06-12 01:17:27,463 n: fit_denses.1.bias
2022-06-12 01:17:27,463 n: fit_denses.2.weight
2022-06-12 01:17:27,463 n: fit_denses.2.bias
2022-06-12 01:17:27,463 n: fit_denses.3.weight
2022-06-12 01:17:27,463 n: fit_denses.3.bias
2022-06-12 01:17:27,463 n: fit_denses.4.weight
2022-06-12 01:17:27,463 n: fit_denses.4.bias
2022-06-12 01:17:27,463 n: fit_denses.5.weight
2022-06-12 01:17:27,463 n: fit_denses.5.bias
2022-06-12 01:17:27,463 n: fit_denses.6.weight
2022-06-12 01:17:27,463 n: fit_denses.6.bias
2022-06-12 01:17:27,463 Total parameters: 72468738
2022-06-12 01:17:29,941 ***** Running evaluation *****
2022-06-12 01:17:29,941   Epoch = 0 iter 9 step
2022-06-12 01:17:29,941   Num examples = 277
2022-06-12 01:17:29,941   Batch size = 32
2022-06-12 01:17:30,179 ***** Eval results *****
2022-06-12 01:17:30,179   acc = 0.4729241877256318
2022-06-12 01:17:30,179   cls_loss = 0.346287296877967
2022-06-12 01:17:30,179   eval_loss = 0.6961779329511855
2022-06-12 01:17:30,179   global_step = 9
2022-06-12 01:17:30,179   loss = 0.346287296877967
2022-06-12 01:17:30,179 ***** Save model *****
2022-06-12 01:17:33,074 ***** Running evaluation *****
2022-06-12 01:17:33,075   Epoch = 0 iter 19 step
2022-06-12 01:17:33,075   Num examples = 277
2022-06-12 01:17:33,075   Batch size = 32
2022-06-12 01:17:33,314 ***** Eval results *****
2022-06-12 01:17:33,314   acc = 0.4729241877256318
2022-06-12 01:17:33,314   cls_loss = 0.3452158366378985
2022-06-12 01:17:33,315   eval_loss = 0.7082627813021342
2022-06-12 01:17:33,315   global_step = 19
2022-06-12 01:17:33,315   loss = 0.3452158366378985
2022-06-12 01:17:35,773 ***** Running evaluation *****
2022-06-12 01:17:35,773   Epoch = 0 iter 29 step
2022-06-12 01:17:35,773   Num examples = 277
2022-06-12 01:17:35,773   Batch size = 32
2022-06-12 01:17:36,012 ***** Eval results *****
2022-06-12 01:17:36,012   acc = 0.5740072202166066
2022-06-12 01:17:36,012   cls_loss = 0.34543240275876275
2022-06-12 01:17:36,012   eval_loss = 0.685810923576355
2022-06-12 01:17:36,013   global_step = 29
2022-06-12 01:17:36,013   loss = 0.34543240275876275
2022-06-12 01:17:36,013 ***** Save model *****
2022-06-12 01:17:38,994 ***** Running evaluation *****
2022-06-12 01:17:38,994   Epoch = 0 iter 39 step
2022-06-12 01:17:38,994   Num examples = 277
2022-06-12 01:17:38,994   Batch size = 32
2022-06-12 01:17:39,233 ***** Eval results *****
2022-06-12 01:17:39,233   acc = 0.555956678700361
2022-06-12 01:17:39,233   cls_loss = 0.34456686713756657
2022-06-12 01:17:39,234   eval_loss = 0.6806966132587857
2022-06-12 01:17:39,234   global_step = 39
2022-06-12 01:17:39,234   loss = 0.34456686713756657
2022-06-12 01:17:41,697 ***** Running evaluation *****
2022-06-12 01:17:41,697   Epoch = 0 iter 49 step
2022-06-12 01:17:41,697   Num examples = 277
2022-06-12 01:17:41,698   Batch size = 32
2022-06-12 01:17:41,940 ***** Eval results *****
2022-06-12 01:17:41,940   acc = 0.5523465703971119
2022-06-12 01:17:41,940   cls_loss = 0.34425406918233753
2022-06-12 01:17:41,940   eval_loss = 0.6843099064297147
2022-06-12 01:17:41,940   global_step = 49
2022-06-12 01:17:41,940   loss = 0.34425406918233753
2022-06-12 01:17:44,400 ***** Running evaluation *****
2022-06-12 01:17:44,400   Epoch = 0 iter 59 step
2022-06-12 01:17:44,400   Num examples = 277
2022-06-12 01:17:44,400   Batch size = 32
2022-06-12 01:17:44,640 ***** Eval results *****
2022-06-12 01:17:44,640   acc = 0.555956678700361
2022-06-12 01:17:44,640   cls_loss = 0.343934859764778
2022-06-12 01:17:44,640   eval_loss = 0.6973560916052924
2022-06-12 01:17:44,640   global_step = 59
2022-06-12 01:17:44,640   loss = 0.343934859764778
2022-06-12 01:17:47,105 ***** Running evaluation *****
2022-06-12 01:17:47,106   Epoch = 0 iter 69 step
2022-06-12 01:17:47,106   Num examples = 277
2022-06-12 01:17:47,106   Batch size = 32
2022-06-12 01:17:47,345 ***** Eval results *****
2022-06-12 01:17:47,346   acc = 0.516245487364621
2022-06-12 01:17:47,346   cls_loss = 0.3430677851041158
2022-06-12 01:17:47,346   eval_loss = 0.7110661864280701
2022-06-12 01:17:47,346   global_step = 69
2022-06-12 01:17:47,346   loss = 0.3430677851041158
2022-06-12 01:17:49,803 ***** Running evaluation *****
2022-06-12 01:17:49,803   Epoch = 1 iter 79 step
2022-06-12 01:17:49,803   Num examples = 277
2022-06-12 01:17:49,803   Batch size = 32
2022-06-12 01:17:50,042 ***** Eval results *****
2022-06-12 01:17:50,042   acc = 0.4981949458483754
2022-06-12 01:17:50,042   cls_loss = 0.31146101653575897
2022-06-12 01:17:50,042   eval_loss = 0.8035527931319343
2022-06-12 01:17:50,042   global_step = 79
2022-06-12 01:17:50,042   loss = 0.31146101653575897
2022-06-12 01:17:52,485 ***** Running evaluation *****
2022-06-12 01:17:52,485   Epoch = 1 iter 89 step
2022-06-12 01:17:52,485   Num examples = 277
2022-06-12 01:17:52,485   Batch size = 32
2022-06-12 01:17:52,723 ***** Eval results *****
2022-06-12 01:17:52,724   acc = 0.5270758122743683
2022-06-12 01:17:52,724   cls_loss = 0.34330809613068897
2022-06-12 01:17:52,724   eval_loss = 0.6916642718844943
2022-06-12 01:17:52,724   global_step = 89
2022-06-12 01:17:52,724   loss = 0.34330809613068897
2022-06-12 01:17:55,201 ***** Running evaluation *****
2022-06-12 01:17:55,202   Epoch = 1 iter 99 step
2022-06-12 01:17:55,202   Num examples = 277
2022-06-12 01:17:55,202   Batch size = 32
2022-06-12 01:17:55,440 ***** Eval results *****
2022-06-12 01:17:55,441   acc = 0.4729241877256318
2022-06-12 01:17:55,441   cls_loss = 0.3448302718726071
2022-06-12 01:17:55,441   eval_loss = 0.6993004216088189
2022-06-12 01:17:55,441   global_step = 99
2022-06-12 01:17:55,441   loss = 0.3448302718726071
2022-06-12 01:17:57,903 ***** Running evaluation *****
2022-06-12 01:17:57,904   Epoch = 1 iter 109 step
2022-06-12 01:17:57,904   Num examples = 277
2022-06-12 01:17:57,904   Batch size = 32
2022-06-12 01:17:58,142 ***** Eval results *****
2022-06-12 01:17:58,142   acc = 0.5667870036101083
2022-06-12 01:17:58,142   cls_loss = 0.34355608839541674
2022-06-12 01:17:58,142   eval_loss = 0.6833094822035896
2022-06-12 01:17:58,142   global_step = 109
2022-06-12 01:17:58,143   loss = 0.34355608839541674
2022-06-12 01:18:00,599 ***** Running evaluation *****
2022-06-12 01:18:00,600   Epoch = 1 iter 119 step
2022-06-12 01:18:00,600   Num examples = 277
2022-06-12 01:18:00,600   Batch size = 32
2022-06-12 01:18:00,844 ***** Eval results *****
2022-06-12 01:18:00,844   acc = 0.5595667870036101
2022-06-12 01:18:00,844   cls_loss = 0.342319056391716
2022-06-12 01:18:00,844   eval_loss = 0.6844501892725626
2022-06-12 01:18:00,845   global_step = 119
2022-06-12 01:18:00,845   loss = 0.342319056391716
2022-06-12 01:18:03,312 ***** Running evaluation *****
2022-06-12 01:18:03,313   Epoch = 1 iter 129 step
2022-06-12 01:18:03,313   Num examples = 277
2022-06-12 01:18:03,313   Batch size = 32
2022-06-12 01:18:03,551 ***** Eval results *****
2022-06-12 01:18:03,551   acc = 0.5379061371841155
2022-06-12 01:18:03,552   cls_loss = 0.3408863968574084
2022-06-12 01:18:03,552   eval_loss = 0.7120920287238227
2022-06-12 01:18:03,552   global_step = 129
2022-06-12 01:18:03,552   loss = 0.3408863968574084
2022-06-12 01:18:06,034 ***** Running evaluation *****
2022-06-12 01:18:06,035   Epoch = 1 iter 139 step
2022-06-12 01:18:06,035   Num examples = 277
2022-06-12 01:18:06,035   Batch size = 32
2022-06-12 01:18:06,274 ***** Eval results *****
2022-06-12 01:18:06,274   acc = 0.5703971119133574
2022-06-12 01:18:06,274   cls_loss = 0.3396120633809797
2022-06-12 01:18:06,274   eval_loss = 0.6799943314658271
2022-06-12 01:18:06,274   global_step = 139
2022-06-12 01:18:06,274   loss = 0.3396120633809797
2022-06-12 01:18:08,757 ***** Running evaluation *****
2022-06-12 01:18:08,758   Epoch = 1 iter 149 step
2022-06-12 01:18:08,758   Num examples = 277
2022-06-12 01:18:08,758   Batch size = 32
2022-06-12 01:18:08,996 ***** Eval results *****
2022-06-12 01:18:08,996   acc = 0.5848375451263538
2022-06-12 01:18:08,997   cls_loss = 0.33838634482688373
2022-06-12 01:18:08,997   eval_loss = 0.6708379718992445
2022-06-12 01:18:08,997   global_step = 149
2022-06-12 01:18:08,997   loss = 0.33838634482688373
2022-06-12 01:18:08,997 ***** Save model *****
2022-06-12 01:18:11,990 ***** Running evaluation *****
2022-06-12 01:18:11,991   Epoch = 2 iter 159 step
2022-06-12 01:18:11,991   Num examples = 277
2022-06-12 01:18:11,991   Batch size = 32
2022-06-12 01:18:12,230 ***** Eval results *****
2022-06-12 01:18:12,230   acc = 0.5487364620938628
2022-06-12 01:18:12,230   cls_loss = 0.3079417824745178
2022-06-12 01:18:12,230   eval_loss = 0.7013109260135226
2022-06-12 01:18:12,230   global_step = 159
2022-06-12 01:18:12,230   loss = 0.3079417824745178
2022-06-12 01:18:14,696 ***** Running evaluation *****
2022-06-12 01:18:14,697   Epoch = 2 iter 169 step
2022-06-12 01:18:14,697   Num examples = 277
2022-06-12 01:18:14,697   Batch size = 32
2022-06-12 01:18:14,936 ***** Eval results *****
2022-06-12 01:18:14,936   acc = 0.5270758122743683
2022-06-12 01:18:14,936   cls_loss = 0.3069767852624257
2022-06-12 01:18:14,936   eval_loss = 0.7658350865046183
2022-06-12 01:18:14,937   global_step = 169
2022-06-12 01:18:14,937   loss = 0.3069767852624257
2022-06-12 01:18:17,435 ***** Running evaluation *****
2022-06-12 01:18:17,436   Epoch = 2 iter 179 step
2022-06-12 01:18:17,436   Num examples = 277
2022-06-12 01:18:17,436   Batch size = 32
2022-06-12 01:18:17,675 ***** Eval results *****
2022-06-12 01:18:17,675   acc = 0.5848375451263538
2022-06-12 01:18:17,675   cls_loss = 0.30991810202598574
2022-06-12 01:18:17,675   eval_loss = 0.7150389817025926
2022-06-12 01:18:17,675   global_step = 179
2022-06-12 01:18:17,675   loss = 0.30991810202598574
2022-06-12 01:18:20,149 ***** Running evaluation *****
2022-06-12 01:18:20,150   Epoch = 2 iter 189 step
2022-06-12 01:18:20,150   Num examples = 277
2022-06-12 01:18:20,150   Batch size = 32
2022-06-12 01:18:20,388 ***** Eval results *****
2022-06-12 01:18:20,388   acc = 0.5126353790613718
2022-06-12 01:18:20,388   cls_loss = 0.31704129236085077
2022-06-12 01:18:20,388   eval_loss = 0.7064258522457547
2022-06-12 01:18:20,388   global_step = 189
2022-06-12 01:18:20,388   loss = 0.31704129236085077
2022-06-12 01:18:22,851 ***** Running evaluation *****
2022-06-12 01:18:22,851   Epoch = 2 iter 199 step
2022-06-12 01:18:22,851   Num examples = 277
2022-06-12 01:18:22,851   Batch size = 32
2022-06-12 01:18:23,090 ***** Eval results *****
2022-06-12 01:18:23,090   acc = 0.5703971119133574
2022-06-12 01:18:23,090   cls_loss = 0.3197718580563863
2022-06-12 01:18:23,090   eval_loss = 0.6831475893656412
2022-06-12 01:18:23,090   global_step = 199
2022-06-12 01:18:23,090   loss = 0.3197718580563863
2022-06-12 01:18:25,560 ***** Running evaluation *****
2022-06-12 01:18:25,560   Epoch = 2 iter 209 step
2022-06-12 01:18:25,560   Num examples = 277
2022-06-12 01:18:25,560   Batch size = 32
2022-06-12 01:18:25,799 ***** Eval results *****
2022-06-12 01:18:25,799   acc = 0.5667870036101083
2022-06-12 01:18:25,800   cls_loss = 0.3182717101140456
2022-06-12 01:18:25,800   eval_loss = 0.714906997150845
2022-06-12 01:18:25,800   global_step = 209
2022-06-12 01:18:25,800   loss = 0.3182717101140456
2022-06-12 01:18:28,265 ***** Running evaluation *****
2022-06-12 01:18:28,266   Epoch = 2 iter 219 step
2022-06-12 01:18:28,266   Num examples = 277
2022-06-12 01:18:28,266   Batch size = 32
2022-06-12 01:18:28,505 ***** Eval results *****
2022-06-12 01:18:28,505   acc = 0.555956678700361
2022-06-12 01:18:28,505   cls_loss = 0.315739316206712
2022-06-12 01:18:28,505   eval_loss = 0.7581212388144599
2022-06-12 01:18:28,505   global_step = 219
2022-06-12 01:18:28,505   loss = 0.315739316206712
2022-06-12 01:18:30,967 ***** Running evaluation *****
2022-06-12 01:18:30,968   Epoch = 2 iter 229 step
2022-06-12 01:18:30,968   Num examples = 277
2022-06-12 01:18:30,968   Batch size = 32
2022-06-12 01:18:31,206 ***** Eval results *****
2022-06-12 01:18:31,206   acc = 0.5595667870036101
2022-06-12 01:18:31,206   cls_loss = 0.31389488697052004
2022-06-12 01:18:31,206   eval_loss = 0.7326792478561401
2022-06-12 01:18:31,206   global_step = 229
2022-06-12 01:18:31,206   loss = 0.31389488697052004
2022-06-12 01:18:33,669 ***** Running evaluation *****
2022-06-12 01:18:33,670   Epoch = 3 iter 239 step
2022-06-12 01:18:33,670   Num examples = 277
2022-06-12 01:18:33,670   Batch size = 32
2022-06-12 01:18:33,908 ***** Eval results *****
2022-06-12 01:18:33,908   acc = 0.555956678700361
2022-06-12 01:18:33,908   cls_loss = 0.2672230862081051
2022-06-12 01:18:33,908   eval_loss = 0.7839826610353258
2022-06-12 01:18:33,908   global_step = 239
2022-06-12 01:18:33,908   loss = 0.2672230862081051
2022-06-12 01:18:36,390 ***** Running evaluation *****
2022-06-12 01:18:36,390   Epoch = 3 iter 249 step
2022-06-12 01:18:36,390   Num examples = 277
2022-06-12 01:18:36,390   Batch size = 32
2022-06-12 01:18:36,628 ***** Eval results *****
2022-06-12 01:18:36,628   acc = 0.5595667870036101
2022-06-12 01:18:36,628   cls_loss = 0.27117108222511077
2022-06-12 01:18:36,628   eval_loss = 0.8138449523184035
2022-06-12 01:18:36,628   global_step = 249
2022-06-12 01:18:36,628   loss = 0.27117108222511077
2022-06-12 01:18:39,098 ***** Running evaluation *****
2022-06-12 01:18:39,099   Epoch = 3 iter 259 step
2022-06-12 01:18:39,099   Num examples = 277
2022-06-12 01:18:39,099   Batch size = 32
2022-06-12 01:18:39,340 ***** Eval results *****
2022-06-12 01:18:39,341   acc = 0.5306859205776173
2022-06-12 01:18:39,341   cls_loss = 0.2694540534700666
2022-06-12 01:18:39,341   eval_loss = 0.7775199082162645
2022-06-12 01:18:39,341   global_step = 259
2022-06-12 01:18:39,341   loss = 0.2694540534700666
2022-06-12 01:18:41,812 ***** Running evaluation *****
2022-06-12 01:18:41,812   Epoch = 3 iter 269 step
2022-06-12 01:18:41,812   Num examples = 277
2022-06-12 01:18:41,812   Batch size = 32
2022-06-12 01:18:42,051 ***** Eval results *****
2022-06-12 01:18:42,051   acc = 0.5451263537906137
2022-06-12 01:18:42,051   cls_loss = 0.2678557755915742
2022-06-12 01:18:42,051   eval_loss = 0.7923567626211379
2022-06-12 01:18:42,051   global_step = 269
2022-06-12 01:18:42,051   loss = 0.2678557755915742
2022-06-12 01:18:44,544 ***** Running evaluation *****
2022-06-12 01:18:44,545   Epoch = 3 iter 279 step
2022-06-12 01:18:44,545   Num examples = 277
2022-06-12 01:18:44,545   Batch size = 32
2022-06-12 01:18:44,784 ***** Eval results *****
2022-06-12 01:18:44,784   acc = 0.5884476534296029
2022-06-12 01:18:44,784   cls_loss = 0.2689624298363924
2022-06-12 01:18:44,784   eval_loss = 0.7546337644259135
2022-06-12 01:18:44,784   global_step = 279
2022-06-12 01:18:44,784   loss = 0.2689624298363924
2022-06-12 01:18:44,785 ***** Save model *****
2022-06-12 01:18:47,742 ***** Running evaluation *****
2022-06-12 01:18:47,743   Epoch = 3 iter 289 step
2022-06-12 01:18:47,743   Num examples = 277
2022-06-12 01:18:47,743   Batch size = 32
2022-06-12 01:18:47,982 ***** Eval results *****
2022-06-12 01:18:47,982   acc = 0.5379061371841155
2022-06-12 01:18:47,982   cls_loss = 0.26797834603950893
2022-06-12 01:18:47,982   eval_loss = 0.7437196307712131
2022-06-12 01:18:47,982   global_step = 289
2022-06-12 01:18:47,982   loss = 0.26797834603950893
2022-06-12 01:18:50,453 ***** Running evaluation *****
2022-06-12 01:18:50,453   Epoch = 3 iter 299 step
2022-06-12 01:18:50,453   Num examples = 277
2022-06-12 01:18:50,454   Batch size = 32
2022-06-12 01:18:50,695 ***** Eval results *****
2022-06-12 01:18:50,695   acc = 0.5487364620938628
2022-06-12 01:18:50,695   cls_loss = 0.2721983978853506
2022-06-12 01:18:50,695   eval_loss = 0.7690853542751737
2022-06-12 01:18:50,695   global_step = 299
2022-06-12 01:18:50,695   loss = 0.2721983978853506
2022-06-12 01:18:53,186 ***** Running evaluation *****
2022-06-12 01:18:53,186   Epoch = 4 iter 309 step
2022-06-12 01:18:53,186   Num examples = 277
2022-06-12 01:18:53,186   Batch size = 32
2022-06-12 01:18:53,426 ***** Eval results *****
2022-06-12 01:18:53,427   acc = 0.5306859205776173
2022-06-12 01:18:53,427   cls_loss = 0.26304125785827637
2022-06-12 01:18:53,427   eval_loss = 0.8018536435233222
2022-06-12 01:18:53,427   global_step = 309
2022-06-12 01:18:53,427   loss = 0.26304125785827637
2022-06-12 01:18:55,897 ***** Running evaluation *****
2022-06-12 01:18:55,898   Epoch = 4 iter 319 step
2022-06-12 01:18:55,898   Num examples = 277
2022-06-12 01:18:55,898   Batch size = 32
2022-06-12 01:18:56,136 ***** Eval results *****
2022-06-12 01:18:56,136   acc = 0.5379061371841155
2022-06-12 01:18:56,136   cls_loss = 0.24574822593819012
2022-06-12 01:18:56,136   eval_loss = 0.8226355248027377
2022-06-12 01:18:56,137   global_step = 319
2022-06-12 01:18:56,137   loss = 0.24574822593819012
2022-06-12 01:18:58,626 ***** Running evaluation *****
2022-06-12 01:18:58,627   Epoch = 4 iter 329 step
2022-06-12 01:18:58,627   Num examples = 277
2022-06-12 01:18:58,627   Batch size = 32
2022-06-12 01:18:58,866 ***** Eval results *****
2022-06-12 01:18:58,866   acc = 0.5487364620938628
2022-06-12 01:18:58,866   cls_loss = 0.24753843460764205
2022-06-12 01:18:58,866   eval_loss = 0.7978655762142606
2022-06-12 01:18:58,866   global_step = 329
2022-06-12 01:18:58,866   loss = 0.24753843460764205
2022-06-12 01:19:01,337 ***** Running evaluation *****
2022-06-12 01:19:01,338   Epoch = 4 iter 339 step
2022-06-12 01:19:01,338   Num examples = 277
2022-06-12 01:19:01,338   Batch size = 32
2022-06-12 01:19:01,576 ***** Eval results *****
2022-06-12 01:19:01,576   acc = 0.5306859205776173
2022-06-12 01:19:01,576   cls_loss = 0.24606178124104777
2022-06-12 01:19:01,576   eval_loss = 0.8316854569647048
2022-06-12 01:19:01,576   global_step = 339
2022-06-12 01:19:01,576   loss = 0.24606178124104777
2022-06-12 01:19:04,055 ***** Running evaluation *****
2022-06-12 01:19:04,056   Epoch = 4 iter 349 step
2022-06-12 01:19:04,056   Num examples = 277
2022-06-12 01:19:04,056   Batch size = 32
2022-06-12 01:19:04,294 ***** Eval results *****
2022-06-12 01:19:04,294   acc = 0.5306859205776173
2022-06-12 01:19:04,294   cls_loss = 0.24489509441503665
2022-06-12 01:19:04,294   eval_loss = 0.8567648861143324
2022-06-12 01:19:04,294   global_step = 349
2022-06-12 01:19:04,294   loss = 0.24489509441503665
2022-06-12 01:19:06,775 ***** Running evaluation *****
2022-06-12 01:19:06,776   Epoch = 4 iter 359 step
2022-06-12 01:19:06,776   Num examples = 277
2022-06-12 01:19:06,776   Batch size = 32
2022-06-12 01:19:07,014 ***** Eval results *****
2022-06-12 01:19:07,014   acc = 0.5306859205776173
2022-06-12 01:19:07,014   cls_loss = 0.24357888949852363
2022-06-12 01:19:07,014   eval_loss = 0.8111448155509101
2022-06-12 01:19:07,014   global_step = 359
2022-06-12 01:19:07,014   loss = 0.24357888949852363
2022-06-12 01:19:09,486 ***** Running evaluation *****
2022-06-12 01:19:09,487   Epoch = 4 iter 369 step
2022-06-12 01:19:09,487   Num examples = 277
2022-06-12 01:19:09,487   Batch size = 32
2022-06-12 01:19:09,725 ***** Eval results *****
2022-06-12 01:19:09,725   acc = 0.5415162454873647
2022-06-12 01:19:09,725   cls_loss = 0.24268500140455904
2022-06-12 01:19:09,725   eval_loss = 0.8603036999702454
2022-06-12 01:19:09,725   global_step = 369
2022-06-12 01:19:09,725   loss = 0.24268500140455904
2022-06-12 01:19:12,202 ***** Running evaluation *****
2022-06-12 01:19:12,203   Epoch = 4 iter 379 step
2022-06-12 01:19:12,203   Num examples = 277
2022-06-12 01:19:12,203   Batch size = 32
2022-06-12 01:19:12,441 ***** Eval results *****
2022-06-12 01:19:12,441   acc = 0.5270758122743683
2022-06-12 01:19:12,442   cls_loss = 0.24277230026856275
2022-06-12 01:19:12,442   eval_loss = 0.8031579388512505
2022-06-12 01:19:12,442   global_step = 379
2022-06-12 01:19:12,442   loss = 0.24277230026856275
2022-06-12 01:19:14,912 ***** Running evaluation *****
2022-06-12 01:19:14,912   Epoch = 5 iter 389 step
2022-06-12 01:19:14,912   Num examples = 277
2022-06-12 01:19:14,912   Batch size = 32
2022-06-12 01:19:15,155 ***** Eval results *****
2022-06-12 01:19:15,155   acc = 0.5415162454873647
2022-06-12 01:19:15,155   cls_loss = 0.23942101001739502
2022-06-12 01:19:15,155   eval_loss = 0.8719265262285868
2022-06-12 01:19:15,155   global_step = 389
2022-06-12 01:19:15,155   loss = 0.23942101001739502
2022-06-12 01:19:17,634 ***** Running evaluation *****
2022-06-12 01:19:17,635   Epoch = 5 iter 399 step
2022-06-12 01:19:17,635   Num examples = 277
2022-06-12 01:19:17,635   Batch size = 32
2022-06-12 01:19:17,873 ***** Eval results *****
2022-06-12 01:19:17,874   acc = 0.5451263537906137
2022-06-12 01:19:17,874   cls_loss = 0.22947247326374054
2022-06-12 01:19:17,874   eval_loss = 0.8877337177594503
2022-06-12 01:19:17,874   global_step = 399
2022-06-12 01:19:17,874   loss = 0.22947247326374054
2022-06-12 01:19:20,330 ***** Running evaluation *****
2022-06-12 01:19:20,331   Epoch = 5 iter 409 step
2022-06-12 01:19:20,331   Num examples = 277
2022-06-12 01:19:20,331   Batch size = 32
2022-06-12 01:19:20,569 ***** Eval results *****
2022-06-12 01:19:20,569   acc = 0.5451263537906137
2022-06-12 01:19:20,569   cls_loss = 0.22683854711552462
2022-06-12 01:19:20,569   eval_loss = 0.8825988305939568
2022-06-12 01:19:20,569   global_step = 409
2022-06-12 01:19:20,569   loss = 0.22683854711552462
2022-06-12 01:19:23,036 ***** Running evaluation *****
2022-06-12 01:19:23,036   Epoch = 5 iter 419 step
2022-06-12 01:19:23,037   Num examples = 277
2022-06-12 01:19:23,037   Batch size = 32
2022-06-12 01:19:23,275 ***** Eval results *****
2022-06-12 01:19:23,275   acc = 0.5342960288808665
2022-06-12 01:19:23,275   cls_loss = 0.22563964040840373
2022-06-12 01:19:23,275   eval_loss = 0.8472185466024611
2022-06-12 01:19:23,275   global_step = 419
2022-06-12 01:19:23,275   loss = 0.22563964040840373
2022-06-12 01:19:25,753 ***** Running evaluation *****
2022-06-12 01:19:25,754   Epoch = 5 iter 429 step
2022-06-12 01:19:25,754   Num examples = 277
2022-06-12 01:19:25,754   Batch size = 32
2022-06-12 01:19:25,993 ***** Eval results *****
2022-06-12 01:19:25,993   acc = 0.5451263537906137
2022-06-12 01:19:25,993   cls_loss = 0.22533280199224298
2022-06-12 01:19:25,993   eval_loss = 0.8719090157084994
2022-06-12 01:19:25,993   global_step = 429
2022-06-12 01:19:25,993   loss = 0.22533280199224298
2022-06-12 01:19:28,468 ***** Running evaluation *****
2022-06-12 01:19:28,469   Epoch = 5 iter 439 step
2022-06-12 01:19:28,469   Num examples = 277
2022-06-12 01:19:28,469   Batch size = 32
2022-06-12 01:19:28,708 ***** Eval results *****
2022-06-12 01:19:28,709   acc = 0.5234657039711191
2022-06-12 01:19:28,709   cls_loss = 0.22615887031511026
2022-06-12 01:19:28,709   eval_loss = 0.8914337025748359
2022-06-12 01:19:28,709   global_step = 439
2022-06-12 01:19:28,709   loss = 0.22615887031511026
2022-06-12 01:19:31,189 ***** Running evaluation *****
2022-06-12 01:19:31,190   Epoch = 5 iter 449 step
2022-06-12 01:19:31,190   Num examples = 277
2022-06-12 01:19:31,190   Batch size = 32
2022-06-12 01:19:31,428 ***** Eval results *****
2022-06-12 01:19:31,429   acc = 0.5234657039711191
2022-06-12 01:19:31,429   cls_loss = 0.22641182132065296
2022-06-12 01:19:31,429   eval_loss = 0.9154920180638632
2022-06-12 01:19:31,429   global_step = 449
2022-06-12 01:19:31,429   loss = 0.22641182132065296
2022-06-12 01:19:31,571 ***** Running evaluation *****
2022-06-12 01:19:31,571   Epoch = 1 iter 6499 step
2022-06-12 01:19:31,571   Num examples = 5463
2022-06-12 01:19:31,572   Batch size = 32
2022-06-12 01:19:31,573 ***** Eval results *****
2022-06-12 01:19:31,573   att_loss = 4.493790297129845
2022-06-12 01:19:31,573   global_step = 6499
2022-06-12 01:19:31,573   loss = 5.502325097301031
2022-06-12 01:19:31,573   rep_loss = 1.0085348016123408
2022-06-12 01:19:31,573 ***** Save model *****
2022-06-12 01:19:33,904 ***** Running evaluation *****
2022-06-12 01:19:33,905   Epoch = 5 iter 459 step
2022-06-12 01:19:33,905   Num examples = 277
2022-06-12 01:19:33,905   Batch size = 32
2022-06-12 01:19:34,144 ***** Eval results *****
2022-06-12 01:19:34,144   acc = 0.516245487364621
2022-06-12 01:19:34,144   cls_loss = 0.22697438078152166
2022-06-12 01:19:34,144   eval_loss = 0.9040180577172173
2022-06-12 01:19:34,144   global_step = 459
2022-06-12 01:19:34,144   loss = 0.22697438078152166
2022-06-12 01:19:36,616 ***** Running evaluation *****
2022-06-12 01:19:36,617   Epoch = 6 iter 469 step
2022-06-12 01:19:36,617   Num examples = 277
2022-06-12 01:19:36,617   Batch size = 32
2022-06-12 01:19:36,856 ***** Eval results *****
2022-06-12 01:19:36,856   acc = 0.51985559566787
2022-06-12 01:19:36,856   cls_loss = 0.21315376673425948
2022-06-12 01:19:36,856   eval_loss = 0.9074417683813307
2022-06-12 01:19:36,856   global_step = 469
2022-06-12 01:19:36,856   loss = 0.21315376673425948
2022-06-12 01:19:39,319 ***** Running evaluation *****
2022-06-12 01:19:39,320   Epoch = 6 iter 479 step
2022-06-12 01:19:39,320   Num examples = 277
2022-06-12 01:19:39,320   Batch size = 32
2022-06-12 01:19:39,560 ***** Eval results *****
2022-06-12 01:19:39,561   acc = 0.5126353790613718
2022-06-12 01:19:39,561   cls_loss = 0.21858460762921503
2022-06-12 01:19:39,561   eval_loss = 0.9251926276418898
2022-06-12 01:19:39,561   global_step = 479
2022-06-12 01:19:39,561   loss = 0.21858460762921503
2022-06-12 01:19:42,031 ***** Running evaluation *****
2022-06-12 01:19:42,031   Epoch = 6 iter 489 step
2022-06-12 01:19:42,031   Num examples = 277
2022-06-12 01:19:42,031   Batch size = 32
2022-06-12 01:19:42,270 ***** Eval results *****
2022-06-12 01:19:42,270   acc = 0.5090252707581228
2022-06-12 01:19:42,270   cls_loss = 0.21575020474416237
2022-06-12 01:19:42,271   eval_loss = 0.9213628570238749
2022-06-12 01:19:42,271   global_step = 489
2022-06-12 01:19:42,271   loss = 0.21575020474416237
2022-06-12 01:19:44,730 ***** Running evaluation *****
2022-06-12 01:19:44,730   Epoch = 6 iter 499 step
2022-06-12 01:19:44,730   Num examples = 277
2022-06-12 01:19:44,730   Batch size = 32
2022-06-12 01:19:44,968 ***** Eval results *****
2022-06-12 01:19:44,968   acc = 0.516245487364621
2022-06-12 01:19:44,968   cls_loss = 0.21677575642998154
2022-06-12 01:19:44,969   eval_loss = 0.9700241949823167
2022-06-12 01:19:44,969   global_step = 499
2022-06-12 01:19:44,969   loss = 0.21677575642998154
2022-06-12 01:19:47,435 ***** Running evaluation *****
2022-06-12 01:19:47,436   Epoch = 6 iter 509 step
2022-06-12 01:19:47,436   Num examples = 277
2022-06-12 01:19:47,436   Batch size = 32
2022-06-12 01:19:47,674 ***** Eval results *****
2022-06-12 01:19:47,674   acc = 0.5270758122743683
2022-06-12 01:19:47,675   cls_loss = 0.21612807505942405
2022-06-12 01:19:47,675   eval_loss = 0.9282264908154806
2022-06-12 01:19:47,675   global_step = 509
2022-06-12 01:19:47,675   loss = 0.21612807505942405
2022-06-12 01:19:50,138 ***** Running evaluation *****
2022-06-12 01:19:50,139   Epoch = 6 iter 519 step
2022-06-12 01:19:50,139   Num examples = 277
2022-06-12 01:19:50,139   Batch size = 32
2022-06-12 01:19:50,377 ***** Eval results *****
2022-06-12 01:19:50,377   acc = 0.5270758122743683
2022-06-12 01:19:50,377   cls_loss = 0.2164704768281234
2022-06-12 01:19:50,377   eval_loss = 0.9214802119466994
2022-06-12 01:19:50,377   global_step = 519
2022-06-12 01:19:50,377   loss = 0.2164704768281234
2022-06-12 01:19:52,844 ***** Running evaluation *****
2022-06-12 01:19:52,844   Epoch = 6 iter 529 step
2022-06-12 01:19:52,845   Num examples = 277
2022-06-12 01:19:52,845   Batch size = 32
2022-06-12 01:19:53,084 ***** Eval results *****
2022-06-12 01:19:53,084   acc = 0.51985559566787
2022-06-12 01:19:53,084   cls_loss = 0.21519377894365965
2022-06-12 01:19:53,084   eval_loss = 0.9699796239535013
2022-06-12 01:19:53,084   global_step = 529
2022-06-12 01:19:53,084   loss = 0.21519377894365965
2022-06-12 01:19:55,560 ***** Running evaluation *****
2022-06-12 01:19:55,561   Epoch = 6 iter 539 step
2022-06-12 01:19:55,561   Num examples = 277
2022-06-12 01:19:55,561   Batch size = 32
2022-06-12 01:19:55,799 ***** Eval results *****
2022-06-12 01:19:55,799   acc = 0.5126353790613718
2022-06-12 01:19:55,800   cls_loss = 0.21522791741730332
2022-06-12 01:19:55,800   eval_loss = 0.990230123202006
2022-06-12 01:19:55,800   global_step = 539
2022-06-12 01:19:55,800   loss = 0.21522791741730332
2022-06-12 01:19:58,265 ***** Running evaluation *****
2022-06-12 01:19:58,265   Epoch = 7 iter 549 step
2022-06-12 01:19:58,265   Num examples = 277
2022-06-12 01:19:58,265   Batch size = 32
2022-06-12 01:19:58,504 ***** Eval results *****
2022-06-12 01:19:58,504   acc = 0.516245487364621
2022-06-12 01:19:58,505   cls_loss = 0.20674657225608825
2022-06-12 01:19:58,505   eval_loss = 0.9555429352654351
2022-06-12 01:19:58,505   global_step = 549
2022-06-12 01:19:58,505   loss = 0.20674657225608825
2022-06-12 01:20:00,980 ***** Running evaluation *****
2022-06-12 01:20:00,980   Epoch = 7 iter 559 step
2022-06-12 01:20:00,981   Num examples = 277
2022-06-12 01:20:00,981   Batch size = 32
2022-06-12 01:20:01,220 ***** Eval results *****
2022-06-12 01:20:01,220   acc = 0.5306859205776173
2022-06-12 01:20:01,220   cls_loss = 0.20870920941233634
2022-06-12 01:20:01,220   eval_loss = 0.9221932888031006
2022-06-12 01:20:01,220   global_step = 559
2022-06-12 01:20:01,220   loss = 0.20870920941233634
2022-06-12 01:20:03,698 ***** Running evaluation *****
2022-06-12 01:20:03,699   Epoch = 7 iter 569 step
2022-06-12 01:20:03,699   Num examples = 277
2022-06-12 01:20:03,699   Batch size = 32
2022-06-12 01:20:03,938 ***** Eval results *****
2022-06-12 01:20:03,938   acc = 0.5342960288808665
2022-06-12 01:20:03,938   cls_loss = 0.20922789921363194
2022-06-12 01:20:03,938   eval_loss = 0.9093330038918389
2022-06-12 01:20:03,938   global_step = 569
2022-06-12 01:20:03,938   loss = 0.20922789921363194
2022-06-12 01:20:06,403 ***** Running evaluation *****
2022-06-12 01:20:06,404   Epoch = 7 iter 579 step
2022-06-12 01:20:06,404   Num examples = 277
2022-06-12 01:20:06,404   Batch size = 32
2022-06-12 01:20:06,643 ***** Eval results *****
2022-06-12 01:20:06,643   acc = 0.5270758122743683
2022-06-12 01:20:06,643   cls_loss = 0.20986324176192284
2022-06-12 01:20:06,644   eval_loss = 0.9666561086972555
2022-06-12 01:20:06,644   global_step = 579
2022-06-12 01:20:06,644   loss = 0.20986324176192284
2022-06-12 01:20:09,101 ***** Running evaluation *****
2022-06-12 01:20:09,102   Epoch = 7 iter 589 step
2022-06-12 01:20:09,102   Num examples = 277
2022-06-12 01:20:09,102   Batch size = 32
2022-06-12 01:20:09,340 ***** Eval results *****
2022-06-12 01:20:09,340   acc = 0.5270758122743683
2022-06-12 01:20:09,341   cls_loss = 0.210053391456604
2022-06-12 01:20:09,341   eval_loss = 0.9375475313928392
2022-06-12 01:20:09,341   global_step = 589
2022-06-12 01:20:09,341   loss = 0.210053391456604
2022-06-12 01:20:11,808 ***** Running evaluation *****
2022-06-12 01:20:11,809   Epoch = 7 iter 599 step
2022-06-12 01:20:11,809   Num examples = 277
2022-06-12 01:20:11,809   Batch size = 32
2022-06-12 01:20:12,047 ***** Eval results *****
2022-06-12 01:20:12,047   acc = 0.5126353790613718
2022-06-12 01:20:12,047   cls_loss = 0.21039118667443593
2022-06-12 01:20:12,047   eval_loss = 0.9894021219677396
2022-06-12 01:20:12,047   global_step = 599
2022-06-12 01:20:12,047   loss = 0.21039118667443593
2022-06-12 01:20:14,513 ***** Running evaluation *****
2022-06-12 01:20:14,513   Epoch = 7 iter 609 step
2022-06-12 01:20:14,513   Num examples = 277
2022-06-12 01:20:14,514   Batch size = 32
2022-06-12 01:20:14,752 ***** Eval results *****
2022-06-12 01:20:14,752   acc = 0.5234657039711191
2022-06-12 01:20:14,753   cls_loss = 0.21132662636893138
2022-06-12 01:20:14,753   eval_loss = 0.9433457321590848
2022-06-12 01:20:14,753   global_step = 609
2022-06-12 01:20:14,753   loss = 0.21132662636893138
2022-06-12 01:20:17,230 ***** Running evaluation *****
2022-06-12 01:20:17,231   Epoch = 8 iter 619 step
2022-06-12 01:20:17,231   Num examples = 277
2022-06-12 01:20:17,231   Batch size = 32
2022-06-12 01:20:17,470 ***** Eval results *****
2022-06-12 01:20:17,470   acc = 0.5270758122743683
2022-06-12 01:20:17,470   cls_loss = 0.21698265274365744
2022-06-12 01:20:17,470   eval_loss = 0.9534349110391405
2022-06-12 01:20:17,470   global_step = 619
2022-06-12 01:20:17,470   loss = 0.21698265274365744
2022-06-12 01:20:19,936 ***** Running evaluation *****
2022-06-12 01:20:19,936   Epoch = 8 iter 629 step
2022-06-12 01:20:19,936   Num examples = 277
2022-06-12 01:20:19,936   Batch size = 32
2022-06-12 01:20:20,174 ***** Eval results *****
2022-06-12 01:20:20,175   acc = 0.5270758122743683
2022-06-12 01:20:20,175   cls_loss = 0.21364320929233843
2022-06-12 01:20:20,175   eval_loss = 0.9219083057509528
2022-06-12 01:20:20,175   global_step = 629
2022-06-12 01:20:20,175   loss = 0.21364320929233843
2022-06-12 01:20:22,648 ***** Running evaluation *****
2022-06-12 01:20:22,648   Epoch = 8 iter 639 step
2022-06-12 01:20:22,648   Num examples = 277
2022-06-12 01:20:22,648   Batch size = 32
2022-06-12 01:20:22,887 ***** Eval results *****
2022-06-12 01:20:22,887   acc = 0.5234657039711191
2022-06-12 01:20:22,887   cls_loss = 0.21042566843654797
2022-06-12 01:20:22,887   eval_loss = 0.9333893259366354
2022-06-12 01:20:22,887   global_step = 639
2022-06-12 01:20:22,888   loss = 0.21042566843654797
2022-06-12 01:20:25,352 ***** Running evaluation *****
2022-06-12 01:20:25,352   Epoch = 8 iter 649 step
2022-06-12 01:20:25,353   Num examples = 277
2022-06-12 01:20:25,353   Batch size = 32
2022-06-12 01:20:25,591 ***** Eval results *****
2022-06-12 01:20:25,591   acc = 0.5270758122743683
2022-06-12 01:20:25,591   cls_loss = 0.20869845242211313
2022-06-12 01:20:25,591   eval_loss = 0.970635387632582
2022-06-12 01:20:25,591   global_step = 649
2022-06-12 01:20:25,591   loss = 0.20869845242211313
2022-06-12 01:20:28,058 ***** Running evaluation *****
2022-06-12 01:20:28,059   Epoch = 8 iter 659 step
2022-06-12 01:20:28,059   Num examples = 277
2022-06-12 01:20:28,059   Batch size = 32
2022-06-12 01:20:28,296 ***** Eval results *****
2022-06-12 01:20:28,297   acc = 0.5306859205776173
2022-06-12 01:20:28,297   cls_loss = 0.2079876734073772
2022-06-12 01:20:28,297   eval_loss = 0.9736553430557251
2022-06-12 01:20:28,297   global_step = 659
2022-06-12 01:20:28,297   loss = 0.2079876734073772
2022-06-12 01:20:30,758 ***** Running evaluation *****
2022-06-12 01:20:30,759   Epoch = 8 iter 669 step
2022-06-12 01:20:30,759   Num examples = 277
2022-06-12 01:20:30,759   Batch size = 32
2022-06-12 01:20:30,997 ***** Eval results *****
2022-06-12 01:20:30,997   acc = 0.5270758122743683
2022-06-12 01:20:30,997   cls_loss = 0.20856564365467936
2022-06-12 01:20:30,997   eval_loss = 0.9405250814225938
2022-06-12 01:20:30,997   global_step = 669
2022-06-12 01:20:30,997   loss = 0.20856564365467936
2022-06-12 01:20:33,463 ***** Running evaluation *****
2022-06-12 01:20:33,463   Epoch = 8 iter 679 step
2022-06-12 01:20:33,463   Num examples = 277
2022-06-12 01:20:33,463   Batch size = 32
2022-06-12 01:20:33,701 ***** Eval results *****
2022-06-12 01:20:33,702   acc = 0.5234657039711191
2022-06-12 01:20:33,702   cls_loss = 0.2093958244437263
2022-06-12 01:20:33,702   eval_loss = 0.9159660140673319
2022-06-12 01:20:33,702   global_step = 679
2022-06-12 01:20:33,702   loss = 0.2093958244437263
2022-06-12 01:20:36,181 ***** Running evaluation *****
2022-06-12 01:20:36,181   Epoch = 8 iter 689 step
2022-06-12 01:20:36,181   Num examples = 277
2022-06-12 01:20:36,181   Batch size = 32
2022-06-12 01:20:36,419 ***** Eval results *****
2022-06-12 01:20:36,419   acc = 0.5379061371841155
2022-06-12 01:20:36,419   cls_loss = 0.20933171290240876
2022-06-12 01:20:36,419   eval_loss = 0.9588941269450717
2022-06-12 01:20:36,419   global_step = 689
2022-06-12 01:20:36,419   loss = 0.20933171290240876
2022-06-12 01:20:38,909 ***** Running evaluation *****
2022-06-12 01:20:38,910   Epoch = 9 iter 699 step
2022-06-12 01:20:38,910   Num examples = 277
2022-06-12 01:20:38,910   Batch size = 32
2022-06-12 01:20:39,148 ***** Eval results *****
2022-06-12 01:20:39,148   acc = 0.51985559566787
2022-06-12 01:20:39,148   cls_loss = 0.21091948946317038
2022-06-12 01:20:39,148   eval_loss = 0.9274707237879435
2022-06-12 01:20:39,148   global_step = 699
2022-06-12 01:20:39,148   loss = 0.21091948946317038
2022-06-12 01:20:41,622 ***** Running evaluation *****
2022-06-12 01:20:41,622   Epoch = 9 iter 709 step
2022-06-12 01:20:41,622   Num examples = 277
2022-06-12 01:20:41,622   Batch size = 32
2022-06-12 01:20:41,861 ***** Eval results *****
2022-06-12 01:20:41,861   acc = 0.5415162454873647
2022-06-12 01:20:41,861   cls_loss = 0.20881478302180767
2022-06-12 01:20:41,861   eval_loss = 0.9575315713882446
2022-06-12 01:20:41,862   global_step = 709
2022-06-12 01:20:41,862   loss = 0.20881478302180767
2022-06-12 01:20:44,336 ***** Running evaluation *****
2022-06-12 01:20:44,336   Epoch = 9 iter 719 step
2022-06-12 01:20:44,337   Num examples = 277
2022-06-12 01:20:44,337   Batch size = 32
2022-06-12 01:20:44,575 ***** Eval results *****
2022-06-12 01:20:44,575   acc = 0.5451263537906137
2022-06-12 01:20:44,575   cls_loss = 0.20944490283727646
2022-06-12 01:20:44,575   eval_loss = 0.974168684747484
2022-06-12 01:20:44,575   global_step = 719
2022-06-12 01:20:44,575   loss = 0.20944490283727646
2022-06-12 01:20:47,052 ***** Running evaluation *****
2022-06-12 01:20:47,052   Epoch = 9 iter 729 step
2022-06-12 01:20:47,052   Num examples = 277
2022-06-12 01:20:47,053   Batch size = 32
2022-06-12 01:20:47,291 ***** Eval results *****
2022-06-12 01:20:47,292   acc = 0.5415162454873647
2022-06-12 01:20:47,292   cls_loss = 0.20871487591001722
2022-06-12 01:20:47,292   eval_loss = 0.9525040984153748
2022-06-12 01:20:47,292   global_step = 729
2022-06-12 01:20:47,292   loss = 0.20871487591001722
2022-06-12 01:20:49,766 ***** Running evaluation *****
2022-06-12 01:20:49,767   Epoch = 9 iter 739 step
2022-06-12 01:20:49,767   Num examples = 277
2022-06-12 01:20:49,767   Batch size = 32
2022-06-12 01:20:50,005 ***** Eval results *****
2022-06-12 01:20:50,006   acc = 0.5342960288808665
2022-06-12 01:20:50,006   cls_loss = 0.20791363942882288
2022-06-12 01:20:50,006   eval_loss = 0.9415838122367859
2022-06-12 01:20:50,006   global_step = 739
2022-06-12 01:20:50,006   loss = 0.20791363942882288
2022-06-12 01:20:52,468 ***** Running evaluation *****
2022-06-12 01:20:52,468   Epoch = 9 iter 749 step
2022-06-12 01:20:52,468   Num examples = 277
2022-06-12 01:20:52,469   Batch size = 32
2022-06-12 01:20:52,708 ***** Eval results *****
2022-06-12 01:20:52,708   acc = 0.5415162454873647
2022-06-12 01:20:52,708   cls_loss = 0.2077589247907911
2022-06-12 01:20:52,708   eval_loss = 0.9449509448475308
2022-06-12 01:20:52,708   global_step = 749
2022-06-12 01:20:52,708   loss = 0.2077589247907911
2022-06-12 01:20:55,171 ***** Running evaluation *****
2022-06-12 01:20:55,171   Epoch = 9 iter 759 step
2022-06-12 01:20:55,171   Num examples = 277
2022-06-12 01:20:55,171   Batch size = 32
2022-06-12 01:20:55,410 ***** Eval results *****
2022-06-12 01:20:55,410   acc = 0.5342960288808665
2022-06-12 01:20:55,410   cls_loss = 0.20745840013930292
2022-06-12 01:20:55,411   eval_loss = 0.965867903497484
2022-06-12 01:20:55,411   global_step = 759
2022-06-12 01:20:55,411   loss = 0.20745840013930292
2022-06-12 01:20:57,883 ***** Running evaluation *****
2022-06-12 01:20:57,884   Epoch = 9 iter 769 step
2022-06-12 01:20:57,884   Num examples = 277
2022-06-12 01:20:57,884   Batch size = 32
2022-06-12 01:20:58,123 ***** Eval results *****
2022-06-12 01:20:58,123   acc = 0.5379061371841155
2022-06-12 01:20:58,123   cls_loss = 0.20722117431853948
2022-06-12 01:20:58,123   eval_loss = 0.9413159489631653
2022-06-12 01:20:58,123   global_step = 769
2022-06-12 01:20:58,123   loss = 0.20722117431853948
2022-06-12 01:21:00,596 ***** Running evaluation *****
2022-06-12 01:21:00,597   Epoch = 10 iter 779 step
2022-06-12 01:21:00,597   Num examples = 277
2022-06-12 01:21:00,597   Batch size = 32
2022-06-12 01:21:00,835 ***** Eval results *****
2022-06-12 01:21:00,835   acc = 0.5342960288808665
2022-06-12 01:21:00,835   cls_loss = 0.20722823176119062
2022-06-12 01:21:00,835   eval_loss = 0.9359095096588135
2022-06-12 01:21:00,835   global_step = 779
2022-06-12 01:21:00,835   loss = 0.20722823176119062
2022-06-12 01:21:03,307 ***** Running evaluation *****
2022-06-12 01:21:03,307   Epoch = 10 iter 789 step
2022-06-12 01:21:03,307   Num examples = 277
2022-06-12 01:21:03,307   Batch size = 32
2022-06-12 01:21:03,546 ***** Eval results *****
2022-06-12 01:21:03,546   acc = 0.5306859205776173
2022-06-12 01:21:03,546   cls_loss = 0.20503967451421837
2022-06-12 01:21:03,546   eval_loss = 0.9555409285757277
2022-06-12 01:21:03,546   global_step = 789
2022-06-12 01:21:03,546   loss = 0.20503967451421837
2022-06-12 01:21:06,012 ***** Running evaluation *****
2022-06-12 01:21:06,012   Epoch = 10 iter 799 step
2022-06-12 01:21:06,012   Num examples = 277
2022-06-12 01:21:06,013   Batch size = 32
2022-06-12 01:21:06,250 ***** Eval results *****
2022-06-12 01:21:06,250   acc = 0.5270758122743683
2022-06-12 01:21:06,251   cls_loss = 0.20506416518112708
2022-06-12 01:21:06,251   eval_loss = 0.9709717101520963
2022-06-12 01:21:06,251   global_step = 799
2022-06-12 01:21:06,251   loss = 0.20506416518112708
2022-06-12 01:21:08,722 ***** Running evaluation *****
2022-06-12 01:21:08,723   Epoch = 10 iter 809 step
2022-06-12 01:21:08,723   Num examples = 277
2022-06-12 01:21:08,723   Batch size = 32
2022-06-12 01:21:08,961 ***** Eval results *****
2022-06-12 01:21:08,962   acc = 0.5306859205776173
2022-06-12 01:21:08,962   cls_loss = 0.20427116102133042
2022-06-12 01:21:08,962   eval_loss = 0.9582050111558702
2022-06-12 01:21:08,962   global_step = 809
2022-06-12 01:21:08,962   loss = 0.20427116102133042
2022-06-12 01:21:11,430 ***** Running evaluation *****
2022-06-12 01:21:11,430   Epoch = 10 iter 819 step
2022-06-12 01:21:11,430   Num examples = 277
2022-06-12 01:21:11,430   Batch size = 32
2022-06-12 01:21:11,669 ***** Eval results *****
2022-06-12 01:21:11,669   acc = 0.5415162454873647
2022-06-12 01:21:11,669   cls_loss = 0.20488364660010047
2022-06-12 01:21:11,669   eval_loss = 1.0028702086872525
2022-06-12 01:21:11,669   global_step = 819
2022-06-12 01:21:11,669   loss = 0.20488364660010047
2022-06-12 01:21:14,135 ***** Running evaluation *****
2022-06-12 01:21:14,135   Epoch = 10 iter 829 step
2022-06-12 01:21:14,135   Num examples = 277
2022-06-12 01:21:14,135   Batch size = 32
2022-06-12 01:21:14,373 ***** Eval results *****
2022-06-12 01:21:14,373   acc = 0.5451263537906137
2022-06-12 01:21:14,374   cls_loss = 0.20686845910751214
2022-06-12 01:21:14,374   eval_loss = 0.9718061950471666
2022-06-12 01:21:14,374   global_step = 829
2022-06-12 01:21:14,374   loss = 0.20686845910751214
2022-06-12 01:21:16,849 ***** Running evaluation *****
2022-06-12 01:21:16,849   Epoch = 10 iter 839 step
2022-06-12 01:21:16,850   Num examples = 277
2022-06-12 01:21:16,850   Batch size = 32
2022-06-12 01:21:17,088 ***** Eval results *****
2022-06-12 01:21:17,088   acc = 0.5379061371841155
2022-06-12 01:21:17,088   cls_loss = 0.2068916919870653
2022-06-12 01:21:17,088   eval_loss = 0.9249577787187364
2022-06-12 01:21:17,088   global_step = 839
2022-06-12 01:21:17,088   loss = 0.2068916919870653
2022-06-12 01:21:19,559 ***** Running evaluation *****
2022-06-12 01:21:19,560   Epoch = 11 iter 849 step
2022-06-12 01:21:19,560   Num examples = 277
2022-06-12 01:21:19,560   Batch size = 32
2022-06-12 01:21:19,799 ***** Eval results *****
2022-06-12 01:21:19,799   acc = 0.5342960288808665
2022-06-12 01:21:19,799   cls_loss = 0.21069549024105072
2022-06-12 01:21:19,799   eval_loss = 0.9005809624989828
2022-06-12 01:21:19,799   global_step = 849
2022-06-12 01:21:19,799   loss = 0.21069549024105072
2022-06-12 01:21:22,271 ***** Running evaluation *****
2022-06-12 01:21:22,272   Epoch = 11 iter 859 step
2022-06-12 01:21:22,272   Num examples = 277
2022-06-12 01:21:22,272   Batch size = 32
2022-06-12 01:21:22,511 ***** Eval results *****
2022-06-12 01:21:22,511   acc = 0.5451263537906137
2022-06-12 01:21:22,511   cls_loss = 0.20975331589579582
2022-06-12 01:21:22,511   eval_loss = 0.9406015740500556
2022-06-12 01:21:22,511   global_step = 859
2022-06-12 01:21:22,511   loss = 0.20975331589579582
2022-06-12 01:21:24,997 ***** Running evaluation *****
2022-06-12 01:21:24,998   Epoch = 11 iter 869 step
2022-06-12 01:21:24,998   Num examples = 277
2022-06-12 01:21:24,998   Batch size = 32
2022-06-12 01:21:25,237 ***** Eval results *****
2022-06-12 01:21:25,238   acc = 0.5270758122743683
2022-06-12 01:21:25,238   cls_loss = 0.21297141909599304
2022-06-12 01:21:25,238   eval_loss = 0.9674467047055563
2022-06-12 01:21:25,238   global_step = 869
2022-06-12 01:21:25,238   loss = 0.21297141909599304
2022-06-12 01:21:27,724 ***** Running evaluation *****
2022-06-12 01:21:27,724   Epoch = 11 iter 879 step
2022-06-12 01:21:27,725   Num examples = 277
2022-06-12 01:21:27,725   Batch size = 32
2022-06-12 01:21:27,965 ***** Eval results *****
2022-06-12 01:21:27,965   acc = 0.5451263537906137
2022-06-12 01:21:27,965   cls_loss = 0.21084103314206004
2022-06-12 01:21:27,965   eval_loss = 0.941930267545912
2022-06-12 01:21:27,965   global_step = 879
2022-06-12 01:21:27,965   loss = 0.21084103314206004
2022-06-12 01:21:30,446 ***** Running evaluation *****
2022-06-12 01:21:30,447   Epoch = 11 iter 889 step
2022-06-12 01:21:30,447   Num examples = 277
2022-06-12 01:21:30,447   Batch size = 32
2022-06-12 01:21:30,688 ***** Eval results *****
2022-06-12 01:21:30,688   acc = 0.5415162454873647
2022-06-12 01:21:30,688   cls_loss = 0.2087997698358127
2022-06-12 01:21:30,688   eval_loss = 0.9152520497639974
2022-06-12 01:21:30,688   global_step = 889
2022-06-12 01:21:30,688   loss = 0.2087997698358127
2022-06-12 01:21:33,173 ***** Running evaluation *****
2022-06-12 01:21:33,174   Epoch = 11 iter 899 step
2022-06-12 01:21:33,174   Num examples = 277
2022-06-12 01:21:33,174   Batch size = 32
2022-06-12 01:21:33,415 ***** Eval results *****
2022-06-12 01:21:33,415   acc = 0.5523465703971119
2022-06-12 01:21:33,415   cls_loss = 0.2091696973030384
2022-06-12 01:21:33,416   eval_loss = 0.9293339120017158
2022-06-12 01:21:33,416   global_step = 899
2022-06-12 01:21:33,416   loss = 0.2091696973030384
2022-06-12 01:21:35,886 ***** Running evaluation *****
2022-06-12 01:21:35,886   Epoch = 11 iter 909 step
2022-06-12 01:21:35,886   Num examples = 277
2022-06-12 01:21:35,886   Batch size = 32
2022-06-12 01:21:36,126 ***** Eval results *****
2022-06-12 01:21:36,126   acc = 0.5451263537906137
2022-06-12 01:21:36,126   cls_loss = 0.2080987876942081
2022-06-12 01:21:36,126   eval_loss = 0.9658028615845574
2022-06-12 01:21:36,126   global_step = 909
2022-06-12 01:21:36,126   loss = 0.2080987876942081
2022-06-12 01:21:38,605 ***** Running evaluation *****
2022-06-12 01:21:38,606   Epoch = 11 iter 919 step
2022-06-12 01:21:38,606   Num examples = 277
2022-06-12 01:21:38,606   Batch size = 32
2022-06-12 01:21:38,848 ***** Eval results *****
2022-06-12 01:21:38,849   acc = 0.5415162454873647
2022-06-12 01:21:38,849   cls_loss = 0.20870059211221006
2022-06-12 01:21:38,849   eval_loss = 0.9373343785603842
2022-06-12 01:21:38,849   global_step = 919
2022-06-12 01:21:38,849   loss = 0.20870059211221006
2022-06-12 01:21:39,417 ***** Running evaluation *****
2022-06-12 01:21:39,419   Epoch = 2 iter 6999 step
2022-06-12 01:21:39,419   Num examples = 5463
2022-06-12 01:21:39,419   Batch size = 32
2022-06-12 01:21:39,420 ***** Eval results *****
2022-06-12 01:21:39,420   att_loss = 4.218339264787586
2022-06-12 01:21:39,420   global_step = 6999
2022-06-12 01:21:39,420   loss = 5.187555873104542
2022-06-12 01:21:39,420   rep_loss = 0.9692166116063958
2022-06-12 01:21:39,421 ***** Save model *****
2022-06-12 01:21:41,336 ***** Running evaluation *****
2022-06-12 01:21:41,336   Epoch = 12 iter 929 step
2022-06-12 01:21:41,336   Num examples = 277
2022-06-12 01:21:41,336   Batch size = 32
2022-06-12 01:21:41,575 ***** Eval results *****
2022-06-12 01:21:41,575   acc = 0.5487364620938628
2022-06-12 01:21:41,576   cls_loss = 0.21057369112968444
2022-06-12 01:21:41,576   eval_loss = 0.9201203717125787
2022-06-12 01:21:41,576   global_step = 929
2022-06-12 01:21:41,576   loss = 0.21057369112968444
2022-06-12 01:21:44,058 ***** Running evaluation *****
2022-06-12 01:21:44,059   Epoch = 12 iter 939 step
2022-06-12 01:21:44,059   Num examples = 277
2022-06-12 01:21:44,059   Batch size = 32
2022-06-12 01:21:44,298 ***** Eval results *****
2022-06-12 01:21:44,298   acc = 0.5487364620938628
2022-06-12 01:21:44,298   cls_loss = 0.2088954418897629
2022-06-12 01:21:44,298   eval_loss = 0.9270696838696798
2022-06-12 01:21:44,298   global_step = 939
2022-06-12 01:21:44,298   loss = 0.2088954418897629
2022-06-12 01:21:46,794 ***** Running evaluation *****
2022-06-12 01:21:46,794   Epoch = 12 iter 949 step
2022-06-12 01:21:46,794   Num examples = 277
2022-06-12 01:21:46,794   Batch size = 32
2022-06-12 01:21:47,036 ***** Eval results *****
2022-06-12 01:21:47,037   acc = 0.5451263537906137
2022-06-12 01:21:47,037   cls_loss = 0.20829738318920135
2022-06-12 01:21:47,037   eval_loss = 0.9340733620855544
2022-06-12 01:21:47,037   global_step = 949
2022-06-12 01:21:47,037   loss = 0.20829738318920135
2022-06-12 01:21:49,512 ***** Running evaluation *****
2022-06-12 01:21:49,512   Epoch = 12 iter 959 step
2022-06-12 01:21:49,512   Num examples = 277
2022-06-12 01:21:49,512   Batch size = 32
2022-06-12 01:21:49,751 ***** Eval results *****
2022-06-12 01:21:49,751   acc = 0.5523465703971119
2022-06-12 01:21:49,752   cls_loss = 0.2064553452389581
2022-06-12 01:21:49,752   eval_loss = 0.9275135331683688
2022-06-12 01:21:49,752   global_step = 959
2022-06-12 01:21:49,752   loss = 0.2064553452389581
2022-06-12 01:21:52,242 ***** Running evaluation *****
2022-06-12 01:21:52,242   Epoch = 12 iter 969 step
2022-06-12 01:21:52,242   Num examples = 277
2022-06-12 01:21:52,242   Batch size = 32
2022-06-12 01:21:52,481 ***** Eval results *****
2022-06-12 01:21:52,481   acc = 0.5487364620938628
2022-06-12 01:21:52,481   cls_loss = 0.2062320676114824
2022-06-12 01:21:52,481   eval_loss = 0.9241794082853529
2022-06-12 01:21:52,481   global_step = 969
2022-06-12 01:21:52,481   loss = 0.2062320676114824
2022-06-12 01:21:54,970 ***** Running evaluation *****
2022-06-12 01:21:54,971   Epoch = 12 iter 979 step
2022-06-12 01:21:54,971   Num examples = 277
2022-06-12 01:21:54,972   Batch size = 32
2022-06-12 01:21:55,214 ***** Eval results *****
2022-06-12 01:21:55,214   acc = 0.5451263537906137
2022-06-12 01:21:55,214   cls_loss = 0.20623435757376932
2022-06-12 01:21:55,214   eval_loss = 0.9345714317427741
2022-06-12 01:21:55,214   global_step = 979
2022-06-12 01:21:55,214   loss = 0.20623435757376932
2022-06-12 01:21:57,712 ***** Running evaluation *****
2022-06-12 01:21:57,712   Epoch = 12 iter 989 step
2022-06-12 01:21:57,712   Num examples = 277
2022-06-12 01:21:57,712   Batch size = 32
2022-06-12 01:21:57,952 ***** Eval results *****
2022-06-12 01:21:57,952   acc = 0.5415162454873647
2022-06-12 01:21:57,952   cls_loss = 0.20600330852545223
2022-06-12 01:21:57,953   eval_loss = 0.940773586432139
2022-06-12 01:21:57,953   global_step = 989
2022-06-12 01:21:57,953   loss = 0.20600330852545223
2022-06-12 01:22:00,449 ***** Running evaluation *****
2022-06-12 01:22:00,449   Epoch = 12 iter 999 step
2022-06-12 01:22:00,449   Num examples = 277
2022-06-12 01:22:00,450   Batch size = 32
2022-06-12 01:22:00,688 ***** Eval results *****
2022-06-12 01:22:00,689   acc = 0.5415162454873647
2022-06-12 01:22:00,689   cls_loss = 0.20621920386950174
2022-06-12 01:22:00,689   eval_loss = 0.9337253173192342
2022-06-12 01:22:00,689   global_step = 999
2022-06-12 01:22:00,689   loss = 0.20621920386950174
2022-06-12 01:22:03,176 ***** Running evaluation *****
2022-06-12 01:22:03,176   Epoch = 13 iter 1009 step
2022-06-12 01:22:03,176   Num examples = 277
2022-06-12 01:22:03,177   Batch size = 32
2022-06-12 01:22:03,415 ***** Eval results *****
2022-06-12 01:22:03,416   acc = 0.5415162454873647
2022-06-12 01:22:03,416   cls_loss = 0.20164186879992485
2022-06-12 01:22:03,416   eval_loss = 0.9362037711673312
2022-06-12 01:22:03,416   global_step = 1009
2022-06-12 01:22:03,416   loss = 0.20164186879992485
2022-06-12 01:22:05,908 ***** Running evaluation *****
2022-06-12 01:22:05,908   Epoch = 13 iter 1019 step
2022-06-12 01:22:05,908   Num examples = 277
2022-06-12 01:22:05,908   Batch size = 32
2022-06-12 01:22:06,147 ***** Eval results *****
2022-06-12 01:22:06,147   acc = 0.5415162454873647
2022-06-12 01:22:06,147   cls_loss = 0.20470937507020104
2022-06-12 01:22:06,147   eval_loss = 0.9385632276535034
2022-06-12 01:22:06,147   global_step = 1019
2022-06-12 01:22:06,147   loss = 0.20470937507020104
2022-06-12 01:22:08,611 ***** Running evaluation *****
2022-06-12 01:22:08,611   Epoch = 13 iter 1029 step
2022-06-12 01:22:08,611   Num examples = 277
2022-06-12 01:22:08,611   Batch size = 32
2022-06-12 01:22:08,850 ***** Eval results *****
2022-06-12 01:22:08,850   acc = 0.5451263537906137
2022-06-12 01:22:08,850   cls_loss = 0.20675849542021751
2022-06-12 01:22:08,850   eval_loss = 0.9384777347246805
2022-06-12 01:22:08,850   global_step = 1029
2022-06-12 01:22:08,850   loss = 0.20675849542021751
2022-06-12 01:22:11,334 ***** Running evaluation *****
2022-06-12 01:22:11,335   Epoch = 13 iter 1039 step
2022-06-12 01:22:11,335   Num examples = 277
2022-06-12 01:22:11,335   Batch size = 32
2022-06-12 01:22:11,574 ***** Eval results *****
2022-06-12 01:22:11,574   acc = 0.5415162454873647
2022-06-12 01:22:11,574   cls_loss = 0.20553625610313916
2022-06-12 01:22:11,574   eval_loss = 0.940797295835283
2022-06-12 01:22:11,574   global_step = 1039
2022-06-12 01:22:11,574   loss = 0.20553625610313916
2022-06-12 01:22:14,060 ***** Running evaluation *****
2022-06-12 01:22:14,060   Epoch = 13 iter 1049 step
2022-06-12 01:22:14,061   Num examples = 277
2022-06-12 01:22:14,061   Batch size = 32
2022-06-12 01:22:14,300 ***** Eval results *****
2022-06-12 01:22:14,300   acc = 0.5415162454873647
2022-06-12 01:22:14,300   cls_loss = 0.20494992099702358
2022-06-12 01:22:14,300   eval_loss = 0.9415298567877876
2022-06-12 01:22:14,301   global_step = 1049
2022-06-12 01:22:14,301   loss = 0.20494992099702358
2022-06-12 01:22:16,774 ***** Running evaluation *****
2022-06-12 01:22:16,775   Epoch = 13 iter 1059 step
2022-06-12 01:22:16,775   Num examples = 277
2022-06-12 01:22:16,775   Batch size = 32
2022-06-12 01:22:17,014 ***** Eval results *****
2022-06-12 01:22:17,014   acc = 0.5451263537906137
2022-06-12 01:22:17,014   cls_loss = 0.2052214186767052
2022-06-12 01:22:17,014   eval_loss = 0.9394967489772372
2022-06-12 01:22:17,014   global_step = 1059
2022-06-12 01:22:17,014   loss = 0.2052214186767052
2022-06-12 01:22:19,491 ***** Running evaluation *****
2022-06-12 01:22:19,492   Epoch = 13 iter 1069 step
2022-06-12 01:22:19,492   Num examples = 277
2022-06-12 01:22:19,492   Batch size = 32
2022-06-12 01:22:19,731 ***** Eval results *****
2022-06-12 01:22:19,731   acc = 0.5415162454873647
2022-06-12 01:22:19,731   cls_loss = 0.20502766594290733
2022-06-12 01:22:19,731   eval_loss = 0.9431225525008308
2022-06-12 01:22:19,731   global_step = 1069
2022-06-12 01:22:19,731   loss = 0.20502766594290733
2022-06-12 01:22:22,196 ***** Running evaluation *****
2022-06-12 01:22:22,197   Epoch = 14 iter 1079 step
2022-06-12 01:22:22,197   Num examples = 277
2022-06-12 01:22:22,197   Batch size = 32
2022-06-12 01:22:22,435 ***** Eval results *****
2022-06-12 01:22:22,435   acc = 0.5379061371841155
2022-06-12 01:22:22,435   cls_loss = 0.17154887318611145
2022-06-12 01:22:22,435   eval_loss = 0.9464325176344978
2022-06-12 01:22:22,435   global_step = 1079
2022-06-12 01:22:22,435   loss = 0.17154887318611145
2022-06-12 01:22:24,916 ***** Running evaluation *****
2022-06-12 01:22:24,916   Epoch = 14 iter 1089 step
2022-06-12 01:22:24,917   Num examples = 277
2022-06-12 01:22:24,917   Batch size = 32
2022-06-12 01:22:25,154 ***** Eval results *****
2022-06-12 01:22:25,154   acc = 0.5487364620938628
2022-06-12 01:22:25,154   cls_loss = 0.20833012597127396
2022-06-12 01:22:25,155   eval_loss = 0.934241877661811
2022-06-12 01:22:25,155   global_step = 1089
2022-06-12 01:22:25,155   loss = 0.20833012597127396
2022-06-12 01:22:27,624 ***** Running evaluation *****
2022-06-12 01:22:27,625   Epoch = 14 iter 1099 step
2022-06-12 01:22:27,625   Num examples = 277
2022-06-12 01:22:27,625   Batch size = 32
2022-06-12 01:22:27,866 ***** Eval results *****
2022-06-12 01:22:27,866   acc = 0.5451263537906137
2022-06-12 01:22:27,866   cls_loss = 0.20849808624812535
2022-06-12 01:22:27,866   eval_loss = 0.9250304765171475
2022-06-12 01:22:27,866   global_step = 1099
2022-06-12 01:22:27,867   loss = 0.20849808624812535
2022-06-12 01:22:30,343 ***** Running evaluation *****
2022-06-12 01:22:30,344   Epoch = 14 iter 1109 step
2022-06-12 01:22:30,344   Num examples = 277
2022-06-12 01:22:30,344   Batch size = 32
2022-06-12 01:22:30,582 ***** Eval results *****
2022-06-12 01:22:30,582   acc = 0.5487364620938628
2022-06-12 01:22:30,582   cls_loss = 0.20719347413509123
2022-06-12 01:22:30,582   eval_loss = 0.9216859406895108
2022-06-12 01:22:30,582   global_step = 1109
2022-06-12 01:22:30,582   loss = 0.20719347413509123
2022-06-12 01:22:33,054 ***** Running evaluation *****
2022-06-12 01:22:33,055   Epoch = 14 iter 1119 step
2022-06-12 01:22:33,055   Num examples = 277
2022-06-12 01:22:33,055   Batch size = 32
2022-06-12 01:22:33,294 ***** Eval results *****
2022-06-12 01:22:33,294   acc = 0.5487364620938628
2022-06-12 01:22:33,294   cls_loss = 0.20717943586954257
2022-06-12 01:22:33,294   eval_loss = 0.9224234422047933
2022-06-12 01:22:33,295   global_step = 1119
2022-06-12 01:22:33,295   loss = 0.20717943586954257
2022-06-12 01:22:35,762 ***** Running evaluation *****
2022-06-12 01:22:35,763   Epoch = 14 iter 1129 step
2022-06-12 01:22:35,763   Num examples = 277
2022-06-12 01:22:35,763   Batch size = 32
2022-06-12 01:22:36,001 ***** Eval results *****
2022-06-12 01:22:36,001   acc = 0.5451263537906137
2022-06-12 01:22:36,001   cls_loss = 0.20716260169066636
2022-06-12 01:22:36,001   eval_loss = 0.9244315889146593
2022-06-12 01:22:36,001   global_step = 1129
2022-06-12 01:22:36,001   loss = 0.20716260169066636
2022-06-12 01:22:38,472 ***** Running evaluation *****
2022-06-12 01:22:38,472   Epoch = 14 iter 1139 step
2022-06-12 01:22:38,472   Num examples = 277
2022-06-12 01:22:38,472   Batch size = 32
2022-06-12 01:22:38,711 ***** Eval results *****
2022-06-12 01:22:38,711   acc = 0.5451263537906137
2022-06-12 01:22:38,711   cls_loss = 0.20611741015168486
2022-06-12 01:22:38,711   eval_loss = 0.9278022050857544
2022-06-12 01:22:38,712   global_step = 1139
2022-06-12 01:22:38,712   loss = 0.20611741015168486
2022-06-12 01:22:41,177 ***** Running evaluation *****
2022-06-12 01:22:41,177   Epoch = 14 iter 1149 step
2022-06-12 01:22:41,177   Num examples = 277
2022-06-12 01:22:41,178   Batch size = 32
2022-06-12 01:22:41,416 ***** Eval results *****
2022-06-12 01:22:41,416   acc = 0.5451263537906137
2022-06-12 01:22:41,416   cls_loss = 0.2057076478508157
2022-06-12 01:22:41,416   eval_loss = 0.9296298093265958
2022-06-12 01:22:41,416   global_step = 1149
2022-06-12 01:22:41,416   loss = 0.2057076478508157
2022-06-12 01:22:42,904 **************S*************
task_name = rte
best_metirc = 0.5884476534296029
**************E*************

2022-06-12 01:22:42,918 Task finish! 
2022-06-12 01:22:42,919 Task cost 5.53269435 minutes, i.e. 0.09221157555555556 hours. 
2022-06-12 01:22:44,974 Task start! 
2022-06-12 01:22:44,994 device: cuda n_gpu: 1
2022-06-12 01:22:44,995 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/CoLA', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=64, max_seq_length=128, no_cuda=False, num_train_epochs=50, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/cola/on_original_data', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='cola', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/cola/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/cola/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 01:22:45,038 Writing example 0 of 8551
2022-06-12 01:22:45,038 *** Example ***
2022-06-12 01:22:45,038 guid: train-0
2022-06-12 01:22:45,038 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2022-06-12 01:22:45,038 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:22:45,038 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:22:45,039 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:22:45,039 label: 1
2022-06-12 01:22:45,039 label_id: 1
2022-06-12 01:22:46,401 Writing example 0 of 1043
2022-06-12 01:22:46,401 *** Example ***
2022-06-12 01:22:46,401 guid: dev-0
2022-06-12 01:22:46,401 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2022-06-12 01:22:46,401 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:22:46,402 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:22:46,402 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 01:22:46,402 label: 1
2022-06-12 01:22:46,402 label_id: 1
2022-06-12 01:22:46,566 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 01:22:52,236 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/cola/on_original_data/pytorch_model.bin
2022-06-12 01:22:53,000 loading model...
2022-06-12 01:22:53,176 done!
2022-06-12 01:22:53,177 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.dense_fit.weight', 'bert.dense_fit.bias']
2022-06-12 01:22:56,371 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 01:22:57,482 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-12 01:22:57,661 loading model...
2022-06-12 01:22:57,699 done!
2022-06-12 01:22:57,699 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-12 01:22:57,699 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-12 01:22:59,116 ***** Running training *****
2022-06-12 01:22:59,132   Num examples = 8551
2022-06-12 01:22:59,148   Batch size = 32
2022-06-12 01:22:59,163   Num steps = 13350
2022-06-12 01:22:59,179 n: bert.embeddings.word_embeddings.weight
2022-06-12 01:22:59,186 n: bert.embeddings.position_embeddings.weight
2022-06-12 01:22:59,186 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 01:22:59,186 n: bert.embeddings.LayerNorm.weight
2022-06-12 01:22:59,186 n: bert.embeddings.LayerNorm.bias
2022-06-12 01:22:59,186 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 01:22:59,186 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 01:22:59,186 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 01:22:59,186 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 01:22:59,186 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 01:22:59,186 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 01:22:59,186 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 01:22:59,186 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 01:22:59,187 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 01:22:59,187 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 01:22:59,187 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 01:22:59,187 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 01:22:59,187 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 01:22:59,187 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 01:22:59,187 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 01:22:59,187 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 01:22:59,187 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 01:22:59,187 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 01:22:59,187 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 01:22:59,187 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 01:22:59,187 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 01:22:59,187 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 01:22:59,187 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 01:22:59,187 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 01:22:59,188 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 01:22:59,188 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 01:22:59,188 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 01:22:59,188 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 01:22:59,188 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 01:22:59,188 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 01:22:59,188 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 01:22:59,188 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 01:22:59,188 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 01:22:59,188 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 01:22:59,188 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 01:22:59,188 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 01:22:59,188 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 01:22:59,189 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 01:22:59,189 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 01:22:59,189 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 01:22:59,190 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 01:22:59,191 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 01:22:59,191 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 01:22:59,192 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 01:22:59,193 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 01:22:59,193 n: bert.pooler.dense.weight
2022-06-12 01:22:59,193 n: bert.pooler.dense.bias
2022-06-12 01:22:59,193 n: classifier.weight
2022-06-12 01:22:59,193 n: classifier.bias
2022-06-12 01:22:59,193 n: fit_denses.0.weight
2022-06-12 01:22:59,193 n: fit_denses.0.bias
2022-06-12 01:22:59,193 n: fit_denses.1.weight
2022-06-12 01:22:59,193 n: fit_denses.1.bias
2022-06-12 01:22:59,193 n: fit_denses.2.weight
2022-06-12 01:22:59,193 n: fit_denses.2.bias
2022-06-12 01:22:59,193 n: fit_denses.3.weight
2022-06-12 01:22:59,193 n: fit_denses.3.bias
2022-06-12 01:22:59,194 n: fit_denses.4.weight
2022-06-12 01:22:59,194 n: fit_denses.4.bias
2022-06-12 01:22:59,194 n: fit_denses.5.weight
2022-06-12 01:22:59,194 n: fit_denses.5.bias
2022-06-12 01:22:59,194 n: fit_denses.6.weight
2022-06-12 01:22:59,194 n: fit_denses.6.bias
2022-06-12 01:22:59,194 Total parameters: 72468738
2022-06-12 01:23:04,243 ***** Running evaluation *****
2022-06-12 01:23:04,244   Epoch = 0 iter 19 step
2022-06-12 01:23:04,244   Num examples = 1043
2022-06-12 01:23:04,244   Batch size = 32
2022-06-12 01:23:04,247 ***** Eval results *****
2022-06-12 01:23:04,247   att_loss = 1.576892382220218
2022-06-12 01:23:04,247   global_step = 19
2022-06-12 01:23:04,247   loss = 3.700702667236328
2022-06-12 01:23:04,247   rep_loss = 2.123810278741937
2022-06-12 01:23:04,247 ***** Save model *****
2022-06-12 01:23:10,099 ***** Running evaluation *****
2022-06-12 01:23:10,100   Epoch = 0 iter 39 step
2022-06-12 01:23:10,100   Num examples = 1043
2022-06-12 01:23:10,100   Batch size = 32
2022-06-12 01:23:10,101 ***** Eval results *****
2022-06-12 01:23:10,101   att_loss = 1.3654440626119957
2022-06-12 01:23:10,102   global_step = 39
2022-06-12 01:23:10,102   loss = 3.171654040996845
2022-06-12 01:23:10,102   rep_loss = 1.8062099799131737
2022-06-12 01:23:10,102 ***** Save model *****
2022-06-12 01:23:15,872 ***** Running evaluation *****
2022-06-12 01:23:15,873   Epoch = 0 iter 59 step
2022-06-12 01:23:15,873   Num examples = 1043
2022-06-12 01:23:15,873   Batch size = 32
2022-06-12 01:23:15,874 ***** Eval results *****
2022-06-12 01:23:15,874   att_loss = 1.276045438596758
2022-06-12 01:23:15,874   global_step = 59
2022-06-12 01:23:15,874   loss = 2.949849811650939
2022-06-12 01:23:15,874   rep_loss = 1.6738043740644293
2022-06-12 01:23:15,874 ***** Save model *****
2022-06-12 01:23:21,540 ***** Running evaluation *****
2022-06-12 01:23:21,541   Epoch = 0 iter 79 step
2022-06-12 01:23:21,541   Num examples = 1043
2022-06-12 01:23:21,541   Batch size = 32
2022-06-12 01:23:21,542 ***** Eval results *****
2022-06-12 01:23:21,542   att_loss = 1.2136157668089564
2022-06-12 01:23:21,542   global_step = 79
2022-06-12 01:23:21,543   loss = 2.8135572777518743
2022-06-12 01:23:21,543   rep_loss = 1.599941507170472
2022-06-12 01:23:21,543 ***** Save model *****
2022-06-12 01:23:27,205 ***** Running evaluation *****
2022-06-12 01:23:27,205   Epoch = 0 iter 99 step
2022-06-12 01:23:27,206   Num examples = 1043
2022-06-12 01:23:27,206   Batch size = 32
2022-06-12 01:23:27,207 ***** Eval results *****
2022-06-12 01:23:27,207   att_loss = 1.1788287427690294
2022-06-12 01:23:27,207   global_step = 99
2022-06-12 01:23:27,207   loss = 2.730541537506412
2022-06-12 01:23:27,207   rep_loss = 1.5517127923291139
2022-06-12 01:23:27,207 ***** Save model *****
2022-06-12 01:23:32,830 ***** Running evaluation *****
2022-06-12 01:23:32,830   Epoch = 0 iter 119 step
2022-06-12 01:23:32,831   Num examples = 1043
2022-06-12 01:23:32,831   Batch size = 32
2022-06-12 01:23:32,832 ***** Eval results *****
2022-06-12 01:23:32,832   att_loss = 1.159924163537867
2022-06-12 01:23:32,832   global_step = 119
2022-06-12 01:23:32,832   loss = 2.677089192286259
2022-06-12 01:23:32,832   rep_loss = 1.517165028748392
2022-06-12 01:23:32,832 ***** Save model *****
2022-06-12 01:23:38,513 ***** Running evaluation *****
2022-06-12 01:23:38,514   Epoch = 0 iter 139 step
2022-06-12 01:23:38,514   Num examples = 1043
2022-06-12 01:23:38,514   Batch size = 32
2022-06-12 01:23:38,515 ***** Eval results *****
2022-06-12 01:23:38,516   att_loss = 1.1396116854475558
2022-06-12 01:23:38,516   global_step = 139
2022-06-12 01:23:38,516   loss = 2.6297840334528644
2022-06-12 01:23:38,516   rep_loss = 1.4901723467188774
2022-06-12 01:23:38,516 ***** Save model *****
2022-06-12 01:23:44,186 ***** Running evaluation *****
2022-06-12 01:23:44,187   Epoch = 0 iter 159 step
2022-06-12 01:23:44,187   Num examples = 1043
2022-06-12 01:23:44,187   Batch size = 32
2022-06-12 01:23:44,188 ***** Eval results *****
2022-06-12 01:23:44,188   att_loss = 1.12229977801161
2022-06-12 01:23:44,188   global_step = 159
2022-06-12 01:23:44,188   loss = 2.5911626006072424
2022-06-12 01:23:44,189   rep_loss = 1.4688628192217845
2022-06-12 01:23:44,189 ***** Save model *****
2022-06-12 01:23:47,464 ***** Running evaluation *****
2022-06-12 01:23:47,465   Epoch = 2 iter 7499 step
2022-06-12 01:23:47,465   Num examples = 5463
2022-06-12 01:23:47,465   Batch size = 32
2022-06-12 01:23:47,466 ***** Eval results *****
2022-06-12 01:23:47,466   att_loss = 4.226506553942109
2022-06-12 01:23:47,466   global_step = 7499
2022-06-12 01:23:47,466   loss = 5.193849560096156
2022-06-12 01:23:47,466   rep_loss = 0.9673430062791358
2022-06-12 01:23:47,467 ***** Save model *****
2022-06-12 01:23:49,826 ***** Running evaluation *****
2022-06-12 01:23:49,827   Epoch = 0 iter 179 step
2022-06-12 01:23:49,827   Num examples = 1043
2022-06-12 01:23:49,827   Batch size = 32
2022-06-12 01:23:49,828 ***** Eval results *****
2022-06-12 01:23:49,828   att_loss = 1.1039615383361305
2022-06-12 01:23:49,828   global_step = 179
2022-06-12 01:23:49,828   loss = 2.5543851506110675
2022-06-12 01:23:49,828   rep_loss = 1.4504236089450686
2022-06-12 01:23:49,829 ***** Save model *****
2022-06-12 01:23:55,495 ***** Running evaluation *****
2022-06-12 01:23:55,496   Epoch = 0 iter 199 step
2022-06-12 01:23:55,496   Num examples = 1043
2022-06-12 01:23:55,496   Batch size = 32
2022-06-12 01:23:55,498 ***** Eval results *****
2022-06-12 01:23:55,498   att_loss = 1.0893201609352725
2022-06-12 01:23:55,498   global_step = 199
2022-06-12 01:23:55,498   loss = 2.5237499601277875
2022-06-12 01:23:55,498   rep_loss = 1.434429797095869
2022-06-12 01:23:55,498 ***** Save model *****
2022-06-12 01:24:01,153 ***** Running evaluation *****
2022-06-12 01:24:01,153   Epoch = 0 iter 219 step
2022-06-12 01:24:01,154   Num examples = 1043
2022-06-12 01:24:01,154   Batch size = 32
2022-06-12 01:24:01,155 ***** Eval results *****
2022-06-12 01:24:01,155   att_loss = 1.0735529750993806
2022-06-12 01:24:01,155   global_step = 219
2022-06-12 01:24:01,155   loss = 2.493958803072368
2022-06-12 01:24:01,155   rep_loss = 1.420405825523481
2022-06-12 01:24:01,156 ***** Save model *****
2022-06-12 01:24:06,839 ***** Running evaluation *****
2022-06-12 01:24:06,840   Epoch = 0 iter 239 step
2022-06-12 01:24:06,840   Num examples = 1043
2022-06-12 01:24:06,840   Batch size = 32
2022-06-12 01:24:06,841 ***** Eval results *****
2022-06-12 01:24:06,841   att_loss = 1.0631870452828986
2022-06-12 01:24:06,841   global_step = 239
2022-06-12 01:24:06,841   loss = 2.470981343520735
2022-06-12 01:24:06,841   rep_loss = 1.4077942974896611
2022-06-12 01:24:06,841 ***** Save model *****
2022-06-12 01:24:12,541 ***** Running evaluation *****
2022-06-12 01:24:12,542   Epoch = 0 iter 259 step
2022-06-12 01:24:12,542   Num examples = 1043
2022-06-12 01:24:12,542   Batch size = 32
2022-06-12 01:24:12,543 ***** Eval results *****
2022-06-12 01:24:12,543   att_loss = 1.0578365240778242
2022-06-12 01:24:12,544   global_step = 259
2022-06-12 01:24:12,544   loss = 2.4563505069629565
2022-06-12 01:24:12,544   rep_loss = 1.3985139821947312
2022-06-12 01:24:12,544 ***** Save model *****
2022-06-12 01:24:18,229 ***** Running evaluation *****
2022-06-12 01:24:18,230   Epoch = 1 iter 279 step
2022-06-12 01:24:18,230   Num examples = 1043
2022-06-12 01:24:18,230   Batch size = 32
2022-06-12 01:24:18,231 ***** Eval results *****
2022-06-12 01:24:18,231   att_loss = 0.9023963610331217
2022-06-12 01:24:18,231   global_step = 279
2022-06-12 01:24:18,231   loss = 2.151929477850596
2022-06-12 01:24:18,231   rep_loss = 1.2495331466197968
2022-06-12 01:24:18,231 ***** Save model *****
2022-06-12 01:24:23,919 ***** Running evaluation *****
2022-06-12 01:24:23,920   Epoch = 1 iter 299 step
2022-06-12 01:24:23,920   Num examples = 1043
2022-06-12 01:24:23,920   Batch size = 32
2022-06-12 01:24:23,921 ***** Eval results *****
2022-06-12 01:24:23,922   att_loss = 0.8929176554083824
2022-06-12 01:24:23,922   global_step = 299
2022-06-12 01:24:23,922   loss = 2.1401310078799725
2022-06-12 01:24:23,922   rep_loss = 1.2472133748233318
2022-06-12 01:24:23,922 ***** Save model *****
2022-06-12 01:24:29,595 ***** Running evaluation *****
2022-06-12 01:24:29,595   Epoch = 1 iter 319 step
2022-06-12 01:24:29,595   Num examples = 1043
2022-06-12 01:24:29,595   Batch size = 32
2022-06-12 01:24:29,597 ***** Eval results *****
2022-06-12 01:24:29,597   att_loss = 0.8943762744848545
2022-06-12 01:24:29,597   global_step = 319
2022-06-12 01:24:29,597   loss = 2.138432427094533
2022-06-12 01:24:29,597   rep_loss = 1.2440561652183533
2022-06-12 01:24:29,598 ***** Save model *****
2022-06-12 01:24:35,280 ***** Running evaluation *****
2022-06-12 01:24:35,281   Epoch = 1 iter 339 step
2022-06-12 01:24:35,281   Num examples = 1043
2022-06-12 01:24:35,281   Batch size = 32
2022-06-12 01:24:35,282 ***** Eval results *****
2022-06-12 01:24:35,282   att_loss = 0.8926805175013013
2022-06-12 01:24:35,282   global_step = 339
2022-06-12 01:24:35,282   loss = 2.1372525029712253
2022-06-12 01:24:35,282   rep_loss = 1.2445720003710852
2022-06-12 01:24:35,283 ***** Save model *****
2022-06-12 01:24:40,925 ***** Running evaluation *****
2022-06-12 01:24:40,926   Epoch = 1 iter 359 step
2022-06-12 01:24:40,926   Num examples = 1043
2022-06-12 01:24:40,926   Batch size = 32
2022-06-12 01:24:40,927 ***** Eval results *****
2022-06-12 01:24:40,928   att_loss = 0.8874363724304282
2022-06-12 01:24:40,928   global_step = 359
2022-06-12 01:24:40,928   loss = 2.130152045384697
2022-06-12 01:24:40,928   rep_loss = 1.242715685263924
2022-06-12 01:24:40,928 ***** Save model *****
2022-06-12 01:24:46,643 ***** Running evaluation *****
2022-06-12 01:24:46,643   Epoch = 1 iter 379 step
2022-06-12 01:24:46,643   Num examples = 1043
2022-06-12 01:24:46,643   Batch size = 32
2022-06-12 01:24:46,645 ***** Eval results *****
2022-06-12 01:24:46,645   att_loss = 0.8897780520575387
2022-06-12 01:24:46,645   global_step = 379
2022-06-12 01:24:46,645   loss = 2.1321575716137886
2022-06-12 01:24:46,645   rep_loss = 1.2423795312643051
2022-06-12 01:24:46,645 ***** Save model *****
2022-06-12 01:24:52,287 ***** Running evaluation *****
2022-06-12 01:24:52,288   Epoch = 1 iter 399 step
2022-06-12 01:24:52,288   Num examples = 1043
2022-06-12 01:24:52,288   Batch size = 32
2022-06-12 01:24:52,289 ***** Eval results *****
2022-06-12 01:24:52,289   att_loss = 0.8928002620285208
2022-06-12 01:24:52,289   global_step = 399
2022-06-12 01:24:52,289   loss = 2.1338155368963876
2022-06-12 01:24:52,290   rep_loss = 1.2410152834473234
2022-06-12 01:24:52,290 ***** Save model *****
2022-06-12 01:24:58,030 ***** Running evaluation *****
2022-06-12 01:24:58,030   Epoch = 1 iter 419 step
2022-06-12 01:24:58,031   Num examples = 1043
2022-06-12 01:24:58,031   Batch size = 32
2022-06-12 01:24:58,032 ***** Eval results *****
2022-06-12 01:24:58,032   att_loss = 0.8882485141879634
2022-06-12 01:24:58,032   global_step = 419
2022-06-12 01:24:58,032   loss = 2.126816765258187
2022-06-12 01:24:58,032   rep_loss = 1.2385682565601248
2022-06-12 01:24:58,032 ***** Save model *****
2022-06-12 01:25:03,717 ***** Running evaluation *****
2022-06-12 01:25:03,718   Epoch = 1 iter 439 step
2022-06-12 01:25:03,718   Num examples = 1043
2022-06-12 01:25:03,718   Batch size = 32
2022-06-12 01:25:03,719 ***** Eval results *****
2022-06-12 01:25:03,720   att_loss = 0.8817737726278083
2022-06-12 01:25:03,720   global_step = 439
2022-06-12 01:25:03,720   loss = 2.117629276458607
2022-06-12 01:25:03,720   rep_loss = 1.2358555107615714
2022-06-12 01:25:03,720 ***** Save model *****
2022-06-12 01:25:09,387 ***** Running evaluation *****
2022-06-12 01:25:09,388   Epoch = 1 iter 459 step
2022-06-12 01:25:09,388   Num examples = 1043
2022-06-12 01:25:09,388   Batch size = 32
2022-06-12 01:25:09,389 ***** Eval results *****
2022-06-12 01:25:09,389   att_loss = 0.8778681565696994
2022-06-12 01:25:09,389   global_step = 459
2022-06-12 01:25:09,389   loss = 2.1118648163974285
2022-06-12 01:25:09,390   rep_loss = 1.2339966669678688
2022-06-12 01:25:09,390 ***** Save model *****
2022-06-12 01:25:15,094 ***** Running evaluation *****
2022-06-12 01:25:15,095   Epoch = 1 iter 479 step
2022-06-12 01:25:15,095   Num examples = 1043
2022-06-12 01:25:15,095   Batch size = 32
2022-06-12 01:25:15,097 ***** Eval results *****
2022-06-12 01:25:15,097   att_loss = 0.8742817893343152
2022-06-12 01:25:15,097   global_step = 479
2022-06-12 01:25:15,097   loss = 2.106250379445418
2022-06-12 01:25:15,097   rep_loss = 1.2319685985457223
2022-06-12 01:25:15,097 ***** Save model *****
2022-06-12 01:25:20,767 ***** Running evaluation *****
2022-06-12 01:25:20,768   Epoch = 1 iter 499 step
2022-06-12 01:25:20,768   Num examples = 1043
2022-06-12 01:25:20,768   Batch size = 32
2022-06-12 01:25:20,769 ***** Eval results *****
2022-06-12 01:25:20,769   att_loss = 0.8721201406984493
2022-06-12 01:25:20,770   global_step = 499
2022-06-12 01:25:20,770   loss = 2.1024628800564797
2022-06-12 01:25:20,770   rep_loss = 1.2303427478362774
2022-06-12 01:25:20,770 ***** Save model *****
2022-06-12 01:25:26,447 ***** Running evaluation *****
2022-06-12 01:25:26,448   Epoch = 1 iter 519 step
2022-06-12 01:25:26,448   Num examples = 1043
2022-06-12 01:25:26,448   Batch size = 32
2022-06-12 01:25:26,450 ***** Eval results *****
2022-06-12 01:25:26,451   att_loss = 0.8719652290382083
2022-06-12 01:25:26,451   global_step = 519
2022-06-12 01:25:26,451   loss = 2.1014485609909843
2022-06-12 01:25:26,451   rep_loss = 1.2294833399946727
2022-06-12 01:25:26,451 ***** Save model *****
2022-06-12 01:25:32,113 ***** Running evaluation *****
2022-06-12 01:25:32,114   Epoch = 2 iter 539 step
2022-06-12 01:25:32,114   Num examples = 1043
2022-06-12 01:25:32,114   Batch size = 32
2022-06-12 01:25:32,115 ***** Eval results *****
2022-06-12 01:25:32,115   att_loss = 0.8284502267837525
2022-06-12 01:25:32,115   global_step = 539
2022-06-12 01:25:32,115   loss = 2.025641345977783
2022-06-12 01:25:32,115   rep_loss = 1.1971911430358886
2022-06-12 01:25:32,115 ***** Save model *****
2022-06-12 01:25:37,785 ***** Running evaluation *****
2022-06-12 01:25:37,786   Epoch = 2 iter 559 step
2022-06-12 01:25:37,786   Num examples = 1043
2022-06-12 01:25:37,786   Batch size = 32
2022-06-12 01:25:37,787 ***** Eval results *****
2022-06-12 01:25:37,787   att_loss = 0.8548844361305237
2022-06-12 01:25:37,787   global_step = 559
2022-06-12 01:25:37,787   loss = 2.062113018035889
2022-06-12 01:25:37,787   rep_loss = 1.2072285747528075
2022-06-12 01:25:37,788 ***** Save model *****
2022-06-12 01:25:43,491 ***** Running evaluation *****
2022-06-12 01:25:43,492   Epoch = 2 iter 579 step
2022-06-12 01:25:43,492   Num examples = 1043
2022-06-12 01:25:43,492   Batch size = 32
2022-06-12 01:25:43,493 ***** Eval results *****
2022-06-12 01:25:43,494   att_loss = 0.8513607025146485
2022-06-12 01:25:43,494   global_step = 579
2022-06-12 01:25:43,494   loss = 2.0523422294192843
2022-06-12 01:25:43,494   rep_loss = 1.2009815216064452
2022-06-12 01:25:43,494 ***** Save model *****
2022-06-12 01:25:49,164 ***** Running evaluation *****
2022-06-12 01:25:49,165   Epoch = 2 iter 599 step
2022-06-12 01:25:49,165   Num examples = 1043
2022-06-12 01:25:49,165   Batch size = 32
2022-06-12 01:25:49,166 ***** Eval results *****
2022-06-12 01:25:49,166   att_loss = 0.8398590775636526
2022-06-12 01:25:49,166   global_step = 599
2022-06-12 01:25:49,166   loss = 2.0365878105163575
2022-06-12 01:25:49,167   rep_loss = 1.1967287338696992
2022-06-12 01:25:49,167 ***** Save model *****
2022-06-12 01:25:54,810 ***** Running evaluation *****
2022-06-12 01:25:54,810   Epoch = 2 iter 619 step
2022-06-12 01:25:54,810   Num examples = 1043
2022-06-12 01:25:54,810   Batch size = 32
2022-06-12 01:25:54,811 ***** Eval results *****
2022-06-12 01:25:54,812   att_loss = 0.8257589466431562
2022-06-12 01:25:54,812   global_step = 619
2022-06-12 01:25:54,812   loss = 2.0193546028698193
2022-06-12 01:25:54,812   rep_loss = 1.1935956506168142
2022-06-12 01:25:54,812 ***** Save model *****
2022-06-12 01:25:55,120 ***** Running evaluation *****
2022-06-12 01:25:55,120   Epoch = 2 iter 7999 step
2022-06-12 01:25:55,120   Num examples = 5463
2022-06-12 01:25:55,120   Batch size = 32
2022-06-12 01:25:55,122 ***** Eval results *****
2022-06-12 01:25:55,122   att_loss = 4.212581522614071
2022-06-12 01:25:55,122   global_step = 7999
2022-06-12 01:25:55,122   loss = 5.17775546052254
2022-06-12 01:25:55,122   rep_loss = 0.9651739376213169
2022-06-12 01:25:55,122 ***** Save model *****
2022-06-12 01:26:00,469 ***** Running evaluation *****
2022-06-12 01:26:00,470   Epoch = 2 iter 639 step
2022-06-12 01:26:00,470   Num examples = 1043
2022-06-12 01:26:00,470   Batch size = 32
2022-06-12 01:26:00,471 ***** Eval results *****
2022-06-12 01:26:00,471   att_loss = 0.8241506338119506
2022-06-12 01:26:00,471   global_step = 639
2022-06-12 01:26:00,471   loss = 2.016752521197001
2022-06-12 01:26:00,471   rep_loss = 1.1926018828437441
2022-06-12 01:26:00,471 ***** Save model *****
2022-06-12 01:26:06,128 ***** Running evaluation *****
2022-06-12 01:26:06,129   Epoch = 2 iter 659 step
2022-06-12 01:26:06,129   Num examples = 1043
2022-06-12 01:26:06,129   Batch size = 32
2022-06-12 01:26:06,130 ***** Eval results *****
2022-06-12 01:26:06,131   att_loss = 0.8219357347488403
2022-06-12 01:26:06,131   global_step = 659
2022-06-12 01:26:06,131   loss = 2.012716457366943
2022-06-12 01:26:06,131   rep_loss = 1.1907807159423829
2022-06-12 01:26:06,131 ***** Save model *****
2022-06-12 01:26:11,798 ***** Running evaluation *****
2022-06-12 01:26:11,799   Epoch = 2 iter 679 step
2022-06-12 01:26:11,799   Num examples = 1043
2022-06-12 01:26:11,799   Batch size = 32
2022-06-12 01:26:11,800 ***** Eval results *****
2022-06-12 01:26:11,801   att_loss = 0.8153333384415199
2022-06-12 01:26:11,801   global_step = 679
2022-06-12 01:26:11,801   loss = 2.004153682445658
2022-06-12 01:26:11,801   rep_loss = 1.1888203366049406
2022-06-12 01:26:11,801 ***** Save model *****
2022-06-12 01:26:17,467 ***** Running evaluation *****
2022-06-12 01:26:17,468   Epoch = 2 iter 699 step
2022-06-12 01:26:17,468   Num examples = 1043
2022-06-12 01:26:17,468   Batch size = 32
2022-06-12 01:26:17,469 ***** Eval results *****
2022-06-12 01:26:17,469   att_loss = 0.81524658203125
2022-06-12 01:26:17,469   global_step = 699
2022-06-12 01:26:17,469   loss = 2.003899841597586
2022-06-12 01:26:17,469   rep_loss = 1.18865325161905
2022-06-12 01:26:17,469 ***** Save model *****
2022-06-12 01:26:23,171 ***** Running evaluation *****
2022-06-12 01:26:23,171   Epoch = 2 iter 719 step
2022-06-12 01:26:23,171   Num examples = 1043
2022-06-12 01:26:23,171   Batch size = 32
2022-06-12 01:26:23,172 ***** Eval results *****
2022-06-12 01:26:23,172   att_loss = 0.813535094905544
2022-06-12 01:26:23,173   global_step = 719
2022-06-12 01:26:23,173   loss = 2.00088063575126
2022-06-12 01:26:23,173   rep_loss = 1.187345534401971
2022-06-12 01:26:23,173 ***** Save model *****
2022-06-12 01:26:28,878 ***** Running evaluation *****
2022-06-12 01:26:28,878   Epoch = 2 iter 739 step
2022-06-12 01:26:28,878   Num examples = 1043
2022-06-12 01:26:28,879   Batch size = 32
2022-06-12 01:26:28,880 ***** Eval results *****
2022-06-12 01:26:28,880   att_loss = 0.8141945443502286
2022-06-12 01:26:28,880   global_step = 739
2022-06-12 01:26:28,880   loss = 2.0009992361068725
2022-06-12 01:26:28,880   rep_loss = 1.1868046865230653
2022-06-12 01:26:28,880 ***** Save model *****
2022-06-12 01:26:34,466 ***** Running evaluation *****
2022-06-12 01:26:34,466   Epoch = 2 iter 759 step
2022-06-12 01:26:34,467   Num examples = 1043
2022-06-12 01:26:34,467   Batch size = 32
2022-06-12 01:26:34,468 ***** Eval results *****
2022-06-12 01:26:34,468   att_loss = 0.8128200631671482
2022-06-12 01:26:34,468   global_step = 759
2022-06-12 01:26:34,468   loss = 1.9983234537972345
2022-06-12 01:26:34,468   rep_loss = 1.1855033858617148
2022-06-12 01:26:34,469 ***** Save model *****
2022-06-12 01:26:40,114 ***** Running evaluation *****
2022-06-12 01:26:40,115   Epoch = 2 iter 779 step
2022-06-12 01:26:40,115   Num examples = 1043
2022-06-12 01:26:40,115   Batch size = 32
2022-06-12 01:26:40,116 ***** Eval results *****
2022-06-12 01:26:40,116   att_loss = 0.812292473170222
2022-06-12 01:26:40,116   global_step = 779
2022-06-12 01:26:40,116   loss = 1.996553838982874
2022-06-12 01:26:40,116   rep_loss = 1.1842613609469668
2022-06-12 01:26:40,116 ***** Save model *****
2022-06-12 01:26:45,750 ***** Running evaluation *****
2022-06-12 01:26:45,751   Epoch = 2 iter 799 step
2022-06-12 01:26:45,751   Num examples = 1043
2022-06-12 01:26:45,751   Batch size = 32
2022-06-12 01:26:45,752 ***** Eval results *****
2022-06-12 01:26:45,752   att_loss = 0.8128647408395443
2022-06-12 01:26:45,752   global_step = 799
2022-06-12 01:26:45,752   loss = 1.9963605358915508
2022-06-12 01:26:45,752   rep_loss = 1.1834957923529283
2022-06-12 01:26:45,753 ***** Save model *****
2022-06-12 01:26:51,474 ***** Running evaluation *****
2022-06-12 01:26:51,475   Epoch = 3 iter 819 step
2022-06-12 01:26:51,475   Num examples = 1043
2022-06-12 01:26:51,475   Batch size = 32
2022-06-12 01:26:51,476 ***** Eval results *****
2022-06-12 01:26:51,476   att_loss = 0.7969208326604631
2022-06-12 01:26:51,476   global_step = 819
2022-06-12 01:26:51,476   loss = 1.961639490392473
2022-06-12 01:26:51,476   rep_loss = 1.1647186676661174
2022-06-12 01:26:51,476 ***** Save model *****
2022-06-12 01:26:57,182 ***** Running evaluation *****
2022-06-12 01:26:57,183   Epoch = 3 iter 839 step
2022-06-12 01:26:57,183   Num examples = 1043
2022-06-12 01:26:57,183   Batch size = 32
2022-06-12 01:26:57,184 ***** Eval results *****
2022-06-12 01:26:57,184   att_loss = 0.7815976833042345
2022-06-12 01:26:57,184   global_step = 839
2022-06-12 01:26:57,184   loss = 1.9360474065730446
2022-06-12 01:26:57,184   rep_loss = 1.1544497201317234
2022-06-12 01:26:57,184 ***** Save model *****
2022-06-12 01:27:02,890 ***** Running evaluation *****
2022-06-12 01:27:02,891   Epoch = 3 iter 859 step
2022-06-12 01:27:02,891   Num examples = 1043
2022-06-12 01:27:02,891   Batch size = 32
2022-06-12 01:27:02,892 ***** Eval results *****
2022-06-12 01:27:02,893   att_loss = 0.7768282407316668
2022-06-12 01:27:02,893   global_step = 859
2022-06-12 01:27:02,893   loss = 1.927353092308702
2022-06-12 01:27:02,893   rep_loss = 1.150524850549369
2022-06-12 01:27:02,893 ***** Save model *****
2022-06-12 01:27:08,546 ***** Running evaluation *****
2022-06-12 01:27:08,547   Epoch = 3 iter 879 step
2022-06-12 01:27:08,547   Num examples = 1043
2022-06-12 01:27:08,547   Batch size = 32
2022-06-12 01:27:08,548 ***** Eval results *****
2022-06-12 01:27:08,548   att_loss = 0.7786826728246151
2022-06-12 01:27:08,548   global_step = 879
2022-06-12 01:27:08,548   loss = 1.9288624494503706
2022-06-12 01:27:08,548   rep_loss = 1.1501797789182417
2022-06-12 01:27:08,548 ***** Save model *****
2022-06-12 01:27:14,222 ***** Running evaluation *****
2022-06-12 01:27:14,223   Epoch = 3 iter 899 step
2022-06-12 01:27:14,223   Num examples = 1043
2022-06-12 01:27:14,223   Batch size = 32
2022-06-12 01:27:14,224 ***** Eval results *****
2022-06-12 01:27:14,224   att_loss = 0.7676488501685006
2022-06-12 01:27:14,225   global_step = 899
2022-06-12 01:27:14,225   loss = 1.9128600383291439
2022-06-12 01:27:14,225   rep_loss = 1.1452111905934859
2022-06-12 01:27:14,225 ***** Save model *****
2022-06-12 01:27:19,861 ***** Running evaluation *****
2022-06-12 01:27:19,862   Epoch = 3 iter 919 step
2022-06-12 01:27:19,862   Num examples = 1043
2022-06-12 01:27:19,862   Batch size = 32
2022-06-12 01:27:19,863 ***** Eval results *****
2022-06-12 01:27:19,863   att_loss = 0.7720588864916462
2022-06-12 01:27:19,863   global_step = 919
2022-06-12 01:27:19,863   loss = 1.9201488272618439
2022-06-12 01:27:19,863   rep_loss = 1.1480899443060666
2022-06-12 01:27:19,863 ***** Save model *****
2022-06-12 01:27:25,505 ***** Running evaluation *****
2022-06-12 01:27:25,505   Epoch = 3 iter 939 step
2022-06-12 01:27:25,506   Num examples = 1043
2022-06-12 01:27:25,506   Batch size = 32
2022-06-12 01:27:25,507 ***** Eval results *****
2022-06-12 01:27:25,507   att_loss = 0.7714788101721501
2022-06-12 01:27:25,507   global_step = 939
2022-06-12 01:27:25,507   loss = 1.9154085093650266
2022-06-12 01:27:25,507   rep_loss = 1.1439297017843828
2022-06-12 01:27:25,507 ***** Save model *****
2022-06-12 01:27:31,125 ***** Running evaluation *****
2022-06-12 01:27:31,126   Epoch = 3 iter 959 step
2022-06-12 01:27:31,126   Num examples = 1043
2022-06-12 01:27:31,126   Batch size = 32
2022-06-12 01:27:31,127 ***** Eval results *****
2022-06-12 01:27:31,127   att_loss = 0.7702041668227956
2022-06-12 01:27:31,127   global_step = 959
2022-06-12 01:27:31,127   loss = 1.9152243650412257
2022-06-12 01:27:31,127   rep_loss = 1.145020201990876
2022-06-12 01:27:31,127 ***** Save model *****
2022-06-12 01:27:36,856 ***** Running evaluation *****
2022-06-12 01:27:36,856   Epoch = 3 iter 979 step
2022-06-12 01:27:36,856   Num examples = 1043
2022-06-12 01:27:36,856   Batch size = 32
2022-06-12 01:27:36,858 ***** Eval results *****
2022-06-12 01:27:36,858   att_loss = 0.7727528800455372
2022-06-12 01:27:36,858   global_step = 979
2022-06-12 01:27:36,858   loss = 1.9177859445636192
2022-06-12 01:27:36,858   rep_loss = 1.145033066862085
2022-06-12 01:27:36,858 ***** Save model *****
2022-06-12 01:27:42,569 ***** Running evaluation *****
2022-06-12 01:27:42,570   Epoch = 3 iter 999 step
2022-06-12 01:27:42,570   Num examples = 1043
2022-06-12 01:27:42,570   Batch size = 32
2022-06-12 01:27:42,571 ***** Eval results *****
2022-06-12 01:27:42,571   att_loss = 0.7696216479696408
2022-06-12 01:27:42,571   global_step = 999
2022-06-12 01:27:42,571   loss = 1.9129097750692656
2022-06-12 01:27:42,571   rep_loss = 1.143288127701692
2022-06-12 01:27:42,571 ***** Save model *****
2022-06-12 01:27:48,270 ***** Running evaluation *****
2022-06-12 01:27:48,270   Epoch = 3 iter 1019 step
2022-06-12 01:27:48,270   Num examples = 1043
2022-06-12 01:27:48,270   Batch size = 32
2022-06-12 01:27:48,271 ***** Eval results *****
2022-06-12 01:27:48,271   att_loss = 0.7700829374680825
2022-06-12 01:27:48,271   global_step = 1019
2022-06-12 01:27:48,271   loss = 1.9127082846580294
2022-06-12 01:27:48,271   rep_loss = 1.1426253482836102
2022-06-12 01:27:48,271 ***** Save model *****
2022-06-12 01:27:53,956 ***** Running evaluation *****
2022-06-12 01:27:53,956   Epoch = 3 iter 1039 step
2022-06-12 01:27:53,956   Num examples = 1043
2022-06-12 01:27:53,956   Batch size = 32
2022-06-12 01:27:53,957 ***** Eval results *****
2022-06-12 01:27:53,957   att_loss = 0.7664323889908671
2022-06-12 01:27:53,958   global_step = 1039
2022-06-12 01:27:53,958   loss = 1.9073728062525517
2022-06-12 01:27:53,958   rep_loss = 1.1409404172616846
2022-06-12 01:27:53,958 ***** Save model *****
2022-06-12 01:27:59,717 ***** Running evaluation *****
2022-06-12 01:27:59,717   Epoch = 3 iter 1059 step
2022-06-12 01:27:59,717   Num examples = 1043
2022-06-12 01:27:59,718   Batch size = 32
2022-06-12 01:27:59,718 ***** Eval results *****
2022-06-12 01:27:59,719   att_loss = 0.7668798836164696
2022-06-12 01:27:59,719   global_step = 1059
2022-06-12 01:27:59,719   loss = 1.9063339529111403
2022-06-12 01:27:59,719   rep_loss = 1.1394540686015935
2022-06-12 01:27:59,719 ***** Save model *****
2022-06-12 01:28:02,957 ***** Running evaluation *****
2022-06-12 01:28:02,958   Epoch = 2 iter 8499 step
2022-06-12 01:28:02,958   Num examples = 5463
2022-06-12 01:28:02,958   Batch size = 32
2022-06-12 01:28:02,959 ***** Eval results *****
2022-06-12 01:28:02,959   att_loss = 4.209532793887871
2022-06-12 01:28:02,960   global_step = 8499
2022-06-12 01:28:02,960   loss = 5.172803958257039
2022-06-12 01:28:02,960   rep_loss = 0.963271162415918
2022-06-12 01:28:02,960 ***** Save model *****
2022-06-12 01:28:05,409 ***** Running evaluation *****
2022-06-12 01:28:05,410   Epoch = 4 iter 1079 step
2022-06-12 01:28:05,410   Num examples = 1043
2022-06-12 01:28:05,410   Batch size = 32
2022-06-12 01:28:05,411 ***** Eval results *****
2022-06-12 01:28:05,411   att_loss = 0.751506279815327
2022-06-12 01:28:05,411   global_step = 1079
2022-06-12 01:28:05,411   loss = 1.8453643105246804
2022-06-12 01:28:05,411   rep_loss = 1.0938580469651655
2022-06-12 01:28:05,411 ***** Save model *****
2022-06-12 01:28:11,063 ***** Running evaluation *****
2022-06-12 01:28:11,064   Epoch = 4 iter 1099 step
2022-06-12 01:28:11,064   Num examples = 1043
2022-06-12 01:28:11,064   Batch size = 32
2022-06-12 01:28:11,066 ***** Eval results *****
2022-06-12 01:28:11,066   att_loss = 0.7094735483969411
2022-06-12 01:28:11,066   global_step = 1099
2022-06-12 01:28:11,066   loss = 1.797913420584894
2022-06-12 01:28:11,066   rep_loss = 1.088439872187953
2022-06-12 01:28:11,067 ***** Save model *****
2022-06-12 01:28:16,733 ***** Running evaluation *****
2022-06-12 01:28:16,734   Epoch = 4 iter 1119 step
2022-06-12 01:28:16,734   Num examples = 1043
2022-06-12 01:28:16,734   Batch size = 32
2022-06-12 01:28:16,735 ***** Eval results *****
2022-06-12 01:28:16,735   att_loss = 0.7093112246662963
2022-06-12 01:28:16,735   global_step = 1119
2022-06-12 01:28:16,735   loss = 1.7965495142282224
2022-06-12 01:28:16,735   rep_loss = 1.0872382883932077
2022-06-12 01:28:16,735 ***** Save model *****
2022-06-12 01:28:22,351 ***** Running evaluation *****
2022-06-12 01:28:22,351   Epoch = 4 iter 1139 step
2022-06-12 01:28:22,351   Num examples = 1043
2022-06-12 01:28:22,351   Batch size = 32
2022-06-12 01:28:22,352 ***** Eval results *****
2022-06-12 01:28:22,352   att_loss = 0.7164497409068363
2022-06-12 01:28:22,353   global_step = 1139
2022-06-12 01:28:22,353   loss = 1.803531806233903
2022-06-12 01:28:22,353   rep_loss = 1.0870820636480627
2022-06-12 01:28:22,353 ***** Save model *****
2022-06-12 01:28:28,056 ***** Running evaluation *****
2022-06-12 01:28:28,056   Epoch = 4 iter 1159 step
2022-06-12 01:28:28,056   Num examples = 1043
2022-06-12 01:28:28,057   Batch size = 32
2022-06-12 01:28:28,058 ***** Eval results *****
2022-06-12 01:28:28,058   att_loss = 0.724111407667726
2022-06-12 01:28:28,058   global_step = 1159
2022-06-12 01:28:28,058   loss = 1.8134774375747849
2022-06-12 01:28:28,058   rep_loss = 1.0893660259770823
2022-06-12 01:28:28,058 ***** Save model *****
2022-06-12 01:28:33,719 ***** Running evaluation *****
2022-06-12 01:28:33,719   Epoch = 4 iter 1179 step
2022-06-12 01:28:33,719   Num examples = 1043
2022-06-12 01:28:33,719   Batch size = 32
2022-06-12 01:28:33,720 ***** Eval results *****
2022-06-12 01:28:33,721   att_loss = 0.720372731621201
2022-06-12 01:28:33,721   global_step = 1179
2022-06-12 01:28:33,721   loss = 1.810028539047585
2022-06-12 01:28:33,721   rep_loss = 1.0896558052784688
2022-06-12 01:28:33,721 ***** Save model *****
2022-06-12 01:28:39,383 ***** Running evaluation *****
2022-06-12 01:28:39,384   Epoch = 4 iter 1199 step
2022-06-12 01:28:39,384   Num examples = 1043
2022-06-12 01:28:39,384   Batch size = 32
2022-06-12 01:28:39,386 ***** Eval results *****
2022-06-12 01:28:39,386   att_loss = 0.7224836249387901
2022-06-12 01:28:39,386   global_step = 1199
2022-06-12 01:28:39,386   loss = 1.814137303192197
2022-06-12 01:28:39,386   rep_loss = 1.091653675523423
2022-06-12 01:28:39,386 ***** Save model *****
2022-06-12 01:28:45,084 ***** Running evaluation *****
2022-06-12 01:28:45,084   Epoch = 4 iter 1219 step
2022-06-12 01:28:45,084   Num examples = 1043
2022-06-12 01:28:45,084   Batch size = 32
2022-06-12 01:28:45,085 ***** Eval results *****
2022-06-12 01:28:45,086   att_loss = 0.7214506710601958
2022-06-12 01:28:45,086   global_step = 1219
2022-06-12 01:28:45,086   loss = 1.8113985322169122
2022-06-12 01:28:45,086   rep_loss = 1.089947859972518
2022-06-12 01:28:45,086 ***** Save model *****
2022-06-12 01:28:50,741 ***** Running evaluation *****
2022-06-12 01:28:50,742   Epoch = 4 iter 1239 step
2022-06-12 01:28:50,742   Num examples = 1043
2022-06-12 01:28:50,742   Batch size = 32
2022-06-12 01:28:50,743 ***** Eval results *****
2022-06-12 01:28:50,743   att_loss = 0.7215280194728695
2022-06-12 01:28:50,743   global_step = 1239
2022-06-12 01:28:50,743   loss = 1.8112786415724726
2022-06-12 01:28:50,743   rep_loss = 1.0897506203567773
2022-06-12 01:28:50,743 ***** Save model *****
2022-06-12 01:28:56,398 ***** Running evaluation *****
2022-06-12 01:28:56,398   Epoch = 4 iter 1259 step
2022-06-12 01:28:56,398   Num examples = 1043
2022-06-12 01:28:56,398   Batch size = 32
2022-06-12 01:28:56,399 ***** Eval results *****
2022-06-12 01:28:56,399   att_loss = 0.7212698534521133
2022-06-12 01:28:56,400   global_step = 1259
2022-06-12 01:28:56,400   loss = 1.8101388278431918
2022-06-12 01:28:56,400   rep_loss = 1.0888689737669461
2022-06-12 01:28:56,400 ***** Save model *****
2022-06-12 01:29:02,085 ***** Running evaluation *****
2022-06-12 01:29:02,085   Epoch = 4 iter 1279 step
2022-06-12 01:29:02,086   Num examples = 1043
2022-06-12 01:29:02,086   Batch size = 32
2022-06-12 01:29:02,087 ***** Eval results *****
2022-06-12 01:29:02,087   att_loss = 0.7206284203235572
2022-06-12 01:29:02,087   global_step = 1279
2022-06-12 01:29:02,087   loss = 1.8100580807545739
2022-06-12 01:29:02,087   rep_loss = 1.089429657041179
2022-06-12 01:29:02,087 ***** Save model *****
2022-06-12 01:29:07,741 ***** Running evaluation *****
2022-06-12 01:29:07,741   Epoch = 4 iter 1299 step
2022-06-12 01:29:07,742   Num examples = 1043
2022-06-12 01:29:07,742   Batch size = 32
2022-06-12 01:29:07,743 ***** Eval results *****
2022-06-12 01:29:07,743   att_loss = 0.7212094579424176
2022-06-12 01:29:07,743   global_step = 1299
2022-06-12 01:29:07,743   loss = 1.8108401283041222
2022-06-12 01:29:07,743   rep_loss = 1.0896306682974746
2022-06-12 01:29:07,743 ***** Save model *****
2022-06-12 01:29:13,382 ***** Running evaluation *****
2022-06-12 01:29:13,383   Epoch = 4 iter 1319 step
2022-06-12 01:29:13,383   Num examples = 1043
2022-06-12 01:29:13,383   Batch size = 32
2022-06-12 01:29:13,384 ***** Eval results *****
2022-06-12 01:29:13,384   att_loss = 0.7203046532266169
2022-06-12 01:29:13,384   global_step = 1319
2022-06-12 01:29:13,384   loss = 1.80947457841668
2022-06-12 01:29:13,384   rep_loss = 1.0891699242401882
2022-06-12 01:29:13,385 ***** Save model *****
2022-06-12 01:29:19,034 ***** Running evaluation *****
2022-06-12 01:29:19,035   Epoch = 5 iter 1339 step
2022-06-12 01:29:19,035   Num examples = 1043
2022-06-12 01:29:19,035   Batch size = 32
2022-06-12 01:29:19,036 ***** Eval results *****
2022-06-12 01:29:19,036   att_loss = 0.6611428558826447
2022-06-12 01:29:19,037   global_step = 1339
2022-06-12 01:29:19,037   loss = 1.6851707994937897
2022-06-12 01:29:19,037   rep_loss = 1.024027943611145
2022-06-12 01:29:19,037 ***** Save model *****
2022-06-12 01:29:24,689 ***** Running evaluation *****
2022-06-12 01:29:24,690   Epoch = 5 iter 1359 step
2022-06-12 01:29:24,690   Num examples = 1043
2022-06-12 01:29:24,690   Batch size = 32
2022-06-12 01:29:24,691 ***** Eval results *****
2022-06-12 01:29:24,691   att_loss = 0.6550087059537569
2022-06-12 01:29:24,691   global_step = 1359
2022-06-12 01:29:24,691   loss = 1.6880267262458801
2022-06-12 01:29:24,691   rep_loss = 1.0330180078744888
2022-06-12 01:29:24,691 ***** Save model *****
2022-06-12 01:29:30,389 ***** Running evaluation *****
2022-06-12 01:29:30,389   Epoch = 5 iter 1379 step
2022-06-12 01:29:30,389   Num examples = 1043
2022-06-12 01:29:30,389   Batch size = 32
2022-06-12 01:29:30,390 ***** Eval results *****
2022-06-12 01:29:30,391   att_loss = 0.6801267076622356
2022-06-12 01:29:30,391   global_step = 1379
2022-06-12 01:29:30,391   loss = 1.7213142189112576
2022-06-12 01:29:30,391   rep_loss = 1.041187501766465
2022-06-12 01:29:30,391 ***** Save model *****
2022-06-12 01:29:36,068 ***** Running evaluation *****
2022-06-12 01:29:36,069   Epoch = 5 iter 1399 step
2022-06-12 01:29:36,069   Num examples = 1043
2022-06-12 01:29:36,069   Batch size = 32
2022-06-12 01:29:36,071 ***** Eval results *****
2022-06-12 01:29:36,071   att_loss = 0.6809981698170304
2022-06-12 01:29:36,071   global_step = 1399
2022-06-12 01:29:36,071   loss = 1.7230811715126038
2022-06-12 01:29:36,071   rep_loss = 1.0420829933136702
2022-06-12 01:29:36,071 ***** Save model *****
2022-06-12 01:29:41,745 ***** Running evaluation *****
2022-06-12 01:29:41,745   Epoch = 5 iter 1419 step
2022-06-12 01:29:41,745   Num examples = 1043
2022-06-12 01:29:41,745   Batch size = 32
2022-06-12 01:29:41,747 ***** Eval results *****
2022-06-12 01:29:41,747   att_loss = 0.685580978790919
2022-06-12 01:29:41,747   global_step = 1419
2022-06-12 01:29:41,747   loss = 1.7300308488664173
2022-06-12 01:29:41,747   rep_loss = 1.0444498643988656
2022-06-12 01:29:41,747 ***** Save model *****
2022-06-12 01:29:47,445 ***** Running evaluation *****
2022-06-12 01:29:47,446   Epoch = 5 iter 1439 step
2022-06-12 01:29:47,446   Num examples = 1043
2022-06-12 01:29:47,446   Batch size = 32
2022-06-12 01:29:47,447 ***** Eval results *****
2022-06-12 01:29:47,447   att_loss = 0.6903700668078202
2022-06-12 01:29:47,447   global_step = 1439
2022-06-12 01:29:47,447   loss = 1.735598080433332
2022-06-12 01:29:47,447   rep_loss = 1.0452280067480528
2022-06-12 01:29:47,447 ***** Save model *****
2022-06-12 01:29:53,105 ***** Running evaluation *****
2022-06-12 01:29:53,106   Epoch = 5 iter 1459 step
2022-06-12 01:29:53,106   Num examples = 1043
2022-06-12 01:29:53,106   Batch size = 32
2022-06-12 01:29:53,107 ***** Eval results *****
2022-06-12 01:29:53,107   att_loss = 0.6875649757923619
2022-06-12 01:29:53,107   global_step = 1459
2022-06-12 01:29:53,107   loss = 1.7329683178855526
2022-06-12 01:29:53,107   rep_loss = 1.0454033334409036
2022-06-12 01:29:53,107 ***** Save model *****
2022-06-12 01:29:58,779 ***** Running evaluation *****
2022-06-12 01:29:58,780   Epoch = 5 iter 1479 step
2022-06-12 01:29:58,780   Num examples = 1043
2022-06-12 01:29:58,780   Batch size = 32
2022-06-12 01:29:58,781 ***** Eval results *****
2022-06-12 01:29:58,781   att_loss = 0.6885630546344651
2022-06-12 01:29:58,781   global_step = 1479
2022-06-12 01:29:58,781   loss = 1.7359972521662712
2022-06-12 01:29:58,781   rep_loss = 1.0474341909090679
2022-06-12 01:29:58,782 ***** Save model *****
2022-06-12 01:30:04,358 ***** Running evaluation *****
2022-06-12 01:30:04,359   Epoch = 5 iter 1499 step
2022-06-12 01:30:04,359   Num examples = 1043
2022-06-12 01:30:04,359   Batch size = 32
2022-06-12 01:30:04,360 ***** Eval results *****
2022-06-12 01:30:04,360   att_loss = 0.6889889236630463
2022-06-12 01:30:04,360   global_step = 1499
2022-06-12 01:30:04,361   loss = 1.7362531125545502
2022-06-12 01:30:04,361   rep_loss = 1.0472641827129736
2022-06-12 01:30:04,361 ***** Save model *****
2022-06-12 01:30:10,053 ***** Running evaluation *****
2022-06-12 01:30:10,054   Epoch = 5 iter 1519 step
2022-06-12 01:30:10,054   Num examples = 1043
2022-06-12 01:30:10,054   Batch size = 32
2022-06-12 01:30:10,055 ***** Eval results *****
2022-06-12 01:30:10,055   att_loss = 0.6913812756538391
2022-06-12 01:30:10,055   global_step = 1519
2022-06-12 01:30:10,055   loss = 1.7418054128470628
2022-06-12 01:30:10,055   rep_loss = 1.0504241313623346
2022-06-12 01:30:10,055 ***** Save model *****
2022-06-12 01:30:10,827 ***** Running evaluation *****
2022-06-12 01:30:10,828   Epoch = 2 iter 8999 step
2022-06-12 01:30:10,828   Num examples = 5463
2022-06-12 01:30:10,828   Batch size = 32
2022-06-12 01:30:10,829 ***** Eval results *****
2022-06-12 01:30:10,829   att_loss = 4.194791861859526
2022-06-12 01:30:10,829   global_step = 8999
2022-06-12 01:30:10,829   loss = 5.155571784479395
2022-06-12 01:30:10,829   rep_loss = 0.960779921283442
2022-06-12 01:30:10,829 ***** Save model *****
2022-06-12 01:30:15,731 ***** Running evaluation *****
2022-06-12 01:30:15,732   Epoch = 5 iter 1539 step
2022-06-12 01:30:15,732   Num examples = 1043
2022-06-12 01:30:15,732   Batch size = 32
2022-06-12 01:30:15,733 ***** Eval results *****
2022-06-12 01:30:15,733   att_loss = 0.6896346816245247
2022-06-12 01:30:15,733   global_step = 1539
2022-06-12 01:30:15,733   loss = 1.73838470788563
2022-06-12 01:30:15,733   rep_loss = 1.0487500212940515
2022-06-12 01:30:15,733 ***** Save model *****
2022-06-12 01:30:21,397 ***** Running evaluation *****
2022-06-12 01:30:21,397   Epoch = 5 iter 1559 step
2022-06-12 01:30:21,397   Num examples = 1043
2022-06-12 01:30:21,397   Batch size = 32
2022-06-12 01:30:21,399 ***** Eval results *****
2022-06-12 01:30:21,399   att_loss = 0.6907994598150253
2022-06-12 01:30:21,399   global_step = 1559
2022-06-12 01:30:21,399   loss = 1.7396975473633833
2022-06-12 01:30:21,399   rep_loss = 1.04889808408916
2022-06-12 01:30:21,400 ***** Save model *****
2022-06-12 01:30:27,122 ***** Running evaluation *****
2022-06-12 01:30:27,122   Epoch = 5 iter 1579 step
2022-06-12 01:30:27,123   Num examples = 1043
2022-06-12 01:30:27,123   Batch size = 32
2022-06-12 01:30:27,124 ***** Eval results *****
2022-06-12 01:30:27,124   att_loss = 0.688425176944889
2022-06-12 01:30:27,124   global_step = 1579
2022-06-12 01:30:27,124   loss = 1.7362382021106657
2022-06-12 01:30:27,124   rep_loss = 1.0478130234558074
2022-06-12 01:30:27,124 ***** Save model *****
2022-06-12 01:30:32,771 ***** Running evaluation *****
2022-06-12 01:30:32,771   Epoch = 5 iter 1599 step
2022-06-12 01:30:32,771   Num examples = 1043
2022-06-12 01:30:32,772   Batch size = 32
2022-06-12 01:30:32,772 ***** Eval results *****
2022-06-12 01:30:32,773   att_loss = 0.6866045772577777
2022-06-12 01:30:32,773   global_step = 1599
2022-06-12 01:30:32,773   loss = 1.7337489155205814
2022-06-12 01:30:32,773   rep_loss = 1.0471443360050519
2022-06-12 01:30:32,773 ***** Save model *****
2022-06-12 01:30:38,430 ***** Running evaluation *****
2022-06-12 01:30:38,431   Epoch = 6 iter 1619 step
2022-06-12 01:30:38,431   Num examples = 1043
2022-06-12 01:30:38,431   Batch size = 32
2022-06-12 01:30:38,432 ***** Eval results *****
2022-06-12 01:30:38,432   att_loss = 0.6728315283270443
2022-06-12 01:30:38,432   global_step = 1619
2022-06-12 01:30:38,432   loss = 1.6942973627763636
2022-06-12 01:30:38,432   rep_loss = 1.021465844967786
2022-06-12 01:30:38,433 ***** Save model *****
2022-06-12 01:30:44,110 ***** Running evaluation *****
2022-06-12 01:30:44,111   Epoch = 6 iter 1639 step
2022-06-12 01:30:44,111   Num examples = 1043
2022-06-12 01:30:44,111   Batch size = 32
2022-06-12 01:30:44,112 ***** Eval results *****
2022-06-12 01:30:44,112   att_loss = 0.6709016432633271
2022-06-12 01:30:44,112   global_step = 1639
2022-06-12 01:30:44,112   loss = 1.6924707019651257
2022-06-12 01:30:44,112   rep_loss = 1.0215690635346077
2022-06-12 01:30:44,112 ***** Save model *****
2022-06-12 01:30:49,783 ***** Running evaluation *****
2022-06-12 01:30:49,784   Epoch = 6 iter 1659 step
2022-06-12 01:30:49,784   Num examples = 1043
2022-06-12 01:30:49,784   Batch size = 32
2022-06-12 01:30:49,785 ***** Eval results *****
2022-06-12 01:30:49,785   att_loss = 0.6678522731128492
2022-06-12 01:30:49,785   global_step = 1659
2022-06-12 01:30:49,785   loss = 1.6890890932919687
2022-06-12 01:30:49,785   rep_loss = 1.021236823316206
2022-06-12 01:30:49,786 ***** Save model *****
2022-06-12 01:30:55,440 ***** Running evaluation *****
2022-06-12 01:30:55,441   Epoch = 6 iter 1679 step
2022-06-12 01:30:55,441   Num examples = 1043
2022-06-12 01:30:55,441   Batch size = 32
2022-06-12 01:30:55,442 ***** Eval results *****
2022-06-12 01:30:55,442   att_loss = 0.6632884477640127
2022-06-12 01:30:55,442   global_step = 1679
2022-06-12 01:30:55,442   loss = 1.6833857623013584
2022-06-12 01:30:55,442   rep_loss = 1.020097312989173
2022-06-12 01:30:55,442 ***** Save model *****
2022-06-12 01:31:01,129 ***** Running evaluation *****
2022-06-12 01:31:01,130   Epoch = 6 iter 1699 step
2022-06-12 01:31:01,130   Num examples = 1043
2022-06-12 01:31:01,130   Batch size = 32
2022-06-12 01:31:01,131 ***** Eval results *****
2022-06-12 01:31:01,131   att_loss = 0.6545079024796633
2022-06-12 01:31:01,131   global_step = 1699
2022-06-12 01:31:01,131   loss = 1.6705485484034746
2022-06-12 01:31:01,132   rep_loss = 1.0160406459238112
2022-06-12 01:31:01,132 ***** Save model *****
2022-06-12 01:31:06,820 ***** Running evaluation *****
2022-06-12 01:31:06,821   Epoch = 6 iter 1719 step
2022-06-12 01:31:06,821   Num examples = 1043
2022-06-12 01:31:06,821   Batch size = 32
2022-06-12 01:31:06,822 ***** Eval results *****
2022-06-12 01:31:06,822   att_loss = 0.6558589731526171
2022-06-12 01:31:06,822   global_step = 1719
2022-06-12 01:31:06,822   loss = 1.6688942491498768
2022-06-12 01:31:06,822   rep_loss = 1.0130352780350254
2022-06-12 01:31:06,823 ***** Save model *****
2022-06-12 01:31:12,483 ***** Running evaluation *****
2022-06-12 01:31:12,484   Epoch = 6 iter 1739 step
2022-06-12 01:31:12,484   Num examples = 1043
2022-06-12 01:31:12,484   Batch size = 32
2022-06-12 01:31:12,485 ***** Eval results *****
2022-06-12 01:31:12,485   att_loss = 0.6578971534749888
2022-06-12 01:31:12,485   global_step = 1739
2022-06-12 01:31:12,485   loss = 1.6720530090540866
2022-06-12 01:31:12,485   rep_loss = 1.0141558586245907
2022-06-12 01:31:12,485 ***** Save model *****
2022-06-12 01:31:18,157 ***** Running evaluation *****
2022-06-12 01:31:18,158   Epoch = 6 iter 1759 step
2022-06-12 01:31:18,158   Num examples = 1043
2022-06-12 01:31:18,158   Batch size = 32
2022-06-12 01:31:18,159 ***** Eval results *****
2022-06-12 01:31:18,159   att_loss = 0.6588517283178439
2022-06-12 01:31:18,159   global_step = 1759
2022-06-12 01:31:18,159   loss = 1.6728814696050753
2022-06-12 01:31:18,159   rep_loss = 1.0140297443244108
2022-06-12 01:31:18,159 ***** Save model *****
2022-06-12 01:31:23,817 ***** Running evaluation *****
2022-06-12 01:31:23,818   Epoch = 6 iter 1779 step
2022-06-12 01:31:23,818   Num examples = 1043
2022-06-12 01:31:23,818   Batch size = 32
2022-06-12 01:31:23,819 ***** Eval results *****
2022-06-12 01:31:23,819   att_loss = 0.6586158548371267
2022-06-12 01:31:23,819   global_step = 1779
2022-06-12 01:31:23,819   loss = 1.6721977541002178
2022-06-12 01:31:23,819   rep_loss = 1.0135819033040838
2022-06-12 01:31:23,819 ***** Save model *****
2022-06-12 01:31:29,510 ***** Running evaluation *****
2022-06-12 01:31:29,511   Epoch = 6 iter 1799 step
2022-06-12 01:31:29,511   Num examples = 1043
2022-06-12 01:31:29,511   Batch size = 32
2022-06-12 01:31:29,512 ***** Eval results *****
2022-06-12 01:31:29,512   att_loss = 0.6584834181112686
2022-06-12 01:31:29,512   global_step = 1799
2022-06-12 01:31:29,512   loss = 1.6723356991249898
2022-06-12 01:31:29,512   rep_loss = 1.013852285552146
2022-06-12 01:31:29,512 ***** Save model *****
2022-06-12 01:31:35,210 ***** Running evaluation *****
2022-06-12 01:31:35,211   Epoch = 6 iter 1819 step
2022-06-12 01:31:35,211   Num examples = 1043
2022-06-12 01:31:35,211   Batch size = 32
2022-06-12 01:31:35,212 ***** Eval results *****
2022-06-12 01:31:35,213   att_loss = 0.6597845642248057
2022-06-12 01:31:35,213   global_step = 1819
2022-06-12 01:31:35,213   loss = 1.6739880747509441
2022-06-12 01:31:35,213   rep_loss = 1.014203516019654
2022-06-12 01:31:35,213 ***** Save model *****
2022-06-12 01:31:40,871 ***** Running evaluation *****
2022-06-12 01:31:40,872   Epoch = 6 iter 1839 step
2022-06-12 01:31:40,872   Num examples = 1043
2022-06-12 01:31:40,872   Batch size = 32
2022-06-12 01:31:40,873 ***** Eval results *****
2022-06-12 01:31:40,873   att_loss = 0.658687554708513
2022-06-12 01:31:40,873   global_step = 1839
2022-06-12 01:31:40,873   loss = 1.6715527219611381
2022-06-12 01:31:40,873   rep_loss = 1.012865173665783
2022-06-12 01:31:40,874 ***** Save model *****
2022-06-12 01:31:46,529 ***** Running evaluation *****
2022-06-12 01:31:46,529   Epoch = 6 iter 1859 step
2022-06-12 01:31:46,529   Num examples = 1043
2022-06-12 01:31:46,529   Batch size = 32
2022-06-12 01:31:46,531 ***** Eval results *****
2022-06-12 01:31:46,531   att_loss = 0.6602303080753593
2022-06-12 01:31:46,531   global_step = 1859
2022-06-12 01:31:46,531   loss = 1.6727009724086361
2022-06-12 01:31:46,531   rep_loss = 1.0124706707112057
2022-06-12 01:31:46,531 ***** Save model *****
2022-06-12 01:31:52,191 ***** Running evaluation *****
2022-06-12 01:31:52,192   Epoch = 7 iter 1879 step
2022-06-12 01:31:52,192   Num examples = 1043
2022-06-12 01:31:52,192   Batch size = 32
2022-06-12 01:31:52,193 ***** Eval results *****
2022-06-12 01:31:52,193   att_loss = 0.6273487687110901
2022-06-12 01:31:52,193   global_step = 1879
2022-06-12 01:31:52,193   loss = 1.6209493041038514
2022-06-12 01:31:52,193   rep_loss = 0.9936005413532257
2022-06-12 01:31:52,194 ***** Save model *****
2022-06-12 01:31:57,828 ***** Running evaluation *****
2022-06-12 01:31:57,828   Epoch = 7 iter 1899 step
2022-06-12 01:31:57,828   Num examples = 1043
2022-06-12 01:31:57,828   Batch size = 32
2022-06-12 01:31:57,829 ***** Eval results *****
2022-06-12 01:31:57,830   att_loss = 0.6461204330126444
2022-06-12 01:31:57,830   global_step = 1899
2022-06-12 01:31:57,830   loss = 1.6360567847887675
2022-06-12 01:31:57,830   rep_loss = 0.9899363537629445
2022-06-12 01:31:57,830 ***** Save model *****
2022-06-12 01:32:03,497 ***** Running evaluation *****
2022-06-12 01:32:03,498   Epoch = 7 iter 1919 step
2022-06-12 01:32:03,498   Num examples = 1043
2022-06-12 01:32:03,498   Batch size = 32
2022-06-12 01:32:03,499 ***** Eval results *****
2022-06-12 01:32:03,499   att_loss = 0.6300076031684876
2022-06-12 01:32:03,499   global_step = 1919
2022-06-12 01:32:03,500   loss = 1.6160387253761292
2022-06-12 01:32:03,500   rep_loss = 0.986031118631363
2022-06-12 01:32:03,500 ***** Save model *****
2022-06-12 01:32:09,235 ***** Running evaluation *****
2022-06-12 01:32:09,236   Epoch = 7 iter 1939 step
2022-06-12 01:32:09,236   Num examples = 1043
2022-06-12 01:32:09,236   Batch size = 32
2022-06-12 01:32:09,238 ***** Eval results *****
2022-06-12 01:32:09,238   att_loss = 0.6232440786702292
2022-06-12 01:32:09,238   global_step = 1939
2022-06-12 01:32:09,238   loss = 1.6074574640819004
2022-06-12 01:32:09,238   rep_loss = 0.9842133811541967
2022-06-12 01:32:09,238 ***** Save model *****
2022-06-12 01:32:14,911 ***** Running evaluation *****
2022-06-12 01:32:14,912   Epoch = 7 iter 1959 step
2022-06-12 01:32:14,912   Num examples = 1043
2022-06-12 01:32:14,912   Batch size = 32
2022-06-12 01:32:14,913 ***** Eval results *****
2022-06-12 01:32:14,913   att_loss = 0.6228919857078128
2022-06-12 01:32:14,913   global_step = 1959
2022-06-12 01:32:14,913   loss = 1.6081141763263278
2022-06-12 01:32:14,913   rep_loss = 0.9852221879694197
2022-06-12 01:32:14,913 ***** Save model *****
2022-06-12 01:32:18,910 ***** Running evaluation *****
2022-06-12 01:32:18,910   Epoch = 2 iter 9499 step
2022-06-12 01:32:18,910   Num examples = 5463
2022-06-12 01:32:18,910   Batch size = 32
2022-06-12 01:32:18,912 ***** Eval results *****
2022-06-12 01:32:18,912   att_loss = 4.177743521537613
2022-06-12 01:32:18,912   global_step = 9499
2022-06-12 01:32:18,912   loss = 5.135897806043589
2022-06-12 01:32:18,912   rep_loss = 0.9581542840619189
2022-06-12 01:32:18,912 ***** Save model *****
2022-06-12 01:32:20,559 ***** Running evaluation *****
2022-06-12 01:32:20,559   Epoch = 7 iter 1979 step
2022-06-12 01:32:20,560   Num examples = 1043
2022-06-12 01:32:20,560   Batch size = 32
2022-06-12 01:32:20,561 ***** Eval results *****
2022-06-12 01:32:20,561   att_loss = 0.6232471997087652
2022-06-12 01:32:20,561   global_step = 1979
2022-06-12 01:32:20,561   loss = 1.6065618525851857
2022-06-12 01:32:20,561   rep_loss = 0.9833146507089788
2022-06-12 01:32:20,561 ***** Save model *****
2022-06-12 01:32:26,219 ***** Running evaluation *****
2022-06-12 01:32:26,220   Epoch = 7 iter 1999 step
2022-06-12 01:32:26,220   Num examples = 1043
2022-06-12 01:32:26,220   Batch size = 32
2022-06-12 01:32:26,222 ***** Eval results *****
2022-06-12 01:32:26,222   att_loss = 0.6261494957483732
2022-06-12 01:32:26,222   global_step = 1999
2022-06-12 01:32:26,222   loss = 1.6091966509819031
2022-06-12 01:32:26,222   rep_loss = 0.9830471538580381
2022-06-12 01:32:26,222 ***** Save model *****
2022-06-12 01:32:31,901 ***** Running evaluation *****
2022-06-12 01:32:31,901   Epoch = 7 iter 2019 step
2022-06-12 01:32:31,901   Num examples = 1043
2022-06-12 01:32:31,902   Batch size = 32
2022-06-12 01:32:31,903 ***** Eval results *****
2022-06-12 01:32:31,903   att_loss = 0.6300863075256348
2022-06-12 01:32:31,903   global_step = 2019
2022-06-12 01:32:31,903   loss = 1.6131197063128153
2022-06-12 01:32:31,903   rep_loss = 0.9830333948135376
2022-06-12 01:32:31,903 ***** Save model *****
2022-06-12 01:32:37,560 ***** Running evaluation *****
2022-06-12 01:32:37,560   Epoch = 7 iter 2039 step
2022-06-12 01:32:37,560   Num examples = 1043
2022-06-12 01:32:37,560   Batch size = 32
2022-06-12 01:32:37,561 ***** Eval results *****
2022-06-12 01:32:37,561   att_loss = 0.6304946944994085
2022-06-12 01:32:37,562   global_step = 2039
2022-06-12 01:32:37,562   loss = 1.6137986484695883
2022-06-12 01:32:37,562   rep_loss = 0.9833039497627931
2022-06-12 01:32:37,562 ***** Save model *****
2022-06-12 01:32:43,234 ***** Running evaluation *****
2022-06-12 01:32:43,235   Epoch = 7 iter 2059 step
2022-06-12 01:32:43,235   Num examples = 1043
2022-06-12 01:32:43,235   Batch size = 32
2022-06-12 01:32:43,236 ***** Eval results *****
2022-06-12 01:32:43,236   att_loss = 0.6330108899819223
2022-06-12 01:32:43,237   global_step = 2059
2022-06-12 01:32:43,237   loss = 1.6179128439802872
2022-06-12 01:32:43,237   rep_loss = 0.9849019499201523
2022-06-12 01:32:43,237 ***** Save model *****
2022-06-12 01:32:48,913 ***** Running evaluation *****
2022-06-12 01:32:48,913   Epoch = 7 iter 2079 step
2022-06-12 01:32:48,913   Num examples = 1043
2022-06-12 01:32:48,914   Batch size = 32
2022-06-12 01:32:48,915 ***** Eval results *****
2022-06-12 01:32:48,915   att_loss = 0.634307112580254
2022-06-12 01:32:48,915   global_step = 2079
2022-06-12 01:32:48,915   loss = 1.61961852482387
2022-06-12 01:32:48,915   rep_loss = 0.985311409121468
2022-06-12 01:32:48,915 ***** Save model *****
2022-06-12 01:32:54,567 ***** Running evaluation *****
2022-06-12 01:32:54,567   Epoch = 7 iter 2099 step
2022-06-12 01:32:54,567   Num examples = 1043
2022-06-12 01:32:54,567   Batch size = 32
2022-06-12 01:32:54,568 ***** Eval results *****
2022-06-12 01:32:54,568   att_loss = 0.6337499050990395
2022-06-12 01:32:54,568   global_step = 2099
2022-06-12 01:32:54,569   loss = 1.6183959349342014
2022-06-12 01:32:54,569   rep_loss = 0.984646026984505
2022-06-12 01:32:54,569 ***** Save model *****
2022-06-12 01:33:00,263 ***** Running evaluation *****
2022-06-12 01:33:00,263   Epoch = 7 iter 2119 step
2022-06-12 01:33:00,264   Num examples = 1043
2022-06-12 01:33:00,264   Batch size = 32
2022-06-12 01:33:00,265 ***** Eval results *****
2022-06-12 01:33:00,265   att_loss = 0.6349089260101318
2022-06-12 01:33:00,265   global_step = 2119
2022-06-12 01:33:00,265   loss = 1.6209469709396362
2022-06-12 01:33:00,265   rep_loss = 0.9860380415916443
2022-06-12 01:33:00,265 ***** Save model *****
2022-06-12 01:33:05,936 ***** Running evaluation *****
2022-06-12 01:33:05,937   Epoch = 8 iter 2139 step
2022-06-12 01:33:05,937   Num examples = 1043
2022-06-12 01:33:05,937   Batch size = 32
2022-06-12 01:33:05,938 ***** Eval results *****
2022-06-12 01:33:05,938   att_loss = 0.6166004538536072
2022-06-12 01:33:05,938   global_step = 2139
2022-06-12 01:33:05,938   loss = 1.5644231637318928
2022-06-12 01:33:05,939   rep_loss = 0.9478227098782858
2022-06-12 01:33:05,939 ***** Save model *****
2022-06-12 01:33:11,638 ***** Running evaluation *****
2022-06-12 01:33:11,639   Epoch = 8 iter 2159 step
2022-06-12 01:33:11,639   Num examples = 1043
2022-06-12 01:33:11,639   Batch size = 32
2022-06-12 01:33:11,640 ***** Eval results *****
2022-06-12 01:33:11,640   att_loss = 0.6184600462084231
2022-06-12 01:33:11,640   global_step = 2159
2022-06-12 01:33:11,640   loss = 1.5790614304335222
2022-06-12 01:33:11,640   rep_loss = 0.9606013868166052
2022-06-12 01:33:11,640 ***** Save model *****
2022-06-12 01:33:17,311 ***** Running evaluation *****
2022-06-12 01:33:17,312   Epoch = 8 iter 2179 step
2022-06-12 01:33:17,312   Num examples = 1043
2022-06-12 01:33:17,312   Batch size = 32
2022-06-12 01:33:17,313 ***** Eval results *****
2022-06-12 01:33:17,314   att_loss = 0.611249633999758
2022-06-12 01:33:17,314   global_step = 2179
2022-06-12 01:33:17,314   loss = 1.5731920807860618
2022-06-12 01:33:17,314   rep_loss = 0.9619424509447675
2022-06-12 01:33:17,314 ***** Save model *****
2022-06-12 01:33:22,964 ***** Running evaluation *****
2022-06-12 01:33:22,964   Epoch = 8 iter 2199 step
2022-06-12 01:33:22,964   Num examples = 1043
2022-06-12 01:33:22,964   Batch size = 32
2022-06-12 01:33:22,965 ***** Eval results *****
2022-06-12 01:33:22,965   att_loss = 0.5974130114865681
2022-06-12 01:33:22,965   global_step = 2199
2022-06-12 01:33:22,966   loss = 1.5512497387235127
2022-06-12 01:33:22,966   rep_loss = 0.9538367296022082
2022-06-12 01:33:22,966 ***** Save model *****
2022-06-12 01:33:28,624 ***** Running evaluation *****
2022-06-12 01:33:28,625   Epoch = 8 iter 2219 step
2022-06-12 01:33:28,625   Num examples = 1043
2022-06-12 01:33:28,625   Batch size = 32
2022-06-12 01:33:28,626 ***** Eval results *****
2022-06-12 01:33:28,626   att_loss = 0.6029157397976841
2022-06-12 01:33:28,626   global_step = 2219
2022-06-12 01:33:28,626   loss = 1.558809668184763
2022-06-12 01:33:28,626   rep_loss = 0.9558939301823995
2022-06-12 01:33:28,627 ***** Save model *****
2022-06-12 01:33:34,292 ***** Running evaluation *****
2022-06-12 01:33:34,293   Epoch = 8 iter 2239 step
2022-06-12 01:33:34,293   Num examples = 1043
2022-06-12 01:33:34,293   Batch size = 32
2022-06-12 01:33:34,294 ***** Eval results *****
2022-06-12 01:33:34,294   att_loss = 0.6073927072066705
2022-06-12 01:33:34,295   global_step = 2239
2022-06-12 01:33:34,295   loss = 1.565185637149996
2022-06-12 01:33:34,295   rep_loss = 0.9577929325474118
2022-06-12 01:33:34,295 ***** Save model *****
2022-06-12 01:33:39,991 ***** Running evaluation *****
2022-06-12 01:33:39,992   Epoch = 8 iter 2259 step
2022-06-12 01:33:39,992   Num examples = 1043
2022-06-12 01:33:39,992   Batch size = 32
2022-06-12 01:33:39,994 ***** Eval results *****
2022-06-12 01:33:39,994   att_loss = 0.6108290032158054
2022-06-12 01:33:39,994   global_step = 2259
2022-06-12 01:33:39,994   loss = 1.5706224809817182
2022-06-12 01:33:39,994   rep_loss = 0.9597934780082082
2022-06-12 01:33:39,994 ***** Save model *****
2022-06-12 01:33:45,670 ***** Running evaluation *****
2022-06-12 01:33:45,671   Epoch = 8 iter 2279 step
2022-06-12 01:33:45,671   Num examples = 1043
2022-06-12 01:33:45,671   Batch size = 32
2022-06-12 01:33:45,672 ***** Eval results *****
2022-06-12 01:33:45,672   att_loss = 0.6130741086456325
2022-06-12 01:33:45,672   global_step = 2279
2022-06-12 01:33:45,672   loss = 1.57233061490359
2022-06-12 01:33:45,672   rep_loss = 0.959256505632734
2022-06-12 01:33:45,672 ***** Save model *****
2022-06-12 01:33:51,368 ***** Running evaluation *****
2022-06-12 01:33:51,369   Epoch = 8 iter 2299 step
2022-06-12 01:33:51,369   Num examples = 1043
2022-06-12 01:33:51,369   Batch size = 32
2022-06-12 01:33:51,370 ***** Eval results *****
2022-06-12 01:33:51,370   att_loss = 0.6114273336401747
2022-06-12 01:33:51,370   global_step = 2299
2022-06-12 01:33:51,370   loss = 1.57003738324335
2022-06-12 01:33:51,370   rep_loss = 0.9586100501516845
2022-06-12 01:33:51,370 ***** Save model *****
2022-06-12 01:33:57,058 ***** Running evaluation *****
2022-06-12 01:33:57,058   Epoch = 8 iter 2319 step
2022-06-12 01:33:57,058   Num examples = 1043
2022-06-12 01:33:57,058   Batch size = 32
2022-06-12 01:33:57,059 ***** Eval results *****
2022-06-12 01:33:57,059   att_loss = 0.6095695933683323
2022-06-12 01:33:57,059   global_step = 2319
2022-06-12 01:33:57,059   loss = 1.569019485041092
2022-06-12 01:33:57,059   rep_loss = 0.9594498921613224
2022-06-12 01:33:57,060 ***** Save model *****
2022-06-12 01:34:02,752 ***** Running evaluation *****
2022-06-12 01:34:02,753   Epoch = 8 iter 2339 step
2022-06-12 01:34:02,753   Num examples = 1043
2022-06-12 01:34:02,753   Batch size = 32
2022-06-12 01:34:02,754 ***** Eval results *****
2022-06-12 01:34:02,754   att_loss = 0.6095628896957548
2022-06-12 01:34:02,755   global_step = 2339
2022-06-12 01:34:02,755   loss = 1.568542846317949
2022-06-12 01:34:02,755   rep_loss = 0.9589799577966699
2022-06-12 01:34:02,755 ***** Save model *****
2022-06-12 01:34:08,401 ***** Running evaluation *****
2022-06-12 01:34:08,402   Epoch = 8 iter 2359 step
2022-06-12 01:34:08,402   Num examples = 1043
2022-06-12 01:34:08,402   Batch size = 32
2022-06-12 01:34:08,403 ***** Eval results *****
2022-06-12 01:34:08,403   att_loss = 0.6116959302949264
2022-06-12 01:34:08,403   global_step = 2359
2022-06-12 01:34:08,403   loss = 1.572278210400466
2022-06-12 01:34:08,403   rep_loss = 0.9605822811746811
2022-06-12 01:34:08,404 ***** Save model *****
2022-06-12 01:34:14,081 ***** Running evaluation *****
2022-06-12 01:34:14,082   Epoch = 8 iter 2379 step
2022-06-12 01:34:14,082   Num examples = 1043
2022-06-12 01:34:14,082   Batch size = 32
2022-06-12 01:34:14,083 ***** Eval results *****
2022-06-12 01:34:14,083   att_loss = 0.6131528694443251
2022-06-12 01:34:14,084   global_step = 2379
2022-06-12 01:34:14,084   loss = 1.5736490855982275
2022-06-12 01:34:14,084   rep_loss = 0.9604962168897621
2022-06-12 01:34:14,084 ***** Save model *****
2022-06-12 01:34:19,770 ***** Running evaluation *****
2022-06-12 01:34:19,771   Epoch = 8 iter 2399 step
2022-06-12 01:34:19,771   Num examples = 1043
2022-06-12 01:34:19,771   Batch size = 32
2022-06-12 01:34:19,772 ***** Eval results *****
2022-06-12 01:34:19,772   att_loss = 0.6153086581384274
2022-06-12 01:34:19,772   global_step = 2399
2022-06-12 01:34:19,772   loss = 1.5767688424868276
2022-06-12 01:34:19,772   rep_loss = 0.9614601844617169
2022-06-12 01:34:19,772 ***** Save model *****
2022-06-12 01:34:25,418 ***** Running evaluation *****
2022-06-12 01:34:25,419   Epoch = 9 iter 2419 step
2022-06-12 01:34:25,419   Num examples = 1043
2022-06-12 01:34:25,419   Batch size = 32
2022-06-12 01:34:25,420 ***** Eval results *****
2022-06-12 01:34:25,420   att_loss = 0.6132684573531151
2022-06-12 01:34:25,420   global_step = 2419
2022-06-12 01:34:25,420   loss = 1.5540681332349777
2022-06-12 01:34:25,420   rep_loss = 0.9407996721565723
2022-06-12 01:34:25,420 ***** Save model *****
2022-06-12 01:34:26,772 ***** Running evaluation *****
2022-06-12 01:34:26,773   Epoch = 3 iter 9999 step
2022-06-12 01:34:26,773   Num examples = 5463
2022-06-12 01:34:26,773   Batch size = 32
2022-06-12 01:34:26,774 ***** Eval results *****
2022-06-12 01:34:26,774   att_loss = 4.022188515133328
2022-06-12 01:34:26,774   global_step = 9999
2022-06-12 01:34:26,774   loss = 4.958178538746304
2022-06-12 01:34:26,774   rep_loss = 0.9359900186459224
2022-06-12 01:34:26,775 ***** Save model *****
2022-06-12 01:34:31,103 ***** Running evaluation *****
2022-06-12 01:34:31,103   Epoch = 9 iter 2439 step
2022-06-12 01:34:31,103   Num examples = 1043
2022-06-12 01:34:31,103   Batch size = 32
2022-06-12 01:34:31,104 ***** Eval results *****
2022-06-12 01:34:31,105   att_loss = 0.608922136326631
2022-06-12 01:34:31,105   global_step = 2439
2022-06-12 01:34:31,105   loss = 1.5551381309827168
2022-06-12 01:34:31,105   rep_loss = 0.9462159954839282
2022-06-12 01:34:31,105 ***** Save model *****
2022-06-12 01:34:36,776 ***** Running evaluation *****
2022-06-12 01:34:36,776   Epoch = 9 iter 2459 step
2022-06-12 01:34:36,776   Num examples = 1043
2022-06-12 01:34:36,776   Batch size = 32
2022-06-12 01:34:36,778 ***** Eval results *****
2022-06-12 01:34:36,778   att_loss = 0.6073676629790238
2022-06-12 01:34:36,778   global_step = 2459
2022-06-12 01:34:36,778   loss = 1.5558667651244573
2022-06-12 01:34:36,778   rep_loss = 0.9484991069350924
2022-06-12 01:34:36,778 ***** Save model *****
2022-06-12 01:34:42,429 ***** Running evaluation *****
2022-06-12 01:34:42,430   Epoch = 9 iter 2479 step
2022-06-12 01:34:42,430   Num examples = 1043
2022-06-12 01:34:42,430   Batch size = 32
2022-06-12 01:34:42,431 ***** Eval results *****
2022-06-12 01:34:42,431   att_loss = 0.6103330901578853
2022-06-12 01:34:42,431   global_step = 2479
2022-06-12 01:34:42,431   loss = 1.5613376027659367
2022-06-12 01:34:42,431   rep_loss = 0.9510045130001871
2022-06-12 01:34:42,431 ***** Save model *****
2022-06-12 01:34:48,103 ***** Running evaluation *****
2022-06-12 01:34:48,104   Epoch = 9 iter 2499 step
2022-06-12 01:34:48,104   Num examples = 1043
2022-06-12 01:34:48,104   Batch size = 32
2022-06-12 01:34:48,105 ***** Eval results *****
2022-06-12 01:34:48,105   att_loss = 0.6058405001337329
2022-06-12 01:34:48,105   global_step = 2499
2022-06-12 01:34:48,105   loss = 1.5531754791736603
2022-06-12 01:34:48,105   rep_loss = 0.9473349799712499
2022-06-12 01:34:48,106 ***** Save model *****
2022-06-12 01:34:53,787 ***** Running evaluation *****
2022-06-12 01:34:53,788   Epoch = 9 iter 2519 step
2022-06-12 01:34:53,788   Num examples = 1043
2022-06-12 01:34:53,788   Batch size = 32
2022-06-12 01:34:53,790 ***** Eval results *****
2022-06-12 01:34:53,790   att_loss = 0.6074656553823372
2022-06-12 01:34:53,790   global_step = 2519
2022-06-12 01:34:53,790   loss = 1.5559916640150135
2022-06-12 01:34:53,790   rep_loss = 0.948526009403426
2022-06-12 01:34:53,790 ***** Save model *****
2022-06-12 01:34:59,470 ***** Running evaluation *****
2022-06-12 01:34:59,471   Epoch = 9 iter 2539 step
2022-06-12 01:34:59,471   Num examples = 1043
2022-06-12 01:34:59,471   Batch size = 32
2022-06-12 01:34:59,472 ***** Eval results *****
2022-06-12 01:34:59,472   att_loss = 0.607936979874092
2022-06-12 01:34:59,473   global_step = 2539
2022-06-12 01:34:59,473   loss = 1.5577128941521925
2022-06-12 01:34:59,473   rep_loss = 0.9497759171268519
2022-06-12 01:34:59,473 ***** Save model *****
2022-06-12 01:35:05,155 ***** Running evaluation *****
2022-06-12 01:35:05,156   Epoch = 9 iter 2559 step
2022-06-12 01:35:05,156   Num examples = 1043
2022-06-12 01:35:05,156   Batch size = 32
2022-06-12 01:35:05,157 ***** Eval results *****
2022-06-12 01:35:05,158   att_loss = 0.608513168608531
2022-06-12 01:35:05,158   global_step = 2559
2022-06-12 01:35:05,158   loss = 1.5579592379239888
2022-06-12 01:35:05,158   rep_loss = 0.9494460721810659
2022-06-12 01:35:05,158 ***** Save model *****
2022-06-12 01:35:10,843 ***** Running evaluation *****
2022-06-12 01:35:10,844   Epoch = 9 iter 2579 step
2022-06-12 01:35:10,844   Num examples = 1043
2022-06-12 01:35:10,844   Batch size = 32
2022-06-12 01:35:10,845 ***** Eval results *****
2022-06-12 01:35:10,845   att_loss = 0.6075283624231815
2022-06-12 01:35:10,845   global_step = 2579
2022-06-12 01:35:10,845   loss = 1.5568781210617586
2022-06-12 01:35:10,845   rep_loss = 0.9493497613478791
2022-06-12 01:35:10,845 ***** Save model *****
2022-06-12 01:35:16,548 ***** Running evaluation *****
2022-06-12 01:35:16,549   Epoch = 9 iter 2599 step
2022-06-12 01:35:16,549   Num examples = 1043
2022-06-12 01:35:16,549   Batch size = 32
2022-06-12 01:35:16,550 ***** Eval results *****
2022-06-12 01:35:16,551   att_loss = 0.6074087812888379
2022-06-12 01:35:16,551   global_step = 2599
2022-06-12 01:35:16,551   loss = 1.555947629772887
2022-06-12 01:35:16,551   rep_loss = 0.9485388519812603
2022-06-12 01:35:16,551 ***** Save model *****
2022-06-12 01:35:22,173 ***** Running evaluation *****
2022-06-12 01:35:22,173   Epoch = 9 iter 2619 step
2022-06-12 01:35:22,173   Num examples = 1043
2022-06-12 01:35:22,173   Batch size = 32
2022-06-12 01:35:22,175 ***** Eval results *****
2022-06-12 01:35:22,175   att_loss = 0.6058487118118339
2022-06-12 01:35:22,175   global_step = 2619
2022-06-12 01:35:22,175   loss = 1.552376577699626
2022-06-12 01:35:22,175   rep_loss = 0.9465278698890297
2022-06-12 01:35:22,175 ***** Save model *****
2022-06-12 01:35:27,812 ***** Running evaluation *****
2022-06-12 01:35:27,813   Epoch = 9 iter 2639 step
2022-06-12 01:35:27,813   Num examples = 1043
2022-06-12 01:35:27,813   Batch size = 32
2022-06-12 01:35:27,814 ***** Eval results *****
2022-06-12 01:35:27,814   att_loss = 0.6057866341986898
2022-06-12 01:35:27,814   global_step = 2639
2022-06-12 01:35:27,814   loss = 1.5521374587285317
2022-06-12 01:35:27,815   rep_loss = 0.9463508275605864
2022-06-12 01:35:27,815 ***** Save model *****
2022-06-12 01:35:33,536 ***** Running evaluation *****
2022-06-12 01:35:33,536   Epoch = 9 iter 2659 step
2022-06-12 01:35:33,537   Num examples = 1043
2022-06-12 01:35:33,537   Batch size = 32
2022-06-12 01:35:33,538 ***** Eval results *****
2022-06-12 01:35:33,538   att_loss = 0.6054499953752384
2022-06-12 01:35:33,538   global_step = 2659
2022-06-12 01:35:33,538   loss = 1.55157122714445
2022-06-12 01:35:33,538   rep_loss = 0.946121234446764
2022-06-12 01:35:33,539 ***** Save model *****
2022-06-12 01:35:39,199 ***** Running evaluation *****
2022-06-12 01:35:39,200   Epoch = 10 iter 2679 step
2022-06-12 01:35:39,200   Num examples = 1043
2022-06-12 01:35:39,200   Batch size = 32
2022-06-12 01:35:39,201 ***** Eval results *****
2022-06-12 01:35:39,201   att_loss = 0.5473816924624972
2022-06-12 01:35:39,201   global_step = 2679
2022-06-12 01:35:39,201   loss = 1.4587488306893244
2022-06-12 01:35:39,202   rep_loss = 0.9113671448495653
2022-06-12 01:35:39,202 ***** Save model *****
2022-06-12 01:35:44,836 ***** Running evaluation *****
2022-06-12 01:35:44,836   Epoch = 10 iter 2699 step
2022-06-12 01:35:44,836   Num examples = 1043
2022-06-12 01:35:44,836   Batch size = 32
2022-06-12 01:35:44,838 ***** Eval results *****
2022-06-12 01:35:44,838   att_loss = 0.5679697250497753
2022-06-12 01:35:44,838   global_step = 2699
2022-06-12 01:35:44,838   loss = 1.4846021964632232
2022-06-12 01:35:44,838   rep_loss = 0.9166324775794457
2022-06-12 01:35:44,839 ***** Save model *****
2022-06-12 01:35:50,517 ***** Running evaluation *****
2022-06-12 01:35:50,518   Epoch = 10 iter 2719 step
2022-06-12 01:35:50,518   Num examples = 1043
2022-06-12 01:35:50,518   Batch size = 32
2022-06-12 01:35:50,519 ***** Eval results *****
2022-06-12 01:35:50,519   att_loss = 0.5670682307408781
2022-06-12 01:35:50,519   global_step = 2719
2022-06-12 01:35:50,519   loss = 1.4805905916252915
2022-06-12 01:35:50,520   rep_loss = 0.9135223578433601
2022-06-12 01:35:50,520 ***** Save model *****
2022-06-12 01:35:56,245 ***** Running evaluation *****
2022-06-12 01:35:56,246   Epoch = 10 iter 2739 step
2022-06-12 01:35:56,246   Num examples = 1043
2022-06-12 01:35:56,246   Batch size = 32
2022-06-12 01:35:56,247 ***** Eval results *****
2022-06-12 01:35:56,247   att_loss = 0.5751790205637614
2022-06-12 01:35:56,247   global_step = 2739
2022-06-12 01:35:56,248   loss = 1.4951205046280571
2022-06-12 01:35:56,248   rep_loss = 0.9199414788812831
2022-06-12 01:35:56,248 ***** Save model *****
2022-06-12 01:36:01,934 ***** Running evaluation *****
2022-06-12 01:36:01,935   Epoch = 10 iter 2759 step
2022-06-12 01:36:01,935   Num examples = 1043
2022-06-12 01:36:01,935   Batch size = 32
2022-06-12 01:36:01,936 ***** Eval results *****
2022-06-12 01:36:01,936   att_loss = 0.5769031154975462
2022-06-12 01:36:01,936   global_step = 2759
2022-06-12 01:36:01,936   loss = 1.497858745328496
2022-06-12 01:36:01,936   rep_loss = 0.9209556284915196
2022-06-12 01:36:01,936 ***** Save model *****
2022-06-12 01:36:07,588 ***** Running evaluation *****
2022-06-12 01:36:07,589   Epoch = 10 iter 2779 step
2022-06-12 01:36:07,589   Num examples = 1043
2022-06-12 01:36:07,589   Batch size = 32
2022-06-12 01:36:07,591 ***** Eval results *****
2022-06-12 01:36:07,591   att_loss = 0.5739457115120844
2022-06-12 01:36:07,591   global_step = 2779
2022-06-12 01:36:07,591   loss = 1.4936920470053996
2022-06-12 01:36:07,591   rep_loss = 0.9197463365869785
2022-06-12 01:36:07,591 ***** Save model *****
2022-06-12 01:36:13,277 ***** Running evaluation *****
2022-06-12 01:36:13,278   Epoch = 10 iter 2799 step
2022-06-12 01:36:13,278   Num examples = 1043
2022-06-12 01:36:13,278   Batch size = 32
2022-06-12 01:36:13,279 ***** Eval results *****
2022-06-12 01:36:13,279   att_loss = 0.5756639540195465
2022-06-12 01:36:13,279   global_step = 2799
2022-06-12 01:36:13,279   loss = 1.4957825231921764
2022-06-12 01:36:13,279   rep_loss = 0.9201185707898103
2022-06-12 01:36:13,279 ***** Save model *****
2022-06-12 01:36:18,983 ***** Running evaluation *****
2022-06-12 01:36:18,984   Epoch = 10 iter 2819 step
2022-06-12 01:36:18,984   Num examples = 1043
2022-06-12 01:36:18,984   Batch size = 32
2022-06-12 01:36:18,986 ***** Eval results *****
2022-06-12 01:36:18,986   att_loss = 0.5750354142397042
2022-06-12 01:36:18,986   global_step = 2819
2022-06-12 01:36:18,986   loss = 1.495318864015925
2022-06-12 01:36:18,986   rep_loss = 0.9202834511763297
2022-06-12 01:36:18,987 ***** Save model *****
2022-06-12 01:36:24,699 ***** Running evaluation *****
2022-06-12 01:36:24,699   Epoch = 10 iter 2839 step
2022-06-12 01:36:24,699   Num examples = 1043
2022-06-12 01:36:24,699   Batch size = 32
2022-06-12 01:36:24,700 ***** Eval results *****
2022-06-12 01:36:24,700   att_loss = 0.5768869235318088
2022-06-12 01:36:24,700   global_step = 2839
2022-06-12 01:36:24,700   loss = 1.4979762029365675
2022-06-12 01:36:24,700   rep_loss = 0.921089279933794
2022-06-12 01:36:24,701 ***** Save model *****
2022-06-12 01:36:30,399 ***** Running evaluation *****
2022-06-12 01:36:30,400   Epoch = 10 iter 2859 step
2022-06-12 01:36:30,401   Num examples = 1043
2022-06-12 01:36:30,401   Batch size = 32
2022-06-12 01:36:30,402 ***** Eval results *****
2022-06-12 01:36:30,402   att_loss = 0.575265181443048
2022-06-12 01:36:30,402   global_step = 2859
2022-06-12 01:36:30,402   loss = 1.4957273731786738
2022-06-12 01:36:30,402   rep_loss = 0.9204621917356259
2022-06-12 01:36:30,402 ***** Save model *****
2022-06-12 01:36:34,544 ***** Running evaluation *****
2022-06-12 01:36:34,544   Epoch = 3 iter 10499 step
2022-06-12 01:36:34,544   Num examples = 5463
2022-06-12 01:36:34,544   Batch size = 32
2022-06-12 01:36:34,546 ***** Eval results *****
2022-06-12 01:36:34,546   att_loss = 4.020459586031297
2022-06-12 01:36:34,546   global_step = 10499
2022-06-12 01:36:34,546   loss = 4.95626330095179
2022-06-12 01:36:34,546   rep_loss = 0.9358037196538027
2022-06-12 01:36:34,546 ***** Save model *****
2022-06-12 01:36:36,131 ***** Running evaluation *****
2022-06-12 01:36:36,132   Epoch = 10 iter 2879 step
2022-06-12 01:36:36,132   Num examples = 1043
2022-06-12 01:36:36,132   Batch size = 32
2022-06-12 01:36:36,133 ***** Eval results *****
2022-06-12 01:36:36,133   att_loss = 0.5793668208510111
2022-06-12 01:36:36,133   global_step = 2879
2022-06-12 01:36:36,133   loss = 1.5008311397150944
2022-06-12 01:36:36,133   rep_loss = 0.9214643182937038
2022-06-12 01:36:36,133 ***** Save model *****
2022-06-12 01:36:41,829 ***** Running evaluation *****
2022-06-12 01:36:41,830   Epoch = 10 iter 2899 step
2022-06-12 01:36:41,830   Num examples = 1043
2022-06-12 01:36:41,830   Batch size = 32
2022-06-12 01:36:41,831 ***** Eval results *****
2022-06-12 01:36:41,831   att_loss = 0.5812041889632112
2022-06-12 01:36:41,832   global_step = 2899
2022-06-12 01:36:41,832   loss = 1.5035133528397073
2022-06-12 01:36:41,832   rep_loss = 0.9223091633559315
2022-06-12 01:36:41,832 ***** Save model *****
2022-06-12 01:36:47,559 ***** Running evaluation *****
2022-06-12 01:36:47,560   Epoch = 10 iter 2919 step
2022-06-12 01:36:47,560   Num examples = 1043
2022-06-12 01:36:47,560   Batch size = 32
2022-06-12 01:36:47,561 ***** Eval results *****
2022-06-12 01:36:47,561   att_loss = 0.5818049840659023
2022-06-12 01:36:47,562   global_step = 2919
2022-06-12 01:36:47,562   loss = 1.504152419097931
2022-06-12 01:36:47,562   rep_loss = 0.9223474367076613
2022-06-12 01:36:47,562 ***** Save model *****
2022-06-12 01:36:53,294 ***** Running evaluation *****
2022-06-12 01:36:53,295   Epoch = 11 iter 2939 step
2022-06-12 01:36:53,295   Num examples = 1043
2022-06-12 01:36:53,295   Batch size = 32
2022-06-12 01:36:53,296 ***** Eval results *****
2022-06-12 01:36:53,296   att_loss = 0.6051508486270905
2022-06-12 01:36:53,296   global_step = 2939
2022-06-12 01:36:53,297   loss = 1.5175055265426636
2022-06-12 01:36:53,297   rep_loss = 0.9123547375202179
2022-06-12 01:36:53,297 ***** Save model *****
2022-06-12 01:36:59,025 ***** Running evaluation *****
2022-06-12 01:36:59,026   Epoch = 11 iter 2959 step
2022-06-12 01:36:59,026   Num examples = 1043
2022-06-12 01:36:59,026   Batch size = 32
2022-06-12 01:36:59,027 ***** Eval results *****
2022-06-12 01:36:59,027   att_loss = 0.5304072323170576
2022-06-12 01:36:59,027   global_step = 2959
2022-06-12 01:36:59,027   loss = 1.4263643297282131
2022-06-12 01:36:59,027   rep_loss = 0.8959571068937128
2022-06-12 01:36:59,027 ***** Save model *****
2022-06-12 01:37:04,677 ***** Running evaluation *****
2022-06-12 01:37:04,678   Epoch = 11 iter 2979 step
2022-06-12 01:37:04,678   Num examples = 1043
2022-06-12 01:37:04,678   Batch size = 32
2022-06-12 01:37:04,679 ***** Eval results *****
2022-06-12 01:37:04,679   att_loss = 0.5584754106544313
2022-06-12 01:37:04,679   global_step = 2979
2022-06-12 01:37:04,679   loss = 1.4630526417777652
2022-06-12 01:37:04,679   rep_loss = 0.9045772410574413
2022-06-12 01:37:04,679 ***** Save model *****
2022-06-12 01:37:10,338 ***** Running evaluation *****
2022-06-12 01:37:10,339   Epoch = 11 iter 2999 step
2022-06-12 01:37:10,339   Num examples = 1043
2022-06-12 01:37:10,339   Batch size = 32
2022-06-12 01:37:10,340 ***** Eval results *****
2022-06-12 01:37:10,341   att_loss = 0.560440273534867
2022-06-12 01:37:10,341   global_step = 2999
2022-06-12 01:37:10,341   loss = 1.464711841075651
2022-06-12 01:37:10,341   rep_loss = 0.9042715718669276
2022-06-12 01:37:10,341 ***** Save model *****
2022-06-12 01:37:16,021 ***** Running evaluation *****
2022-06-12 01:37:16,022   Epoch = 11 iter 3019 step
2022-06-12 01:37:16,022   Num examples = 1043
2022-06-12 01:37:16,022   Batch size = 32
2022-06-12 01:37:16,023 ***** Eval results *****
2022-06-12 01:37:16,023   att_loss = 0.567563542142147
2022-06-12 01:37:16,023   global_step = 3019
2022-06-12 01:37:16,023   loss = 1.47565542197809
2022-06-12 01:37:16,023   rep_loss = 0.9080918838338154
2022-06-12 01:37:16,023 ***** Save model *****
2022-06-12 01:37:21,698 ***** Running evaluation *****
2022-06-12 01:37:21,699   Epoch = 11 iter 3039 step
2022-06-12 01:37:21,699   Num examples = 1043
2022-06-12 01:37:21,699   Batch size = 32
2022-06-12 01:37:21,700 ***** Eval results *****
2022-06-12 01:37:21,700   att_loss = 0.5719073949491277
2022-06-12 01:37:21,700   global_step = 3039
2022-06-12 01:37:21,701   loss = 1.4826827540117151
2022-06-12 01:37:21,701   rep_loss = 0.910775363445282
2022-06-12 01:37:21,701 ***** Save model *****
2022-06-12 01:37:27,440 ***** Running evaluation *****
2022-06-12 01:37:27,441   Epoch = 11 iter 3059 step
2022-06-12 01:37:27,441   Num examples = 1043
2022-06-12 01:37:27,441   Batch size = 32
2022-06-12 01:37:27,442 ***** Eval results *****
2022-06-12 01:37:27,442   att_loss = 0.5717099924556545
2022-06-12 01:37:27,442   global_step = 3059
2022-06-12 01:37:27,442   loss = 1.4831688706992103
2022-06-12 01:37:27,442   rep_loss = 0.9114588816634944
2022-06-12 01:37:27,443 ***** Save model *****
2022-06-12 01:37:33,116 ***** Running evaluation *****
2022-06-12 01:37:33,116   Epoch = 11 iter 3079 step
2022-06-12 01:37:33,116   Num examples = 1043
2022-06-12 01:37:33,116   Batch size = 32
2022-06-12 01:37:33,118 ***** Eval results *****
2022-06-12 01:37:33,118   att_loss = 0.5730161436007056
2022-06-12 01:37:33,118   global_step = 3079
2022-06-12 01:37:33,118   loss = 1.4855777604479186
2022-06-12 01:37:33,118   rep_loss = 0.91256161978547
2022-06-12 01:37:33,118 ***** Save model *****
2022-06-12 01:37:38,821 ***** Running evaluation *****
2022-06-12 01:37:38,821   Epoch = 11 iter 3099 step
2022-06-12 01:37:38,822   Num examples = 1043
2022-06-12 01:37:38,822   Batch size = 32
2022-06-12 01:37:38,823 ***** Eval results *****
2022-06-12 01:37:38,823   att_loss = 0.5735380217249011
2022-06-12 01:37:38,823   global_step = 3099
2022-06-12 01:37:38,823   loss = 1.487689910111604
2022-06-12 01:37:38,823   rep_loss = 0.9141518907782472
2022-06-12 01:37:38,823 ***** Save model *****
2022-06-12 01:37:44,498 ***** Running evaluation *****
2022-06-12 01:37:44,499   Epoch = 11 iter 3119 step
2022-06-12 01:37:44,499   Num examples = 1043
2022-06-12 01:37:44,499   Batch size = 32
2022-06-12 01:37:44,500 ***** Eval results *****
2022-06-12 01:37:44,500   att_loss = 0.5715400046044654
2022-06-12 01:37:44,500   global_step = 3119
2022-06-12 01:37:44,500   loss = 1.4850063690772424
2022-06-12 01:37:44,501   rep_loss = 0.9134663657827692
2022-06-12 01:37:44,501 ***** Save model *****
2022-06-12 01:37:50,189 ***** Running evaluation *****
2022-06-12 01:37:50,190   Epoch = 11 iter 3139 step
2022-06-12 01:37:50,190   Num examples = 1043
2022-06-12 01:37:50,190   Batch size = 32
2022-06-12 01:37:50,192 ***** Eval results *****
2022-06-12 01:37:50,192   att_loss = 0.5743257456486768
2022-06-12 01:37:50,192   global_step = 3139
2022-06-12 01:37:50,192   loss = 1.4887040570230767
2022-06-12 01:37:50,192   rep_loss = 0.9143783119645449
2022-06-12 01:37:50,192 ***** Save model *****
2022-06-12 01:37:55,851 ***** Running evaluation *****
2022-06-12 01:37:55,851   Epoch = 11 iter 3159 step
2022-06-12 01:37:55,851   Num examples = 1043
2022-06-12 01:37:55,852   Batch size = 32
2022-06-12 01:37:55,853 ***** Eval results *****
2022-06-12 01:37:55,853   att_loss = 0.5731988772078678
2022-06-12 01:37:55,853   global_step = 3159
2022-06-12 01:37:55,853   loss = 1.4870521700060046
2022-06-12 01:37:55,853   rep_loss = 0.9138532949460519
2022-06-12 01:37:55,853 ***** Save model *****
2022-06-12 01:38:01,520 ***** Running evaluation *****
2022-06-12 01:38:01,521   Epoch = 11 iter 3179 step
2022-06-12 01:38:01,521   Num examples = 1043
2022-06-12 01:38:01,521   Batch size = 32
2022-06-12 01:38:01,522 ***** Eval results *****
2022-06-12 01:38:01,522   att_loss = 0.5746989973074148
2022-06-12 01:38:01,522   global_step = 3179
2022-06-12 01:38:01,522   loss = 1.4894360761997127
2022-06-12 01:38:01,523   rep_loss = 0.9147370814784499
2022-06-12 01:38:01,523 ***** Save model *****
2022-06-12 01:38:07,213 ***** Running evaluation *****
2022-06-12 01:38:07,214   Epoch = 11 iter 3199 step
2022-06-12 01:38:07,214   Num examples = 1043
2022-06-12 01:38:07,214   Batch size = 32
2022-06-12 01:38:07,215 ***** Eval results *****
2022-06-12 01:38:07,215   att_loss = 0.5747848415875253
2022-06-12 01:38:07,215   global_step = 3199
2022-06-12 01:38:07,215   loss = 1.4896111629391444
2022-06-12 01:38:07,215   rep_loss = 0.9148263248778482
2022-06-12 01:38:07,215 ***** Save model *****
2022-06-12 01:38:12,900 ***** Running evaluation *****
2022-06-12 01:38:12,901   Epoch = 12 iter 3219 step
2022-06-12 01:38:12,901   Num examples = 1043
2022-06-12 01:38:12,901   Batch size = 32
2022-06-12 01:38:12,902 ***** Eval results *****
2022-06-12 01:38:12,902   att_loss = 0.5743093411127727
2022-06-12 01:38:12,902   global_step = 3219
2022-06-12 01:38:12,903   loss = 1.4883408784866332
2022-06-12 01:38:12,903   rep_loss = 0.9140315175056457
2022-06-12 01:38:12,903 ***** Save model *****
2022-06-12 01:38:18,574 ***** Running evaluation *****
2022-06-12 01:38:18,575   Epoch = 12 iter 3239 step
2022-06-12 01:38:18,575   Num examples = 1043
2022-06-12 01:38:18,575   Batch size = 32
2022-06-12 01:38:18,576 ***** Eval results *****
2022-06-12 01:38:18,576   att_loss = 0.558799901178905
2022-06-12 01:38:18,576   global_step = 3239
2022-06-12 01:38:18,576   loss = 1.4621989522661483
2022-06-12 01:38:18,576   rep_loss = 0.9033990434237889
2022-06-12 01:38:18,576 ***** Save model *****
2022-06-12 01:38:24,272 ***** Running evaluation *****
2022-06-12 01:38:24,273   Epoch = 12 iter 3259 step
2022-06-12 01:38:24,273   Num examples = 1043
2022-06-12 01:38:24,273   Batch size = 32
2022-06-12 01:38:24,274 ***** Eval results *****
2022-06-12 01:38:24,274   att_loss = 0.5626805879852989
2022-06-12 01:38:24,274   global_step = 3259
2022-06-12 01:38:24,274   loss = 1.4667966019023548
2022-06-12 01:38:24,274   rep_loss = 0.9041160052472895
2022-06-12 01:38:24,274 ***** Save model *****
2022-06-12 01:38:29,925 ***** Running evaluation *****
2022-06-12 01:38:29,926   Epoch = 12 iter 3279 step
2022-06-12 01:38:29,926   Num examples = 1043
2022-06-12 01:38:29,926   Batch size = 32
2022-06-12 01:38:29,927 ***** Eval results *****
2022-06-12 01:38:29,927   att_loss = 0.5613321399688721
2022-06-12 01:38:29,927   global_step = 3279
2022-06-12 01:38:29,927   loss = 1.4638242197036744
2022-06-12 01:38:29,927   rep_loss = 0.9024920725822448
2022-06-12 01:38:29,927 ***** Save model *****
2022-06-12 01:38:35,616 ***** Running evaluation *****
2022-06-12 01:38:35,617   Epoch = 12 iter 3299 step
2022-06-12 01:38:35,617   Num examples = 1043
2022-06-12 01:38:35,617   Batch size = 32
2022-06-12 01:38:35,619 ***** Eval results *****
2022-06-12 01:38:35,619   att_loss = 0.5593600470768778
2022-06-12 01:38:35,619   global_step = 3299
2022-06-12 01:38:35,619   loss = 1.4599376452596564
2022-06-12 01:38:35,619   rep_loss = 0.9005775915948968
2022-06-12 01:38:35,619 ***** Save model *****
2022-06-12 01:38:41,300 ***** Running evaluation *****
2022-06-12 01:38:41,301   Epoch = 12 iter 3319 step
2022-06-12 01:38:41,301   Num examples = 1043
2022-06-12 01:38:41,301   Batch size = 32
2022-06-12 01:38:41,302 ***** Eval results *****
2022-06-12 01:38:41,302   att_loss = 0.5595364767572154
2022-06-12 01:38:41,302   global_step = 3319
2022-06-12 01:38:41,302   loss = 1.4595010363537333
2022-06-12 01:38:41,302   rep_loss = 0.899964551821999
2022-06-12 01:38:41,302 ***** Save model *****
2022-06-12 01:38:42,706 ***** Running evaluation *****
2022-06-12 01:38:42,707   Epoch = 3 iter 10999 step
2022-06-12 01:38:42,707   Num examples = 5463
2022-06-12 01:38:42,707   Batch size = 32
2022-06-12 01:38:42,708 ***** Eval results *****
2022-06-12 01:38:42,709   att_loss = 4.015531525571467
2022-06-12 01:38:42,709   global_step = 10999
2022-06-12 01:38:42,709   loss = 4.950284344867124
2022-06-12 01:38:42,709   rep_loss = 0.9347528161638874
2022-06-12 01:38:42,709 ***** Save model *****
2022-06-12 01:38:46,971 ***** Running evaluation *****
2022-06-12 01:38:46,971   Epoch = 12 iter 3339 step
2022-06-12 01:38:46,971   Num examples = 1043
2022-06-12 01:38:46,971   Batch size = 32
2022-06-12 01:38:46,972 ***** Eval results *****
2022-06-12 01:38:46,972   att_loss = 0.5634339451789856
2022-06-12 01:38:46,973   global_step = 3339
2022-06-12 01:38:46,973   loss = 1.465567085478041
2022-06-12 01:38:46,973   rep_loss = 0.9021331327932852
2022-06-12 01:38:46,973 ***** Save model *****
2022-06-12 01:38:52,637 ***** Running evaluation *****
2022-06-12 01:38:52,637   Epoch = 12 iter 3359 step
2022-06-12 01:38:52,638   Num examples = 1043
2022-06-12 01:38:52,638   Batch size = 32
2022-06-12 01:38:52,639 ***** Eval results *****
2022-06-12 01:38:52,639   att_loss = 0.5629941082769825
2022-06-12 01:38:52,639   global_step = 3359
2022-06-12 01:38:52,640   loss = 1.4641071035015967
2022-06-12 01:38:52,640   rep_loss = 0.9011129890718768
2022-06-12 01:38:52,640 ***** Save model *****
2022-06-12 01:38:58,333 ***** Running evaluation *****
2022-06-12 01:38:58,334   Epoch = 12 iter 3379 step
2022-06-12 01:38:58,334   Num examples = 1043
2022-06-12 01:38:58,334   Batch size = 32
2022-06-12 01:38:58,335 ***** Eval results *****
2022-06-12 01:38:58,335   att_loss = 0.5672433229855128
2022-06-12 01:38:58,335   global_step = 3379
2022-06-12 01:38:58,335   loss = 1.4697548191887992
2022-06-12 01:38:58,335   rep_loss = 0.9025114910943167
2022-06-12 01:38:58,336 ***** Save model *****
2022-06-12 01:39:04,044 ***** Running evaluation *****
2022-06-12 01:39:04,044   Epoch = 12 iter 3399 step
2022-06-12 01:39:04,044   Num examples = 1043
2022-06-12 01:39:04,045   Batch size = 32
2022-06-12 01:39:04,046 ***** Eval results *****
2022-06-12 01:39:04,046   att_loss = 0.5644766243604513
2022-06-12 01:39:04,046   global_step = 3399
2022-06-12 01:39:04,046   loss = 1.4657438547183306
2022-06-12 01:39:04,046   rep_loss = 0.9012672250087445
2022-06-12 01:39:04,047 ***** Save model *****
2022-06-12 01:39:09,739 ***** Running evaluation *****
2022-06-12 01:39:09,740   Epoch = 12 iter 3419 step
2022-06-12 01:39:09,740   Num examples = 1043
2022-06-12 01:39:09,740   Batch size = 32
2022-06-12 01:39:09,741 ***** Eval results *****
2022-06-12 01:39:09,741   att_loss = 0.5633131260095641
2022-06-12 01:39:09,741   global_step = 3419
2022-06-12 01:39:09,741   loss = 1.4638910537542298
2022-06-12 01:39:09,741   rep_loss = 0.9005779230317404
2022-06-12 01:39:09,741 ***** Save model *****
2022-06-12 01:39:15,430 ***** Running evaluation *****
2022-06-12 01:39:15,431   Epoch = 12 iter 3439 step
2022-06-12 01:39:15,431   Num examples = 1043
2022-06-12 01:39:15,431   Batch size = 32
2022-06-12 01:39:15,433 ***** Eval results *****
2022-06-12 01:39:15,433   att_loss = 0.5626692212642508
2022-06-12 01:39:15,433   global_step = 3439
2022-06-12 01:39:15,433   loss = 1.4625466975759953
2022-06-12 01:39:15,433   rep_loss = 0.8998774736485583
2022-06-12 01:39:15,433 ***** Save model *****
2022-06-12 01:39:21,135 ***** Running evaluation *****
2022-06-12 01:39:21,136   Epoch = 12 iter 3459 step
2022-06-12 01:39:21,136   Num examples = 1043
2022-06-12 01:39:21,136   Batch size = 32
2022-06-12 01:39:21,137 ***** Eval results *****
2022-06-12 01:39:21,137   att_loss = 0.5612403118142895
2022-06-12 01:39:21,137   global_step = 3459
2022-06-12 01:39:21,137   loss = 1.4604250791026097
2022-06-12 01:39:21,137   rep_loss = 0.8991847648340113
2022-06-12 01:39:21,137 ***** Save model *****
2022-06-12 01:39:26,806 ***** Running evaluation *****
2022-06-12 01:39:26,806   Epoch = 13 iter 3479 step
2022-06-12 01:39:26,806   Num examples = 1043
2022-06-12 01:39:26,806   Batch size = 32
2022-06-12 01:39:26,807 ***** Eval results *****
2022-06-12 01:39:26,807   att_loss = 0.5721505135297775
2022-06-12 01:39:26,807   global_step = 3479
2022-06-12 01:39:26,807   loss = 1.4726895093917847
2022-06-12 01:39:26,807   rep_loss = 0.9005389958620071
2022-06-12 01:39:26,807 ***** Save model *****
2022-06-12 01:39:32,470 ***** Running evaluation *****
2022-06-12 01:39:32,471   Epoch = 13 iter 3499 step
2022-06-12 01:39:32,471   Num examples = 1043
2022-06-12 01:39:32,471   Batch size = 32
2022-06-12 01:39:32,472 ***** Eval results *****
2022-06-12 01:39:32,472   att_loss = 0.5602429966841426
2022-06-12 01:39:32,472   global_step = 3499
2022-06-12 01:39:32,472   loss = 1.4529130629130773
2022-06-12 01:39:32,472   rep_loss = 0.8926700736795153
2022-06-12 01:39:32,472 ***** Save model *****
2022-06-12 01:39:38,135 ***** Running evaluation *****
2022-06-12 01:39:38,136   Epoch = 13 iter 3519 step
2022-06-12 01:39:38,136   Num examples = 1043
2022-06-12 01:39:38,136   Batch size = 32
2022-06-12 01:39:38,137 ***** Eval results *****
2022-06-12 01:39:38,137   att_loss = 0.5592480699221293
2022-06-12 01:39:38,137   global_step = 3519
2022-06-12 01:39:38,137   loss = 1.4512331436077754
2022-06-12 01:39:38,137   rep_loss = 0.8919850736856461
2022-06-12 01:39:38,137 ***** Save model *****
2022-06-12 01:39:43,831 ***** Running evaluation *****
2022-06-12 01:39:43,831   Epoch = 13 iter 3539 step
2022-06-12 01:39:43,831   Num examples = 1043
2022-06-12 01:39:43,831   Batch size = 32
2022-06-12 01:39:43,833 ***** Eval results *****
2022-06-12 01:39:43,833   att_loss = 0.559069874970352
2022-06-12 01:39:43,833   global_step = 3539
2022-06-12 01:39:43,833   loss = 1.4506344953004051
2022-06-12 01:39:43,833   rep_loss = 0.891564626027556
2022-06-12 01:39:43,833 ***** Save model *****
2022-06-12 01:39:49,498 ***** Running evaluation *****
2022-06-12 01:39:49,499   Epoch = 13 iter 3559 step
2022-06-12 01:39:49,499   Num examples = 1043
2022-06-12 01:39:49,499   Batch size = 32
2022-06-12 01:39:49,500 ***** Eval results *****
2022-06-12 01:39:49,500   att_loss = 0.5622557374564084
2022-06-12 01:39:49,501   global_step = 3559
2022-06-12 01:39:49,501   loss = 1.453180269761519
2022-06-12 01:39:49,501   rep_loss = 0.8909245390783657
2022-06-12 01:39:49,501 ***** Save model *****
2022-06-12 01:39:55,223 ***** Running evaluation *****
2022-06-12 01:39:55,224   Epoch = 13 iter 3579 step
2022-06-12 01:39:55,224   Num examples = 1043
2022-06-12 01:39:55,224   Batch size = 32
2022-06-12 01:39:55,225 ***** Eval results *****
2022-06-12 01:39:55,225   att_loss = 0.5601211899408588
2022-06-12 01:39:55,225   global_step = 3579
2022-06-12 01:39:55,225   loss = 1.4511460838494477
2022-06-12 01:39:55,225   rep_loss = 0.891024899703485
2022-06-12 01:39:55,225 ***** Save model *****
2022-06-12 01:40:00,922 ***** Running evaluation *****
2022-06-12 01:40:00,922   Epoch = 13 iter 3599 step
2022-06-12 01:40:00,922   Num examples = 1043
2022-06-12 01:40:00,922   Batch size = 32
2022-06-12 01:40:00,924 ***** Eval results *****
2022-06-12 01:40:00,924   att_loss = 0.5588837238028646
2022-06-12 01:40:00,924   global_step = 3599
2022-06-12 01:40:00,924   loss = 1.450294683687389
2022-06-12 01:40:00,924   rep_loss = 0.8914109654724598
2022-06-12 01:40:00,925 ***** Save model *****
2022-06-12 01:40:06,608 ***** Running evaluation *****
2022-06-12 01:40:06,609   Epoch = 13 iter 3619 step
2022-06-12 01:40:06,609   Num examples = 1043
2022-06-12 01:40:06,609   Batch size = 32
2022-06-12 01:40:06,610 ***** Eval results *****
2022-06-12 01:40:06,610   att_loss = 0.5588095135785438
2022-06-12 01:40:06,610   global_step = 3619
2022-06-12 01:40:06,611   loss = 1.4503966449080288
2022-06-12 01:40:06,611   rep_loss = 0.8915871337458894
2022-06-12 01:40:06,611 ***** Save model *****
2022-06-12 01:40:12,296 ***** Running evaluation *****
2022-06-12 01:40:12,297   Epoch = 13 iter 3639 step
2022-06-12 01:40:12,297   Num examples = 1043
2022-06-12 01:40:12,297   Batch size = 32
2022-06-12 01:40:12,298 ***** Eval results *****
2022-06-12 01:40:12,299   att_loss = 0.5553240545448803
2022-06-12 01:40:12,299   global_step = 3639
2022-06-12 01:40:12,299   loss = 1.4453974720977603
2022-06-12 01:40:12,299   rep_loss = 0.8900734207459858
2022-06-12 01:40:12,299 ***** Save model *****
2022-06-12 01:40:17,958 ***** Running evaluation *****
2022-06-12 01:40:17,959   Epoch = 13 iter 3659 step
2022-06-12 01:40:17,959   Num examples = 1043
2022-06-12 01:40:17,959   Batch size = 32
2022-06-12 01:40:17,960 ***** Eval results *****
2022-06-12 01:40:17,960   att_loss = 0.5553874149918556
2022-06-12 01:40:17,960   global_step = 3659
2022-06-12 01:40:17,960   loss = 1.445361744216148
2022-06-12 01:40:17,960   rep_loss = 0.8899743316021371
2022-06-12 01:40:17,960 ***** Save model *****
2022-06-12 01:40:23,626 ***** Running evaluation *****
2022-06-12 01:40:23,627   Epoch = 13 iter 3679 step
2022-06-12 01:40:23,627   Num examples = 1043
2022-06-12 01:40:23,627   Batch size = 32
2022-06-12 01:40:23,628 ***** Eval results *****
2022-06-12 01:40:23,628   att_loss = 0.5546701301175815
2022-06-12 01:40:23,629   global_step = 3679
2022-06-12 01:40:23,629   loss = 1.4449774651573255
2022-06-12 01:40:23,629   rep_loss = 0.890307336472548
2022-06-12 01:40:23,629 ***** Save model *****
2022-06-12 01:40:29,221 ***** Running evaluation *****
2022-06-12 01:40:29,221   Epoch = 13 iter 3699 step
2022-06-12 01:40:29,222   Num examples = 1043
2022-06-12 01:40:29,222   Batch size = 32
2022-06-12 01:40:29,223 ***** Eval results *****
2022-06-12 01:40:29,223   att_loss = 0.555603666525138
2022-06-12 01:40:29,223   global_step = 3699
2022-06-12 01:40:29,223   loss = 1.4460840256590592
2022-06-12 01:40:29,223   rep_loss = 0.890480359656769
2022-06-12 01:40:29,223 ***** Save model *****
2022-06-12 01:40:34,896 ***** Running evaluation *****
2022-06-12 01:40:34,896   Epoch = 13 iter 3719 step
2022-06-12 01:40:34,896   Num examples = 1043
2022-06-12 01:40:34,896   Batch size = 32
2022-06-12 01:40:34,898 ***** Eval results *****
2022-06-12 01:40:34,898   att_loss = 0.5567520805183919
2022-06-12 01:40:34,898   global_step = 3719
2022-06-12 01:40:34,898   loss = 1.4484401451003166
2022-06-12 01:40:34,898   rep_loss = 0.8916880659038021
2022-06-12 01:40:34,898 ***** Save model *****
2022-06-12 01:40:40,526 ***** Running evaluation *****
2022-06-12 01:40:40,527   Epoch = 14 iter 3739 step
2022-06-12 01:40:40,527   Num examples = 1043
2022-06-12 01:40:40,527   Batch size = 32
2022-06-12 01:40:40,528 ***** Eval results *****
2022-06-12 01:40:40,528   att_loss = 0.4928353428840637
2022-06-12 01:40:40,528   global_step = 3739
2022-06-12 01:40:40,529   loss = 1.3537342548370361
2022-06-12 01:40:40,529   rep_loss = 0.8608989119529724
2022-06-12 01:40:40,529 ***** Save model *****
2022-06-12 01:40:46,201 ***** Running evaluation *****
2022-06-12 01:40:46,202   Epoch = 14 iter 3759 step
2022-06-12 01:40:46,202   Num examples = 1043
2022-06-12 01:40:46,202   Batch size = 32
2022-06-12 01:40:46,204 ***** Eval results *****
2022-06-12 01:40:46,204   att_loss = 0.5286804295721508
2022-06-12 01:40:46,204   global_step = 3759
2022-06-12 01:40:46,204   loss = 1.3950007415953136
2022-06-12 01:40:46,204   rep_loss = 0.86632030634653
2022-06-12 01:40:46,204 ***** Save model *****
2022-06-12 01:40:50,694 ***** Running evaluation *****
2022-06-12 01:40:50,694   Epoch = 3 iter 11499 step
2022-06-12 01:40:50,694   Num examples = 5463
2022-06-12 01:40:50,694   Batch size = 32
2022-06-12 01:40:50,696 ***** Eval results *****
2022-06-12 01:40:50,696   att_loss = 4.007553667397726
2022-06-12 01:40:50,696   global_step = 11499
2022-06-12 01:40:50,696   loss = 4.940634491472017
2022-06-12 01:40:50,696   rep_loss = 0.9330808222293854
2022-06-12 01:40:50,696 ***** Save model *****
2022-06-12 01:40:51,914 ***** Running evaluation *****
2022-06-12 01:40:51,915   Epoch = 14 iter 3779 step
2022-06-12 01:40:51,915   Num examples = 1043
2022-06-12 01:40:51,915   Batch size = 32
2022-06-12 01:40:51,916 ***** Eval results *****
2022-06-12 01:40:51,916   att_loss = 0.5228443712722964
2022-06-12 01:40:51,916   global_step = 3779
2022-06-12 01:40:51,916   loss = 1.3902762924752585
2022-06-12 01:40:51,916   rep_loss = 0.8674319153878747
2022-06-12 01:40:51,916 ***** Save model *****
2022-06-12 01:40:57,592 ***** Running evaluation *****
2022-06-12 01:40:57,593   Epoch = 14 iter 3799 step
2022-06-12 01:40:57,593   Num examples = 1043
2022-06-12 01:40:57,593   Batch size = 32
2022-06-12 01:40:57,594 ***** Eval results *****
2022-06-12 01:40:57,594   att_loss = 0.5248936927709423
2022-06-12 01:40:57,594   global_step = 3799
2022-06-12 01:40:57,595   loss = 1.3952805409665967
2022-06-12 01:40:57,595   rep_loss = 0.8703868437985904
2022-06-12 01:40:57,595 ***** Save model *****
2022-06-12 01:41:03,239 ***** Running evaluation *****
2022-06-12 01:41:03,240   Epoch = 14 iter 3819 step
2022-06-12 01:41:03,240   Num examples = 1043
2022-06-12 01:41:03,240   Batch size = 32
2022-06-12 01:41:03,241 ***** Eval results *****
2022-06-12 01:41:03,241   att_loss = 0.5282864382973423
2022-06-12 01:41:03,241   global_step = 3819
2022-06-12 01:41:03,241   loss = 1.399901916951309
2022-06-12 01:41:03,241   rep_loss = 0.8716154760784574
2022-06-12 01:41:03,242 ***** Save model *****
2022-06-12 01:41:08,884 ***** Running evaluation *****
2022-06-12 01:41:08,885   Epoch = 14 iter 3839 step
2022-06-12 01:41:08,885   Num examples = 1043
2022-06-12 01:41:08,885   Batch size = 32
2022-06-12 01:41:08,886 ***** Eval results *****
2022-06-12 01:41:08,886   att_loss = 0.5311352666651848
2022-06-12 01:41:08,886   global_step = 3839
2022-06-12 01:41:08,886   loss = 1.4039832483423818
2022-06-12 01:41:08,886   rep_loss = 0.8728479802018345
2022-06-12 01:41:08,886 ***** Save model *****
2022-06-12 01:41:14,585 ***** Running evaluation *****
2022-06-12 01:41:14,586   Epoch = 14 iter 3859 step
2022-06-12 01:41:14,586   Num examples = 1043
2022-06-12 01:41:14,586   Batch size = 32
2022-06-12 01:41:14,587 ***** Eval results *****
2022-06-12 01:41:14,587   att_loss = 0.5347980504686182
2022-06-12 01:41:14,587   global_step = 3859
2022-06-12 01:41:14,587   loss = 1.4091106840401642
2022-06-12 01:41:14,587   rep_loss = 0.8743126343104465
2022-06-12 01:41:14,587 ***** Save model *****
2022-06-12 01:41:20,276 ***** Running evaluation *****
2022-06-12 01:41:20,277   Epoch = 14 iter 3879 step
2022-06-12 01:41:20,277   Num examples = 1043
2022-06-12 01:41:20,277   Batch size = 32
2022-06-12 01:41:20,278 ***** Eval results *****
2022-06-12 01:41:20,278   att_loss = 0.5375368669100687
2022-06-12 01:41:20,278   global_step = 3879
2022-06-12 01:41:20,279   loss = 1.4125764107873253
2022-06-12 01:41:20,279   rep_loss = 0.8750395449340767
2022-06-12 01:41:20,279 ***** Save model *****
2022-06-12 01:41:25,946 ***** Running evaluation *****
2022-06-12 01:41:25,946   Epoch = 14 iter 3899 step
2022-06-12 01:41:25,947   Num examples = 1043
2022-06-12 01:41:25,947   Batch size = 32
2022-06-12 01:41:25,948 ***** Eval results *****
2022-06-12 01:41:25,948   att_loss = 0.5413071885242225
2022-06-12 01:41:25,948   global_step = 3899
2022-06-12 01:41:25,948   loss = 1.4176437558594697
2022-06-12 01:41:25,948   rep_loss = 0.8763365671501396
2022-06-12 01:41:25,948 ***** Save model *****
2022-06-12 01:41:31,595 ***** Running evaluation *****
2022-06-12 01:41:31,595   Epoch = 14 iter 3919 step
2022-06-12 01:41:31,596   Num examples = 1043
2022-06-12 01:41:31,596   Batch size = 32
2022-06-12 01:41:31,597 ***** Eval results *****
2022-06-12 01:41:31,597   att_loss = 0.5445587812866295
2022-06-12 01:41:31,597   global_step = 3919
2022-06-12 01:41:31,597   loss = 1.4225761060556654
2022-06-12 01:41:31,597   rep_loss = 0.8780173234518062
2022-06-12 01:41:31,597 ***** Save model *****
2022-06-12 01:41:37,293 ***** Running evaluation *****
2022-06-12 01:41:37,294   Epoch = 14 iter 3939 step
2022-06-12 01:41:37,294   Num examples = 1043
2022-06-12 01:41:37,294   Batch size = 32
2022-06-12 01:41:37,296 ***** Eval results *****
2022-06-12 01:41:37,296   att_loss = 0.5430132457569464
2022-06-12 01:41:37,296   global_step = 3939
2022-06-12 01:41:37,296   loss = 1.4205502491092208
2022-06-12 01:41:37,296   rep_loss = 0.877537001424761
2022-06-12 01:41:37,296 ***** Save model *****
2022-06-12 01:41:42,971 ***** Running evaluation *****
2022-06-12 01:41:42,972   Epoch = 14 iter 3959 step
2022-06-12 01:41:42,972   Num examples = 1043
2022-06-12 01:41:42,972   Batch size = 32
2022-06-12 01:41:42,973 ***** Eval results *****
2022-06-12 01:41:42,973   att_loss = 0.5446320480890403
2022-06-12 01:41:42,973   global_step = 3959
2022-06-12 01:41:42,973   loss = 1.4231811033654536
2022-06-12 01:41:42,973   rep_loss = 0.8785490528490748
2022-06-12 01:41:42,974 ***** Save model *****
2022-06-12 01:41:48,637 ***** Running evaluation *****
2022-06-12 01:41:48,638   Epoch = 14 iter 3979 step
2022-06-12 01:41:48,638   Num examples = 1043
2022-06-12 01:41:48,638   Batch size = 32
2022-06-12 01:41:48,639 ***** Eval results *****
2022-06-12 01:41:48,639   att_loss = 0.5475267369717484
2022-06-12 01:41:48,639   global_step = 3979
2022-06-12 01:41:48,639   loss = 1.4275303243106827
2022-06-12 01:41:48,639   rep_loss = 0.8800035843710682
2022-06-12 01:41:48,639 ***** Save model *****
2022-06-12 01:41:54,322 ***** Running evaluation *****
2022-06-12 01:41:54,323   Epoch = 14 iter 3999 step
2022-06-12 01:41:54,323   Num examples = 1043
2022-06-12 01:41:54,323   Batch size = 32
2022-06-12 01:41:54,324 ***** Eval results *****
2022-06-12 01:41:54,324   att_loss = 0.5476056621677574
2022-06-12 01:41:54,324   global_step = 3999
2022-06-12 01:41:54,324   loss = 1.4282697717805475
2022-06-12 01:41:54,324   rep_loss = 0.8806641079000129
2022-06-12 01:41:54,324 ***** Save model *****
2022-06-12 01:42:00,005 ***** Running evaluation *****
2022-06-12 01:42:00,005   Epoch = 15 iter 4019 step
2022-06-12 01:42:00,005   Num examples = 1043
2022-06-12 01:42:00,006   Batch size = 32
2022-06-12 01:42:00,007 ***** Eval results *****
2022-06-12 01:42:00,007   att_loss = 0.5443068018981388
2022-06-12 01:42:00,007   global_step = 4019
2022-06-12 01:42:00,007   loss = 1.4155033486230033
2022-06-12 01:42:00,007   rep_loss = 0.8711965552398137
2022-06-12 01:42:00,007 ***** Save model *****
2022-06-12 01:42:05,652 ***** Running evaluation *****
2022-06-12 01:42:05,653   Epoch = 15 iter 4039 step
2022-06-12 01:42:05,653   Num examples = 1043
2022-06-12 01:42:05,653   Batch size = 32
2022-06-12 01:42:05,654 ***** Eval results *****
2022-06-12 01:42:05,654   att_loss = 0.5375624158803154
2022-06-12 01:42:05,654   global_step = 4039
2022-06-12 01:42:05,654   loss = 1.4095014684340532
2022-06-12 01:42:05,654   rep_loss = 0.8719390525537378
2022-06-12 01:42:05,654 ***** Save model *****
2022-06-12 01:42:11,323 ***** Running evaluation *****
2022-06-12 01:42:11,324   Epoch = 15 iter 4059 step
2022-06-12 01:42:11,324   Num examples = 1043
2022-06-12 01:42:11,324   Batch size = 32
2022-06-12 01:42:11,325 ***** Eval results *****
2022-06-12 01:42:11,325   att_loss = 0.5346842099119116
2022-06-12 01:42:11,326   global_step = 4059
2022-06-12 01:42:11,326   loss = 1.403739094734192
2022-06-12 01:42:11,326   rep_loss = 0.8690548804071214
2022-06-12 01:42:11,326 ***** Save model *****
2022-06-12 01:42:16,972 ***** Running evaluation *****
2022-06-12 01:42:16,972   Epoch = 15 iter 4079 step
2022-06-12 01:42:16,972   Num examples = 1043
2022-06-12 01:42:16,972   Batch size = 32
2022-06-12 01:42:16,974 ***** Eval results *****
2022-06-12 01:42:16,974   att_loss = 0.5409564834994238
2022-06-12 01:42:16,974   global_step = 4079
2022-06-12 01:42:16,974   loss = 1.4116040887059391
2022-06-12 01:42:16,974   rep_loss = 0.8706476060119835
2022-06-12 01:42:16,974 ***** Save model *****
2022-06-12 01:42:22,655 ***** Running evaluation *****
2022-06-12 01:42:22,655   Epoch = 15 iter 4099 step
2022-06-12 01:42:22,656   Num examples = 1043
2022-06-12 01:42:22,656   Batch size = 32
2022-06-12 01:42:22,657 ***** Eval results *****
2022-06-12 01:42:22,657   att_loss = 0.5402941681603168
2022-06-12 01:42:22,657   global_step = 4099
2022-06-12 01:42:22,657   loss = 1.410839834111802
2022-06-12 01:42:22,658   rep_loss = 0.8705456675367153
2022-06-12 01:42:22,658 ***** Save model *****
2022-06-12 01:42:28,354 ***** Running evaluation *****
2022-06-12 01:42:28,355   Epoch = 15 iter 4119 step
2022-06-12 01:42:28,355   Num examples = 1043
2022-06-12 01:42:28,355   Batch size = 32
2022-06-12 01:42:28,356 ***** Eval results *****
2022-06-12 01:42:28,356   att_loss = 0.5417871781085667
2022-06-12 01:42:28,357   global_step = 4119
2022-06-12 01:42:28,357   loss = 1.4120376936176366
2022-06-12 01:42:28,357   rep_loss = 0.8702505152476462
2022-06-12 01:42:28,357 ***** Save model *****
2022-06-12 01:42:34,004 ***** Running evaluation *****
2022-06-12 01:42:34,005   Epoch = 15 iter 4139 step
2022-06-12 01:42:34,005   Num examples = 1043
2022-06-12 01:42:34,005   Batch size = 32
2022-06-12 01:42:34,006 ***** Eval results *****
2022-06-12 01:42:34,007   att_loss = 0.5486419194225055
2022-06-12 01:42:34,007   global_step = 4139
2022-06-12 01:42:34,007   loss = 1.4222242004835783
2022-06-12 01:42:34,007   rep_loss = 0.8735822830627213
2022-06-12 01:42:34,007 ***** Save model *****
2022-06-12 01:42:39,649 ***** Running evaluation *****
2022-06-12 01:42:39,650   Epoch = 15 iter 4159 step
2022-06-12 01:42:39,650   Num examples = 1043
2022-06-12 01:42:39,650   Batch size = 32
2022-06-12 01:42:39,651 ***** Eval results *****
2022-06-12 01:42:39,651   att_loss = 0.5438429481023318
2022-06-12 01:42:39,651   global_step = 4159
2022-06-12 01:42:39,651   loss = 1.41590866562608
2022-06-12 01:42:39,651   rep_loss = 0.8720657198460071
2022-06-12 01:42:39,651 ***** Save model *****
2022-06-12 01:42:45,301 ***** Running evaluation *****
2022-06-12 01:42:45,301   Epoch = 15 iter 4179 step
2022-06-12 01:42:45,302   Num examples = 1043
2022-06-12 01:42:45,302   Batch size = 32
2022-06-12 01:42:45,303 ***** Eval results *****
2022-06-12 01:42:45,303   att_loss = 0.5436668606667683
2022-06-12 01:42:45,303   global_step = 4179
2022-06-12 01:42:45,303   loss = 1.4160965571458313
2022-06-12 01:42:45,303   rep_loss = 0.8724297000758949
2022-06-12 01:42:45,303 ***** Save model *****
2022-06-12 01:42:51,019 ***** Running evaluation *****
2022-06-12 01:42:51,019   Epoch = 15 iter 4199 step
2022-06-12 01:42:51,020   Num examples = 1043
2022-06-12 01:42:51,020   Batch size = 32
2022-06-12 01:42:51,021 ***** Eval results *****
2022-06-12 01:42:51,021   att_loss = 0.5417448976605209
2022-06-12 01:42:51,021   global_step = 4199
2022-06-12 01:42:51,021   loss = 1.4148611285022854
2022-06-12 01:42:51,021   rep_loss = 0.873116235450371
2022-06-12 01:42:51,022 ***** Save model *****
2022-06-12 01:42:56,674 ***** Running evaluation *****
2022-06-12 01:42:56,675   Epoch = 15 iter 4219 step
2022-06-12 01:42:56,675   Num examples = 1043
2022-06-12 01:42:56,675   Batch size = 32
2022-06-12 01:42:56,676 ***** Eval results *****
2022-06-12 01:42:56,676   att_loss = 0.5416681540903644
2022-06-12 01:42:56,677   global_step = 4219
2022-06-12 01:42:56,677   loss = 1.4152877525748493
2022-06-12 01:42:56,677   rep_loss = 0.8736196015482751
2022-06-12 01:42:56,677 ***** Save model *****
2022-06-12 01:42:58,783 ***** Running evaluation *****
2022-06-12 01:42:58,784   Epoch = 3 iter 11999 step
2022-06-12 01:42:58,784   Num examples = 5463
2022-06-12 01:42:58,784   Batch size = 32
2022-06-12 01:42:58,785 ***** Eval results *****
2022-06-12 01:42:58,785   att_loss = 4.00056224306789
2022-06-12 01:42:58,786   global_step = 11999
2022-06-12 01:42:58,786   loss = 4.932447425706671
2022-06-12 01:42:58,786   rep_loss = 0.9318851803694296
2022-06-12 01:42:58,786 ***** Save model *****
2022-06-12 01:43:02,351 ***** Running evaluation *****
2022-06-12 01:43:02,352   Epoch = 15 iter 4239 step
2022-06-12 01:43:02,352   Num examples = 1043
2022-06-12 01:43:02,352   Batch size = 32
2022-06-12 01:43:02,354 ***** Eval results *****
2022-06-12 01:43:02,354   att_loss = 0.5422193604147333
2022-06-12 01:43:02,354   global_step = 4239
2022-06-12 01:43:02,354   loss = 1.4165627513176355
2022-06-12 01:43:02,354   rep_loss = 0.87434339370483
2022-06-12 01:43:02,354 ***** Save model *****
2022-06-12 01:43:08,006 ***** Running evaluation *****
2022-06-12 01:43:08,006   Epoch = 15 iter 4259 step
2022-06-12 01:43:08,006   Num examples = 1043
2022-06-12 01:43:08,006   Batch size = 32
2022-06-12 01:43:08,007 ***** Eval results *****
2022-06-12 01:43:08,008   att_loss = 0.5434628795451066
2022-06-12 01:43:08,008   global_step = 4259
2022-06-12 01:43:08,008   loss = 1.418353573074491
2022-06-12 01:43:08,008   rep_loss = 0.874890695406696
2022-06-12 01:43:08,008 ***** Save model *****
2022-06-12 01:43:13,692 ***** Running evaluation *****
2022-06-12 01:43:13,692   Epoch = 16 iter 4279 step
2022-06-12 01:43:13,692   Num examples = 1043
2022-06-12 01:43:13,692   Batch size = 32
2022-06-12 01:43:13,693 ***** Eval results *****
2022-06-12 01:43:13,693   att_loss = 0.5618068746158055
2022-06-12 01:43:13,693   global_step = 4279
2022-06-12 01:43:13,693   loss = 1.4475786685943604
2022-06-12 01:43:13,694   rep_loss = 0.8857717684337071
2022-06-12 01:43:13,694 ***** Save model *****
2022-06-12 01:43:19,390 ***** Running evaluation *****
2022-06-12 01:43:19,391   Epoch = 16 iter 4299 step
2022-06-12 01:43:19,391   Num examples = 1043
2022-06-12 01:43:19,391   Batch size = 32
2022-06-12 01:43:19,393 ***** Eval results *****
2022-06-12 01:43:19,393   att_loss = 0.522198384558713
2022-06-12 01:43:19,393   global_step = 4299
2022-06-12 01:43:19,393   loss = 1.3830329444673326
2022-06-12 01:43:19,393   rep_loss = 0.8608345521820916
2022-06-12 01:43:19,393 ***** Save model *****
2022-06-12 01:43:25,048 ***** Running evaluation *****
2022-06-12 01:43:25,048   Epoch = 16 iter 4319 step
2022-06-12 01:43:25,048   Num examples = 1043
2022-06-12 01:43:25,048   Batch size = 32
2022-06-12 01:43:25,050 ***** Eval results *****
2022-06-12 01:43:25,050   att_loss = 0.5359775893231655
2022-06-12 01:43:25,050   global_step = 4319
2022-06-12 01:43:25,050   loss = 1.4037800443933366
2022-06-12 01:43:25,050   rep_loss = 0.8678024461928834
2022-06-12 01:43:25,050 ***** Save model *****
2022-06-12 01:43:30,717 ***** Running evaluation *****
2022-06-12 01:43:30,717   Epoch = 16 iter 4339 step
2022-06-12 01:43:30,717   Num examples = 1043
2022-06-12 01:43:30,717   Batch size = 32
2022-06-12 01:43:30,718 ***** Eval results *****
2022-06-12 01:43:30,719   att_loss = 0.5333709836895786
2022-06-12 01:43:30,719   global_step = 4339
2022-06-12 01:43:30,719   loss = 1.399075497442217
2022-06-12 01:43:30,719   rep_loss = 0.8657045061908551
2022-06-12 01:43:30,719 ***** Save model *****
2022-06-12 01:43:36,448 ***** Running evaluation *****
2022-06-12 01:43:36,449   Epoch = 16 iter 4359 step
2022-06-12 01:43:36,449   Num examples = 1043
2022-06-12 01:43:36,449   Batch size = 32
2022-06-12 01:43:36,451 ***** Eval results *****
2022-06-12 01:43:36,451   att_loss = 0.5363950585496837
2022-06-12 01:43:36,451   global_step = 4359
2022-06-12 01:43:36,451   loss = 1.401760663109264
2022-06-12 01:43:36,451   rep_loss = 0.8653655977084719
2022-06-12 01:43:36,451 ***** Save model *****
2022-06-12 01:43:42,109 ***** Running evaluation *****
2022-06-12 01:43:42,109   Epoch = 16 iter 4379 step
2022-06-12 01:43:42,109   Num examples = 1043
2022-06-12 01:43:42,109   Batch size = 32
2022-06-12 01:43:42,110 ***** Eval results *****
2022-06-12 01:43:42,110   att_loss = 0.5371501963829326
2022-06-12 01:43:42,110   global_step = 4379
2022-06-12 01:43:42,110   loss = 1.4032023409816707
2022-06-12 01:43:42,110   rep_loss = 0.8660521362429467
2022-06-12 01:43:42,111 ***** Save model *****
2022-06-12 01:43:47,772 ***** Running evaluation *****
2022-06-12 01:43:47,773   Epoch = 16 iter 4399 step
2022-06-12 01:43:47,773   Num examples = 1043
2022-06-12 01:43:47,773   Batch size = 32
2022-06-12 01:43:47,774 ***** Eval results *****
2022-06-12 01:43:47,774   att_loss = 0.5350823529123321
2022-06-12 01:43:47,774   global_step = 4399
2022-06-12 01:43:47,774   loss = 1.4000224428852712
2022-06-12 01:43:47,774   rep_loss = 0.8649400810557087
2022-06-12 01:43:47,774 ***** Save model *****
2022-06-12 01:43:53,429 ***** Running evaluation *****
2022-06-12 01:43:53,430   Epoch = 16 iter 4419 step
2022-06-12 01:43:53,430   Num examples = 1043
2022-06-12 01:43:53,430   Batch size = 32
2022-06-12 01:43:53,431 ***** Eval results *****
2022-06-12 01:43:53,431   att_loss = 0.5348878934675333
2022-06-12 01:43:53,431   global_step = 4419
2022-06-12 01:43:53,431   loss = 1.3995853111046512
2022-06-12 01:43:53,431   rep_loss = 0.8646974085139579
2022-06-12 01:43:53,431 ***** Save model *****
2022-06-12 01:43:59,029 ***** Running evaluation *****
2022-06-12 01:43:59,030   Epoch = 16 iter 4439 step
2022-06-12 01:43:59,030   Num examples = 1043
2022-06-12 01:43:59,030   Batch size = 32
2022-06-12 01:43:59,031 ***** Eval results *****
2022-06-12 01:43:59,031   att_loss = 0.5380471744937098
2022-06-12 01:43:59,032   global_step = 4439
2022-06-12 01:43:59,032   loss = 1.4036947902805077
2022-06-12 01:43:59,032   rep_loss = 0.8656476100761733
2022-06-12 01:43:59,032 ***** Save model *****
2022-06-12 01:44:04,753 ***** Running evaluation *****
2022-06-12 01:44:04,754   Epoch = 16 iter 4459 step
2022-06-12 01:44:04,754   Num examples = 1043
2022-06-12 01:44:04,754   Batch size = 32
2022-06-12 01:44:04,755 ***** Eval results *****
2022-06-12 01:44:04,755   att_loss = 0.5385694642436696
2022-06-12 01:44:04,755   global_step = 4459
2022-06-12 01:44:04,755   loss = 1.4044518375141735
2022-06-12 01:44:04,755   rep_loss = 0.8658823692862363
2022-06-12 01:44:04,755 ***** Save model *****
2022-06-12 01:44:10,419 ***** Running evaluation *****
2022-06-12 01:44:10,420   Epoch = 16 iter 4479 step
2022-06-12 01:44:10,420   Num examples = 1043
2022-06-12 01:44:10,420   Batch size = 32
2022-06-12 01:44:10,421 ***** Eval results *****
2022-06-12 01:44:10,421   att_loss = 0.5365860433394206
2022-06-12 01:44:10,421   global_step = 4479
2022-06-12 01:44:10,422   loss = 1.4012729873979726
2022-06-12 01:44:10,422   rep_loss = 0.8646869400273198
2022-06-12 01:44:10,422 ***** Save model *****
2022-06-12 01:44:16,094 ***** Running evaluation *****
2022-06-12 01:44:16,094   Epoch = 16 iter 4499 step
2022-06-12 01:44:16,094   Num examples = 1043
2022-06-12 01:44:16,094   Batch size = 32
2022-06-12 01:44:16,095 ***** Eval results *****
2022-06-12 01:44:16,095   att_loss = 0.5341946560118167
2022-06-12 01:44:16,095   global_step = 4499
2022-06-12 01:44:16,096   loss = 1.3982393195450569
2022-06-12 01:44:16,096   rep_loss = 0.8640446615639237
2022-06-12 01:44:16,096 ***** Save model *****
2022-06-12 01:44:21,718 ***** Running evaluation *****
2022-06-12 01:44:21,719   Epoch = 16 iter 4519 step
2022-06-12 01:44:21,719   Num examples = 1043
2022-06-12 01:44:21,719   Batch size = 32
2022-06-12 01:44:21,720 ***** Eval results *****
2022-06-12 01:44:21,720   att_loss = 0.5336278109897968
2022-06-12 01:44:21,720   global_step = 4519
2022-06-12 01:44:21,721   loss = 1.3976651367388273
2022-06-12 01:44:21,721   rep_loss = 0.8640373235772013
2022-06-12 01:44:21,721 ***** Save model *****
2022-06-12 01:44:27,381 ***** Running evaluation *****
2022-06-12 01:44:27,381   Epoch = 16 iter 4539 step
2022-06-12 01:44:27,381   Num examples = 1043
2022-06-12 01:44:27,382   Batch size = 32
2022-06-12 01:44:27,383 ***** Eval results *****
2022-06-12 01:44:27,383   att_loss = 0.5354585878634721
2022-06-12 01:44:27,383   global_step = 4539
2022-06-12 01:44:27,383   loss = 1.4001387478260512
2022-06-12 01:44:27,383   rep_loss = 0.8646801571720995
2022-06-12 01:44:27,383 ***** Save model *****
2022-06-12 01:44:33,060 ***** Running evaluation *****
2022-06-12 01:44:33,061   Epoch = 17 iter 4559 step
2022-06-12 01:44:33,061   Num examples = 1043
2022-06-12 01:44:33,061   Batch size = 32
2022-06-12 01:44:33,063 ***** Eval results *****
2022-06-12 01:44:33,064   att_loss = 0.5238286569714546
2022-06-12 01:44:33,064   global_step = 4559
2022-06-12 01:44:33,064   loss = 1.3790386319160461
2022-06-12 01:44:33,064   rep_loss = 0.8552099734544754
2022-06-12 01:44:33,064 ***** Save model *****
2022-06-12 01:44:38,717 ***** Running evaluation *****
2022-06-12 01:44:38,718   Epoch = 17 iter 4579 step
2022-06-12 01:44:38,718   Num examples = 1043
2022-06-12 01:44:38,718   Batch size = 32
2022-06-12 01:44:38,719 ***** Eval results *****
2022-06-12 01:44:38,719   att_loss = 0.5267537660896778
2022-06-12 01:44:38,719   global_step = 4579
2022-06-12 01:44:38,719   loss = 1.3831632375717162
2022-06-12 01:44:38,719   rep_loss = 0.8564094692468643
2022-06-12 01:44:38,719 ***** Save model *****
2022-06-12 01:44:44,391 ***** Running evaluation *****
2022-06-12 01:44:44,392   Epoch = 17 iter 4599 step
2022-06-12 01:44:44,392   Num examples = 1043
2022-06-12 01:44:44,392   Batch size = 32
2022-06-12 01:44:44,393 ***** Eval results *****
2022-06-12 01:44:44,393   att_loss = 0.528717718521754
2022-06-12 01:44:44,393   global_step = 4599
2022-06-12 01:44:44,393   loss = 1.3846628983815512
2022-06-12 01:44:44,393   rep_loss = 0.8559451738993327
2022-06-12 01:44:44,394 ***** Save model *****
2022-06-12 01:44:50,080 ***** Running evaluation *****
2022-06-12 01:44:50,080   Epoch = 17 iter 4619 step
2022-06-12 01:44:50,080   Num examples = 1043
2022-06-12 01:44:50,081   Batch size = 32
2022-06-12 01:44:50,082 ***** Eval results *****
2022-06-12 01:44:50,082   att_loss = 0.5298167508095503
2022-06-12 01:44:50,082   global_step = 4619
2022-06-12 01:44:50,082   loss = 1.387872788310051
2022-06-12 01:44:50,082   rep_loss = 0.8580560304224492
2022-06-12 01:44:50,082 ***** Save model *****
2022-06-12 01:44:55,723 ***** Running evaluation *****
2022-06-12 01:44:55,723   Epoch = 17 iter 4639 step
2022-06-12 01:44:55,723   Num examples = 1043
2022-06-12 01:44:55,724   Batch size = 32
2022-06-12 01:44:55,724 ***** Eval results *****
2022-06-12 01:44:55,725   att_loss = 0.5301475721597672
2022-06-12 01:44:55,725   global_step = 4639
2022-06-12 01:44:55,725   loss = 1.3878710520267488
2022-06-12 01:44:55,725   rep_loss = 0.8577234721183777
2022-06-12 01:44:55,725 ***** Save model *****
2022-06-12 01:45:01,397 ***** Running evaluation *****
2022-06-12 01:45:01,398   Epoch = 17 iter 4659 step
2022-06-12 01:45:01,398   Num examples = 1043
2022-06-12 01:45:01,398   Batch size = 32
2022-06-12 01:45:01,399 ***** Eval results *****
2022-06-12 01:45:01,399   att_loss = 0.531248644242684
2022-06-12 01:45:01,399   global_step = 4659
2022-06-12 01:45:01,400   loss = 1.389560310045878
2022-06-12 01:45:01,400   rep_loss = 0.8583116615811984
2022-06-12 01:45:01,400 ***** Save model *****
2022-06-12 01:45:06,690 ***** Running evaluation *****
2022-06-12 01:45:06,690   Epoch = 3 iter 12499 step
2022-06-12 01:45:06,690   Num examples = 5463
2022-06-12 01:45:06,690   Batch size = 32
2022-06-12 01:45:06,692 ***** Eval results *****
2022-06-12 01:45:06,692   att_loss = 3.999431451398935
2022-06-12 01:45:06,692   global_step = 12499
2022-06-12 01:45:06,692   loss = 4.930323769708178
2022-06-12 01:45:06,692   rep_loss = 0.930892315751581
2022-06-12 01:45:06,692 ***** Save model *****
2022-06-12 01:45:07,087 ***** Running evaluation *****
2022-06-12 01:45:07,088   Epoch = 17 iter 4679 step
2022-06-12 01:45:07,088   Num examples = 1043
2022-06-12 01:45:07,088   Batch size = 32
2022-06-12 01:45:07,089 ***** Eval results *****
2022-06-12 01:45:07,089   att_loss = 0.5300142122166497
2022-06-12 01:45:07,089   global_step = 4679
2022-06-12 01:45:07,090   loss = 1.3871336374964034
2022-06-12 01:45:07,090   rep_loss = 0.8571194227252688
2022-06-12 01:45:07,090 ***** Save model *****
2022-06-12 01:45:12,751 ***** Running evaluation *****
2022-06-12 01:45:12,751   Epoch = 17 iter 4699 step
2022-06-12 01:45:12,752   Num examples = 1043
2022-06-12 01:45:12,752   Batch size = 32
2022-06-12 01:45:12,753 ***** Eval results *****
2022-06-12 01:45:12,753   att_loss = 0.5320534003898502
2022-06-12 01:45:12,753   global_step = 4699
2022-06-12 01:45:12,753   loss = 1.3899678826332091
2022-06-12 01:45:12,753   rep_loss = 0.8579144798219204
2022-06-12 01:45:12,753 ***** Save model *****
2022-06-12 01:45:18,411 ***** Running evaluation *****
2022-06-12 01:45:18,411   Epoch = 17 iter 4719 step
2022-06-12 01:45:18,411   Num examples = 1043
2022-06-12 01:45:18,412   Batch size = 32
2022-06-12 01:45:18,413 ***** Eval results *****
2022-06-12 01:45:18,413   att_loss = 0.5303486259447203
2022-06-12 01:45:18,413   global_step = 4719
2022-06-12 01:45:18,413   loss = 1.387760349114736
2022-06-12 01:45:18,413   rep_loss = 0.8574117203553517
2022-06-12 01:45:18,413 ***** Save model *****
2022-06-12 01:45:24,095 ***** Running evaluation *****
2022-06-12 01:45:24,096   Epoch = 17 iter 4739 step
2022-06-12 01:45:24,096   Num examples = 1043
2022-06-12 01:45:24,096   Batch size = 32
2022-06-12 01:45:24,097 ***** Eval results *****
2022-06-12 01:45:24,097   att_loss = 0.5342076629400253
2022-06-12 01:45:24,097   global_step = 4739
2022-06-12 01:45:24,097   loss = 1.393843476176262
2022-06-12 01:45:24,097   rep_loss = 0.859635811150074
2022-06-12 01:45:24,097 ***** Save model *****
2022-06-12 01:45:29,767 ***** Running evaluation *****
2022-06-12 01:45:29,767   Epoch = 17 iter 4759 step
2022-06-12 01:45:29,768   Num examples = 1043
2022-06-12 01:45:29,768   Batch size = 32
2022-06-12 01:45:29,769 ***** Eval results *****
2022-06-12 01:45:29,769   att_loss = 0.5338203925978053
2022-06-12 01:45:29,769   global_step = 4759
2022-06-12 01:45:29,769   loss = 1.3931802652098917
2022-06-12 01:45:29,769   rep_loss = 0.8593598704446446
2022-06-12 01:45:29,769 ***** Save model *****
2022-06-12 01:45:35,445 ***** Running evaluation *****
2022-06-12 01:45:35,446   Epoch = 17 iter 4779 step
2022-06-12 01:45:35,446   Num examples = 1043
2022-06-12 01:45:35,446   Batch size = 32
2022-06-12 01:45:35,448 ***** Eval results *****
2022-06-12 01:45:35,448   att_loss = 0.5323613811284303
2022-06-12 01:45:35,448   global_step = 4779
2022-06-12 01:45:35,448   loss = 1.391022390127182
2022-06-12 01:45:35,449   rep_loss = 0.8586610071361065
2022-06-12 01:45:35,449 ***** Save model *****
2022-06-12 01:45:41,160 ***** Running evaluation *****
2022-06-12 01:45:41,161   Epoch = 17 iter 4799 step
2022-06-12 01:45:41,161   Num examples = 1043
2022-06-12 01:45:41,161   Batch size = 32
2022-06-12 01:45:41,162 ***** Eval results *****
2022-06-12 01:45:41,162   att_loss = 0.5322134364109773
2022-06-12 01:45:41,162   global_step = 4799
2022-06-12 01:45:41,162   loss = 1.3899424053155458
2022-06-12 01:45:41,162   rep_loss = 0.8577289677583254
2022-06-12 01:45:41,162 ***** Save model *****
2022-06-12 01:45:46,870 ***** Running evaluation *****
2022-06-12 01:45:46,871   Epoch = 18 iter 4819 step
2022-06-12 01:45:46,871   Num examples = 1043
2022-06-12 01:45:46,871   Batch size = 32
2022-06-12 01:45:46,872 ***** Eval results *****
2022-06-12 01:45:46,872   att_loss = 0.5248823876564319
2022-06-12 01:45:46,872   global_step = 4819
2022-06-12 01:45:46,872   loss = 1.375531820150522
2022-06-12 01:45:46,872   rep_loss = 0.8506494347865765
2022-06-12 01:45:46,872 ***** Save model *****
2022-06-12 01:45:52,563 ***** Running evaluation *****
2022-06-12 01:45:52,564   Epoch = 18 iter 4839 step
2022-06-12 01:45:52,564   Num examples = 1043
2022-06-12 01:45:52,564   Batch size = 32
2022-06-12 01:45:52,565 ***** Eval results *****
2022-06-12 01:45:52,565   att_loss = 0.5162286288810499
2022-06-12 01:45:52,565   global_step = 4839
2022-06-12 01:45:52,565   loss = 1.3625311490261194
2022-06-12 01:45:52,565   rep_loss = 0.8463025255636736
2022-06-12 01:45:52,565 ***** Save model *****
2022-06-12 01:45:58,219 ***** Running evaluation *****
2022-06-12 01:45:58,220   Epoch = 18 iter 4859 step
2022-06-12 01:45:58,220   Num examples = 1043
2022-06-12 01:45:58,220   Batch size = 32
2022-06-12 01:45:58,221 ***** Eval results *****
2022-06-12 01:45:58,221   att_loss = 0.5113269905999022
2022-06-12 01:45:58,222   global_step = 4859
2022-06-12 01:45:58,222   loss = 1.3578999987188376
2022-06-12 01:45:58,222   rep_loss = 0.8465730120550912
2022-06-12 01:45:58,222 ***** Save model *****
2022-06-12 01:46:03,896 ***** Running evaluation *****
2022-06-12 01:46:03,897   Epoch = 18 iter 4879 step
2022-06-12 01:46:03,897   Num examples = 1043
2022-06-12 01:46:03,897   Batch size = 32
2022-06-12 01:46:03,898 ***** Eval results *****
2022-06-12 01:46:03,898   att_loss = 0.5196357157132397
2022-06-12 01:46:03,898   global_step = 4879
2022-06-12 01:46:03,898   loss = 1.3706211178270105
2022-06-12 01:46:03,899   rep_loss = 0.8509854053797787
2022-06-12 01:46:03,899 ***** Save model *****
2022-06-12 01:46:09,594 ***** Running evaluation *****
2022-06-12 01:46:09,595   Epoch = 18 iter 4899 step
2022-06-12 01:46:09,595   Num examples = 1043
2022-06-12 01:46:09,595   Batch size = 32
2022-06-12 01:46:09,596 ***** Eval results *****
2022-06-12 01:46:09,597   att_loss = 0.5235383122838954
2022-06-12 01:46:09,597   global_step = 4899
2022-06-12 01:46:09,597   loss = 1.3752064051166657
2022-06-12 01:46:09,597   rep_loss = 0.8516680963577763
2022-06-12 01:46:09,597 ***** Save model *****
2022-06-12 01:46:15,279 ***** Running evaluation *****
2022-06-12 01:46:15,280   Epoch = 18 iter 4919 step
2022-06-12 01:46:15,280   Num examples = 1043
2022-06-12 01:46:15,280   Batch size = 32
2022-06-12 01:46:15,281 ***** Eval results *****
2022-06-12 01:46:15,282   att_loss = 0.5268179017358121
2022-06-12 01:46:15,282   global_step = 4919
2022-06-12 01:46:15,282   loss = 1.3802606500355543
2022-06-12 01:46:15,282   rep_loss = 0.853442751728328
2022-06-12 01:46:15,282 ***** Save model *****
2022-06-12 01:46:20,927 ***** Running evaluation *****
2022-06-12 01:46:20,928   Epoch = 18 iter 4939 step
2022-06-12 01:46:20,928   Num examples = 1043
2022-06-12 01:46:20,928   Batch size = 32
2022-06-12 01:46:20,929 ***** Eval results *****
2022-06-12 01:46:20,930   att_loss = 0.5259070996951339
2022-06-12 01:46:20,930   global_step = 4939
2022-06-12 01:46:20,930   loss = 1.3790722502801651
2022-06-12 01:46:20,930   rep_loss = 0.8531651555147386
2022-06-12 01:46:20,930 ***** Save model *****
2022-06-12 01:46:26,594 ***** Running evaluation *****
2022-06-12 01:46:26,594   Epoch = 18 iter 4959 step
2022-06-12 01:46:26,594   Num examples = 1043
2022-06-12 01:46:26,594   Batch size = 32
2022-06-12 01:46:26,596 ***** Eval results *****
2022-06-12 01:46:26,596   att_loss = 0.5284135370862251
2022-06-12 01:46:26,596   global_step = 4959
2022-06-12 01:46:26,596   loss = 1.3822860312617682
2022-06-12 01:46:26,596   rep_loss = 0.8538725006034951
2022-06-12 01:46:26,596 ***** Save model *****
2022-06-12 01:46:32,289 ***** Running evaluation *****
2022-06-12 01:46:32,290   Epoch = 18 iter 4979 step
2022-06-12 01:46:32,290   Num examples = 1043
2022-06-12 01:46:32,290   Batch size = 32
2022-06-12 01:46:32,291 ***** Eval results *****
2022-06-12 01:46:32,291   att_loss = 0.5261958915029648
2022-06-12 01:46:32,291   global_step = 4979
2022-06-12 01:46:32,291   loss = 1.3782080150064016
2022-06-12 01:46:32,292   rep_loss = 0.852012129188273
2022-06-12 01:46:32,292 ***** Save model *****
2022-06-12 01:46:37,967 ***** Running evaluation *****
2022-06-12 01:46:37,967   Epoch = 18 iter 4999 step
2022-06-12 01:46:37,967   Num examples = 1043
2022-06-12 01:46:37,967   Batch size = 32
2022-06-12 01:46:37,968 ***** Eval results *****
2022-06-12 01:46:37,968   att_loss = 0.5248238189541614
2022-06-12 01:46:37,969   global_step = 4999
2022-06-12 01:46:37,969   loss = 1.3764527264036663
2022-06-12 01:46:37,969   rep_loss = 0.8516289143982329
2022-06-12 01:46:37,969 ***** Save model *****
2022-06-12 01:46:43,603 ***** Running evaluation *****
2022-06-12 01:46:43,604   Epoch = 18 iter 5019 step
2022-06-12 01:46:43,604   Num examples = 1043
2022-06-12 01:46:43,604   Batch size = 32
2022-06-12 01:46:43,605 ***** Eval results *****
2022-06-12 01:46:43,605   att_loss = 0.5227682556904537
2022-06-12 01:46:43,605   global_step = 5019
2022-06-12 01:46:43,605   loss = 1.3730253887848116
2022-06-12 01:46:43,605   rep_loss = 0.8502571398103741
2022-06-12 01:46:43,605 ***** Save model *****
2022-06-12 01:46:49,304 ***** Running evaluation *****
2022-06-12 01:46:49,305   Epoch = 18 iter 5039 step
2022-06-12 01:46:49,305   Num examples = 1043
2022-06-12 01:46:49,305   Batch size = 32
2022-06-12 01:46:49,306 ***** Eval results *****
2022-06-12 01:46:49,306   att_loss = 0.5243157424640246
2022-06-12 01:46:49,306   global_step = 5039
2022-06-12 01:46:49,306   loss = 1.3747864721159055
2022-06-12 01:46:49,306   rep_loss = 0.8504707355356012
2022-06-12 01:46:49,306 ***** Save model *****
2022-06-12 01:46:54,972 ***** Running evaluation *****
2022-06-12 01:46:54,972   Epoch = 18 iter 5059 step
2022-06-12 01:46:54,972   Num examples = 1043
2022-06-12 01:46:54,972   Batch size = 32
2022-06-12 01:46:54,973 ***** Eval results *****
2022-06-12 01:46:54,973   att_loss = 0.5245337311929393
2022-06-12 01:46:54,974   global_step = 5059
2022-06-12 01:46:54,974   loss = 1.3747437650507146
2022-06-12 01:46:54,974   rep_loss = 0.8502100395119708
2022-06-12 01:46:54,974 ***** Save model *****
2022-06-12 01:47:00,661 ***** Running evaluation *****
2022-06-12 01:47:00,662   Epoch = 19 iter 5079 step
2022-06-12 01:47:00,662   Num examples = 1043
2022-06-12 01:47:00,662   Batch size = 32
2022-06-12 01:47:00,663 ***** Eval results *****
2022-06-12 01:47:00,663   att_loss = 0.5417498201131821
2022-06-12 01:47:00,663   global_step = 5079
2022-06-12 01:47:00,663   loss = 1.4032561580340068
2022-06-12 01:47:00,663   rep_loss = 0.861506332953771
2022-06-12 01:47:00,664 ***** Save model *****
2022-06-12 01:47:06,326 ***** Running evaluation *****
2022-06-12 01:47:06,327   Epoch = 19 iter 5099 step
2022-06-12 01:47:06,327   Num examples = 1043
2022-06-12 01:47:06,327   Batch size = 32
2022-06-12 01:47:06,328 ***** Eval results *****
2022-06-12 01:47:06,328   att_loss = 0.502340093255043
2022-06-12 01:47:06,328   global_step = 5099
2022-06-12 01:47:06,328   loss = 1.3380440748654878
2022-06-12 01:47:06,329   rep_loss = 0.8357039873416607
2022-06-12 01:47:06,329 ***** Save model *****
2022-06-12 01:47:11,994 ***** Running evaluation *****
2022-06-12 01:47:11,995   Epoch = 19 iter 5119 step
2022-06-12 01:47:11,995   Num examples = 1043
2022-06-12 01:47:11,995   Batch size = 32
2022-06-12 01:47:11,996 ***** Eval results *****
2022-06-12 01:47:11,996   att_loss = 0.5184765529373417
2022-06-12 01:47:11,996   global_step = 5119
2022-06-12 01:47:11,996   loss = 1.3625405456708826
2022-06-12 01:47:11,996   rep_loss = 0.8440640024516893
2022-06-12 01:47:11,996 ***** Save model *****
2022-06-12 01:47:14,463 ***** Running evaluation *****
2022-06-12 01:47:14,463   Epoch = 3 iter 12999 step
2022-06-12 01:47:14,463   Num examples = 5463
2022-06-12 01:47:14,463   Batch size = 32
2022-06-12 01:47:14,465 ***** Eval results *****
2022-06-12 01:47:14,465   att_loss = 3.988304025572051
2022-06-12 01:47:14,465   global_step = 12999
2022-06-12 01:47:14,465   loss = 4.917511688013497
2022-06-12 01:47:14,465   rep_loss = 0.9292076600235213
2022-06-12 01:47:14,465 ***** Save model *****
2022-06-12 01:47:17,665 ***** Running evaluation *****
2022-06-12 01:47:17,666   Epoch = 19 iter 5139 step
2022-06-12 01:47:17,666   Num examples = 1043
2022-06-12 01:47:17,666   Batch size = 32
2022-06-12 01:47:17,667 ***** Eval results *****
2022-06-12 01:47:17,667   att_loss = 0.5206203411022822
2022-06-12 01:47:17,667   global_step = 5139
2022-06-12 01:47:17,667   loss = 1.3660878575209416
2022-06-12 01:47:17,667   rep_loss = 0.8454675231919144
2022-06-12 01:47:17,668 ***** Save model *****
2022-06-12 01:47:23,329 ***** Running evaluation *****
2022-06-12 01:47:23,330   Epoch = 19 iter 5159 step
2022-06-12 01:47:23,330   Num examples = 1043
2022-06-12 01:47:23,330   Batch size = 32
2022-06-12 01:47:23,331 ***** Eval results *****
2022-06-12 01:47:23,331   att_loss = 0.51914916135544
2022-06-12 01:47:23,331   global_step = 5159
2022-06-12 01:47:23,331   loss = 1.365041576152624
2022-06-12 01:47:23,331   rep_loss = 0.8458924210348795
2022-06-12 01:47:23,332 ***** Save model *****
2022-06-12 01:47:28,948 ***** Running evaluation *****
2022-06-12 01:47:28,948   Epoch = 19 iter 5179 step
2022-06-12 01:47:28,948   Num examples = 1043
2022-06-12 01:47:28,949   Batch size = 32
2022-06-12 01:47:28,950 ***** Eval results *****
2022-06-12 01:47:28,950   att_loss = 0.5179516636538055
2022-06-12 01:47:28,950   global_step = 5179
2022-06-12 01:47:28,950   loss = 1.3631133448402837
2022-06-12 01:47:28,950   rep_loss = 0.8451616865284038
2022-06-12 01:47:28,950 ***** Save model *****
2022-06-12 01:47:34,623 ***** Running evaluation *****
2022-06-12 01:47:34,623   Epoch = 19 iter 5199 step
2022-06-12 01:47:34,623   Num examples = 1043
2022-06-12 01:47:34,623   Batch size = 32
2022-06-12 01:47:34,624 ***** Eval results *****
2022-06-12 01:47:34,624   att_loss = 0.5151857577619099
2022-06-12 01:47:34,624   global_step = 5199
2022-06-12 01:47:34,624   loss = 1.359001440661294
2022-06-12 01:47:34,624   rep_loss = 0.8438156857377007
2022-06-12 01:47:34,625 ***** Save model *****
2022-06-12 01:47:40,299 ***** Running evaluation *****
2022-06-12 01:47:40,300   Epoch = 19 iter 5219 step
2022-06-12 01:47:40,300   Num examples = 1043
2022-06-12 01:47:40,300   Batch size = 32
2022-06-12 01:47:40,301 ***** Eval results *****
2022-06-12 01:47:40,301   att_loss = 0.5188043366556299
2022-06-12 01:47:40,301   global_step = 5219
2022-06-12 01:47:40,301   loss = 1.3638369245071933
2022-06-12 01:47:40,302   rep_loss = 0.8450325874433126
2022-06-12 01:47:40,302 ***** Save model *****
2022-06-12 01:47:45,994 ***** Running evaluation *****
2022-06-12 01:47:45,995   Epoch = 19 iter 5239 step
2022-06-12 01:47:45,995   Num examples = 1043
2022-06-12 01:47:45,995   Batch size = 32
2022-06-12 01:47:45,996 ***** Eval results *****
2022-06-12 01:47:45,996   att_loss = 0.518987376287759
2022-06-12 01:47:45,996   global_step = 5239
2022-06-12 01:47:45,996   loss = 1.3642886704709156
2022-06-12 01:47:45,996   rep_loss = 0.8453012931059642
2022-06-12 01:47:45,996 ***** Save model *****
2022-06-12 01:47:51,638 ***** Running evaluation *****
2022-06-12 01:47:51,639   Epoch = 19 iter 5259 step
2022-06-12 01:47:51,639   Num examples = 1043
2022-06-12 01:47:51,639   Batch size = 32
2022-06-12 01:47:51,640 ***** Eval results *****
2022-06-12 01:47:51,640   att_loss = 0.5182531982339839
2022-06-12 01:47:51,640   global_step = 5259
2022-06-12 01:47:51,640   loss = 1.3633102678483533
2022-06-12 01:47:51,640   rep_loss = 0.8450570689734592
2022-06-12 01:47:51,640 ***** Save model *****
2022-06-12 01:47:57,315 ***** Running evaluation *****
2022-06-12 01:47:57,315   Epoch = 19 iter 5279 step
2022-06-12 01:47:57,316   Num examples = 1043
2022-06-12 01:47:57,316   Batch size = 32
2022-06-12 01:47:57,317 ***** Eval results *****
2022-06-12 01:47:57,317   att_loss = 0.5179155088454774
2022-06-12 01:47:57,317   global_step = 5279
2022-06-12 01:47:57,317   loss = 1.363113013864721
2022-06-12 01:47:57,317   rep_loss = 0.8451975054532579
2022-06-12 01:47:57,317 ***** Save model *****
2022-06-12 01:48:02,949 ***** Running evaluation *****
2022-06-12 01:48:02,950   Epoch = 19 iter 5299 step
2022-06-12 01:48:02,950   Num examples = 1043
2022-06-12 01:48:02,950   Batch size = 32
2022-06-12 01:48:02,951 ***** Eval results *****
2022-06-12 01:48:02,951   att_loss = 0.5181295072610399
2022-06-12 01:48:02,951   global_step = 5299
2022-06-12 01:48:02,951   loss = 1.363498551128185
2022-06-12 01:48:02,951   rep_loss = 0.845369043867145
2022-06-12 01:48:02,951 ***** Save model *****
2022-06-12 01:48:08,660 ***** Running evaluation *****
2022-06-12 01:48:08,661   Epoch = 19 iter 5319 step
2022-06-12 01:48:08,661   Num examples = 1043
2022-06-12 01:48:08,661   Batch size = 32
2022-06-12 01:48:08,662 ***** Eval results *****
2022-06-12 01:48:08,662   att_loss = 0.5185455788926381
2022-06-12 01:48:08,662   global_step = 5319
2022-06-12 01:48:08,662   loss = 1.3638765589008486
2022-06-12 01:48:08,663   rep_loss = 0.8453309802505059
2022-06-12 01:48:08,663 ***** Save model *****
2022-06-12 01:48:14,344 ***** Running evaluation *****
2022-06-12 01:48:14,344   Epoch = 19 iter 5339 step
2022-06-12 01:48:14,344   Num examples = 1043
2022-06-12 01:48:14,344   Batch size = 32
2022-06-12 01:48:14,346 ***** Eval results *****
2022-06-12 01:48:14,346   att_loss = 0.5179228507038346
2022-06-12 01:48:14,346   global_step = 5339
2022-06-12 01:48:14,346   loss = 1.3626175495914947
2022-06-12 01:48:14,346   rep_loss = 0.8446946986635825
2022-06-12 01:48:14,346 ***** Save model *****
2022-06-12 01:48:20,070 ***** Running evaluation *****
2022-06-12 01:48:20,071   Epoch = 20 iter 5359 step
2022-06-12 01:48:20,071   Num examples = 1043
2022-06-12 01:48:20,071   Batch size = 32
2022-06-12 01:48:20,072 ***** Eval results *****
2022-06-12 01:48:20,072   att_loss = 0.49072348287231043
2022-06-12 01:48:20,072   global_step = 5359
2022-06-12 01:48:20,072   loss = 1.3200950120624744
2022-06-12 01:48:20,072   rep_loss = 0.8293715338957938
2022-06-12 01:48:20,072 ***** Save model *****
2022-06-12 01:48:25,712 ***** Running evaluation *****
2022-06-12 01:48:25,712   Epoch = 20 iter 5379 step
2022-06-12 01:48:25,713   Num examples = 1043
2022-06-12 01:48:25,713   Batch size = 32
2022-06-12 01:48:25,714 ***** Eval results *****
2022-06-12 01:48:25,714   att_loss = 0.5087474477596772
2022-06-12 01:48:25,714   global_step = 5379
2022-06-12 01:48:25,714   loss = 1.345230961457277
2022-06-12 01:48:25,714   rep_loss = 0.8364835182825724
2022-06-12 01:48:25,714 ***** Save model *****
2022-06-12 01:48:31,381 ***** Running evaluation *****
2022-06-12 01:48:31,382   Epoch = 20 iter 5399 step
2022-06-12 01:48:31,382   Num examples = 1043
2022-06-12 01:48:31,382   Batch size = 32
2022-06-12 01:48:31,384 ***** Eval results *****
2022-06-12 01:48:31,384   att_loss = 0.5103387186082743
2022-06-12 01:48:31,384   global_step = 5399
2022-06-12 01:48:31,384   loss = 1.3491802377215887
2022-06-12 01:48:31,384   rep_loss = 0.8388415241645555
2022-06-12 01:48:31,384 ***** Save model *****
2022-06-12 01:48:37,109 ***** Running evaluation *****
2022-06-12 01:48:37,110   Epoch = 20 iter 5419 step
2022-06-12 01:48:37,110   Num examples = 1043
2022-06-12 01:48:37,110   Batch size = 32
2022-06-12 01:48:37,111 ***** Eval results *****
2022-06-12 01:48:37,111   att_loss = 0.5051808964602554
2022-06-12 01:48:37,111   global_step = 5419
2022-06-12 01:48:37,111   loss = 1.3413554158391832
2022-06-12 01:48:37,111   rep_loss = 0.8361745197561723
2022-06-12 01:48:37,112 ***** Save model *****
2022-06-12 01:48:42,777 ***** Running evaluation *****
2022-06-12 01:48:42,777   Epoch = 20 iter 5439 step
2022-06-12 01:48:42,777   Num examples = 1043
2022-06-12 01:48:42,777   Batch size = 32
2022-06-12 01:48:42,778 ***** Eval results *****
2022-06-12 01:48:42,778   att_loss = 0.5083116982320343
2022-06-12 01:48:42,779   global_step = 5439
2022-06-12 01:48:42,779   loss = 1.3458237611886226
2022-06-12 01:48:42,779   rep_loss = 0.837512063257622
2022-06-12 01:48:42,779 ***** Save model *****
2022-06-12 01:48:48,460 ***** Running evaluation *****
2022-06-12 01:48:48,461   Epoch = 20 iter 5459 step
2022-06-12 01:48:48,461   Num examples = 1043
2022-06-12 01:48:48,461   Batch size = 32
2022-06-12 01:48:48,462 ***** Eval results *****
2022-06-12 01:48:48,462   att_loss = 0.5123951535264984
2022-06-12 01:48:48,462   global_step = 5459
2022-06-12 01:48:48,462   loss = 1.350975573563776
2022-06-12 01:48:48,463   rep_loss = 0.8385804215399157
2022-06-12 01:48:48,463 ***** Save model *****
2022-06-12 01:48:54,156 ***** Running evaluation *****
2022-06-12 01:48:54,157   Epoch = 20 iter 5479 step
2022-06-12 01:48:54,157   Num examples = 1043
2022-06-12 01:48:54,157   Batch size = 32
2022-06-12 01:48:54,159 ***** Eval results *****
2022-06-12 01:48:54,159   att_loss = 0.5138837720850389
2022-06-12 01:48:54,159   global_step = 5479
2022-06-12 01:48:54,159   loss = 1.3531657603147218
2022-06-12 01:48:54,159   rep_loss = 0.8392819886584933
2022-06-12 01:48:54,159 ***** Save model *****
2022-06-12 01:48:59,824 ***** Running evaluation *****
2022-06-12 01:48:59,824   Epoch = 20 iter 5499 step
2022-06-12 01:48:59,825   Num examples = 1043
2022-06-12 01:48:59,825   Batch size = 32
2022-06-12 01:48:59,826 ***** Eval results *****
2022-06-12 01:48:59,826   att_loss = 0.5141620390445182
2022-06-12 01:48:59,826   global_step = 5499
2022-06-12 01:48:59,826   loss = 1.3527513779934097
2022-06-12 01:48:59,826   rep_loss = 0.8385893383865837
2022-06-12 01:48:59,826 ***** Save model *****
2022-06-12 01:49:05,492 ***** Running evaluation *****
2022-06-12 01:49:05,492   Epoch = 20 iter 5519 step
2022-06-12 01:49:05,492   Num examples = 1043
2022-06-12 01:49:05,492   Batch size = 32
2022-06-12 01:49:05,493 ***** Eval results *****
2022-06-12 01:49:05,494   att_loss = 0.5130321107097178
2022-06-12 01:49:05,494   global_step = 5519
2022-06-12 01:49:05,494   loss = 1.3510718625351037
2022-06-12 01:49:05,494   rep_loss = 0.8380397511594122
2022-06-12 01:49:05,494 ***** Save model *****
2022-06-12 01:49:11,143 ***** Running evaluation *****
2022-06-12 01:49:11,144   Epoch = 20 iter 5539 step
2022-06-12 01:49:11,144   Num examples = 1043
2022-06-12 01:49:11,144   Batch size = 32
2022-06-12 01:49:11,145 ***** Eval results *****
2022-06-12 01:49:11,145   att_loss = 0.5119329734363748
2022-06-12 01:49:11,145   global_step = 5539
2022-06-12 01:49:11,145   loss = 1.350196117731794
2022-06-12 01:49:11,145   rep_loss = 0.8382631444451797
2022-06-12 01:49:11,145 ***** Save model *****
2022-06-12 01:49:16,820 ***** Running evaluation *****
2022-06-12 01:49:16,820   Epoch = 20 iter 5559 step
2022-06-12 01:49:16,821   Num examples = 1043
2022-06-12 01:49:16,821   Batch size = 32
2022-06-12 01:49:16,822 ***** Eval results *****
2022-06-12 01:49:16,822   att_loss = 0.5127275512643057
2022-06-12 01:49:16,822   global_step = 5559
2022-06-12 01:49:16,822   loss = 1.3507583076006746
2022-06-12 01:49:16,822   rep_loss = 0.8380307560642016
2022-06-12 01:49:16,822 ***** Save model *****
2022-06-12 01:49:22,240 ***** Running evaluation *****
2022-06-12 01:49:22,240   Epoch = 4 iter 13499 step
2022-06-12 01:49:22,240   Num examples = 5463
2022-06-12 01:49:22,240   Batch size = 32
2022-06-12 01:49:22,241 ***** Eval results *****
2022-06-12 01:49:22,242   att_loss = 3.837376284657884
2022-06-12 01:49:22,242   global_step = 13499
2022-06-12 01:49:22,242   loss = 4.751083915298049
2022-06-12 01:49:22,242   rep_loss = 0.913707634740731
2022-06-12 01:49:22,242 ***** Save model *****
2022-06-12 01:49:22,502 ***** Running evaluation *****
2022-06-12 01:49:22,503   Epoch = 20 iter 5579 step
2022-06-12 01:49:22,503   Num examples = 1043
2022-06-12 01:49:22,503   Batch size = 32
2022-06-12 01:49:22,504 ***** Eval results *****
2022-06-12 01:49:22,504   att_loss = 0.5138944165477194
2022-06-12 01:49:22,504   global_step = 5579
2022-06-12 01:49:22,504   loss = 1.3520692701619041
2022-06-12 01:49:22,505   rep_loss = 0.8381748541129683
2022-06-12 01:49:22,505 ***** Save model *****
2022-06-12 01:49:28,166 ***** Running evaluation *****
2022-06-12 01:49:28,167   Epoch = 20 iter 5599 step
2022-06-12 01:49:28,167   Num examples = 1043
2022-06-12 01:49:28,167   Batch size = 32
2022-06-12 01:49:28,168 ***** Eval results *****
2022-06-12 01:49:28,169   att_loss = 0.5134860412954824
2022-06-12 01:49:28,169   global_step = 5599
2022-06-12 01:49:28,169   loss = 1.351901192002315
2022-06-12 01:49:28,169   rep_loss = 0.8384151513972338
2022-06-12 01:49:28,169 ***** Save model *****
2022-06-12 01:49:33,879 ***** Running evaluation *****
2022-06-12 01:49:33,879   Epoch = 21 iter 5619 step
2022-06-12 01:49:33,879   Num examples = 1043
2022-06-12 01:49:33,879   Batch size = 32
2022-06-12 01:49:33,880 ***** Eval results *****
2022-06-12 01:49:33,880   att_loss = 0.5293213799595833
2022-06-12 01:49:33,880   global_step = 5619
2022-06-12 01:49:33,880   loss = 1.378242512543996
2022-06-12 01:49:33,880   rep_loss = 0.8489211350679398
2022-06-12 01:49:33,881 ***** Save model *****
2022-06-12 01:49:39,579 ***** Running evaluation *****
2022-06-12 01:49:39,580   Epoch = 21 iter 5639 step
2022-06-12 01:49:39,580   Num examples = 1043
2022-06-12 01:49:39,580   Batch size = 32
2022-06-12 01:49:39,581 ***** Eval results *****
2022-06-12 01:49:39,582   att_loss = 0.5189910149201751
2022-06-12 01:49:39,582   global_step = 5639
2022-06-12 01:49:39,582   loss = 1.361661672592163
2022-06-12 01:49:39,582   rep_loss = 0.8426706548780203
2022-06-12 01:49:39,582 ***** Save model *****
2022-06-12 01:49:45,280 ***** Running evaluation *****
2022-06-12 01:49:45,280   Epoch = 21 iter 5659 step
2022-06-12 01:49:45,281   Num examples = 1043
2022-06-12 01:49:45,281   Batch size = 32
2022-06-12 01:49:45,282 ***** Eval results *****
2022-06-12 01:49:45,282   att_loss = 0.514547256896129
2022-06-12 01:49:45,282   global_step = 5659
2022-06-12 01:49:45,282   loss = 1.3513009708661299
2022-06-12 01:49:45,282   rep_loss = 0.8367537030806909
2022-06-12 01:49:45,282 ***** Save model *****
2022-06-12 01:49:50,921 ***** Running evaluation *****
2022-06-12 01:49:50,921   Epoch = 21 iter 5679 step
2022-06-12 01:49:50,921   Num examples = 1043
2022-06-12 01:49:50,921   Batch size = 32
2022-06-12 01:49:50,923 ***** Eval results *****
2022-06-12 01:49:50,923   att_loss = 0.5092117256588407
2022-06-12 01:49:50,923   global_step = 5679
2022-06-12 01:49:50,923   loss = 1.3438867098755307
2022-06-12 01:49:50,923   rep_loss = 0.8346749767661095
2022-06-12 01:49:50,923 ***** Save model *****
2022-06-12 01:49:56,562 ***** Running evaluation *****
2022-06-12 01:49:56,562   Epoch = 21 iter 5699 step
2022-06-12 01:49:56,562   Num examples = 1043
2022-06-12 01:49:56,562   Batch size = 32
2022-06-12 01:49:56,563 ***** Eval results *****
2022-06-12 01:49:56,563   att_loss = 0.5070910207603289
2022-06-12 01:49:56,563   global_step = 5699
2022-06-12 01:49:56,563   loss = 1.338023691073708
2022-06-12 01:49:56,563   rep_loss = 0.8309326651303665
2022-06-12 01:49:56,563 ***** Save model *****
2022-06-12 01:50:02,221 ***** Running evaluation *****
2022-06-12 01:50:02,222   Epoch = 21 iter 5719 step
2022-06-12 01:50:02,222   Num examples = 1043
2022-06-12 01:50:02,222   Batch size = 32
2022-06-12 01:50:02,224 ***** Eval results *****
2022-06-12 01:50:02,224   att_loss = 0.507030670398048
2022-06-12 01:50:02,224   global_step = 5719
2022-06-12 01:50:02,224   loss = 1.3370378123862403
2022-06-12 01:50:02,224   rep_loss = 0.8300071390611785
2022-06-12 01:50:02,224 ***** Save model *****
2022-06-12 01:50:07,952 ***** Running evaluation *****
2022-06-12 01:50:07,953   Epoch = 21 iter 5739 step
2022-06-12 01:50:07,953   Num examples = 1043
2022-06-12 01:50:07,953   Batch size = 32
2022-06-12 01:50:07,954 ***** Eval results *****
2022-06-12 01:50:07,954   att_loss = 0.5070543483351216
2022-06-12 01:50:07,954   global_step = 5739
2022-06-12 01:50:07,954   loss = 1.3379387286576359
2022-06-12 01:50:07,955   rep_loss = 0.8308843780647625
2022-06-12 01:50:07,955 ***** Save model *****
2022-06-12 01:50:13,621 ***** Running evaluation *****
2022-06-12 01:50:13,622   Epoch = 21 iter 5759 step
2022-06-12 01:50:13,622   Num examples = 1043
2022-06-12 01:50:13,622   Batch size = 32
2022-06-12 01:50:13,623 ***** Eval results *****
2022-06-12 01:50:13,623   att_loss = 0.5060725531687862
2022-06-12 01:50:13,623   global_step = 5759
2022-06-12 01:50:13,624   loss = 1.3362688484944796
2022-06-12 01:50:13,624   rep_loss = 0.8301962943453538
2022-06-12 01:50:13,624 ***** Save model *****
2022-06-12 01:50:19,299 ***** Running evaluation *****
2022-06-12 01:50:19,300   Epoch = 21 iter 5779 step
2022-06-12 01:50:19,300   Num examples = 1043
2022-06-12 01:50:19,300   Batch size = 32
2022-06-12 01:50:19,301 ***** Eval results *****
2022-06-12 01:50:19,301   att_loss = 0.5064100973134817
2022-06-12 01:50:19,301   global_step = 5779
2022-06-12 01:50:19,301   loss = 1.3370720688686815
2022-06-12 01:50:19,301   rep_loss = 0.8306619691294294
2022-06-12 01:50:19,301 ***** Save model *****
2022-06-12 01:50:24,977 ***** Running evaluation *****
2022-06-12 01:50:24,977   Epoch = 21 iter 5799 step
2022-06-12 01:50:24,977   Num examples = 1043
2022-06-12 01:50:24,977   Batch size = 32
2022-06-12 01:50:24,979 ***** Eval results *****
2022-06-12 01:50:24,979   att_loss = 0.5071662454865873
2022-06-12 01:50:24,979   global_step = 5799
2022-06-12 01:50:24,979   loss = 1.3373948236306508
2022-06-12 01:50:24,979   rep_loss = 0.8302285770575205
2022-06-12 01:50:24,979 ***** Save model *****
2022-06-12 01:50:30,661 ***** Running evaluation *****
2022-06-12 01:50:30,662   Epoch = 21 iter 5819 step
2022-06-12 01:50:30,662   Num examples = 1043
2022-06-12 01:50:30,662   Batch size = 32
2022-06-12 01:50:30,663 ***** Eval results *****
2022-06-12 01:50:30,663   att_loss = 0.5079030776923558
2022-06-12 01:50:30,663   global_step = 5819
2022-06-12 01:50:30,663   loss = 1.338872713862725
2022-06-12 01:50:30,663   rep_loss = 0.8309696342022914
2022-06-12 01:50:30,663 ***** Save model *****
2022-06-12 01:50:36,290 ***** Running evaluation *****
2022-06-12 01:50:36,290   Epoch = 21 iter 5839 step
2022-06-12 01:50:36,290   Num examples = 1043
2022-06-12 01:50:36,290   Batch size = 32
2022-06-12 01:50:36,291 ***** Eval results *****
2022-06-12 01:50:36,291   att_loss = 0.5096933679847881
2022-06-12 01:50:36,291   global_step = 5839
2022-06-12 01:50:36,291   loss = 1.3417354005164113
2022-06-12 01:50:36,291   rep_loss = 0.8320420304762906
2022-06-12 01:50:36,291 ***** Save model *****
2022-06-12 01:50:41,933 ***** Running evaluation *****
2022-06-12 01:50:41,933   Epoch = 21 iter 5859 step
2022-06-12 01:50:41,934   Num examples = 1043
2022-06-12 01:50:41,934   Batch size = 32
2022-06-12 01:50:41,935 ***** Eval results *****
2022-06-12 01:50:41,935   att_loss = 0.5100425478248369
2022-06-12 01:50:41,935   global_step = 5859
2022-06-12 01:50:41,935   loss = 1.3425001454731775
2022-06-12 01:50:41,935   rep_loss = 0.8324575968204982
2022-06-12 01:50:41,935 ***** Save model *****
2022-06-12 01:50:47,613 ***** Running evaluation *****
2022-06-12 01:50:47,614   Epoch = 22 iter 5879 step
2022-06-12 01:50:47,614   Num examples = 1043
2022-06-12 01:50:47,614   Batch size = 32
2022-06-12 01:50:47,615 ***** Eval results *****
2022-06-12 01:50:47,615   att_loss = 0.4898948907852173
2022-06-12 01:50:47,615   global_step = 5879
2022-06-12 01:50:47,615   loss = 1.3096549272537232
2022-06-12 01:50:47,615   rep_loss = 0.8197600483894348
2022-06-12 01:50:47,615 ***** Save model *****
2022-06-12 01:50:53,328 ***** Running evaluation *****
2022-06-12 01:50:53,329   Epoch = 22 iter 5899 step
2022-06-12 01:50:53,329   Num examples = 1043
2022-06-12 01:50:53,329   Batch size = 32
2022-06-12 01:50:53,331 ***** Eval results *****
2022-06-12 01:50:53,331   att_loss = 0.5014323008060455
2022-06-12 01:50:53,331   global_step = 5899
2022-06-12 01:50:53,331   loss = 1.3283008766174316
2022-06-12 01:50:53,331   rep_loss = 0.8268685698509216
2022-06-12 01:50:53,331 ***** Save model *****
2022-06-12 01:50:58,927 ***** Running evaluation *****
2022-06-12 01:50:58,927   Epoch = 22 iter 5919 step
2022-06-12 01:50:58,928   Num examples = 1043
2022-06-12 01:50:58,928   Batch size = 32
2022-06-12 01:50:58,929 ***** Eval results *****
2022-06-12 01:50:58,929   att_loss = 0.508735454082489
2022-06-12 01:50:58,929   global_step = 5919
2022-06-12 01:50:58,929   loss = 1.3367737346225315
2022-06-12 01:50:58,929   rep_loss = 0.8280382712682088
2022-06-12 01:50:58,929 ***** Save model *****
2022-06-12 01:51:04,609 ***** Running evaluation *****
2022-06-12 01:51:04,609   Epoch = 22 iter 5939 step
2022-06-12 01:51:04,609   Num examples = 1043
2022-06-12 01:51:04,609   Batch size = 32
2022-06-12 01:51:04,610 ***** Eval results *****
2022-06-12 01:51:04,610   att_loss = 0.5092231067327353
2022-06-12 01:51:04,610   global_step = 5939
2022-06-12 01:51:04,610   loss = 1.339659228691688
2022-06-12 01:51:04,611   rep_loss = 0.8304361187494718
2022-06-12 01:51:04,611 ***** Save model *****
2022-06-12 01:51:10,283 ***** Running evaluation *****
2022-06-12 01:51:10,284   Epoch = 22 iter 5959 step
2022-06-12 01:51:10,284   Num examples = 1043
2022-06-12 01:51:10,284   Batch size = 32
2022-06-12 01:51:10,285 ***** Eval results *****
2022-06-12 01:51:10,285   att_loss = 0.5044074661591473
2022-06-12 01:51:10,285   global_step = 5959
2022-06-12 01:51:10,286   loss = 1.3330307048909804
2022-06-12 01:51:10,286   rep_loss = 0.8286232359269086
2022-06-12 01:51:10,286 ***** Save model *****
2022-06-12 01:51:15,983 ***** Running evaluation *****
2022-06-12 01:51:15,984   Epoch = 22 iter 5979 step
2022-06-12 01:51:15,984   Num examples = 1043
2022-06-12 01:51:15,984   Batch size = 32
2022-06-12 01:51:15,985 ***** Eval results *****
2022-06-12 01:51:15,986   att_loss = 0.5031168247972216
2022-06-12 01:51:15,986   global_step = 5979
2022-06-12 01:51:15,986   loss = 1.3309314296359107
2022-06-12 01:51:15,986   rep_loss = 0.8278146022842043
2022-06-12 01:51:15,986 ***** Save model *****
2022-06-12 01:51:21,680 ***** Running evaluation *****
2022-06-12 01:51:21,681   Epoch = 22 iter 5999 step
2022-06-12 01:51:21,681   Num examples = 1043
2022-06-12 01:51:21,681   Batch size = 32
2022-06-12 01:51:21,682 ***** Eval results *****
2022-06-12 01:51:21,683   att_loss = 0.5041741950511932
2022-06-12 01:51:21,683   global_step = 5999
2022-06-12 01:51:21,683   loss = 1.3325045585632325
2022-06-12 01:51:21,683   rep_loss = 0.828330361366272
2022-06-12 01:51:21,683 ***** Save model *****
2022-06-12 01:51:27,342 ***** Running evaluation *****
2022-06-12 01:51:27,342   Epoch = 22 iter 6019 step
2022-06-12 01:51:27,342   Num examples = 1043
2022-06-12 01:51:27,342   Batch size = 32
2022-06-12 01:51:27,343 ***** Eval results *****
2022-06-12 01:51:27,343   att_loss = 0.502333460799579
2022-06-12 01:51:27,344   global_step = 6019
2022-06-12 01:51:27,344   loss = 1.3291583850465971
2022-06-12 01:51:27,344   rep_loss = 0.826824923219352
2022-06-12 01:51:27,344 ***** Save model *****
2022-06-12 01:51:29,895 ***** Running evaluation *****
2022-06-12 01:51:29,896   Epoch = 4 iter 13999 step
2022-06-12 01:51:29,896   Num examples = 5463
2022-06-12 01:51:29,896   Batch size = 32
2022-06-12 01:51:29,897 ***** Eval results *****
2022-06-12 01:51:29,897   att_loss = 3.855214973794586
2022-06-12 01:51:29,897   global_step = 13999
2022-06-12 01:51:29,897   loss = 4.768783239440329
2022-06-12 01:51:29,897   rep_loss = 0.9135682703773137
2022-06-12 01:51:29,898 ***** Save model *****
2022-06-12 01:51:33,000 ***** Running evaluation *****
2022-06-12 01:51:33,001   Epoch = 22 iter 6039 step
2022-06-12 01:51:33,001   Num examples = 1043
2022-06-12 01:51:33,001   Batch size = 32
2022-06-12 01:51:33,002 ***** Eval results *****
2022-06-12 01:51:33,002   att_loss = 0.503818504557465
2022-06-12 01:51:33,002   global_step = 6039
2022-06-12 01:51:33,002   loss = 1.331611775629448
2022-06-12 01:51:33,002   rep_loss = 0.8277932698076421
2022-06-12 01:51:33,003 ***** Save model *****
2022-06-12 01:51:38,674 ***** Running evaluation *****
2022-06-12 01:51:38,674   Epoch = 22 iter 6059 step
2022-06-12 01:51:38,674   Num examples = 1043
2022-06-12 01:51:38,674   Batch size = 32
2022-06-12 01:51:38,676 ***** Eval results *****
2022-06-12 01:51:38,676   att_loss = 0.5050611636123141
2022-06-12 01:51:38,676   global_step = 6059
2022-06-12 01:51:38,676   loss = 1.3324231927459305
2022-06-12 01:51:38,676   rep_loss = 0.8273620283281481
2022-06-12 01:51:38,676 ***** Save model *****
2022-06-12 01:51:44,349 ***** Running evaluation *****
2022-06-12 01:51:44,350   Epoch = 22 iter 6079 step
2022-06-12 01:51:44,350   Num examples = 1043
2022-06-12 01:51:44,350   Batch size = 32
2022-06-12 01:51:44,351 ***** Eval results *****
2022-06-12 01:51:44,351   att_loss = 0.5058580138334414
2022-06-12 01:51:44,351   global_step = 6079
2022-06-12 01:51:44,351   loss = 1.33346142245502
2022-06-12 01:51:44,351   rep_loss = 0.8276034076039384
2022-06-12 01:51:44,351 ***** Save model *****
2022-06-12 01:51:50,029 ***** Running evaluation *****
2022-06-12 01:51:50,030   Epoch = 22 iter 6099 step
2022-06-12 01:51:50,030   Num examples = 1043
2022-06-12 01:51:50,030   Batch size = 32
2022-06-12 01:51:50,031 ***** Eval results *****
2022-06-12 01:51:50,031   att_loss = 0.5063357155852848
2022-06-12 01:51:50,031   global_step = 6099
2022-06-12 01:51:50,031   loss = 1.3346749761369494
2022-06-12 01:51:50,031   rep_loss = 0.8283392601543003
2022-06-12 01:51:50,031 ***** Save model *****
2022-06-12 01:51:55,707 ***** Running evaluation *****
2022-06-12 01:51:55,708   Epoch = 22 iter 6119 step
2022-06-12 01:51:55,708   Num examples = 1043
2022-06-12 01:51:55,708   Batch size = 32
2022-06-12 01:51:55,709 ***** Eval results *****
2022-06-12 01:51:55,709   att_loss = 0.5065490307856579
2022-06-12 01:51:55,709   global_step = 6119
2022-06-12 01:51:55,709   loss = 1.3349463020052228
2022-06-12 01:51:55,709   rep_loss = 0.8283972710979228
2022-06-12 01:51:55,709 ***** Save model *****
2022-06-12 01:52:01,399 ***** Running evaluation *****
2022-06-12 01:52:01,400   Epoch = 22 iter 6139 step
2022-06-12 01:52:01,400   Num examples = 1043
2022-06-12 01:52:01,400   Batch size = 32
2022-06-12 01:52:01,402 ***** Eval results *****
2022-06-12 01:52:01,402   att_loss = 0.5061155459790859
2022-06-12 01:52:01,402   global_step = 6139
2022-06-12 01:52:01,402   loss = 1.334299977320545
2022-06-12 01:52:01,402   rep_loss = 0.8281844323536135
2022-06-12 01:52:01,402 ***** Save model *****
2022-06-12 01:52:07,052 ***** Running evaluation *****
2022-06-12 01:52:07,052   Epoch = 23 iter 6159 step
2022-06-12 01:52:07,052   Num examples = 1043
2022-06-12 01:52:07,052   Batch size = 32
2022-06-12 01:52:07,053 ***** Eval results *****
2022-06-12 01:52:07,054   att_loss = 0.5068368564049403
2022-06-12 01:52:07,054   global_step = 6159
2022-06-12 01:52:07,054   loss = 1.337561653720008
2022-06-12 01:52:07,054   rep_loss = 0.830724812216229
2022-06-12 01:52:07,054 ***** Save model *****
2022-06-12 01:52:12,701 ***** Running evaluation *****
2022-06-12 01:52:12,701   Epoch = 23 iter 6179 step
2022-06-12 01:52:12,701   Num examples = 1043
2022-06-12 01:52:12,701   Batch size = 32
2022-06-12 01:52:12,702 ***** Eval results *****
2022-06-12 01:52:12,703   att_loss = 0.5056367963552475
2022-06-12 01:52:12,703   global_step = 6179
2022-06-12 01:52:12,703   loss = 1.3353834560042934
2022-06-12 01:52:12,703   rep_loss = 0.8297466667074906
2022-06-12 01:52:12,703 ***** Save model *****
2022-06-12 01:52:18,385 ***** Running evaluation *****
2022-06-12 01:52:18,386   Epoch = 23 iter 6199 step
2022-06-12 01:52:18,386   Num examples = 1043
2022-06-12 01:52:18,386   Batch size = 32
2022-06-12 01:52:18,387 ***** Eval results *****
2022-06-12 01:52:18,387   att_loss = 0.5057106187631344
2022-06-12 01:52:18,387   global_step = 6199
2022-06-12 01:52:18,387   loss = 1.332774189011804
2022-06-12 01:52:18,387   rep_loss = 0.8270635769285005
2022-06-12 01:52:18,388 ***** Save model *****
2022-06-12 01:52:24,048 ***** Running evaluation *****
2022-06-12 01:52:24,049   Epoch = 23 iter 6219 step
2022-06-12 01:52:24,050   Num examples = 1043
2022-06-12 01:52:24,050   Batch size = 32
2022-06-12 01:52:24,051 ***** Eval results *****
2022-06-12 01:52:24,051   att_loss = 0.5036248174997476
2022-06-12 01:52:24,051   global_step = 6219
2022-06-12 01:52:24,051   loss = 1.3281629207806709
2022-06-12 01:52:24,051   rep_loss = 0.824538107865896
2022-06-12 01:52:24,052 ***** Save model *****
2022-06-12 01:52:29,766 ***** Running evaluation *****
2022-06-12 01:52:29,767   Epoch = 23 iter 6239 step
2022-06-12 01:52:29,767   Num examples = 1043
2022-06-12 01:52:29,767   Batch size = 32
2022-06-12 01:52:29,768 ***** Eval results *****
2022-06-12 01:52:29,768   att_loss = 0.504852123710574
2022-06-12 01:52:29,768   global_step = 6239
2022-06-12 01:52:29,768   loss = 1.3287223480185684
2022-06-12 01:52:29,768   rep_loss = 0.8238702294777851
2022-06-12 01:52:29,768 ***** Save model *****
2022-06-12 01:52:35,438 ***** Running evaluation *****
2022-06-12 01:52:35,439   Epoch = 23 iter 6259 step
2022-06-12 01:52:35,439   Num examples = 1043
2022-06-12 01:52:35,439   Batch size = 32
2022-06-12 01:52:35,440 ***** Eval results *****
2022-06-12 01:52:35,440   att_loss = 0.5033463469501269
2022-06-12 01:52:35,441   global_step = 6259
2022-06-12 01:52:35,441   loss = 1.3271588218414176
2022-06-12 01:52:35,441   rep_loss = 0.8238124786797216
2022-06-12 01:52:35,441 ***** Save model *****
2022-06-12 01:52:41,128 ***** Running evaluation *****
2022-06-12 01:52:41,129   Epoch = 23 iter 6279 step
2022-06-12 01:52:41,129   Num examples = 1043
2022-06-12 01:52:41,129   Batch size = 32
2022-06-12 01:52:41,130 ***** Eval results *****
2022-06-12 01:52:41,130   att_loss = 0.5030026887206064
2022-06-12 01:52:41,130   global_step = 6279
2022-06-12 01:52:41,130   loss = 1.3258426042570584
2022-06-12 01:52:41,130   rep_loss = 0.8228399187758348
2022-06-12 01:52:41,131 ***** Save model *****
2022-06-12 01:52:46,802 ***** Running evaluation *****
2022-06-12 01:52:46,803   Epoch = 23 iter 6299 step
2022-06-12 01:52:46,803   Num examples = 1043
2022-06-12 01:52:46,803   Batch size = 32
2022-06-12 01:52:46,804 ***** Eval results *****
2022-06-12 01:52:46,804   att_loss = 0.5020462683861768
2022-06-12 01:52:46,804   global_step = 6299
2022-06-12 01:52:46,804   loss = 1.3248869806905337
2022-06-12 01:52:46,804   rep_loss = 0.8228407136247128
2022-06-12 01:52:46,804 ***** Save model *****
2022-06-12 01:52:52,476 ***** Running evaluation *****
2022-06-12 01:52:52,477   Epoch = 23 iter 6319 step
2022-06-12 01:52:52,477   Num examples = 1043
2022-06-12 01:52:52,477   Batch size = 32
2022-06-12 01:52:52,478 ***** Eval results *****
2022-06-12 01:52:52,478   att_loss = 0.5030693524004368
2022-06-12 01:52:52,478   global_step = 6319
2022-06-12 01:52:52,478   loss = 1.3267343446110071
2022-06-12 01:52:52,478   rep_loss = 0.8236649937174293
2022-06-12 01:52:52,479 ***** Save model *****
2022-06-12 01:52:58,152 ***** Running evaluation *****
2022-06-12 01:52:58,153   Epoch = 23 iter 6339 step
2022-06-12 01:52:58,153   Num examples = 1043
2022-06-12 01:52:58,153   Batch size = 32
2022-06-12 01:52:58,155 ***** Eval results *****
2022-06-12 01:52:58,155   att_loss = 0.5042523384997339
2022-06-12 01:52:58,155   global_step = 6339
2022-06-12 01:52:58,155   loss = 1.3280873978980865
2022-06-12 01:52:58,155   rep_loss = 0.8238350598499028
2022-06-12 01:52:58,155 ***** Save model *****
2022-06-12 01:53:03,820 ***** Running evaluation *****
2022-06-12 01:53:03,820   Epoch = 23 iter 6359 step
2022-06-12 01:53:03,820   Num examples = 1043
2022-06-12 01:53:03,820   Batch size = 32
2022-06-12 01:53:03,821 ***** Eval results *****
2022-06-12 01:53:03,821   att_loss = 0.504513442379619
2022-06-12 01:53:03,821   global_step = 6359
2022-06-12 01:53:03,822   loss = 1.3278896496930253
2022-06-12 01:53:03,822   rep_loss = 0.8233762069032826
2022-06-12 01:53:03,822 ***** Save model *****
2022-06-12 01:53:09,461 ***** Running evaluation *****
2022-06-12 01:53:09,462   Epoch = 23 iter 6379 step
2022-06-12 01:53:09,462   Num examples = 1043
2022-06-12 01:53:09,462   Batch size = 32
2022-06-12 01:53:09,463 ***** Eval results *****
2022-06-12 01:53:09,463   att_loss = 0.5053934190704041
2022-06-12 01:53:09,463   global_step = 6379
2022-06-12 01:53:09,463   loss = 1.3288897255889507
2022-06-12 01:53:09,463   rep_loss = 0.8234963066437665
2022-06-12 01:53:09,463 ***** Save model *****
2022-06-12 01:53:15,139 ***** Running evaluation *****
2022-06-12 01:53:15,139   Epoch = 23 iter 6399 step
2022-06-12 01:53:15,139   Num examples = 1043
2022-06-12 01:53:15,139   Batch size = 32
2022-06-12 01:53:15,140 ***** Eval results *****
2022-06-12 01:53:15,140   att_loss = 0.503660206993421
2022-06-12 01:53:15,141   global_step = 6399
2022-06-12 01:53:15,141   loss = 1.3264250196227731
2022-06-12 01:53:15,141   rep_loss = 0.8227648122828136
2022-06-12 01:53:15,141 ***** Save model *****
2022-06-12 01:53:20,785 ***** Running evaluation *****
2022-06-12 01:53:20,786   Epoch = 24 iter 6419 step
2022-06-12 01:53:20,786   Num examples = 1043
2022-06-12 01:53:20,786   Batch size = 32
2022-06-12 01:53:20,788 ***** Eval results *****
2022-06-12 01:53:20,788   att_loss = 0.4801149747588418
2022-06-12 01:53:20,788   global_step = 6419
2022-06-12 01:53:20,788   loss = 1.2910637855529785
2022-06-12 01:53:20,788   rep_loss = 0.8109488162127408
2022-06-12 01:53:20,788 ***** Save model *****
2022-06-12 01:53:26,499 ***** Running evaluation *****
2022-06-12 01:53:26,499   Epoch = 24 iter 6439 step
2022-06-12 01:53:26,499   Num examples = 1043
2022-06-12 01:53:26,499   Batch size = 32
2022-06-12 01:53:26,500 ***** Eval results *****
2022-06-12 01:53:26,500   att_loss = 0.4876066311713188
2022-06-12 01:53:26,500   global_step = 6439
2022-06-12 01:53:26,500   loss = 1.297314836132911
2022-06-12 01:53:26,500   rep_loss = 0.8097082011161312
2022-06-12 01:53:26,500 ***** Save model *****
2022-06-12 01:53:32,170 ***** Running evaluation *****
2022-06-12 01:53:32,170   Epoch = 24 iter 6459 step
2022-06-12 01:53:32,170   Num examples = 1043
2022-06-12 01:53:32,170   Batch size = 32
2022-06-12 01:53:32,171 ***** Eval results *****
2022-06-12 01:53:32,171   att_loss = 0.4945186084392024
2022-06-12 01:53:32,172   global_step = 6459
2022-06-12 01:53:32,172   loss = 1.3066146467246262
2022-06-12 01:53:32,172   rep_loss = 0.8120960382854238
2022-06-12 01:53:32,172 ***** Save model *****
2022-06-12 01:53:37,601 ***** Running evaluation *****
2022-06-12 01:53:37,601   Epoch = 4 iter 14499 step
2022-06-12 01:53:37,601   Num examples = 5463
2022-06-12 01:53:37,601   Batch size = 32
2022-06-12 01:53:37,602 ***** Eval results *****
2022-06-12 01:53:37,603   att_loss = 3.860406725476122
2022-06-12 01:53:37,603   global_step = 14499
2022-06-12 01:53:37,603   loss = 4.773637917165479
2022-06-12 01:53:37,603   rep_loss = 0.9132311946547615
2022-06-12 01:53:37,603 ***** Save model *****
2022-06-12 01:53:37,851 ***** Running evaluation *****
2022-06-12 01:53:37,852   Epoch = 24 iter 6479 step
2022-06-12 01:53:37,852   Num examples = 1043
2022-06-12 01:53:37,852   Batch size = 32
2022-06-12 01:53:37,853 ***** Eval results *****
2022-06-12 01:53:37,853   att_loss = 0.49530074881835723
2022-06-12 01:53:37,853   global_step = 6479
2022-06-12 01:53:37,853   loss = 1.307652814287535
2022-06-12 01:53:37,853   rep_loss = 0.81235207050619
2022-06-12 01:53:37,854 ***** Save model *****
2022-06-12 01:53:43,524 ***** Running evaluation *****
2022-06-12 01:53:43,525   Epoch = 24 iter 6499 step
2022-06-12 01:53:43,525   Num examples = 1043
2022-06-12 01:53:43,525   Batch size = 32
2022-06-12 01:53:43,526 ***** Eval results *****
2022-06-12 01:53:43,526   att_loss = 0.49485202051781035
2022-06-12 01:53:43,526   global_step = 6499
2022-06-12 01:53:43,526   loss = 1.3086821901929246
2022-06-12 01:53:43,527   rep_loss = 0.813830173932589
2022-06-12 01:53:43,527 ***** Save model *****
2022-06-12 01:53:49,208 ***** Running evaluation *****
2022-06-12 01:53:49,209   Epoch = 24 iter 6519 step
2022-06-12 01:53:49,209   Num examples = 1043
2022-06-12 01:53:49,210   Batch size = 32
2022-06-12 01:53:49,212 ***** Eval results *****
2022-06-12 01:53:49,212   att_loss = 0.4990937162089992
2022-06-12 01:53:49,212   global_step = 6519
2022-06-12 01:53:49,212   loss = 1.3149518032331724
2022-06-12 01:53:49,212   rep_loss = 0.8158580897090671
2022-06-12 01:53:49,212 ***** Save model *****
2022-06-12 01:53:54,868 ***** Running evaluation *****
2022-06-12 01:53:54,868   Epoch = 24 iter 6539 step
2022-06-12 01:53:54,868   Num examples = 1043
2022-06-12 01:53:54,868   Batch size = 32
2022-06-12 01:53:54,870 ***** Eval results *****
2022-06-12 01:53:54,870   att_loss = 0.49695753163963785
2022-06-12 01:53:54,870   global_step = 6539
2022-06-12 01:53:54,870   loss = 1.3125751164123303
2022-06-12 01:53:54,870   rep_loss = 0.8156175868201802
2022-06-12 01:53:54,870 ***** Save model *****
2022-06-12 01:54:00,502 ***** Running evaluation *****
2022-06-12 01:54:00,503   Epoch = 24 iter 6559 step
2022-06-12 01:54:00,503   Num examples = 1043
2022-06-12 01:54:00,503   Batch size = 32
2022-06-12 01:54:00,504 ***** Eval results *****
2022-06-12 01:54:00,504   att_loss = 0.49755523240329413
2022-06-12 01:54:00,504   global_step = 6559
2022-06-12 01:54:00,504   loss = 1.3139460702605594
2022-06-12 01:54:00,504   rep_loss = 0.8163908408177609
2022-06-12 01:54:00,504 ***** Save model *****
2022-06-12 01:54:06,174 ***** Running evaluation *****
2022-06-12 01:54:06,175   Epoch = 24 iter 6579 step
2022-06-12 01:54:06,175   Num examples = 1043
2022-06-12 01:54:06,175   Batch size = 32
2022-06-12 01:54:06,176 ***** Eval results *****
2022-06-12 01:54:06,176   att_loss = 0.4980961673440989
2022-06-12 01:54:06,176   global_step = 6579
2022-06-12 01:54:06,176   loss = 1.3145657052770692
2022-06-12 01:54:06,176   rep_loss = 0.8164695386301007
2022-06-12 01:54:06,176 ***** Save model *****
2022-06-12 01:54:11,884 ***** Running evaluation *****
2022-06-12 01:54:11,885   Epoch = 24 iter 6599 step
2022-06-12 01:54:11,885   Num examples = 1043
2022-06-12 01:54:11,885   Batch size = 32
2022-06-12 01:54:11,886 ***** Eval results *****
2022-06-12 01:54:11,886   att_loss = 0.49752273562690974
2022-06-12 01:54:11,886   global_step = 6599
2022-06-12 01:54:11,886   loss = 1.3134953451406268
2022-06-12 01:54:11,886   rep_loss = 0.8159726102938827
2022-06-12 01:54:11,886 ***** Save model *****
2022-06-12 01:54:17,546 ***** Running evaluation *****
2022-06-12 01:54:17,547   Epoch = 24 iter 6619 step
2022-06-12 01:54:17,547   Num examples = 1043
2022-06-12 01:54:17,547   Batch size = 32
2022-06-12 01:54:17,548 ***** Eval results *****
2022-06-12 01:54:17,548   att_loss = 0.4984660562463281
2022-06-12 01:54:17,548   global_step = 6619
2022-06-12 01:54:17,548   loss = 1.3151054625262582
2022-06-12 01:54:17,548   rep_loss = 0.8166394058562003
2022-06-12 01:54:17,548 ***** Save model *****
2022-06-12 01:54:23,250 ***** Running evaluation *****
2022-06-12 01:54:23,251   Epoch = 24 iter 6639 step
2022-06-12 01:54:23,251   Num examples = 1043
2022-06-12 01:54:23,251   Batch size = 32
2022-06-12 01:54:23,253 ***** Eval results *****
2022-06-12 01:54:23,253   att_loss = 0.49904592444886375
2022-06-12 01:54:23,253   global_step = 6639
2022-06-12 01:54:23,254   loss = 1.3161788347995642
2022-06-12 01:54:23,254   rep_loss = 0.8171329093185854
2022-06-12 01:54:23,254 ***** Save model *****
2022-06-12 01:54:28,918 ***** Running evaluation *****
2022-06-12 01:54:28,918   Epoch = 24 iter 6659 step
2022-06-12 01:54:28,919   Num examples = 1043
2022-06-12 01:54:28,919   Batch size = 32
2022-06-12 01:54:28,920 ***** Eval results *****
2022-06-12 01:54:28,920   att_loss = 0.498732324377949
2022-06-12 01:54:28,920   global_step = 6659
2022-06-12 01:54:28,920   loss = 1.315963724694879
2022-06-12 01:54:28,920   rep_loss = 0.8172313981797116
2022-06-12 01:54:28,920 ***** Save model *****
2022-06-12 01:54:34,552 ***** Running evaluation *****
2022-06-12 01:54:34,552   Epoch = 25 iter 6679 step
2022-06-12 01:54:34,552   Num examples = 1043
2022-06-12 01:54:34,552   Batch size = 32
2022-06-12 01:54:34,553 ***** Eval results *****
2022-06-12 01:54:34,553   att_loss = 0.47296077013015747
2022-06-12 01:54:34,553   global_step = 6679
2022-06-12 01:54:34,553   loss = 1.2866829633712769
2022-06-12 01:54:34,553   rep_loss = 0.8137221783399582
2022-06-12 01:54:34,554 ***** Save model *****
2022-06-12 01:54:40,177 ***** Running evaluation *****
2022-06-12 01:54:40,178   Epoch = 25 iter 6699 step
2022-06-12 01:54:40,178   Num examples = 1043
2022-06-12 01:54:40,178   Batch size = 32
2022-06-12 01:54:40,179 ***** Eval results *****
2022-06-12 01:54:40,179   att_loss = 0.49901507546504337
2022-06-12 01:54:40,179   global_step = 6699
2022-06-12 01:54:40,179   loss = 1.3133114476998646
2022-06-12 01:54:40,179   rep_loss = 0.8142963722348213
2022-06-12 01:54:40,179 ***** Save model *****
2022-06-12 01:54:45,857 ***** Running evaluation *****
2022-06-12 01:54:45,857   Epoch = 25 iter 6719 step
2022-06-12 01:54:45,857   Num examples = 1043
2022-06-12 01:54:45,858   Batch size = 32
2022-06-12 01:54:45,859 ***** Eval results *****
2022-06-12 01:54:45,859   att_loss = 0.5035357868129556
2022-06-12 01:54:45,859   global_step = 6719
2022-06-12 01:54:45,859   loss = 1.3165045841173693
2022-06-12 01:54:45,859   rep_loss = 0.8129687945951115
2022-06-12 01:54:45,859 ***** Save model *****
2022-06-12 01:54:51,489 ***** Running evaluation *****
2022-06-12 01:54:51,490   Epoch = 25 iter 6739 step
2022-06-12 01:54:51,490   Num examples = 1043
2022-06-12 01:54:51,490   Batch size = 32
2022-06-12 01:54:51,491 ***** Eval results *****
2022-06-12 01:54:51,491   att_loss = 0.5010990132577717
2022-06-12 01:54:51,491   global_step = 6739
2022-06-12 01:54:51,491   loss = 1.3151104338467121
2022-06-12 01:54:51,491   rep_loss = 0.8140114173293114
2022-06-12 01:54:51,491 ***** Save model *****
2022-06-12 01:54:57,170 ***** Running evaluation *****
2022-06-12 01:54:57,170   Epoch = 25 iter 6759 step
2022-06-12 01:54:57,170   Num examples = 1043
2022-06-12 01:54:57,170   Batch size = 32
2022-06-12 01:54:57,172 ***** Eval results *****
2022-06-12 01:54:57,172   att_loss = 0.5001089689987046
2022-06-12 01:54:57,172   global_step = 6759
2022-06-12 01:54:57,172   loss = 1.3149984124160947
2022-06-12 01:54:57,172   rep_loss = 0.8148894409338633
2022-06-12 01:54:57,172 ***** Save model *****
2022-06-12 01:55:02,812 ***** Running evaluation *****
2022-06-12 01:55:02,812   Epoch = 25 iter 6779 step
2022-06-12 01:55:02,812   Num examples = 1043
2022-06-12 01:55:02,812   Batch size = 32
2022-06-12 01:55:02,813 ***** Eval results *****
2022-06-12 01:55:02,813   att_loss = 0.5002758293770827
2022-06-12 01:55:02,813   global_step = 6779
2022-06-12 01:55:02,813   loss = 1.3134620075042431
2022-06-12 01:55:02,814   rep_loss = 0.8131861766943564
2022-06-12 01:55:02,814 ***** Save model *****
2022-06-12 01:55:08,452 ***** Running evaluation *****
2022-06-12 01:55:08,453   Epoch = 25 iter 6799 step
2022-06-12 01:55:08,453   Num examples = 1043
2022-06-12 01:55:08,453   Batch size = 32
2022-06-12 01:55:08,454 ***** Eval results *****
2022-06-12 01:55:08,455   att_loss = 0.4989163022368185
2022-06-12 01:55:08,455   global_step = 6799
2022-06-12 01:55:08,455   loss = 1.3133997369197108
2022-06-12 01:55:08,455   rep_loss = 0.8144834310777725
2022-06-12 01:55:08,455 ***** Save model *****
2022-06-12 01:55:14,086 ***** Running evaluation *****
2022-06-12 01:55:14,086   Epoch = 25 iter 6819 step
2022-06-12 01:55:14,086   Num examples = 1043
2022-06-12 01:55:14,086   Batch size = 32
2022-06-12 01:55:14,087 ***** Eval results *****
2022-06-12 01:55:14,087   att_loss = 0.4985930559535821
2022-06-12 01:55:14,087   global_step = 6819
2022-06-12 01:55:14,087   loss = 1.3122867287860975
2022-06-12 01:55:14,087   rep_loss = 0.8136936674515406
2022-06-12 01:55:14,088 ***** Save model *****
2022-06-12 01:55:19,729 ***** Running evaluation *****
2022-06-12 01:55:19,730   Epoch = 25 iter 6839 step
2022-06-12 01:55:19,730   Num examples = 1043
2022-06-12 01:55:19,730   Batch size = 32
2022-06-12 01:55:19,731 ***** Eval results *****
2022-06-12 01:55:19,731   att_loss = 0.4964252332361733
2022-06-12 01:55:19,731   global_step = 6839
2022-06-12 01:55:19,731   loss = 1.3101473559693593
2022-06-12 01:55:19,731   rep_loss = 0.8137221180084275
2022-06-12 01:55:19,731 ***** Save model *****
2022-06-12 01:55:25,417 ***** Running evaluation *****
2022-06-12 01:55:25,417   Epoch = 25 iter 6859 step
2022-06-12 01:55:25,418   Num examples = 1043
2022-06-12 01:55:25,418   Batch size = 32
2022-06-12 01:55:25,419 ***** Eval results *****
2022-06-12 01:55:25,419   att_loss = 0.4966301435361738
2022-06-12 01:55:25,419   global_step = 6859
2022-06-12 01:55:25,419   loss = 1.310152539740438
2022-06-12 01:55:25,419   rep_loss = 0.8135223919930665
2022-06-12 01:55:25,419 ***** Save model *****
2022-06-12 01:55:31,105 ***** Running evaluation *****
2022-06-12 01:55:31,106   Epoch = 25 iter 6879 step
2022-06-12 01:55:31,106   Num examples = 1043
2022-06-12 01:55:31,106   Batch size = 32
2022-06-12 01:55:31,107 ***** Eval results *****
2022-06-12 01:55:31,107   att_loss = 0.49526795540370194
2022-06-12 01:55:31,107   global_step = 6879
2022-06-12 01:55:31,107   loss = 1.3081082240039228
2022-06-12 01:55:31,107   rep_loss = 0.8128402642175263
2022-06-12 01:55:31,108 ***** Save model *****
2022-06-12 01:55:36,843 ***** Running evaluation *****
2022-06-12 01:55:36,845   Epoch = 25 iter 6899 step
2022-06-12 01:55:36,845   Num examples = 1043
2022-06-12 01:55:36,845   Batch size = 32
2022-06-12 01:55:36,846 ***** Eval results *****
2022-06-12 01:55:36,846   att_loss = 0.4939462070220283
2022-06-12 01:55:36,846   global_step = 6899
2022-06-12 01:55:36,846   loss = 1.306407900793212
2022-06-12 01:55:36,846   rep_loss = 0.8124616904450315
2022-06-12 01:55:36,846 ***** Save model *****
2022-06-12 01:55:42,545 ***** Running evaluation *****
2022-06-12 01:55:42,546   Epoch = 25 iter 6919 step
2022-06-12 01:55:42,546   Num examples = 1043
2022-06-12 01:55:42,546   Batch size = 32
2022-06-12 01:55:42,547 ***** Eval results *****
2022-06-12 01:55:42,547   att_loss = 0.4946725817977405
2022-06-12 01:55:42,547   global_step = 6919
2022-06-12 01:55:42,547   loss = 1.3073480310010128
2022-06-12 01:55:42,547   rep_loss = 0.8126754462718964
2022-06-12 01:55:42,547 ***** Save model *****
2022-06-12 01:55:45,882 ***** Running evaluation *****
2022-06-12 01:55:45,883   Epoch = 4 iter 14999 step
2022-06-12 01:55:45,883   Num examples = 5463
2022-06-12 01:55:45,883   Batch size = 32
2022-06-12 01:55:45,884 ***** Eval results *****
2022-06-12 01:55:45,884   att_loss = 3.8579425534216845
2022-06-12 01:55:45,885   global_step = 14999
2022-06-12 01:55:45,885   loss = 4.77007358187959
2022-06-12 01:55:45,885   rep_loss = 0.9121310297393899
2022-06-12 01:55:45,885 ***** Save model *****
2022-06-12 01:55:48,223 ***** Running evaluation *****
2022-06-12 01:55:48,224   Epoch = 25 iter 6939 step
2022-06-12 01:55:48,224   Num examples = 1043
2022-06-12 01:55:48,224   Batch size = 32
2022-06-12 01:55:48,225 ***** Eval results *****
2022-06-12 01:55:48,229   att_loss = 0.49527178106434416
2022-06-12 01:55:48,230   global_step = 6939
2022-06-12 01:55:48,230   loss = 1.3080320267966299
2022-06-12 01:55:48,230   rep_loss = 0.8127602435874216
2022-06-12 01:55:48,230 ***** Save model *****
2022-06-12 01:55:53,897 ***** Running evaluation *****
2022-06-12 01:55:53,898   Epoch = 26 iter 6959 step
2022-06-12 01:55:53,898   Num examples = 1043
2022-06-12 01:55:53,898   Batch size = 32
2022-06-12 01:55:53,899 ***** Eval results *****
2022-06-12 01:55:53,899   att_loss = 0.48627787828445435
2022-06-12 01:55:53,899   global_step = 6959
2022-06-12 01:55:53,899   loss = 1.2913451124640072
2022-06-12 01:55:53,899   rep_loss = 0.8050672306733972
2022-06-12 01:55:53,899 ***** Save model *****
2022-06-12 01:55:59,557 ***** Running evaluation *****
2022-06-12 01:55:59,558   Epoch = 26 iter 6979 step
2022-06-12 01:55:59,558   Num examples = 1043
2022-06-12 01:55:59,558   Batch size = 32
2022-06-12 01:55:59,559 ***** Eval results *****
2022-06-12 01:55:59,559   att_loss = 0.4846930407189034
2022-06-12 01:55:59,559   global_step = 6979
2022-06-12 01:55:59,559   loss = 1.290034616315687
2022-06-12 01:55:59,560   rep_loss = 0.8053415755967837
2022-06-12 01:55:59,560 ***** Save model *****
2022-06-12 01:56:05,234 ***** Running evaluation *****
2022-06-12 01:56:05,235   Epoch = 26 iter 6999 step
2022-06-12 01:56:05,235   Num examples = 1043
2022-06-12 01:56:05,235   Batch size = 32
2022-06-12 01:56:05,236 ***** Eval results *****
2022-06-12 01:56:05,236   att_loss = 0.4911479322533858
2022-06-12 01:56:05,236   global_step = 6999
2022-06-12 01:56:05,236   loss = 1.2999544290074132
2022-06-12 01:56:05,236   rep_loss = 0.8088064977997228
2022-06-12 01:56:05,236 ***** Save model *****
2022-06-12 01:56:10,878 ***** Running evaluation *****
2022-06-12 01:56:10,879   Epoch = 26 iter 7019 step
2022-06-12 01:56:10,879   Num examples = 1043
2022-06-12 01:56:10,879   Batch size = 32
2022-06-12 01:56:10,880 ***** Eval results *****
2022-06-12 01:56:10,880   att_loss = 0.4884912925107138
2022-06-12 01:56:10,880   global_step = 7019
2022-06-12 01:56:10,880   loss = 1.2955751186841493
2022-06-12 01:56:10,880   rep_loss = 0.8070838281086513
2022-06-12 01:56:10,880 ***** Save model *****
2022-06-12 01:56:16,582 ***** Running evaluation *****
2022-06-12 01:56:16,583   Epoch = 26 iter 7039 step
2022-06-12 01:56:16,583   Num examples = 1043
2022-06-12 01:56:16,583   Batch size = 32
2022-06-12 01:56:16,584 ***** Eval results *****
2022-06-12 01:56:16,584   att_loss = 0.4891502918656339
2022-06-12 01:56:16,584   global_step = 7039
2022-06-12 01:56:16,584   loss = 1.2971281083588748
2022-06-12 01:56:16,584   rep_loss = 0.8079778195656452
2022-06-12 01:56:16,584 ***** Save model *****
2022-06-12 01:56:22,227 ***** Running evaluation *****
2022-06-12 01:56:22,227   Epoch = 26 iter 7059 step
2022-06-12 01:56:22,227   Num examples = 1043
2022-06-12 01:56:22,227   Batch size = 32
2022-06-12 01:56:22,228 ***** Eval results *****
2022-06-12 01:56:22,229   att_loss = 0.49185166858200335
2022-06-12 01:56:22,229   global_step = 7059
2022-06-12 01:56:22,229   loss = 1.3016704759027204
2022-06-12 01:56:22,229   rep_loss = 0.8098188098679241
2022-06-12 01:56:22,229 ***** Save model *****
2022-06-12 01:56:27,885 ***** Running evaluation *****
2022-06-12 01:56:27,886   Epoch = 26 iter 7079 step
2022-06-12 01:56:27,886   Num examples = 1043
2022-06-12 01:56:27,886   Batch size = 32
2022-06-12 01:56:27,887 ***** Eval results *****
2022-06-12 01:56:27,887   att_loss = 0.4933680537408286
2022-06-12 01:56:27,887   global_step = 7079
2022-06-12 01:56:27,887   loss = 1.3041896228372616
2022-06-12 01:56:27,887   rep_loss = 0.810821572359461
2022-06-12 01:56:27,887 ***** Save model *****
2022-06-12 01:56:33,539 ***** Running evaluation *****
2022-06-12 01:56:33,539   Epoch = 26 iter 7099 step
2022-06-12 01:56:33,539   Num examples = 1043
2022-06-12 01:56:33,539   Batch size = 32
2022-06-12 01:56:33,541 ***** Eval results *****
2022-06-12 01:56:33,541   att_loss = 0.49495167337405455
2022-06-12 01:56:33,541   global_step = 7099
2022-06-12 01:56:33,541   loss = 1.3074082591731078
2022-06-12 01:56:33,541   rep_loss = 0.8124565892158799
2022-06-12 01:56:33,541 ***** Save model *****
2022-06-12 01:56:39,226 ***** Running evaluation *****
2022-06-12 01:56:39,227   Epoch = 26 iter 7119 step
2022-06-12 01:56:39,227   Num examples = 1043
2022-06-12 01:56:39,227   Batch size = 32
2022-06-12 01:56:39,228 ***** Eval results *****
2022-06-12 01:56:39,228   att_loss = 0.49599686551228755
2022-06-12 01:56:39,228   global_step = 7119
2022-06-12 01:56:39,229   loss = 1.3086473773428275
2022-06-12 01:56:39,229   rep_loss = 0.8126505148612847
2022-06-12 01:56:39,229 ***** Save model *****
2022-06-12 01:56:44,884 ***** Running evaluation *****
2022-06-12 01:56:44,885   Epoch = 26 iter 7139 step
2022-06-12 01:56:44,885   Num examples = 1043
2022-06-12 01:56:44,885   Batch size = 32
2022-06-12 01:56:44,886 ***** Eval results *****
2022-06-12 01:56:44,886   att_loss = 0.49610812440136365
2022-06-12 01:56:44,886   global_step = 7139
2022-06-12 01:56:44,886   loss = 1.3080629654947271
2022-06-12 01:56:44,886   rep_loss = 0.8119548447241033
2022-06-12 01:56:44,887 ***** Save model *****
2022-06-12 01:56:50,519 ***** Running evaluation *****
2022-06-12 01:56:50,520   Epoch = 26 iter 7159 step
2022-06-12 01:56:50,520   Num examples = 1043
2022-06-12 01:56:50,520   Batch size = 32
2022-06-12 01:56:50,521 ***** Eval results *****
2022-06-12 01:56:50,521   att_loss = 0.49611660329976937
2022-06-12 01:56:50,522   global_step = 7159
2022-06-12 01:56:50,522   loss = 1.3074237267542546
2022-06-12 01:56:50,522   rep_loss = 0.8113071267505945
2022-06-12 01:56:50,522 ***** Save model *****
2022-06-12 01:56:56,179 ***** Running evaluation *****
2022-06-12 01:56:56,180   Epoch = 26 iter 7179 step
2022-06-12 01:56:56,180   Num examples = 1043
2022-06-12 01:56:56,180   Batch size = 32
2022-06-12 01:56:56,181 ***** Eval results *****
2022-06-12 01:56:56,181   att_loss = 0.49567074149469786
2022-06-12 01:56:56,181   global_step = 7179
2022-06-12 01:56:56,181   loss = 1.3065702387049227
2022-06-12 01:56:56,181   rep_loss = 0.8108994993479443
2022-06-12 01:56:56,181 ***** Save model *****
2022-06-12 01:57:01,833 ***** Running evaluation *****
2022-06-12 01:57:01,834   Epoch = 26 iter 7199 step
2022-06-12 01:57:01,834   Num examples = 1043
2022-06-12 01:57:01,834   Batch size = 32
2022-06-12 01:57:01,835 ***** Eval results *****
2022-06-12 01:57:01,835   att_loss = 0.4948744162743194
2022-06-12 01:57:01,835   global_step = 7199
2022-06-12 01:57:01,835   loss = 1.3055192886159577
2022-06-12 01:57:01,835   rep_loss = 0.8106448740810736
2022-06-12 01:57:01,835 ***** Save model *****
2022-06-12 01:57:07,494 ***** Running evaluation *****
2022-06-12 01:57:07,494   Epoch = 27 iter 7219 step
2022-06-12 01:57:07,494   Num examples = 1043
2022-06-12 01:57:07,495   Batch size = 32
2022-06-12 01:57:07,496 ***** Eval results *****
2022-06-12 01:57:07,496   att_loss = 0.5151849031448364
2022-06-12 01:57:07,496   global_step = 7219
2022-06-12 01:57:07,496   loss = 1.3308149456977845
2022-06-12 01:57:07,496   rep_loss = 0.815630030632019
2022-06-12 01:57:07,496 ***** Save model *****
2022-06-12 01:57:13,163 ***** Running evaluation *****
2022-06-12 01:57:13,163   Epoch = 27 iter 7239 step
2022-06-12 01:57:13,163   Num examples = 1043
2022-06-12 01:57:13,164   Batch size = 32
2022-06-12 01:57:13,165 ***** Eval results *****
2022-06-12 01:57:13,165   att_loss = 0.5097490380207698
2022-06-12 01:57:13,165   global_step = 7239
2022-06-12 01:57:13,165   loss = 1.324842377503713
2022-06-12 01:57:13,165   rep_loss = 0.815093328555425
2022-06-12 01:57:13,165 ***** Save model *****
2022-06-12 01:57:18,803 ***** Running evaluation *****
2022-06-12 01:57:18,804   Epoch = 27 iter 7259 step
2022-06-12 01:57:18,804   Num examples = 1043
2022-06-12 01:57:18,804   Batch size = 32
2022-06-12 01:57:18,805 ***** Eval results *****
2022-06-12 01:57:18,805   att_loss = 0.49978588342666624
2022-06-12 01:57:18,805   global_step = 7259
2022-06-12 01:57:18,805   loss = 1.3101274800300597
2022-06-12 01:57:18,805   rep_loss = 0.8103415858745575
2022-06-12 01:57:18,806 ***** Save model *****
2022-06-12 01:57:24,441 ***** Running evaluation *****
2022-06-12 01:57:24,442   Epoch = 27 iter 7279 step
2022-06-12 01:57:24,442   Num examples = 1043
2022-06-12 01:57:24,443   Batch size = 32
2022-06-12 01:57:24,444 ***** Eval results *****
2022-06-12 01:57:24,444   att_loss = 0.4990948608943394
2022-06-12 01:57:24,444   global_step = 7279
2022-06-12 01:57:24,444   loss = 1.309130721432822
2022-06-12 01:57:24,444   rep_loss = 0.8100358528750283
2022-06-12 01:57:24,445 ***** Save model *****
2022-06-12 01:57:30,094 ***** Running evaluation *****
2022-06-12 01:57:30,094   Epoch = 27 iter 7299 step
2022-06-12 01:57:30,094   Num examples = 1043
2022-06-12 01:57:30,094   Batch size = 32
2022-06-12 01:57:30,096 ***** Eval results *****
2022-06-12 01:57:30,096   att_loss = 0.49268103109465705
2022-06-12 01:57:30,096   global_step = 7299
2022-06-12 01:57:30,096   loss = 1.3012136379877726
2022-06-12 01:57:30,096   rep_loss = 0.8085326002703772
2022-06-12 01:57:30,096 ***** Save model *****
2022-06-12 01:57:35,770 ***** Running evaluation *****
2022-06-12 01:57:35,770   Epoch = 27 iter 7319 step
2022-06-12 01:57:35,770   Num examples = 1043
2022-06-12 01:57:35,771   Batch size = 32
2022-06-12 01:57:35,772 ***** Eval results *****
2022-06-12 01:57:35,772   att_loss = 0.4950764661485499
2022-06-12 01:57:35,772   global_step = 7319
2022-06-12 01:57:35,772   loss = 1.3043354825540023
2022-06-12 01:57:35,772   rep_loss = 0.8092590104449879
2022-06-12 01:57:35,772 ***** Save model *****
2022-06-12 01:57:41,412 ***** Running evaluation *****
2022-06-12 01:57:41,412   Epoch = 27 iter 7339 step
2022-06-12 01:57:41,413   Num examples = 1043
2022-06-12 01:57:41,413   Batch size = 32
2022-06-12 01:57:41,414 ***** Eval results *****
2022-06-12 01:57:41,414   att_loss = 0.4947547830068148
2022-06-12 01:57:41,414   global_step = 7339
2022-06-12 01:57:41,414   loss = 1.3032811421614428
2022-06-12 01:57:41,414   rep_loss = 0.8085263541111579
2022-06-12 01:57:41,414 ***** Save model *****
2022-06-12 01:57:47,104 ***** Running evaluation *****
2022-06-12 01:57:47,105   Epoch = 27 iter 7359 step
2022-06-12 01:57:47,105   Num examples = 1043
2022-06-12 01:57:47,105   Batch size = 32
2022-06-12 01:57:47,106 ***** Eval results *****
2022-06-12 01:57:47,106   att_loss = 0.49422412157058715
2022-06-12 01:57:47,107   global_step = 7359
2022-06-12 01:57:47,107   loss = 1.303058831691742
2022-06-12 01:57:47,107   rep_loss = 0.8088347061475119
2022-06-12 01:57:47,107 ***** Save model *****
2022-06-12 01:57:52,816 ***** Running evaluation *****
2022-06-12 01:57:52,817   Epoch = 27 iter 7379 step
2022-06-12 01:57:52,817   Num examples = 1043
2022-06-12 01:57:52,817   Batch size = 32
2022-06-12 01:57:52,818 ***** Eval results *****
2022-06-12 01:57:52,818   att_loss = 0.4935804190004573
2022-06-12 01:57:52,818   global_step = 7379
2022-06-12 01:57:52,818   loss = 1.301499269289129
2022-06-12 01:57:52,818   rep_loss = 0.8079188473084393
2022-06-12 01:57:52,819 ***** Save model *****
2022-06-12 01:57:53,854 ***** Running evaluation *****
2022-06-12 01:57:53,855   Epoch = 4 iter 15499 step
2022-06-12 01:57:53,855   Num examples = 5463
2022-06-12 01:57:53,855   Batch size = 32
2022-06-12 01:57:53,856 ***** Eval results *****
2022-06-12 01:57:53,856   att_loss = 3.862453158584233
2022-06-12 01:57:53,856   global_step = 15499
2022-06-12 01:57:53,857   loss = 4.774253145906506
2022-06-12 01:57:53,857   rep_loss = 0.9117999876441923
2022-06-12 01:57:53,857 ***** Save model *****
2022-06-12 01:57:58,482 ***** Running evaluation *****
2022-06-12 01:57:58,483   Epoch = 27 iter 7399 step
2022-06-12 01:57:58,483   Num examples = 1043
2022-06-12 01:57:58,483   Batch size = 32
2022-06-12 01:57:58,484 ***** Eval results *****
2022-06-12 01:57:58,484   att_loss = 0.4955850434930701
2022-06-12 01:57:58,484   global_step = 7399
2022-06-12 01:57:58,484   loss = 1.3042577160032172
2022-06-12 01:57:58,484   rep_loss = 0.8086726690593519
2022-06-12 01:57:58,485 ***** Save model *****
2022-06-12 01:58:04,133 ***** Running evaluation *****
2022-06-12 01:58:04,134   Epoch = 27 iter 7419 step
2022-06-12 01:58:04,134   Num examples = 1043
2022-06-12 01:58:04,134   Batch size = 32
2022-06-12 01:58:04,135 ***** Eval results *****
2022-06-12 01:58:04,135   att_loss = 0.4943378221421015
2022-06-12 01:58:04,135   global_step = 7419
2022-06-12 01:58:04,135   loss = 1.3025640527407327
2022-06-12 01:58:04,135   rep_loss = 0.8082262274764833
2022-06-12 01:58:04,135 ***** Save model *****
2022-06-12 01:58:09,766 ***** Running evaluation *****
2022-06-12 01:58:09,766   Epoch = 27 iter 7439 step
2022-06-12 01:58:09,766   Num examples = 1043
2022-06-12 01:58:09,767   Batch size = 32
2022-06-12 01:58:09,768 ***** Eval results *****
2022-06-12 01:58:09,768   att_loss = 0.4914533679899962
2022-06-12 01:58:09,768   global_step = 7439
2022-06-12 01:58:09,768   loss = 1.2990171930064325
2022-06-12 01:58:09,768   rep_loss = 0.8075638219066288
2022-06-12 01:58:09,768 ***** Save model *****
2022-06-12 01:58:15,432 ***** Running evaluation *****
2022-06-12 01:58:15,433   Epoch = 27 iter 7459 step
2022-06-12 01:58:15,433   Num examples = 1043
2022-06-12 01:58:15,433   Batch size = 32
2022-06-12 01:58:15,434 ***** Eval results *****
2022-06-12 01:58:15,434   att_loss = 0.49098705637454987
2022-06-12 01:58:15,434   global_step = 7459
2022-06-12 01:58:15,434   loss = 1.298365339756012
2022-06-12 01:58:15,434   rep_loss = 0.8073782808780671
2022-06-12 01:58:15,434 ***** Save model *****
2022-06-12 01:58:21,099 ***** Running evaluation *****
2022-06-12 01:58:21,099   Epoch = 28 iter 7479 step
2022-06-12 01:58:21,099   Num examples = 1043
2022-06-12 01:58:21,099   Batch size = 32
2022-06-12 01:58:21,101 ***** Eval results *****
2022-06-12 01:58:21,101   att_loss = 0.49037397901217145
2022-06-12 01:58:21,101   global_step = 7479
2022-06-12 01:58:21,101   loss = 1.291336178779602
2022-06-12 01:58:21,101   rep_loss = 0.8009621699651083
2022-06-12 01:58:21,101 ***** Save model *****
2022-06-12 01:58:26,812 ***** Running evaluation *****
2022-06-12 01:58:26,813   Epoch = 28 iter 7499 step
2022-06-12 01:58:26,813   Num examples = 1043
2022-06-12 01:58:26,813   Batch size = 32
2022-06-12 01:58:26,814 ***** Eval results *****
2022-06-12 01:58:26,814   att_loss = 0.49203469442284625
2022-06-12 01:58:26,814   global_step = 7499
2022-06-12 01:58:26,814   loss = 1.2960671808408655
2022-06-12 01:58:26,814   rep_loss = 0.8040324786435002
2022-06-12 01:58:26,814 ***** Save model *****
2022-06-12 01:58:32,495 ***** Running evaluation *****
2022-06-12 01:58:32,496   Epoch = 28 iter 7519 step
2022-06-12 01:58:32,496   Num examples = 1043
2022-06-12 01:58:32,496   Batch size = 32
2022-06-12 01:58:32,497 ***** Eval results *****
2022-06-12 01:58:32,497   att_loss = 0.48597973030666974
2022-06-12 01:58:32,497   global_step = 7519
2022-06-12 01:58:32,497   loss = 1.2879530413206233
2022-06-12 01:58:32,497   rep_loss = 0.8019733082416446
2022-06-12 01:58:32,498 ***** Save model *****
2022-06-12 01:58:38,151 ***** Running evaluation *****
2022-06-12 01:58:38,152   Epoch = 28 iter 7539 step
2022-06-12 01:58:38,152   Num examples = 1043
2022-06-12 01:58:38,152   Batch size = 32
2022-06-12 01:58:38,153 ***** Eval results *****
2022-06-12 01:58:38,153   att_loss = 0.4844763430338057
2022-06-12 01:58:38,153   global_step = 7539
2022-06-12 01:58:38,153   loss = 1.2862443318442693
2022-06-12 01:58:38,153   rep_loss = 0.8017679831338307
2022-06-12 01:58:38,153 ***** Save model *****
2022-06-12 01:58:43,821 ***** Running evaluation *****
2022-06-12 01:58:43,821   Epoch = 28 iter 7559 step
2022-06-12 01:58:43,821   Num examples = 1043
2022-06-12 01:58:43,822   Batch size = 32
2022-06-12 01:58:43,823 ***** Eval results *****
2022-06-12 01:58:43,823   att_loss = 0.4873250895235912
2022-06-12 01:58:43,823   global_step = 7559
2022-06-12 01:58:43,823   loss = 1.2895778877189361
2022-06-12 01:58:43,823   rep_loss = 0.8022527946047036
2022-06-12 01:58:43,823 ***** Save model *****
2022-06-12 01:58:49,487 ***** Running evaluation *****
2022-06-12 01:58:49,488   Epoch = 28 iter 7579 step
2022-06-12 01:58:49,488   Num examples = 1043
2022-06-12 01:58:49,488   Batch size = 32
2022-06-12 01:58:49,489 ***** Eval results *****
2022-06-12 01:58:49,489   att_loss = 0.4908142914471117
2022-06-12 01:58:49,489   global_step = 7579
2022-06-12 01:58:49,489   loss = 1.2950960594473533
2022-06-12 01:58:49,489   rep_loss = 0.8042817653961551
2022-06-12 01:58:49,489 ***** Save model *****
2022-06-12 01:58:55,181 ***** Running evaluation *****
2022-06-12 01:58:55,182   Epoch = 28 iter 7599 step
2022-06-12 01:58:55,182   Num examples = 1043
2022-06-12 01:58:55,182   Batch size = 32
2022-06-12 01:58:55,183 ***** Eval results *****
2022-06-12 01:58:55,183   att_loss = 0.489544386786174
2022-06-12 01:58:55,183   global_step = 7599
2022-06-12 01:58:55,183   loss = 1.2943338252664582
2022-06-12 01:58:55,183   rep_loss = 0.8047894346035593
2022-06-12 01:58:55,183 ***** Save model *****
2022-06-12 01:59:00,840 ***** Running evaluation *****
2022-06-12 01:59:00,841   Epoch = 28 iter 7619 step
2022-06-12 01:59:00,841   Num examples = 1043
2022-06-12 01:59:00,841   Batch size = 32
2022-06-12 01:59:00,842 ***** Eval results *****
2022-06-12 01:59:00,842   att_loss = 0.4914822305415894
2022-06-12 01:59:00,842   global_step = 7619
2022-06-12 01:59:00,842   loss = 1.2962271858762193
2022-06-12 01:59:00,842   rep_loss = 0.8047449517916966
2022-06-12 01:59:00,842 ***** Save model *****
2022-06-12 01:59:06,513 ***** Running evaluation *****
2022-06-12 01:59:06,513   Epoch = 28 iter 7639 step
2022-06-12 01:59:06,513   Num examples = 1043
2022-06-12 01:59:06,514   Batch size = 32
2022-06-12 01:59:06,515 ***** Eval results *****
2022-06-12 01:59:06,515   att_loss = 0.49389690124184077
2022-06-12 01:59:06,515   global_step = 7639
2022-06-12 01:59:06,515   loss = 1.2995311343596756
2022-06-12 01:59:06,515   rep_loss = 0.8056342298267809
2022-06-12 01:59:06,515 ***** Save model *****
2022-06-12 01:59:12,226 ***** Running evaluation *****
2022-06-12 01:59:12,226   Epoch = 28 iter 7659 step
2022-06-12 01:59:12,226   Num examples = 1043
2022-06-12 01:59:12,226   Batch size = 32
2022-06-12 01:59:12,227 ***** Eval results *****
2022-06-12 01:59:12,227   att_loss = 0.4946975309014972
2022-06-12 01:59:12,227   global_step = 7659
2022-06-12 01:59:12,227   loss = 1.3005956111709929
2022-06-12 01:59:12,228   rep_loss = 0.8058980775009739
2022-06-12 01:59:12,228 ***** Save model *****
2022-06-12 01:59:17,904 ***** Running evaluation *****
2022-06-12 01:59:17,905   Epoch = 28 iter 7679 step
2022-06-12 01:59:17,905   Num examples = 1043
2022-06-12 01:59:17,905   Batch size = 32
2022-06-12 01:59:17,906 ***** Eval results *****
2022-06-12 01:59:17,906   att_loss = 0.49402430448038825
2022-06-12 01:59:17,906   global_step = 7679
2022-06-12 01:59:17,906   loss = 1.2998930697370632
2022-06-12 01:59:17,906   rep_loss = 0.8058687633481519
2022-06-12 01:59:17,907 ***** Save model *****
2022-06-12 01:59:23,573 ***** Running evaluation *****
2022-06-12 01:59:23,573   Epoch = 28 iter 7699 step
2022-06-12 01:59:23,573   Num examples = 1043
2022-06-12 01:59:23,573   Batch size = 32
2022-06-12 01:59:23,574 ***** Eval results *****
2022-06-12 01:59:23,574   att_loss = 0.4932262717073808
2022-06-12 01:59:23,574   global_step = 7699
2022-06-12 01:59:23,574   loss = 1.2984098484697897
2022-06-12 01:59:23,574   rep_loss = 0.8051835750250539
2022-06-12 01:59:23,574 ***** Save model *****
2022-06-12 01:59:29,323 ***** Running evaluation *****
2022-06-12 01:59:29,323   Epoch = 28 iter 7719 step
2022-06-12 01:59:29,323   Num examples = 1043
2022-06-12 01:59:29,323   Batch size = 32
2022-06-12 01:59:29,324 ***** Eval results *****
2022-06-12 01:59:29,324   att_loss = 0.49119042651152905
2022-06-12 01:59:29,324   global_step = 7719
2022-06-12 01:59:29,324   loss = 1.2956207588376332
2022-06-12 01:59:29,324   rep_loss = 0.8044303307317412
2022-06-12 01:59:29,325 ***** Save model *****
2022-06-12 01:59:35,012 ***** Running evaluation *****
2022-06-12 01:59:35,012   Epoch = 28 iter 7739 step
2022-06-12 01:59:35,012   Num examples = 1043
2022-06-12 01:59:35,012   Batch size = 32
2022-06-12 01:59:35,013 ***** Eval results *****
2022-06-12 01:59:35,014   att_loss = 0.48878618870851204
2022-06-12 01:59:35,014   global_step = 7739
2022-06-12 01:59:35,014   loss = 1.2928912018641774
2022-06-12 01:59:35,014   rep_loss = 0.8041050112292794
2022-06-12 01:59:35,014 ***** Save model *****
2022-06-12 01:59:40,697 ***** Running evaluation *****
2022-06-12 01:59:40,697   Epoch = 29 iter 7759 step
2022-06-12 01:59:40,697   Num examples = 1043
2022-06-12 01:59:40,697   Batch size = 32
2022-06-12 01:59:40,698 ***** Eval results *****
2022-06-12 01:59:40,698   att_loss = 0.47605716437101364
2022-06-12 01:59:40,699   global_step = 7759
2022-06-12 01:59:40,699   loss = 1.2732434645295143
2022-06-12 01:59:40,699   rep_loss = 0.7971862927079201
2022-06-12 01:59:40,699 ***** Save model *****
2022-06-12 01:59:46,323 ***** Running evaluation *****
2022-06-12 01:59:46,324   Epoch = 29 iter 7779 step
2022-06-12 01:59:46,324   Num examples = 1043
2022-06-12 01:59:46,324   Batch size = 32
2022-06-12 01:59:46,325 ***** Eval results *****
2022-06-12 01:59:46,325   att_loss = 0.4819468963477347
2022-06-12 01:59:46,325   global_step = 7779
2022-06-12 01:59:46,325   loss = 1.2790219949351416
2022-06-12 01:59:46,325   rep_loss = 0.7970750878254572
2022-06-12 01:59:46,325 ***** Save model *****
2022-06-12 01:59:52,009 ***** Running evaluation *****
2022-06-12 01:59:52,010   Epoch = 29 iter 7799 step
2022-06-12 01:59:52,010   Num examples = 1043
2022-06-12 01:59:52,010   Batch size = 32
2022-06-12 01:59:52,011 ***** Eval results *****
2022-06-12 01:59:52,011   att_loss = 0.47753359537039486
2022-06-12 01:59:52,011   global_step = 7799
2022-06-12 01:59:52,011   loss = 1.2726190005029951
2022-06-12 01:59:52,011   rep_loss = 0.7950853960854667
2022-06-12 01:59:52,011 ***** Save model *****
2022-06-12 01:59:57,691 ***** Running evaluation *****
2022-06-12 01:59:57,692   Epoch = 29 iter 7819 step
2022-06-12 01:59:57,692   Num examples = 1043
2022-06-12 01:59:57,692   Batch size = 32
2022-06-12 01:59:57,693 ***** Eval results *****
2022-06-12 01:59:57,693   att_loss = 0.4796756635371007
2022-06-12 01:59:57,693   global_step = 7819
2022-06-12 01:59:57,693   loss = 1.2751109521639974
2022-06-12 01:59:57,693   rep_loss = 0.7954352827448594
2022-06-12 01:59:57,693 ***** Save model *****
2022-06-12 02:00:01,739 ***** Running evaluation *****
2022-06-12 02:00:01,739   Epoch = 4 iter 15999 step
2022-06-12 02:00:01,739   Num examples = 5463
2022-06-12 02:00:01,739   Batch size = 32
2022-06-12 02:00:01,741 ***** Eval results *****
2022-06-12 02:00:01,741   att_loss = 3.8593213723047843
2022-06-12 02:00:01,741   global_step = 15999
2022-06-12 02:00:01,741   loss = 4.770528093580122
2022-06-12 02:00:01,741   rep_loss = 0.9112067203936726
2022-06-12 02:00:01,741 ***** Save model *****
2022-06-12 02:00:03,352 ***** Running evaluation *****
2022-06-12 02:00:03,353   Epoch = 29 iter 7839 step
2022-06-12 02:00:03,353   Num examples = 1043
2022-06-12 02:00:03,353   Batch size = 32
2022-06-12 02:00:03,354 ***** Eval results *****
2022-06-12 02:00:03,354   att_loss = 0.483754294924438
2022-06-12 02:00:03,354   global_step = 7839
2022-06-12 02:00:03,354   loss = 1.2818339218695958
2022-06-12 02:00:03,355   rep_loss = 0.7980796210467815
2022-06-12 02:00:03,355 ***** Save model *****
2022-06-12 02:00:08,992 ***** Running evaluation *****
2022-06-12 02:00:08,993   Epoch = 29 iter 7859 step
2022-06-12 02:00:08,993   Num examples = 1043
2022-06-12 02:00:08,993   Batch size = 32
2022-06-12 02:00:08,994 ***** Eval results *****
2022-06-12 02:00:08,994   att_loss = 0.4787855402662836
2022-06-12 02:00:08,995   global_step = 7859
2022-06-12 02:00:08,995   loss = 1.2749362197415581
2022-06-12 02:00:08,995   rep_loss = 0.7961506735661934
2022-06-12 02:00:08,995 ***** Save model *****
2022-06-12 02:00:14,721 ***** Running evaluation *****
2022-06-12 02:00:14,721   Epoch = 29 iter 7879 step
2022-06-12 02:00:14,721   Num examples = 1043
2022-06-12 02:00:14,721   Batch size = 32
2022-06-12 02:00:14,722 ***** Eval results *****
2022-06-12 02:00:14,723   att_loss = 0.478684129741262
2022-06-12 02:00:14,723   global_step = 7879
2022-06-12 02:00:14,723   loss = 1.2746417873045977
2022-06-12 02:00:14,723   rep_loss = 0.7959576520849677
2022-06-12 02:00:14,723 ***** Save model *****
2022-06-12 02:00:20,390 ***** Running evaluation *****
2022-06-12 02:00:20,391   Epoch = 29 iter 7899 step
2022-06-12 02:00:20,391   Num examples = 1043
2022-06-12 02:00:20,391   Batch size = 32
2022-06-12 02:00:20,392 ***** Eval results *****
2022-06-12 02:00:20,392   att_loss = 0.47874590315115756
2022-06-12 02:00:20,393   global_step = 7899
2022-06-12 02:00:20,393   loss = 1.2740171605195754
2022-06-12 02:00:20,393   rep_loss = 0.7952712529744858
2022-06-12 02:00:20,393 ***** Save model *****
2022-06-12 02:00:26,047 ***** Running evaluation *****
2022-06-12 02:00:26,048   Epoch = 29 iter 7919 step
2022-06-12 02:00:26,048   Num examples = 1043
2022-06-12 02:00:26,048   Batch size = 32
2022-06-12 02:00:26,049 ***** Eval results *****
2022-06-12 02:00:26,049   att_loss = 0.4784240873361176
2022-06-12 02:00:26,049   global_step = 7919
2022-06-12 02:00:26,049   loss = 1.274432161314921
2022-06-12 02:00:26,049   rep_loss = 0.796008069745519
2022-06-12 02:00:26,049 ***** Save model *****
2022-06-12 02:00:31,694 ***** Running evaluation *****
2022-06-12 02:00:31,694   Epoch = 29 iter 7939 step
2022-06-12 02:00:31,694   Num examples = 1043
2022-06-12 02:00:31,694   Batch size = 32
2022-06-12 02:00:31,695 ***** Eval results *****
2022-06-12 02:00:31,695   att_loss = 0.4795968251264825
2022-06-12 02:00:31,695   global_step = 7939
2022-06-12 02:00:31,696   loss = 1.2761371920303421
2022-06-12 02:00:31,696   rep_loss = 0.7965403631025431
2022-06-12 02:00:31,696 ***** Save model *****
2022-06-12 02:00:37,365 ***** Running evaluation *****
2022-06-12 02:00:37,366   Epoch = 29 iter 7959 step
2022-06-12 02:00:37,366   Num examples = 1043
2022-06-12 02:00:37,366   Batch size = 32
2022-06-12 02:00:37,367 ***** Eval results *****
2022-06-12 02:00:37,367   att_loss = 0.4811268335691205
2022-06-12 02:00:37,367   global_step = 7959
2022-06-12 02:00:37,367   loss = 1.2787036934384592
2022-06-12 02:00:37,367   rep_loss = 0.7975768551782325
2022-06-12 02:00:37,367 ***** Save model *****
2022-06-12 02:00:43,046 ***** Running evaluation *****
2022-06-12 02:00:43,047   Epoch = 29 iter 7979 step
2022-06-12 02:00:43,047   Num examples = 1043
2022-06-12 02:00:43,047   Batch size = 32
2022-06-12 02:00:43,048 ***** Eval results *****
2022-06-12 02:00:43,048   att_loss = 0.4826237206994477
2022-06-12 02:00:43,048   global_step = 7979
2022-06-12 02:00:43,049   loss = 1.2802800217927512
2022-06-12 02:00:43,049   rep_loss = 0.7976562969260297
2022-06-12 02:00:43,049 ***** Save model *****
2022-06-12 02:00:48,717 ***** Running evaluation *****
2022-06-12 02:00:48,718   Epoch = 29 iter 7999 step
2022-06-12 02:00:48,718   Num examples = 1043
2022-06-12 02:00:48,718   Batch size = 32
2022-06-12 02:00:48,719 ***** Eval results *****
2022-06-12 02:00:48,719   att_loss = 0.4835098800249398
2022-06-12 02:00:48,720   global_step = 7999
2022-06-12 02:00:48,720   loss = 1.2813540948554873
2022-06-12 02:00:48,720   rep_loss = 0.7978442115709186
2022-06-12 02:00:48,720 ***** Save model *****
2022-06-12 02:00:54,447 ***** Running evaluation *****
2022-06-12 02:00:54,447   Epoch = 30 iter 8019 step
2022-06-12 02:00:54,448   Num examples = 1043
2022-06-12 02:00:54,448   Batch size = 32
2022-06-12 02:00:54,449 ***** Eval results *****
2022-06-12 02:00:54,449   att_loss = 0.47424114412731594
2022-06-12 02:00:54,449   global_step = 8019
2022-06-12 02:00:54,449   loss = 1.2646712859471638
2022-06-12 02:00:54,449   rep_loss = 0.7904301418198479
2022-06-12 02:00:54,449 ***** Save model *****
2022-06-12 02:01:00,144 ***** Running evaluation *****
2022-06-12 02:01:00,144   Epoch = 30 iter 8039 step
2022-06-12 02:01:00,144   Num examples = 1043
2022-06-12 02:01:00,145   Batch size = 32
2022-06-12 02:01:00,146 ***** Eval results *****
2022-06-12 02:01:00,146   att_loss = 0.4558256134904664
2022-06-12 02:01:00,146   global_step = 8039
2022-06-12 02:01:00,146   loss = 1.2414811027461086
2022-06-12 02:01:00,146   rep_loss = 0.7856554841173107
2022-06-12 02:01:00,146 ***** Save model *****
2022-06-12 02:01:05,798 ***** Running evaluation *****
2022-06-12 02:01:05,799   Epoch = 30 iter 8059 step
2022-06-12 02:01:05,799   Num examples = 1043
2022-06-12 02:01:05,799   Batch size = 32
2022-06-12 02:01:05,800 ***** Eval results *****
2022-06-12 02:01:05,800   att_loss = 0.46465869095860696
2022-06-12 02:01:05,800   global_step = 8059
2022-06-12 02:01:05,800   loss = 1.2534267148193048
2022-06-12 02:01:05,800   rep_loss = 0.7887680202114339
2022-06-12 02:01:05,800 ***** Save model *****
2022-06-12 02:01:11,444 ***** Running evaluation *****
2022-06-12 02:01:11,445   Epoch = 30 iter 8079 step
2022-06-12 02:01:11,445   Num examples = 1043
2022-06-12 02:01:11,445   Batch size = 32
2022-06-12 02:01:11,446 ***** Eval results *****
2022-06-12 02:01:11,446   att_loss = 0.4654586686604265
2022-06-12 02:01:11,446   global_step = 8079
2022-06-12 02:01:11,446   loss = 1.2540062113084656
2022-06-12 02:01:11,446   rep_loss = 0.7885475391926973
2022-06-12 02:01:11,446 ***** Save model *****
2022-06-12 02:01:17,122 ***** Running evaluation *****
2022-06-12 02:01:17,123   Epoch = 30 iter 8099 step
2022-06-12 02:01:17,123   Num examples = 1043
2022-06-12 02:01:17,123   Batch size = 32
2022-06-12 02:01:17,124 ***** Eval results *****
2022-06-12 02:01:17,124   att_loss = 0.4737027555369259
2022-06-12 02:01:17,124   global_step = 8099
2022-06-12 02:01:17,124   loss = 1.2651345863770902
2022-06-12 02:01:17,124   rep_loss = 0.7914318261521586
2022-06-12 02:01:17,124 ***** Save model *****
2022-06-12 02:01:22,789 ***** Running evaluation *****
2022-06-12 02:01:22,790   Epoch = 30 iter 8119 step
2022-06-12 02:01:22,790   Num examples = 1043
2022-06-12 02:01:22,790   Batch size = 32
2022-06-12 02:01:22,791 ***** Eval results *****
2022-06-12 02:01:22,792   att_loss = 0.47315800326679824
2022-06-12 02:01:22,792   global_step = 8119
2022-06-12 02:01:22,792   loss = 1.2640679320064159
2022-06-12 02:01:22,792   rep_loss = 0.7909099262788755
2022-06-12 02:01:22,792 ***** Save model *****
2022-06-12 02:01:28,472 ***** Running evaluation *****
2022-06-12 02:01:28,473   Epoch = 30 iter 8139 step
2022-06-12 02:01:28,473   Num examples = 1043
2022-06-12 02:01:28,473   Batch size = 32
2022-06-12 02:01:28,474 ***** Eval results *****
2022-06-12 02:01:28,474   att_loss = 0.4755987104519393
2022-06-12 02:01:28,474   global_step = 8139
2022-06-12 02:01:28,474   loss = 1.266861500666123
2022-06-12 02:01:28,474   rep_loss = 0.7912627855936686
2022-06-12 02:01:28,474 ***** Save model *****
2022-06-12 02:01:34,168 ***** Running evaluation *****
2022-06-12 02:01:34,168   Epoch = 30 iter 8159 step
2022-06-12 02:01:34,169   Num examples = 1043
2022-06-12 02:01:34,169   Batch size = 32
2022-06-12 02:01:34,170 ***** Eval results *****
2022-06-12 02:01:34,170   att_loss = 0.47835926141514873
2022-06-12 02:01:34,170   global_step = 8159
2022-06-12 02:01:34,170   loss = 1.2699881400037931
2022-06-12 02:01:34,170   rep_loss = 0.791628873988286
2022-06-12 02:01:34,170 ***** Save model *****
2022-06-12 02:01:39,838 ***** Running evaluation *****
2022-06-12 02:01:39,838   Epoch = 30 iter 8179 step
2022-06-12 02:01:39,839   Num examples = 1043
2022-06-12 02:01:39,839   Batch size = 32
2022-06-12 02:01:39,840 ***** Eval results *****
2022-06-12 02:01:39,840   att_loss = 0.47874379369634146
2022-06-12 02:01:39,840   global_step = 8179
2022-06-12 02:01:39,840   loss = 1.2699332865032218
2022-06-12 02:01:39,840   rep_loss = 0.791189490338049
2022-06-12 02:01:39,840 ***** Save model *****
2022-06-12 02:01:45,503 ***** Running evaluation *****
2022-06-12 02:01:45,504   Epoch = 30 iter 8199 step
2022-06-12 02:01:45,504   Num examples = 1043
2022-06-12 02:01:45,504   Batch size = 32
2022-06-12 02:01:45,505 ***** Eval results *****
2022-06-12 02:01:45,506   att_loss = 0.4770065585456828
2022-06-12 02:01:45,506   global_step = 8199
2022-06-12 02:01:45,506   loss = 1.2678169459892958
2022-06-12 02:01:45,506   rep_loss = 0.7908103841322439
2022-06-12 02:01:45,506 ***** Save model *****
2022-06-12 02:01:51,170 ***** Running evaluation *****
2022-06-12 02:01:51,171   Epoch = 30 iter 8219 step
2022-06-12 02:01:51,171   Num examples = 1043
2022-06-12 02:01:51,171   Batch size = 32
2022-06-12 02:01:51,172 ***** Eval results *****
2022-06-12 02:01:51,172   att_loss = 0.4780729705351962
2022-06-12 02:01:51,172   global_step = 8219
2022-06-12 02:01:51,172   loss = 1.2693756681880313
2022-06-12 02:01:51,172   rep_loss = 0.7913026952287227
2022-06-12 02:01:51,172 ***** Save model *****
2022-06-12 02:01:56,842 ***** Running evaluation *****
2022-06-12 02:01:56,842   Epoch = 30 iter 8239 step
2022-06-12 02:01:56,842   Num examples = 1043
2022-06-12 02:01:56,842   Batch size = 32
2022-06-12 02:01:56,843 ***** Eval results *****
2022-06-12 02:01:56,843   att_loss = 0.4766888198113337
2022-06-12 02:01:56,843   global_step = 8239
2022-06-12 02:01:56,843   loss = 1.2674172210901584
2022-06-12 02:01:56,844   rep_loss = 0.7907283993267076
2022-06-12 02:01:56,844 ***** Save model *****
2022-06-12 02:02:02,504 ***** Running evaluation *****
2022-06-12 02:02:02,505   Epoch = 30 iter 8259 step
2022-06-12 02:02:02,505   Num examples = 1043
2022-06-12 02:02:02,505   Batch size = 32
2022-06-12 02:02:02,506 ***** Eval results *****
2022-06-12 02:02:02,506   att_loss = 0.47763127232172403
2022-06-12 02:02:02,506   global_step = 8259
2022-06-12 02:02:02,506   loss = 1.2690591649358052
2022-06-12 02:02:02,506   rep_loss = 0.7914278914172008
2022-06-12 02:02:02,507 ***** Save model *****
2022-06-12 02:02:08,170 ***** Running evaluation *****
2022-06-12 02:02:08,171   Epoch = 31 iter 8279 step
2022-06-12 02:02:08,171   Num examples = 1043
2022-06-12 02:02:08,171   Batch size = 32
2022-06-12 02:02:08,172 ***** Eval results *****
2022-06-12 02:02:08,172   att_loss = 0.45418351888656616
2022-06-12 02:02:08,172   global_step = 8279
2022-06-12 02:02:08,172   loss = 1.2399023175239563
2022-06-12 02:02:08,172   rep_loss = 0.7857187688350677
2022-06-12 02:02:08,172 ***** Save model *****
2022-06-12 02:02:09,645 ***** Running evaluation *****
2022-06-12 02:02:09,645   Epoch = 5 iter 16499 step
2022-06-12 02:02:09,645   Num examples = 5463
2022-06-12 02:02:09,645   Batch size = 32
2022-06-12 02:02:09,646 ***** Eval results *****
2022-06-12 02:02:09,647   att_loss = 3.8131692373930517
2022-06-12 02:02:09,647   global_step = 16499
2022-06-12 02:02:09,647   loss = 4.716782883032044
2022-06-12 02:02:09,647   rep_loss = 0.9036136425253171
2022-06-12 02:02:09,647 ***** Save model *****
2022-06-12 02:02:13,862 ***** Running evaluation *****
2022-06-12 02:02:13,863   Epoch = 31 iter 8299 step
2022-06-12 02:02:13,863   Num examples = 1043
2022-06-12 02:02:13,863   Batch size = 32
2022-06-12 02:02:13,864 ***** Eval results *****
2022-06-12 02:02:13,864   att_loss = 0.48351426828991284
2022-06-12 02:02:13,864   global_step = 8299
2022-06-12 02:02:13,864   loss = 1.277707739309831
2022-06-12 02:02:13,864   rep_loss = 0.7941934818571265
2022-06-12 02:02:13,864 ***** Save model *****
2022-06-12 02:02:19,512 ***** Running evaluation *****
2022-06-12 02:02:19,513   Epoch = 31 iter 8319 step
2022-06-12 02:02:19,513   Num examples = 1043
2022-06-12 02:02:19,513   Batch size = 32
2022-06-12 02:02:19,514 ***** Eval results *****
2022-06-12 02:02:19,514   att_loss = 0.47696838066691444
2022-06-12 02:02:19,514   global_step = 8319
2022-06-12 02:02:19,514   loss = 1.2680249639919825
2022-06-12 02:02:19,514   rep_loss = 0.7910565847442264
2022-06-12 02:02:19,514 ***** Save model *****
2022-06-12 02:02:25,198 ***** Running evaluation *****
2022-06-12 02:02:25,199   Epoch = 31 iter 8339 step
2022-06-12 02:02:25,199   Num examples = 1043
2022-06-12 02:02:25,199   Batch size = 32
2022-06-12 02:02:25,201 ***** Eval results *****
2022-06-12 02:02:25,201   att_loss = 0.4790341104230573
2022-06-12 02:02:25,201   global_step = 8339
2022-06-12 02:02:25,201   loss = 1.2706692564872004
2022-06-12 02:02:25,201   rep_loss = 0.7916351508709693
2022-06-12 02:02:25,201 ***** Save model *****
2022-06-12 02:02:30,877 ***** Running evaluation *****
2022-06-12 02:02:30,878   Epoch = 31 iter 8359 step
2022-06-12 02:02:30,878   Num examples = 1043
2022-06-12 02:02:30,878   Batch size = 32
2022-06-12 02:02:30,879 ***** Eval results *****
2022-06-12 02:02:30,879   att_loss = 0.4743265400572521
2022-06-12 02:02:30,879   global_step = 8359
2022-06-12 02:02:30,880   loss = 1.2634033953271262
2022-06-12 02:02:30,880   rep_loss = 0.7890768596311895
2022-06-12 02:02:30,880 ***** Save model *****
2022-06-12 02:02:36,565 ***** Running evaluation *****
2022-06-12 02:02:36,566   Epoch = 31 iter 8379 step
2022-06-12 02:02:36,566   Num examples = 1043
2022-06-12 02:02:36,566   Batch size = 32
2022-06-12 02:02:36,567 ***** Eval results *****
2022-06-12 02:02:36,567   att_loss = 0.47676465002929463
2022-06-12 02:02:36,567   global_step = 8379
2022-06-12 02:02:36,567   loss = 1.2661948285850824
2022-06-12 02:02:36,568   rep_loss = 0.7894301794323266
2022-06-12 02:02:36,568 ***** Save model *****
2022-06-12 02:02:42,209 ***** Running evaluation *****
2022-06-12 02:02:42,210   Epoch = 31 iter 8399 step
2022-06-12 02:02:42,210   Num examples = 1043
2022-06-12 02:02:42,210   Batch size = 32
2022-06-12 02:02:42,211 ***** Eval results *****
2022-06-12 02:02:42,211   att_loss = 0.47671959610258946
2022-06-12 02:02:42,211   global_step = 8399
2022-06-12 02:02:42,211   loss = 1.266069257845644
2022-06-12 02:02:42,211   rep_loss = 0.7893496629644613
2022-06-12 02:02:42,211 ***** Save model *****
2022-06-12 02:02:47,891 ***** Running evaluation *****
2022-06-12 02:02:47,891   Epoch = 31 iter 8419 step
2022-06-12 02:02:47,891   Num examples = 1043
2022-06-12 02:02:47,891   Batch size = 32
2022-06-12 02:02:47,892 ***** Eval results *****
2022-06-12 02:02:47,893   att_loss = 0.47581920384521215
2022-06-12 02:02:47,893   global_step = 8419
2022-06-12 02:02:47,893   loss = 1.2649194405112467
2022-06-12 02:02:47,893   rep_loss = 0.7891002368759101
2022-06-12 02:02:47,893 ***** Save model *****
2022-06-12 02:02:53,624 ***** Running evaluation *****
2022-06-12 02:02:53,624   Epoch = 31 iter 8439 step
2022-06-12 02:02:53,624   Num examples = 1043
2022-06-12 02:02:53,624   Batch size = 32
2022-06-12 02:02:53,625 ***** Eval results *****
2022-06-12 02:02:53,625   att_loss = 0.47559728501019655
2022-06-12 02:02:53,625   global_step = 8439
2022-06-12 02:02:53,626   loss = 1.265223910043269
2022-06-12 02:02:53,626   rep_loss = 0.7896266252170374
2022-06-12 02:02:53,626 ***** Save model *****
2022-06-12 02:02:59,274 ***** Running evaluation *****
2022-06-12 02:02:59,275   Epoch = 31 iter 8459 step
2022-06-12 02:02:59,275   Num examples = 1043
2022-06-12 02:02:59,275   Batch size = 32
2022-06-12 02:02:59,276 ***** Eval results *****
2022-06-12 02:02:59,276   att_loss = 0.47649847576906396
2022-06-12 02:02:59,276   global_step = 8459
2022-06-12 02:02:59,276   loss = 1.266623250075749
2022-06-12 02:02:59,276   rep_loss = 0.790124774306685
2022-06-12 02:02:59,276 ***** Save model *****
2022-06-12 02:03:04,913 ***** Running evaluation *****
2022-06-12 02:03:04,914   Epoch = 31 iter 8479 step
2022-06-12 02:03:04,914   Num examples = 1043
2022-06-12 02:03:04,914   Batch size = 32
2022-06-12 02:03:04,915 ***** Eval results *****
2022-06-12 02:03:04,915   att_loss = 0.4771435167824868
2022-06-12 02:03:04,915   global_step = 8479
2022-06-12 02:03:04,915   loss = 1.2679221588786285
2022-06-12 02:03:04,915   rep_loss = 0.7907786419486055
2022-06-12 02:03:04,916 ***** Save model *****
2022-06-12 02:03:10,571 ***** Running evaluation *****
2022-06-12 02:03:10,572   Epoch = 31 iter 8499 step
2022-06-12 02:03:10,572   Num examples = 1043
2022-06-12 02:03:10,572   Batch size = 32
2022-06-12 02:03:10,573 ***** Eval results *****
2022-06-12 02:03:10,573   att_loss = 0.47732779705846634
2022-06-12 02:03:10,574   global_step = 8499
2022-06-12 02:03:10,574   loss = 1.268293844686972
2022-06-12 02:03:10,574   rep_loss = 0.7909660489709528
2022-06-12 02:03:10,574 ***** Save model *****
2022-06-12 02:03:16,188 ***** Running evaluation *****
2022-06-12 02:03:16,188   Epoch = 31 iter 8519 step
2022-06-12 02:03:16,188   Num examples = 1043
2022-06-12 02:03:16,189   Batch size = 32
2022-06-12 02:03:16,190 ***** Eval results *****
2022-06-12 02:03:16,190   att_loss = 0.4763375946805497
2022-06-12 02:03:16,190   global_step = 8519
2022-06-12 02:03:16,191   loss = 1.2667509258286027
2022-06-12 02:03:16,191   rep_loss = 0.7904133326258541
2022-06-12 02:03:16,191 ***** Save model *****
2022-06-12 02:03:21,844 ***** Running evaluation *****
2022-06-12 02:03:21,844   Epoch = 31 iter 8539 step
2022-06-12 02:03:21,844   Num examples = 1043
2022-06-12 02:03:21,844   Batch size = 32
2022-06-12 02:03:21,845 ***** Eval results *****
2022-06-12 02:03:21,846   att_loss = 0.4766749988757927
2022-06-12 02:03:21,846   global_step = 8539
2022-06-12 02:03:21,846   loss = 1.2669854182323426
2022-06-12 02:03:21,846   rep_loss = 0.7903104217452858
2022-06-12 02:03:21,846 ***** Save model *****
2022-06-12 02:03:27,541 ***** Running evaluation *****
2022-06-12 02:03:27,542   Epoch = 32 iter 8559 step
2022-06-12 02:03:27,542   Num examples = 1043
2022-06-12 02:03:27,542   Batch size = 32
2022-06-12 02:03:27,543 ***** Eval results *****
2022-06-12 02:03:27,543   att_loss = 0.4431032419204712
2022-06-12 02:03:27,543   global_step = 8559
2022-06-12 02:03:27,543   loss = 1.2299561341603598
2022-06-12 02:03:27,543   rep_loss = 0.7868528842926026
2022-06-12 02:03:27,544 ***** Save model *****
2022-06-12 02:03:33,243 ***** Running evaluation *****
2022-06-12 02:03:33,244   Epoch = 32 iter 8579 step
2022-06-12 02:03:33,244   Num examples = 1043
2022-06-12 02:03:33,244   Batch size = 32
2022-06-12 02:03:33,246 ***** Eval results *****
2022-06-12 02:03:33,246   att_loss = 0.457168972492218
2022-06-12 02:03:33,246   global_step = 8579
2022-06-12 02:03:33,246   loss = 1.2413853747504098
2022-06-12 02:03:33,246   rep_loss = 0.7842163988522121
2022-06-12 02:03:33,247 ***** Save model *****
2022-06-12 02:03:38,957 ***** Running evaluation *****
2022-06-12 02:03:38,959   Epoch = 32 iter 8599 step
2022-06-12 02:03:38,959   Num examples = 1043
2022-06-12 02:03:38,959   Batch size = 32
2022-06-12 02:03:38,960 ***** Eval results *****
2022-06-12 02:03:38,960   att_loss = 0.46867790005423804
2022-06-12 02:03:38,960   global_step = 8599
2022-06-12 02:03:38,960   loss = 1.2552513491023671
2022-06-12 02:03:38,960   rep_loss = 0.7865734479644082
2022-06-12 02:03:38,961 ***** Save model *****
2022-06-12 02:03:44,619 ***** Running evaluation *****
2022-06-12 02:03:44,620   Epoch = 32 iter 8619 step
2022-06-12 02:03:44,620   Num examples = 1043
2022-06-12 02:03:44,620   Batch size = 32
2022-06-12 02:03:44,621 ***** Eval results *****
2022-06-12 02:03:44,621   att_loss = 0.47109946807225545
2022-06-12 02:03:44,621   global_step = 8619
2022-06-12 02:03:44,621   loss = 1.2571814235051473
2022-06-12 02:03:44,621   rep_loss = 0.7860819578170777
2022-06-12 02:03:44,621 ***** Save model *****
2022-06-12 02:03:50,307 ***** Running evaluation *****
2022-06-12 02:03:50,308   Epoch = 32 iter 8639 step
2022-06-12 02:03:50,308   Num examples = 1043
2022-06-12 02:03:50,308   Batch size = 32
2022-06-12 02:03:50,309 ***** Eval results *****
2022-06-12 02:03:50,309   att_loss = 0.47843144159567985
2022-06-12 02:03:50,309   global_step = 8639
2022-06-12 02:03:50,309   loss = 1.26745882912686
2022-06-12 02:03:50,309   rep_loss = 0.789027390354558
2022-06-12 02:03:50,309 ***** Save model *****
2022-06-12 02:03:55,960 ***** Running evaluation *****
2022-06-12 02:03:55,961   Epoch = 32 iter 8659 step
2022-06-12 02:03:55,961   Num examples = 1043
2022-06-12 02:03:55,961   Batch size = 32
2022-06-12 02:03:55,962 ***** Eval results *****
2022-06-12 02:03:55,962   att_loss = 0.4778618309808814
2022-06-12 02:03:55,962   global_step = 8659
2022-06-12 02:03:55,963   loss = 1.2665149149687394
2022-06-12 02:03:55,963   rep_loss = 0.788653086061063
2022-06-12 02:03:55,963 ***** Save model *****
2022-06-12 02:04:01,612 ***** Running evaluation *****
2022-06-12 02:04:01,613   Epoch = 32 iter 8679 step
2022-06-12 02:04:01,613   Num examples = 1043
2022-06-12 02:04:01,613   Batch size = 32
2022-06-12 02:04:01,614 ***** Eval results *****
2022-06-12 02:04:01,614   att_loss = 0.47691957155863446
2022-06-12 02:04:01,614   global_step = 8679
2022-06-12 02:04:01,614   loss = 1.2646785268077143
2022-06-12 02:04:01,614   rep_loss = 0.7877589561321118
2022-06-12 02:04:01,615 ***** Save model *****
2022-06-12 02:04:07,269 ***** Running evaluation *****
2022-06-12 02:04:07,270   Epoch = 32 iter 8699 step
2022-06-12 02:04:07,270   Num examples = 1043
2022-06-12 02:04:07,270   Batch size = 32
2022-06-12 02:04:07,271 ***** Eval results *****
2022-06-12 02:04:07,271   att_loss = 0.47799681828868007
2022-06-12 02:04:07,271   global_step = 8699
2022-06-12 02:04:07,271   loss = 1.2664114767505277
2022-06-12 02:04:07,271   rep_loss = 0.7884146598077589
2022-06-12 02:04:07,271 ***** Save model *****
2022-06-12 02:04:12,952 ***** Running evaluation *****
2022-06-12 02:04:12,953   Epoch = 32 iter 8719 step
2022-06-12 02:04:12,953   Num examples = 1043
2022-06-12 02:04:12,953   Batch size = 32
2022-06-12 02:04:12,954 ***** Eval results *****
2022-06-12 02:04:12,954   att_loss = 0.4759516767093113
2022-06-12 02:04:12,954   global_step = 8719
2022-06-12 02:04:12,954   loss = 1.2630385330745153
2022-06-12 02:04:12,954   rep_loss = 0.7870868563652038
2022-06-12 02:04:12,954 ***** Save model *****
2022-06-12 02:04:17,625 ***** Running evaluation *****
2022-06-12 02:04:17,625   Epoch = 5 iter 16999 step
2022-06-12 02:04:17,625   Num examples = 5463
2022-06-12 02:04:17,625   Batch size = 32
2022-06-12 02:04:17,626 ***** Eval results *****
2022-06-12 02:04:17,627   att_loss = 3.772014031650892
2022-06-12 02:04:17,627   global_step = 16999
2022-06-12 02:04:17,627   loss = 4.672702603535697
2022-06-12 02:04:17,627   rep_loss = 0.9006885765854868
2022-06-12 02:04:17,627 ***** Save model *****
2022-06-12 02:04:18,587 ***** Running evaluation *****
2022-06-12 02:04:18,587   Epoch = 32 iter 8739 step
2022-06-12 02:04:18,587   Num examples = 1043
2022-06-12 02:04:18,587   Batch size = 32
2022-06-12 02:04:18,588 ***** Eval results *****
2022-06-12 02:04:18,588   att_loss = 0.4759259512791267
2022-06-12 02:04:18,588   global_step = 8739
2022-06-12 02:04:18,588   loss = 1.2630109664721367
2022-06-12 02:04:18,589   rep_loss = 0.7870850159571721
2022-06-12 02:04:18,589 ***** Save model *****
2022-06-12 02:04:24,255 ***** Running evaluation *****
2022-06-12 02:04:24,256   Epoch = 32 iter 8759 step
2022-06-12 02:04:24,256   Num examples = 1043
2022-06-12 02:04:24,256   Batch size = 32
2022-06-12 02:04:24,258 ***** Eval results *****
2022-06-12 02:04:24,258   att_loss = 0.47615419764851413
2022-06-12 02:04:24,258   global_step = 8759
2022-06-12 02:04:24,258   loss = 1.26369493340337
2022-06-12 02:04:24,258   rep_loss = 0.7875407376954722
2022-06-12 02:04:24,258 ***** Save model *****
2022-06-12 02:04:29,929 ***** Running evaluation *****
2022-06-12 02:04:29,930   Epoch = 32 iter 8779 step
2022-06-12 02:04:29,930   Num examples = 1043
2022-06-12 02:04:29,930   Batch size = 32
2022-06-12 02:04:29,931 ***** Eval results *****
2022-06-12 02:04:29,931   att_loss = 0.474957608288907
2022-06-12 02:04:29,931   global_step = 8779
2022-06-12 02:04:29,931   loss = 1.2621232682086052
2022-06-12 02:04:29,931   rep_loss = 0.7871656615683373
2022-06-12 02:04:29,931 ***** Save model *****
2022-06-12 02:04:35,552 ***** Running evaluation *****
2022-06-12 02:04:35,553   Epoch = 32 iter 8799 step
2022-06-12 02:04:35,553   Num examples = 1043
2022-06-12 02:04:35,553   Batch size = 32
2022-06-12 02:04:35,554 ***** Eval results *****
2022-06-12 02:04:35,554   att_loss = 0.4759060172473683
2022-06-12 02:04:35,554   global_step = 8799
2022-06-12 02:04:35,554   loss = 1.2643603016348446
2022-06-12 02:04:35,554   rep_loss = 0.7884542843874763
2022-06-12 02:04:35,554 ***** Save model *****
2022-06-12 02:04:41,243 ***** Running evaluation *****
2022-06-12 02:04:41,243   Epoch = 33 iter 8819 step
2022-06-12 02:04:41,243   Num examples = 1043
2022-06-12 02:04:41,243   Batch size = 32
2022-06-12 02:04:41,245 ***** Eval results *****
2022-06-12 02:04:41,245   att_loss = 0.4977201782166958
2022-06-12 02:04:41,245   global_step = 8819
2022-06-12 02:04:41,245   loss = 1.2887268215417862
2022-06-12 02:04:41,245   rep_loss = 0.7910066694021225
2022-06-12 02:04:41,245 ***** Save model *****
2022-06-12 02:04:46,901 ***** Running evaluation *****
2022-06-12 02:04:46,901   Epoch = 33 iter 8839 step
2022-06-12 02:04:46,901   Num examples = 1043
2022-06-12 02:04:46,901   Batch size = 32
2022-06-12 02:04:46,902 ***** Eval results *****
2022-06-12 02:04:46,902   att_loss = 0.47912741133144926
2022-06-12 02:04:46,903   global_step = 8839
2022-06-12 02:04:46,903   loss = 1.2658916413784027
2022-06-12 02:04:46,903   rep_loss = 0.7867642364331654
2022-06-12 02:04:46,903 ***** Save model *****
2022-06-12 02:04:52,570 ***** Running evaluation *****
2022-06-12 02:04:52,571   Epoch = 33 iter 8859 step
2022-06-12 02:04:52,571   Num examples = 1043
2022-06-12 02:04:52,571   Batch size = 32
2022-06-12 02:04:52,572 ***** Eval results *****
2022-06-12 02:04:52,572   att_loss = 0.48396339764197666
2022-06-12 02:04:52,572   global_step = 8859
2022-06-12 02:04:52,572   loss = 1.2725562552611034
2022-06-12 02:04:52,572   rep_loss = 0.788592861344417
2022-06-12 02:04:52,572 ***** Save model *****
2022-06-12 02:04:58,237 ***** Running evaluation *****
2022-06-12 02:04:58,238   Epoch = 33 iter 8879 step
2022-06-12 02:04:58,238   Num examples = 1043
2022-06-12 02:04:58,238   Batch size = 32
2022-06-12 02:04:58,239 ***** Eval results *****
2022-06-12 02:04:58,240   att_loss = 0.48019710840547786
2022-06-12 02:04:58,240   global_step = 8879
2022-06-12 02:04:58,240   loss = 1.2662958555361803
2022-06-12 02:04:58,240   rep_loss = 0.7860987519516665
2022-06-12 02:04:58,240 ***** Save model *****
2022-06-12 02:05:03,892 ***** Running evaluation *****
2022-06-12 02:05:03,893   Epoch = 33 iter 8899 step
2022-06-12 02:05:03,893   Num examples = 1043
2022-06-12 02:05:03,893   Batch size = 32
2022-06-12 02:05:03,894 ***** Eval results *****
2022-06-12 02:05:03,894   att_loss = 0.48152996091680095
2022-06-12 02:05:03,894   global_step = 8899
2022-06-12 02:05:03,895   loss = 1.268932899290865
2022-06-12 02:05:03,895   rep_loss = 0.7874029420993545
2022-06-12 02:05:03,895 ***** Save model *****
2022-06-12 02:05:09,552 ***** Running evaluation *****
2022-06-12 02:05:09,553   Epoch = 33 iter 8919 step
2022-06-12 02:05:09,553   Num examples = 1043
2022-06-12 02:05:09,553   Batch size = 32
2022-06-12 02:05:09,554 ***** Eval results *****
2022-06-12 02:05:09,554   att_loss = 0.47906309697363114
2022-06-12 02:05:09,554   global_step = 8919
2022-06-12 02:05:09,554   loss = 1.266352896337156
2022-06-12 02:05:09,554   rep_loss = 0.787289802674894
2022-06-12 02:05:09,554 ***** Save model *****
2022-06-12 02:05:15,260 ***** Running evaluation *****
2022-06-12 02:05:15,261   Epoch = 33 iter 8939 step
2022-06-12 02:05:15,261   Num examples = 1043
2022-06-12 02:05:15,261   Batch size = 32
2022-06-12 02:05:15,262 ***** Eval results *****
2022-06-12 02:05:15,263   att_loss = 0.4771934493910521
2022-06-12 02:05:15,263   global_step = 8939
2022-06-12 02:05:15,263   loss = 1.2643723906949162
2022-06-12 02:05:15,263   rep_loss = 0.7871789466589689
2022-06-12 02:05:15,263 ***** Save model *****
2022-06-12 02:05:20,944 ***** Running evaluation *****
2022-06-12 02:05:20,945   Epoch = 33 iter 8959 step
2022-06-12 02:05:20,945   Num examples = 1043
2022-06-12 02:05:20,945   Batch size = 32
2022-06-12 02:05:20,946 ***** Eval results *****
2022-06-12 02:05:20,946   att_loss = 0.4748288266159393
2022-06-12 02:05:20,946   global_step = 8959
2022-06-12 02:05:20,946   loss = 1.261220012967651
2022-06-12 02:05:20,946   rep_loss = 0.7863911913858878
2022-06-12 02:05:20,946 ***** Save model *****
2022-06-12 02:05:26,695 ***** Running evaluation *****
2022-06-12 02:05:26,696   Epoch = 33 iter 8979 step
2022-06-12 02:05:26,696   Num examples = 1043
2022-06-12 02:05:26,696   Batch size = 32
2022-06-12 02:05:26,698 ***** Eval results *****
2022-06-12 02:05:26,698   att_loss = 0.47637011554269565
2022-06-12 02:05:26,698   global_step = 8979
2022-06-12 02:05:26,698   loss = 1.263421661797024
2022-06-12 02:05:26,698   rep_loss = 0.787051552108356
2022-06-12 02:05:26,698 ***** Save model *****
2022-06-12 02:05:32,402 ***** Running evaluation *****
2022-06-12 02:05:32,403   Epoch = 33 iter 8999 step
2022-06-12 02:05:32,403   Num examples = 1043
2022-06-12 02:05:32,403   Batch size = 32
2022-06-12 02:05:32,404 ***** Eval results *****
2022-06-12 02:05:32,404   att_loss = 0.4760184397405766
2022-06-12 02:05:32,404   global_step = 8999
2022-06-12 02:05:32,404   loss = 1.2626028675982293
2022-06-12 02:05:32,404   rep_loss = 0.7865844324548193
2022-06-12 02:05:32,404 ***** Save model *****
2022-06-12 02:05:38,052 ***** Running evaluation *****
2022-06-12 02:05:38,053   Epoch = 33 iter 9019 step
2022-06-12 02:05:38,053   Num examples = 1043
2022-06-12 02:05:38,053   Batch size = 32
2022-06-12 02:05:38,055 ***** Eval results *****
2022-06-12 02:05:38,055   att_loss = 0.4751789693075877
2022-06-12 02:05:38,055   global_step = 9019
2022-06-12 02:05:38,055   loss = 1.2616589344464815
2022-06-12 02:05:38,055   rep_loss = 0.7864799697238666
2022-06-12 02:05:38,055 ***** Save model *****
2022-06-12 02:05:43,775 ***** Running evaluation *****
2022-06-12 02:05:43,776   Epoch = 33 iter 9039 step
2022-06-12 02:05:43,776   Num examples = 1043
2022-06-12 02:05:43,776   Batch size = 32
2022-06-12 02:05:43,777 ***** Eval results *****
2022-06-12 02:05:43,777   att_loss = 0.4740400782279801
2022-06-12 02:05:43,777   global_step = 9039
2022-06-12 02:05:43,777   loss = 1.2603398493507452
2022-06-12 02:05:43,777   rep_loss = 0.7862997758283949
2022-06-12 02:05:43,777 ***** Save model *****
2022-06-12 02:05:49,441 ***** Running evaluation *****
2022-06-12 02:05:49,442   Epoch = 33 iter 9059 step
2022-06-12 02:05:49,442   Num examples = 1043
2022-06-12 02:05:49,442   Batch size = 32
2022-06-12 02:05:49,443 ***** Eval results *****
2022-06-12 02:05:49,443   att_loss = 0.4725597306124626
2022-06-12 02:05:49,443   global_step = 9059
2022-06-12 02:05:49,443   loss = 1.257695121149863
2022-06-12 02:05:49,443   rep_loss = 0.7851353939021787
2022-06-12 02:05:49,443 ***** Save model *****
2022-06-12 02:05:55,202 ***** Running evaluation *****
2022-06-12 02:05:55,203   Epoch = 34 iter 9079 step
2022-06-12 02:05:55,203   Num examples = 1043
2022-06-12 02:05:55,204   Batch size = 32
2022-06-12 02:05:55,205 ***** Eval results *****
2022-06-12 02:05:55,206   att_loss = 0.4325675368309021
2022-06-12 02:05:55,206   global_step = 9079
2022-06-12 02:05:55,206   loss = 1.1910228729248047
2022-06-12 02:05:55,206   rep_loss = 0.7584553360939026
2022-06-12 02:05:55,206 ***** Save model *****
2022-06-12 02:06:00,870 ***** Running evaluation *****
2022-06-12 02:06:00,871   Epoch = 34 iter 9099 step
2022-06-12 02:06:00,871   Num examples = 1043
2022-06-12 02:06:00,871   Batch size = 32
2022-06-12 02:06:00,872 ***** Eval results *****
2022-06-12 02:06:00,872   att_loss = 0.4768878576301393
2022-06-12 02:06:00,872   global_step = 9099
2022-06-12 02:06:00,872   loss = 1.259191524414789
2022-06-12 02:06:00,873   rep_loss = 0.7823036738804409
2022-06-12 02:06:00,873 ***** Save model *****
2022-06-12 02:06:06,525 ***** Running evaluation *****
2022-06-12 02:06:06,525   Epoch = 34 iter 9119 step
2022-06-12 02:06:06,525   Num examples = 1043
2022-06-12 02:06:06,525   Batch size = 32
2022-06-12 02:06:06,526 ***** Eval results *****
2022-06-12 02:06:06,527   att_loss = 0.47599965988135917
2022-06-12 02:06:06,527   global_step = 9119
2022-06-12 02:06:06,527   loss = 1.2587095266435204
2022-06-12 02:06:06,527   rep_loss = 0.7827098820267654
2022-06-12 02:06:06,527 ***** Save model *****
2022-06-12 02:06:12,169 ***** Running evaluation *****
2022-06-12 02:06:12,170   Epoch = 34 iter 9139 step
2022-06-12 02:06:12,170   Num examples = 1043
2022-06-12 02:06:12,170   Batch size = 32
2022-06-12 02:06:12,171 ***** Eval results *****
2022-06-12 02:06:12,171   att_loss = 0.4791876815381597
2022-06-12 02:06:12,171   global_step = 9139
2022-06-12 02:06:12,171   loss = 1.2637515419819316
2022-06-12 02:06:12,171   rep_loss = 0.7845638697264624
2022-06-12 02:06:12,171 ***** Save model *****
2022-06-12 02:06:17,816 ***** Running evaluation *****
2022-06-12 02:06:17,817   Epoch = 34 iter 9159 step
2022-06-12 02:06:17,817   Num examples = 1043
2022-06-12 02:06:17,817   Batch size = 32
2022-06-12 02:06:17,818 ***** Eval results *****
2022-06-12 02:06:17,818   att_loss = 0.47749780394412855
2022-06-12 02:06:17,818   global_step = 9159
2022-06-12 02:06:17,818   loss = 1.261851359296728
2022-06-12 02:06:17,818   rep_loss = 0.7843535638149873
2022-06-12 02:06:17,819 ***** Save model *****
2022-06-12 02:06:23,480 ***** Running evaluation *****
2022-06-12 02:06:23,480   Epoch = 34 iter 9179 step
2022-06-12 02:06:23,480   Num examples = 1043
2022-06-12 02:06:23,480   Batch size = 32
2022-06-12 02:06:23,481 ***** Eval results *****
2022-06-12 02:06:23,481   att_loss = 0.47347532788125596
2022-06-12 02:06:23,481   global_step = 9179
2022-06-12 02:06:23,482   loss = 1.2567872458165235
2022-06-12 02:06:23,482   rep_loss = 0.783311923541645
2022-06-12 02:06:23,482 ***** Save model *****
2022-06-12 02:06:25,772 ***** Running evaluation *****
2022-06-12 02:06:25,772   Epoch = 5 iter 17499 step
2022-06-12 02:06:25,772   Num examples = 5463
2022-06-12 02:06:25,772   Batch size = 32
2022-06-12 02:06:25,774 ***** Eval results *****
2022-06-12 02:06:25,774   att_loss = 3.775896361054987
2022-06-12 02:06:25,774   global_step = 17499
2022-06-12 02:06:25,774   loss = 4.676228584225636
2022-06-12 02:06:25,774   rep_loss = 0.9003322260089652
2022-06-12 02:06:25,774 ***** Save model *****
2022-06-12 02:06:29,136 ***** Running evaluation *****
2022-06-12 02:06:29,137   Epoch = 34 iter 9199 step
2022-06-12 02:06:29,137   Num examples = 1043
2022-06-12 02:06:29,137   Batch size = 32
2022-06-12 02:06:29,138 ***** Eval results *****
2022-06-12 02:06:29,138   att_loss = 0.470511445329209
2022-06-12 02:06:29,138   global_step = 9199
2022-06-12 02:06:29,138   loss = 1.251465011234126
2022-06-12 02:06:29,138   rep_loss = 0.7809535703383201
2022-06-12 02:06:29,139 ***** Save model *****
2022-06-12 02:06:34,806 ***** Running evaluation *****
2022-06-12 02:06:34,806   Epoch = 34 iter 9219 step
2022-06-12 02:06:34,806   Num examples = 1043
2022-06-12 02:06:34,806   Batch size = 32
2022-06-12 02:06:34,807 ***** Eval results *****
2022-06-12 02:06:34,807   att_loss = 0.47069540535304566
2022-06-12 02:06:34,807   global_step = 9219
2022-06-12 02:06:34,807   loss = 1.2525210448190676
2022-06-12 02:06:34,807   rep_loss = 0.7818256434819377
2022-06-12 02:06:34,808 ***** Save model *****
2022-06-12 02:06:40,472 ***** Running evaluation *****
2022-06-12 02:06:40,473   Epoch = 34 iter 9239 step
2022-06-12 02:06:40,473   Num examples = 1043
2022-06-12 02:06:40,473   Batch size = 32
2022-06-12 02:06:40,474 ***** Eval results *****
2022-06-12 02:06:40,474   att_loss = 0.47242849583951585
2022-06-12 02:06:40,474   global_step = 9239
2022-06-12 02:06:40,474   loss = 1.2547901158007035
2022-06-12 02:06:40,474   rep_loss = 0.7823616214420485
2022-06-12 02:06:40,474 ***** Save model *****
2022-06-12 02:06:46,067 ***** Running evaluation *****
2022-06-12 02:06:46,067   Epoch = 34 iter 9259 step
2022-06-12 02:06:46,067   Num examples = 1043
2022-06-12 02:06:46,068   Batch size = 32
2022-06-12 02:06:46,069 ***** Eval results *****
2022-06-12 02:06:46,069   att_loss = 0.4707080344798157
2022-06-12 02:06:46,069   global_step = 9259
2022-06-12 02:06:46,069   loss = 1.2528060358532227
2022-06-12 02:06:46,069   rep_loss = 0.7820980005501383
2022-06-12 02:06:46,069 ***** Save model *****
2022-06-12 02:06:51,813 ***** Running evaluation *****
2022-06-12 02:06:51,814   Epoch = 34 iter 9279 step
2022-06-12 02:06:51,814   Num examples = 1043
2022-06-12 02:06:51,814   Batch size = 32
2022-06-12 02:06:51,815 ***** Eval results *****
2022-06-12 02:06:51,815   att_loss = 0.4710665623940046
2022-06-12 02:06:51,815   global_step = 9279
2022-06-12 02:06:51,815   loss = 1.2531194894468014
2022-06-12 02:06:51,815   rep_loss = 0.7820529255700942
2022-06-12 02:06:51,815 ***** Save model *****
2022-06-12 02:06:57,489 ***** Running evaluation *****
2022-06-12 02:06:57,490   Epoch = 34 iter 9299 step
2022-06-12 02:06:57,490   Num examples = 1043
2022-06-12 02:06:57,490   Batch size = 32
2022-06-12 02:06:57,491 ***** Eval results *****
2022-06-12 02:06:57,491   att_loss = 0.4718349751304178
2022-06-12 02:06:57,491   global_step = 9299
2022-06-12 02:06:57,491   loss = 1.2542866096237666
2022-06-12 02:06:57,491   rep_loss = 0.7824516331448275
2022-06-12 02:06:57,491 ***** Save model *****
2022-06-12 02:07:03,184 ***** Running evaluation *****
2022-06-12 02:07:03,185   Epoch = 34 iter 9319 step
2022-06-12 02:07:03,185   Num examples = 1043
2022-06-12 02:07:03,185   Batch size = 32
2022-06-12 02:07:03,186 ***** Eval results *****
2022-06-12 02:07:03,186   att_loss = 0.47114150133370364
2022-06-12 02:07:03,186   global_step = 9319
2022-06-12 02:07:03,186   loss = 1.2535404872102855
2022-06-12 02:07:03,186   rep_loss = 0.7823989843926489
2022-06-12 02:07:03,186 ***** Save model *****
2022-06-12 02:07:08,830 ***** Running evaluation *****
2022-06-12 02:07:08,831   Epoch = 34 iter 9339 step
2022-06-12 02:07:08,831   Num examples = 1043
2022-06-12 02:07:08,831   Batch size = 32
2022-06-12 02:07:08,832 ***** Eval results *****
2022-06-12 02:07:08,832   att_loss = 0.47171164952018707
2022-06-12 02:07:08,832   global_step = 9339
2022-06-12 02:07:08,832   loss = 1.2545551902032903
2022-06-12 02:07:08,832   rep_loss = 0.7828435395412519
2022-06-12 02:07:08,832 ***** Save model *****
2022-06-12 02:07:14,501 ***** Running evaluation *****
2022-06-12 02:07:14,502   Epoch = 35 iter 9359 step
2022-06-12 02:07:14,502   Num examples = 1043
2022-06-12 02:07:14,502   Batch size = 32
2022-06-12 02:07:14,503 ***** Eval results *****
2022-06-12 02:07:14,503   att_loss = 0.47228881078107016
2022-06-12 02:07:14,503   global_step = 9359
2022-06-12 02:07:14,503   loss = 1.2514385666166032
2022-06-12 02:07:14,503   rep_loss = 0.7791497579642704
2022-06-12 02:07:14,504 ***** Save model *****
2022-06-12 02:07:20,162 ***** Running evaluation *****
2022-06-12 02:07:20,163   Epoch = 35 iter 9379 step
2022-06-12 02:07:20,163   Num examples = 1043
2022-06-12 02:07:20,163   Batch size = 32
2022-06-12 02:07:20,164 ***** Eval results *****
2022-06-12 02:07:20,164   att_loss = 0.4742002250517116
2022-06-12 02:07:20,164   global_step = 9379
2022-06-12 02:07:20,164   loss = 1.2545764376135433
2022-06-12 02:07:20,164   rep_loss = 0.7803762134383706
2022-06-12 02:07:20,164 ***** Save model *****
2022-06-12 02:07:25,823 ***** Running evaluation *****
2022-06-12 02:07:25,823   Epoch = 35 iter 9399 step
2022-06-12 02:07:25,823   Num examples = 1043
2022-06-12 02:07:25,823   Batch size = 32
2022-06-12 02:07:25,824 ***** Eval results *****
2022-06-12 02:07:25,824   att_loss = 0.47459592421849567
2022-06-12 02:07:25,824   global_step = 9399
2022-06-12 02:07:25,824   loss = 1.2563361481383994
2022-06-12 02:07:25,825   rep_loss = 0.7817402294388524
2022-06-12 02:07:25,825 ***** Save model *****
2022-06-12 02:07:31,460 ***** Running evaluation *****
2022-06-12 02:07:31,460   Epoch = 35 iter 9419 step
2022-06-12 02:07:31,460   Num examples = 1043
2022-06-12 02:07:31,460   Batch size = 32
2022-06-12 02:07:31,462 ***** Eval results *****
2022-06-12 02:07:31,462   att_loss = 0.4726004379021155
2022-06-12 02:07:31,462   global_step = 9419
2022-06-12 02:07:31,462   loss = 1.2533927398758966
2022-06-12 02:07:31,462   rep_loss = 0.7807923039874515
2022-06-12 02:07:31,462 ***** Save model *****
2022-06-12 02:07:37,096 ***** Running evaluation *****
2022-06-12 02:07:37,097   Epoch = 35 iter 9439 step
2022-06-12 02:07:37,097   Num examples = 1043
2022-06-12 02:07:37,097   Batch size = 32
2022-06-12 02:07:37,098 ***** Eval results *****
2022-06-12 02:07:37,098   att_loss = 0.47289741768481885
2022-06-12 02:07:37,098   global_step = 9439
2022-06-12 02:07:37,098   loss = 1.2539027510805334
2022-06-12 02:07:37,098   rep_loss = 0.7810053349809444
2022-06-12 02:07:37,098 ***** Save model *****
2022-06-12 02:07:42,762 ***** Running evaluation *****
2022-06-12 02:07:42,762   Epoch = 35 iter 9459 step
2022-06-12 02:07:42,762   Num examples = 1043
2022-06-12 02:07:42,762   Batch size = 32
2022-06-12 02:07:42,763 ***** Eval results *****
2022-06-12 02:07:42,764   att_loss = 0.470843542301864
2022-06-12 02:07:42,764   global_step = 9459
2022-06-12 02:07:42,764   loss = 1.251184414353287
2022-06-12 02:07:42,764   rep_loss = 0.7803408744042379
2022-06-12 02:07:42,764 ***** Save model *****
2022-06-12 02:07:48,458 ***** Running evaluation *****
2022-06-12 02:07:48,459   Epoch = 35 iter 9479 step
2022-06-12 02:07:48,459   Num examples = 1043
2022-06-12 02:07:48,459   Batch size = 32
2022-06-12 02:07:48,460 ***** Eval results *****
2022-06-12 02:07:48,460   att_loss = 0.47061475399714797
2022-06-12 02:07:48,460   global_step = 9479
2022-06-12 02:07:48,460   loss = 1.2498499359657516
2022-06-12 02:07:48,460   rep_loss = 0.7792351837478467
2022-06-12 02:07:48,461 ***** Save model *****
2022-06-12 02:07:54,125 ***** Running evaluation *****
2022-06-12 02:07:54,125   Epoch = 35 iter 9499 step
2022-06-12 02:07:54,126   Num examples = 1043
2022-06-12 02:07:54,126   Batch size = 32
2022-06-12 02:07:54,127 ***** Eval results *****
2022-06-12 02:07:54,127   att_loss = 0.47001244514793544
2022-06-12 02:07:54,127   global_step = 9499
2022-06-12 02:07:54,127   loss = 1.2493425839907164
2022-06-12 02:07:54,127   rep_loss = 0.7793301401974319
2022-06-12 02:07:54,127 ***** Save model *****
2022-06-12 02:07:59,785 ***** Running evaluation *****
2022-06-12 02:07:59,785   Epoch = 35 iter 9519 step
2022-06-12 02:07:59,785   Num examples = 1043
2022-06-12 02:07:59,785   Batch size = 32
2022-06-12 02:07:59,786 ***** Eval results *****
2022-06-12 02:07:59,786   att_loss = 0.4698675002517371
2022-06-12 02:07:59,786   global_step = 9519
2022-06-12 02:07:59,786   loss = 1.249618269931311
2022-06-12 02:07:59,787   rep_loss = 0.7797507701934069
2022-06-12 02:07:59,787 ***** Save model *****
2022-06-12 02:08:05,428 ***** Running evaluation *****
2022-06-12 02:08:05,428   Epoch = 35 iter 9539 step
2022-06-12 02:08:05,428   Num examples = 1043
2022-06-12 02:08:05,428   Batch size = 32
2022-06-12 02:08:05,429 ***** Eval results *****
2022-06-12 02:08:05,429   att_loss = 0.4692957459651318
2022-06-12 02:08:05,429   global_step = 9539
2022-06-12 02:08:05,430   loss = 1.2486492389256192
2022-06-12 02:08:05,430   rep_loss = 0.7793534929604874
2022-06-12 02:08:05,430 ***** Save model *****
2022-06-12 02:08:11,135 ***** Running evaluation *****
2022-06-12 02:08:11,135   Epoch = 35 iter 9559 step
2022-06-12 02:08:11,135   Num examples = 1043
2022-06-12 02:08:11,135   Batch size = 32
2022-06-12 02:08:11,136 ***** Eval results *****
2022-06-12 02:08:11,137   att_loss = 0.4671346919837399
2022-06-12 02:08:11,137   global_step = 9559
2022-06-12 02:08:11,137   loss = 1.2460937834231653
2022-06-12 02:08:11,137   rep_loss = 0.7789590915786886
2022-06-12 02:08:11,137 ***** Save model *****
2022-06-12 02:08:16,767 ***** Running evaluation *****
2022-06-12 02:08:16,768   Epoch = 35 iter 9579 step
2022-06-12 02:08:16,768   Num examples = 1043
2022-06-12 02:08:16,768   Batch size = 32
2022-06-12 02:08:16,769 ***** Eval results *****
2022-06-12 02:08:16,769   att_loss = 0.4696220588735026
2022-06-12 02:08:16,769   global_step = 9579
2022-06-12 02:08:16,769   loss = 1.2495342630606432
2022-06-12 02:08:16,770   rep_loss = 0.7799122048239423
2022-06-12 02:08:16,770 ***** Save model *****
2022-06-12 02:08:22,458 ***** Running evaluation *****
2022-06-12 02:08:22,459   Epoch = 35 iter 9599 step
2022-06-12 02:08:22,459   Num examples = 1043
2022-06-12 02:08:22,459   Batch size = 32
2022-06-12 02:08:22,461 ***** Eval results *****
2022-06-12 02:08:22,461   att_loss = 0.4696681376282624
2022-06-12 02:08:22,461   global_step = 9599
2022-06-12 02:08:22,461   loss = 1.2496622242326811
2022-06-12 02:08:22,461   rep_loss = 0.7799940871910787
2022-06-12 02:08:22,461 ***** Save model *****
2022-06-12 02:08:28,152 ***** Running evaluation *****
2022-06-12 02:08:28,152   Epoch = 36 iter 9619 step
2022-06-12 02:08:28,153   Num examples = 1043
2022-06-12 02:08:28,153   Batch size = 32
2022-06-12 02:08:28,154 ***** Eval results *****
2022-06-12 02:08:28,154   att_loss = 0.46510657242366243
2022-06-12 02:08:28,154   global_step = 9619
2022-06-12 02:08:28,154   loss = 1.2341037818363734
2022-06-12 02:08:28,154   rep_loss = 0.7689971923828125
2022-06-12 02:08:28,154 ***** Save model *****
2022-06-12 02:08:33,628 ***** Running evaluation *****
2022-06-12 02:08:33,629   Epoch = 5 iter 17999 step
2022-06-12 02:08:33,629   Num examples = 5463
2022-06-12 02:08:33,629   Batch size = 32
2022-06-12 02:08:33,630 ***** Eval results *****
2022-06-12 02:08:33,630   att_loss = 3.7721817038196392
2022-06-12 02:08:33,630   global_step = 17999
2022-06-12 02:08:33,630   loss = 4.6719902265028095
2022-06-12 02:08:33,630   rep_loss = 0.899808528446655
2022-06-12 02:08:33,631 ***** Save model *****
2022-06-12 02:08:33,807 ***** Running evaluation *****
2022-06-12 02:08:33,808   Epoch = 36 iter 9639 step
2022-06-12 02:08:33,808   Num examples = 1043
2022-06-12 02:08:33,808   Batch size = 32
2022-06-12 02:08:33,809 ***** Eval results *****
2022-06-12 02:08:33,809   att_loss = 0.4713122844696045
2022-06-12 02:08:33,809   global_step = 9639
2022-06-12 02:08:33,809   loss = 1.2412450313568115
2022-06-12 02:08:33,809   rep_loss = 0.7699327424720481
2022-06-12 02:08:33,809 ***** Save model *****
2022-06-12 02:08:39,474 ***** Running evaluation *****
2022-06-12 02:08:39,474   Epoch = 36 iter 9659 step
2022-06-12 02:08:39,474   Num examples = 1043
2022-06-12 02:08:39,474   Batch size = 32
2022-06-12 02:08:39,475 ***** Eval results *****
2022-06-12 02:08:39,475   att_loss = 0.4726296130647051
2022-06-12 02:08:39,475   global_step = 9659
2022-06-12 02:08:39,476   loss = 1.2469886769639684
2022-06-12 02:08:39,476   rep_loss = 0.7743590638992635
2022-06-12 02:08:39,476 ***** Save model *****
2022-06-12 02:08:45,147 ***** Running evaluation *****
2022-06-12 02:08:45,147   Epoch = 36 iter 9679 step
2022-06-12 02:08:45,147   Num examples = 1043
2022-06-12 02:08:45,147   Batch size = 32
2022-06-12 02:08:45,149 ***** Eval results *****
2022-06-12 02:08:45,149   att_loss = 0.47371362661247823
2022-06-12 02:08:45,149   global_step = 9679
2022-06-12 02:08:45,149   loss = 1.2483742076959183
2022-06-12 02:08:45,149   rep_loss = 0.7746605864211694
2022-06-12 02:08:45,149 ***** Save model *****
2022-06-12 02:08:50,834 ***** Running evaluation *****
2022-06-12 02:08:50,835   Epoch = 36 iter 9699 step
2022-06-12 02:08:50,835   Num examples = 1043
2022-06-12 02:08:50,835   Batch size = 32
2022-06-12 02:08:50,836 ***** Eval results *****
2022-06-12 02:08:50,837   att_loss = 0.47409847275964145
2022-06-12 02:08:50,837   global_step = 9699
2022-06-12 02:08:50,837   loss = 1.2500143667747234
2022-06-12 02:08:50,837   rep_loss = 0.7759158967555254
2022-06-12 02:08:50,838 ***** Save model *****
2022-06-12 02:08:56,487 ***** Running evaluation *****
2022-06-12 02:08:56,488   Epoch = 36 iter 9719 step
2022-06-12 02:08:56,488   Num examples = 1043
2022-06-12 02:08:56,488   Batch size = 32
2022-06-12 02:08:56,490 ***** Eval results *****
2022-06-12 02:08:56,490   att_loss = 0.471368962637732
2022-06-12 02:08:56,490   global_step = 9719
2022-06-12 02:08:56,490   loss = 1.2475986614405552
2022-06-12 02:08:56,490   rep_loss = 0.7762296985242968
2022-06-12 02:08:56,490 ***** Save model *****
2022-06-12 02:09:02,159 ***** Running evaluation *****
2022-06-12 02:09:02,160   Epoch = 36 iter 9739 step
2022-06-12 02:09:02,160   Num examples = 1043
2022-06-12 02:09:02,160   Batch size = 32
2022-06-12 02:09:02,161 ***** Eval results *****
2022-06-12 02:09:02,162   att_loss = 0.4709710185452709
2022-06-12 02:09:02,162   global_step = 9739
2022-06-12 02:09:02,162   loss = 1.2467150218843475
2022-06-12 02:09:02,162   rep_loss = 0.7757440031044126
2022-06-12 02:09:02,162 ***** Save model *****
2022-06-12 02:09:07,798 ***** Running evaluation *****
2022-06-12 02:09:07,798   Epoch = 36 iter 9759 step
2022-06-12 02:09:07,798   Num examples = 1043
2022-06-12 02:09:07,798   Batch size = 32
2022-06-12 02:09:07,799 ***** Eval results *****
2022-06-12 02:09:07,799   att_loss = 0.4674524855451519
2022-06-12 02:09:07,800   global_step = 9759
2022-06-12 02:09:07,800   loss = 1.2420255124163466
2022-06-12 02:09:07,800   rep_loss = 0.7745730248438257
2022-06-12 02:09:07,800 ***** Save model *****
2022-06-12 02:09:13,491 ***** Running evaluation *****
2022-06-12 02:09:13,491   Epoch = 36 iter 9779 step
2022-06-12 02:09:13,492   Num examples = 1043
2022-06-12 02:09:13,492   Batch size = 32
2022-06-12 02:09:13,493 ***** Eval results *****
2022-06-12 02:09:13,493   att_loss = 0.46784507800005154
2022-06-12 02:09:13,493   global_step = 9779
2022-06-12 02:09:13,493   loss = 1.242762706950753
2022-06-12 02:09:13,493   rep_loss = 0.774917625024647
2022-06-12 02:09:13,493 ***** Save model *****
2022-06-12 02:09:19,143 ***** Running evaluation *****
2022-06-12 02:09:19,144   Epoch = 36 iter 9799 step
2022-06-12 02:09:19,144   Num examples = 1043
2022-06-12 02:09:19,144   Batch size = 32
2022-06-12 02:09:19,145 ***** Eval results *****
2022-06-12 02:09:19,145   att_loss = 0.4688836489131744
2022-06-12 02:09:19,145   global_step = 9799
2022-06-12 02:09:19,145   loss = 1.2445811881101068
2022-06-12 02:09:19,145   rep_loss = 0.7756975369657425
2022-06-12 02:09:19,146 ***** Save model *****
2022-06-12 02:09:24,758 ***** Running evaluation *****
2022-06-12 02:09:24,759   Epoch = 36 iter 9819 step
2022-06-12 02:09:24,759   Num examples = 1043
2022-06-12 02:09:24,759   Batch size = 32
2022-06-12 02:09:24,760 ***** Eval results *****
2022-06-12 02:09:24,760   att_loss = 0.46822125324304553
2022-06-12 02:09:24,760   global_step = 9819
2022-06-12 02:09:24,760   loss = 1.244141047127581
2022-06-12 02:09:24,760   rep_loss = 0.7759197915809742
2022-06-12 02:09:24,760 ***** Save model *****
2022-06-12 02:09:30,413 ***** Running evaluation *****
2022-06-12 02:09:30,413   Epoch = 36 iter 9839 step
2022-06-12 02:09:30,413   Num examples = 1043
2022-06-12 02:09:30,413   Batch size = 32
2022-06-12 02:09:30,414 ***** Eval results *****
2022-06-12 02:09:30,415   att_loss = 0.46832195607050925
2022-06-12 02:09:30,415   global_step = 9839
2022-06-12 02:09:30,415   loss = 1.244366470937687
2022-06-12 02:09:30,415   rep_loss = 0.7760445121101346
2022-06-12 02:09:30,415 ***** Save model *****
2022-06-12 02:09:36,081 ***** Running evaluation *****
2022-06-12 02:09:36,082   Epoch = 36 iter 9859 step
2022-06-12 02:09:36,082   Num examples = 1043
2022-06-12 02:09:36,082   Batch size = 32
2022-06-12 02:09:36,083 ***** Eval results *****
2022-06-12 02:09:36,083   att_loss = 0.4671549859799837
2022-06-12 02:09:36,083   global_step = 9859
2022-06-12 02:09:36,083   loss = 1.242826325690698
2022-06-12 02:09:36,083   rep_loss = 0.7756713372975709
2022-06-12 02:09:36,083 ***** Save model *****
2022-06-12 02:09:41,706 ***** Running evaluation *****
2022-06-12 02:09:41,706   Epoch = 36 iter 9879 step
2022-06-12 02:09:41,706   Num examples = 1043
2022-06-12 02:09:41,706   Batch size = 32
2022-06-12 02:09:41,707 ***** Eval results *****
2022-06-12 02:09:41,707   att_loss = 0.46645009361402817
2022-06-12 02:09:41,707   global_step = 9879
2022-06-12 02:09:41,707   loss = 1.2418998984361855
2022-06-12 02:09:41,708   rep_loss = 0.7754498025897737
2022-06-12 02:09:41,708 ***** Save model *****
2022-06-12 02:09:47,387 ***** Running evaluation *****
2022-06-12 02:09:47,387   Epoch = 37 iter 9899 step
2022-06-12 02:09:47,387   Num examples = 1043
2022-06-12 02:09:47,387   Batch size = 32
2022-06-12 02:09:47,388 ***** Eval results *****
2022-06-12 02:09:47,388   att_loss = 0.4696795970201492
2022-06-12 02:09:47,388   global_step = 9899
2022-06-12 02:09:47,388   loss = 1.2391567885875703
2022-06-12 02:09:47,388   rep_loss = 0.769477191567421
2022-06-12 02:09:47,388 ***** Save model *****
2022-06-12 02:09:53,069 ***** Running evaluation *****
2022-06-12 02:09:53,070   Epoch = 37 iter 9919 step
2022-06-12 02:09:53,070   Num examples = 1043
2022-06-12 02:09:53,070   Batch size = 32
2022-06-12 02:09:53,071 ***** Eval results *****
2022-06-12 02:09:53,071   att_loss = 0.4600697971880436
2022-06-12 02:09:53,071   global_step = 9919
2022-06-12 02:09:53,071   loss = 1.2293474137783051
2022-06-12 02:09:53,071   rep_loss = 0.7692776128649712
2022-06-12 02:09:53,072 ***** Save model *****
2022-06-12 02:09:58,728 ***** Running evaluation *****
2022-06-12 02:09:58,729   Epoch = 37 iter 9939 step
2022-06-12 02:09:58,729   Num examples = 1043
2022-06-12 02:09:58,729   Batch size = 32
2022-06-12 02:09:58,730 ***** Eval results *****
2022-06-12 02:09:58,730   att_loss = 0.4540024926265081
2022-06-12 02:09:58,730   global_step = 9939
2022-06-12 02:09:58,730   loss = 1.2196612238883973
2022-06-12 02:09:58,730   rep_loss = 0.7656587292750676
2022-06-12 02:09:58,730 ***** Save model *****
2022-06-12 02:10:04,395 ***** Running evaluation *****
2022-06-12 02:10:04,395   Epoch = 37 iter 9959 step
2022-06-12 02:10:04,395   Num examples = 1043
2022-06-12 02:10:04,395   Batch size = 32
2022-06-12 02:10:04,396 ***** Eval results *****
2022-06-12 02:10:04,396   att_loss = 0.4538270838558674
2022-06-12 02:10:04,396   global_step = 9959
2022-06-12 02:10:04,396   loss = 1.2209605261683465
2022-06-12 02:10:04,397   rep_loss = 0.7671334438025952
2022-06-12 02:10:04,397 ***** Save model *****
2022-06-12 02:10:10,068 ***** Running evaluation *****
2022-06-12 02:10:10,069   Epoch = 37 iter 9979 step
2022-06-12 02:10:10,069   Num examples = 1043
2022-06-12 02:10:10,069   Batch size = 32
2022-06-12 02:10:10,070 ***** Eval results *****
2022-06-12 02:10:10,071   att_loss = 0.45974985480308533
2022-06-12 02:10:10,071   global_step = 9979
2022-06-12 02:10:10,071   loss = 1.229875614643097
2022-06-12 02:10:10,071   rep_loss = 0.7701257610321045
2022-06-12 02:10:10,071 ***** Save model *****
2022-06-12 02:10:15,734 ***** Running evaluation *****
2022-06-12 02:10:15,735   Epoch = 37 iter 9999 step
2022-06-12 02:10:15,735   Num examples = 1043
2022-06-12 02:10:15,735   Batch size = 32
2022-06-12 02:10:15,736 ***** Eval results *****
2022-06-12 02:10:15,736   att_loss = 0.4597189264992873
2022-06-12 02:10:15,736   global_step = 9999
2022-06-12 02:10:15,736   loss = 1.2304227689901988
2022-06-12 02:10:15,736   rep_loss = 0.7707038482030233
2022-06-12 02:10:15,736 ***** Save model *****
2022-06-12 02:10:21,351 ***** Running evaluation *****
2022-06-12 02:10:21,351   Epoch = 37 iter 10019 step
2022-06-12 02:10:21,351   Num examples = 1043
2022-06-12 02:10:21,351   Batch size = 32
2022-06-12 02:10:21,352 ***** Eval results *****
2022-06-12 02:10:21,352   att_loss = 0.4604010939598083
2022-06-12 02:10:21,352   global_step = 10019
2022-06-12 02:10:21,353   loss = 1.2307707216058459
2022-06-12 02:10:21,353   rep_loss = 0.7703696331807546
2022-06-12 02:10:21,353 ***** Save model *****
2022-06-12 02:10:27,006 ***** Running evaluation *****
2022-06-12 02:10:27,007   Epoch = 37 iter 10039 step
2022-06-12 02:10:27,007   Num examples = 1043
2022-06-12 02:10:27,007   Batch size = 32
2022-06-12 02:10:27,008 ***** Eval results *****
2022-06-12 02:10:27,008   att_loss = 0.45997090768069027
2022-06-12 02:10:27,008   global_step = 10039
2022-06-12 02:10:27,009   loss = 1.230861335247755
2022-06-12 02:10:27,009   rep_loss = 0.7708904318511486
2022-06-12 02:10:27,009 ***** Save model *****
2022-06-12 02:10:32,654 ***** Running evaluation *****
2022-06-12 02:10:32,655   Epoch = 37 iter 10059 step
2022-06-12 02:10:32,655   Num examples = 1043
2022-06-12 02:10:32,655   Batch size = 32
2022-06-12 02:10:32,656 ***** Eval results *****
2022-06-12 02:10:32,656   att_loss = 0.46088861160808137
2022-06-12 02:10:32,656   global_step = 10059
2022-06-12 02:10:32,656   loss = 1.2320318950547113
2022-06-12 02:10:32,656   rep_loss = 0.7711432880825466
2022-06-12 02:10:32,656 ***** Save model *****
2022-06-12 02:10:38,342 ***** Running evaluation *****
2022-06-12 02:10:38,343   Epoch = 37 iter 10079 step
2022-06-12 02:10:38,343   Num examples = 1043
2022-06-12 02:10:38,343   Batch size = 32
2022-06-12 02:10:38,344 ***** Eval results *****
2022-06-12 02:10:38,345   att_loss = 0.46275945514440536
2022-06-12 02:10:38,345   global_step = 10079
2022-06-12 02:10:38,345   loss = 1.2350862210988998
2022-06-12 02:10:38,345   rep_loss = 0.772326770722866
2022-06-12 02:10:38,345 ***** Save model *****
2022-06-12 02:10:41,507 ***** Running evaluation *****
2022-06-12 02:10:41,508   Epoch = 5 iter 18499 step
2022-06-12 02:10:41,508   Num examples = 5463
2022-06-12 02:10:41,508   Batch size = 32
2022-06-12 02:10:41,509 ***** Eval results *****
2022-06-12 02:10:41,510   att_loss = 3.7650559808790067
2022-06-12 02:10:41,510   global_step = 18499
2022-06-12 02:10:41,510   loss = 4.66417244906278
2022-06-12 02:10:41,510   rep_loss = 0.899116472094106
2022-06-12 02:10:41,510 ***** Save model *****
2022-06-12 02:10:44,002 ***** Running evaluation *****
2022-06-12 02:10:44,002   Epoch = 37 iter 10099 step
2022-06-12 02:10:44,002   Num examples = 1043
2022-06-12 02:10:44,002   Batch size = 32
2022-06-12 02:10:44,003 ***** Eval results *****
2022-06-12 02:10:44,003   att_loss = 0.4639206741343845
2022-06-12 02:10:44,004   global_step = 10099
2022-06-12 02:10:44,004   loss = 1.2367019214413382
2022-06-12 02:10:44,004   rep_loss = 0.7727812515063719
2022-06-12 02:10:44,004 ***** Save model *****
2022-06-12 02:10:49,671 ***** Running evaluation *****
2022-06-12 02:10:49,672   Epoch = 37 iter 10119 step
2022-06-12 02:10:49,672   Num examples = 1043
2022-06-12 02:10:49,672   Batch size = 32
2022-06-12 02:10:49,673 ***** Eval results *****
2022-06-12 02:10:49,673   att_loss = 0.4646787049869696
2022-06-12 02:10:49,673   global_step = 10119
2022-06-12 02:10:49,673   loss = 1.2378103787700334
2022-06-12 02:10:49,673   rep_loss = 0.7731316777567069
2022-06-12 02:10:49,673 ***** Save model *****
2022-06-12 02:10:55,317 ***** Running evaluation *****
2022-06-12 02:10:55,317   Epoch = 37 iter 10139 step
2022-06-12 02:10:55,317   Num examples = 1043
2022-06-12 02:10:55,317   Batch size = 32
2022-06-12 02:10:55,318 ***** Eval results *****
2022-06-12 02:10:55,318   att_loss = 0.4644947674411994
2022-06-12 02:10:55,318   global_step = 10139
2022-06-12 02:10:55,318   loss = 1.2377554636735182
2022-06-12 02:10:55,319   rep_loss = 0.7732607002441699
2022-06-12 02:10:55,319 ***** Save model *****
2022-06-12 02:11:00,968 ***** Running evaluation *****
2022-06-12 02:11:00,968   Epoch = 38 iter 10159 step
2022-06-12 02:11:00,968   Num examples = 1043
2022-06-12 02:11:00,968   Batch size = 32
2022-06-12 02:11:00,971 ***** Eval results *****
2022-06-12 02:11:00,971   att_loss = 0.45402512642053455
2022-06-12 02:11:00,972   global_step = 10159
2022-06-12 02:11:00,972   loss = 1.220214687860929
2022-06-12 02:11:00,972   rep_loss = 0.7661895614403945
2022-06-12 02:11:00,972 ***** Save model *****
2022-06-12 02:11:06,622 ***** Running evaluation *****
2022-06-12 02:11:06,622   Epoch = 38 iter 10179 step
2022-06-12 02:11:06,622   Num examples = 1043
2022-06-12 02:11:06,622   Batch size = 32
2022-06-12 02:11:06,623 ***** Eval results *****
2022-06-12 02:11:06,624   att_loss = 0.46609760143540124
2022-06-12 02:11:06,624   global_step = 10179
2022-06-12 02:11:06,624   loss = 1.2370136940118037
2022-06-12 02:11:06,624   rep_loss = 0.7709160862546979
2022-06-12 02:11:06,624 ***** Save model *****
2022-06-12 02:11:12,335 ***** Running evaluation *****
2022-06-12 02:11:12,336   Epoch = 38 iter 10199 step
2022-06-12 02:11:12,336   Num examples = 1043
2022-06-12 02:11:12,336   Batch size = 32
2022-06-12 02:11:12,337 ***** Eval results *****
2022-06-12 02:11:12,337   att_loss = 0.46092671212160363
2022-06-12 02:11:12,337   global_step = 10199
2022-06-12 02:11:12,337   loss = 1.2301816985292255
2022-06-12 02:11:12,337   rep_loss = 0.769254983596082
2022-06-12 02:11:12,337 ***** Save model *****
2022-06-12 02:11:17,962 ***** Running evaluation *****
2022-06-12 02:11:17,962   Epoch = 38 iter 10219 step
2022-06-12 02:11:17,962   Num examples = 1043
2022-06-12 02:11:17,962   Batch size = 32
2022-06-12 02:11:17,964 ***** Eval results *****
2022-06-12 02:11:17,964   att_loss = 0.4607844838540848
2022-06-12 02:11:17,964   global_step = 10219
2022-06-12 02:11:17,964   loss = 1.2299733194586349
2022-06-12 02:11:17,964   rep_loss = 0.7691888343797971
2022-06-12 02:11:17,964 ***** Save model *****
2022-06-12 02:11:23,606 ***** Running evaluation *****
2022-06-12 02:11:23,606   Epoch = 38 iter 10239 step
2022-06-12 02:11:23,606   Num examples = 1043
2022-06-12 02:11:23,606   Batch size = 32
2022-06-12 02:11:23,607 ***** Eval results *****
2022-06-12 02:11:23,607   att_loss = 0.45691323056015914
2022-06-12 02:11:23,607   global_step = 10239
2022-06-12 02:11:23,607   loss = 1.225259829592961
2022-06-12 02:11:23,607   rep_loss = 0.7683465980714367
2022-06-12 02:11:23,608 ***** Save model *****
2022-06-12 02:11:29,237 ***** Running evaluation *****
2022-06-12 02:11:29,238   Epoch = 38 iter 10259 step
2022-06-12 02:11:29,238   Num examples = 1043
2022-06-12 02:11:29,238   Batch size = 32
2022-06-12 02:11:29,239 ***** Eval results *****
2022-06-12 02:11:29,239   att_loss = 0.45682170908008
2022-06-12 02:11:29,239   global_step = 10259
2022-06-12 02:11:29,239   loss = 1.2262430929504664
2022-06-12 02:11:29,239   rep_loss = 0.7694213838703865
2022-06-12 02:11:29,239 ***** Save model *****
2022-06-12 02:11:34,853 ***** Running evaluation *****
2022-06-12 02:11:34,853   Epoch = 38 iter 10279 step
2022-06-12 02:11:34,854   Num examples = 1043
2022-06-12 02:11:34,854   Batch size = 32
2022-06-12 02:11:34,855 ***** Eval results *****
2022-06-12 02:11:34,855   att_loss = 0.4586502976883623
2022-06-12 02:11:34,855   global_step = 10279
2022-06-12 02:11:34,855   loss = 1.2285789036213006
2022-06-12 02:11:34,855   rep_loss = 0.7699286041403175
2022-06-12 02:11:34,855 ***** Save model *****
2022-06-12 02:11:40,482 ***** Running evaluation *****
2022-06-12 02:11:40,482   Epoch = 38 iter 10299 step
2022-06-12 02:11:40,482   Num examples = 1043
2022-06-12 02:11:40,482   Batch size = 32
2022-06-12 02:11:40,483 ***** Eval results *****
2022-06-12 02:11:40,483   att_loss = 0.45727915896309745
2022-06-12 02:11:40,483   global_step = 10299
2022-06-12 02:11:40,483   loss = 1.2269241731930403
2022-06-12 02:11:40,483   rep_loss = 0.7696450122820786
2022-06-12 02:11:40,483 ***** Save model *****
2022-06-12 02:11:46,155 ***** Running evaluation *****
2022-06-12 02:11:46,155   Epoch = 38 iter 10319 step
2022-06-12 02:11:46,156   Num examples = 1043
2022-06-12 02:11:46,156   Batch size = 32
2022-06-12 02:11:46,157 ***** Eval results *****
2022-06-12 02:11:46,157   att_loss = 0.4602092554458993
2022-06-12 02:11:46,157   global_step = 10319
2022-06-12 02:11:46,157   loss = 1.2305679569354635
2022-06-12 02:11:46,157   rep_loss = 0.7703586982164768
2022-06-12 02:11:46,157 ***** Save model *****
2022-06-12 02:11:51,792 ***** Running evaluation *****
2022-06-12 02:11:51,792   Epoch = 38 iter 10339 step
2022-06-12 02:11:51,792   Num examples = 1043
2022-06-12 02:11:51,792   Batch size = 32
2022-06-12 02:11:51,793 ***** Eval results *****
2022-06-12 02:11:51,793   att_loss = 0.4596413749178456
2022-06-12 02:11:51,793   global_step = 10339
2022-06-12 02:11:51,793   loss = 1.2298863094705375
2022-06-12 02:11:51,793   rep_loss = 0.770244930383455
2022-06-12 02:11:51,794 ***** Save model *****
2022-06-12 02:11:57,425 ***** Running evaluation *****
2022-06-12 02:11:57,426   Epoch = 38 iter 10359 step
2022-06-12 02:11:57,426   Num examples = 1043
2022-06-12 02:11:57,426   Batch size = 32
2022-06-12 02:11:57,427 ***** Eval results *****
2022-06-12 02:11:57,427   att_loss = 0.4601026678309194
2022-06-12 02:11:57,427   global_step = 10359
2022-06-12 02:11:57,427   loss = 1.2304943883922739
2022-06-12 02:11:57,427   rep_loss = 0.7703917160840101
2022-06-12 02:11:57,427 ***** Save model *****
2022-06-12 02:12:03,080 ***** Running evaluation *****
2022-06-12 02:12:03,080   Epoch = 38 iter 10379 step
2022-06-12 02:12:03,080   Num examples = 1043
2022-06-12 02:12:03,080   Batch size = 32
2022-06-12 02:12:03,081 ***** Eval results *****
2022-06-12 02:12:03,081   att_loss = 0.46062190325475044
2022-06-12 02:12:03,082   global_step = 10379
2022-06-12 02:12:03,082   loss = 1.2309270739043732
2022-06-12 02:12:03,082   rep_loss = 0.7703051664286928
2022-06-12 02:12:03,082 ***** Save model *****
2022-06-12 02:12:08,755 ***** Running evaluation *****
2022-06-12 02:12:08,756   Epoch = 38 iter 10399 step
2022-06-12 02:12:08,756   Num examples = 1043
2022-06-12 02:12:08,756   Batch size = 32
2022-06-12 02:12:08,757 ***** Eval results *****
2022-06-12 02:12:08,757   att_loss = 0.4608988447387228
2022-06-12 02:12:08,757   global_step = 10399
2022-06-12 02:12:08,757   loss = 1.2310518982853342
2022-06-12 02:12:08,757   rep_loss = 0.7701530494237606
2022-06-12 02:12:08,757 ***** Save model *****
2022-06-12 02:12:14,414 ***** Running evaluation *****
2022-06-12 02:12:14,414   Epoch = 39 iter 10419 step
2022-06-12 02:12:14,415   Num examples = 1043
2022-06-12 02:12:14,415   Batch size = 32
2022-06-12 02:12:14,416 ***** Eval results *****
2022-06-12 02:12:14,416   att_loss = 0.4481871376434962
2022-06-12 02:12:14,416   global_step = 10419
2022-06-12 02:12:14,416   loss = 1.2359108328819275
2022-06-12 02:12:14,416   rep_loss = 0.7877236803372701
2022-06-12 02:12:14,416 ***** Save model *****
2022-06-12 02:12:20,041 ***** Running evaluation *****
2022-06-12 02:12:20,041   Epoch = 39 iter 10439 step
2022-06-12 02:12:20,041   Num examples = 1043
2022-06-12 02:12:20,041   Batch size = 32
2022-06-12 02:12:20,042 ***** Eval results *****
2022-06-12 02:12:20,043   att_loss = 0.457649555343848
2022-06-12 02:12:20,043   global_step = 10439
2022-06-12 02:12:20,043   loss = 1.2316363178766692
2022-06-12 02:12:20,043   rep_loss = 0.7739867613865778
2022-06-12 02:12:20,043 ***** Save model *****
2022-06-12 02:12:25,713 ***** Running evaluation *****
2022-06-12 02:12:25,713   Epoch = 39 iter 10459 step
2022-06-12 02:12:25,713   Num examples = 1043
2022-06-12 02:12:25,713   Batch size = 32
2022-06-12 02:12:25,715 ***** Eval results *****
2022-06-12 02:12:25,715   att_loss = 0.4540946295727854
2022-06-12 02:12:25,715   global_step = 10459
2022-06-12 02:12:25,715   loss = 1.2240081014840498
2022-06-12 02:12:25,715   rep_loss = 0.7699134738548942
2022-06-12 02:12:25,715 ***** Save model *****
2022-06-12 02:12:31,364 ***** Running evaluation *****
2022-06-12 02:12:31,365   Epoch = 39 iter 10479 step
2022-06-12 02:12:31,365   Num examples = 1043
2022-06-12 02:12:31,365   Batch size = 32
2022-06-12 02:12:31,366 ***** Eval results *****
2022-06-12 02:12:31,366   att_loss = 0.4580715980493661
2022-06-12 02:12:31,366   global_step = 10479
2022-06-12 02:12:31,366   loss = 1.227874781146194
2022-06-12 02:12:31,366   rep_loss = 0.7698031781297742
2022-06-12 02:12:31,367 ***** Save model *****
2022-06-12 02:12:37,024 ***** Running evaluation *****
2022-06-12 02:12:37,025   Epoch = 39 iter 10499 step
2022-06-12 02:12:37,025   Num examples = 1043
2022-06-12 02:12:37,025   Batch size = 32
2022-06-12 02:12:37,026 ***** Eval results *****
2022-06-12 02:12:37,027   att_loss = 0.45946857194567836
2022-06-12 02:12:37,027   global_step = 10499
2022-06-12 02:12:37,027   loss = 1.2290865266045858
2022-06-12 02:12:37,027   rep_loss = 0.769617953272753
2022-06-12 02:12:37,027 ***** Save model *****
2022-06-12 02:12:42,660 ***** Running evaluation *****
2022-06-12 02:12:42,661   Epoch = 39 iter 10519 step
2022-06-12 02:12:42,661   Num examples = 1043
2022-06-12 02:12:42,661   Batch size = 32
2022-06-12 02:12:42,662 ***** Eval results *****
2022-06-12 02:12:42,662   att_loss = 0.4593691198893313
2022-06-12 02:12:42,662   global_step = 10519
2022-06-12 02:12:42,662   loss = 1.2286104897283159
2022-06-12 02:12:42,662   rep_loss = 0.7692413673085986
2022-06-12 02:12:42,662 ***** Save model *****
2022-06-12 02:12:48,315 ***** Running evaluation *****
2022-06-12 02:12:48,315   Epoch = 39 iter 10539 step
2022-06-12 02:12:48,315   Num examples = 1043
2022-06-12 02:12:48,315   Batch size = 32
2022-06-12 02:12:48,316 ***** Eval results *****
2022-06-12 02:12:48,317   att_loss = 0.4588622008524244
2022-06-12 02:12:48,317   global_step = 10539
2022-06-12 02:12:48,317   loss = 1.2269836020848108
2022-06-12 02:12:48,317   rep_loss = 0.7681213981575437
2022-06-12 02:12:48,317 ***** Save model *****
2022-06-12 02:12:49,385 ***** Running evaluation *****
2022-06-12 02:12:49,385   Epoch = 5 iter 18999 step
2022-06-12 02:12:49,385   Num examples = 5463
2022-06-12 02:12:49,385   Batch size = 32
2022-06-12 02:12:49,386 ***** Eval results *****
2022-06-12 02:12:49,387   att_loss = 3.7644076662346007
2022-06-12 02:12:49,387   global_step = 18999
2022-06-12 02:12:49,387   loss = 4.663045972155728
2022-06-12 02:12:49,387   rep_loss = 0.8986383099264509
2022-06-12 02:12:49,387 ***** Save model *****
2022-06-12 02:12:53,961 ***** Running evaluation *****
2022-06-12 02:12:53,962   Epoch = 39 iter 10559 step
2022-06-12 02:12:53,962   Num examples = 1043
2022-06-12 02:12:53,962   Batch size = 32
2022-06-12 02:12:53,963 ***** Eval results *****
2022-06-12 02:12:53,963   att_loss = 0.4604133024607619
2022-06-12 02:12:53,963   global_step = 10559
2022-06-12 02:12:53,963   loss = 1.2288355745681345
2022-06-12 02:12:53,963   rep_loss = 0.7684222676166116
2022-06-12 02:12:53,964 ***** Save model *****
2022-06-12 02:12:59,596 ***** Running evaluation *****
2022-06-12 02:12:59,596   Epoch = 39 iter 10579 step
2022-06-12 02:12:59,596   Num examples = 1043
2022-06-12 02:12:59,596   Batch size = 32
2022-06-12 02:12:59,597 ***** Eval results *****
2022-06-12 02:12:59,597   att_loss = 0.46126285877572487
2022-06-12 02:12:59,597   global_step = 10579
2022-06-12 02:12:59,597   loss = 1.230048435280122
2022-06-12 02:12:59,597   rep_loss = 0.7687855711184353
2022-06-12 02:12:59,597 ***** Save model *****
2022-06-12 02:13:05,207 ***** Running evaluation *****
2022-06-12 02:13:05,208   Epoch = 39 iter 10599 step
2022-06-12 02:13:05,208   Num examples = 1043
2022-06-12 02:13:05,208   Batch size = 32
2022-06-12 02:13:05,209 ***** Eval results *****
2022-06-12 02:13:05,209   att_loss = 0.46067942198245754
2022-06-12 02:13:05,209   global_step = 10599
2022-06-12 02:13:05,209   loss = 1.229763021392207
2022-06-12 02:13:05,209   rep_loss = 0.7690835941222406
2022-06-12 02:13:05,209 ***** Save model *****
2022-06-12 02:13:10,847 ***** Running evaluation *****
2022-06-12 02:13:10,847   Epoch = 39 iter 10619 step
2022-06-12 02:13:10,847   Num examples = 1043
2022-06-12 02:13:10,847   Batch size = 32
2022-06-12 02:13:10,848 ***** Eval results *****
2022-06-12 02:13:10,848   att_loss = 0.4603105226477373
2022-06-12 02:13:10,848   global_step = 10619
2022-06-12 02:13:10,848   loss = 1.2291805049748097
2022-06-12 02:13:10,848   rep_loss = 0.7688699789996286
2022-06-12 02:13:10,848 ***** Save model *****
2022-06-12 02:13:16,485 ***** Running evaluation *****
2022-06-12 02:13:16,486   Epoch = 39 iter 10639 step
2022-06-12 02:13:16,486   Num examples = 1043
2022-06-12 02:13:16,486   Batch size = 32
2022-06-12 02:13:16,487 ***** Eval results *****
2022-06-12 02:13:16,487   att_loss = 0.4598978565329999
2022-06-12 02:13:16,487   global_step = 10639
2022-06-12 02:13:16,487   loss = 1.2286034537627635
2022-06-12 02:13:16,487   rep_loss = 0.7687055935374404
2022-06-12 02:13:16,487 ***** Save model *****
2022-06-12 02:13:22,141 ***** Running evaluation *****
2022-06-12 02:13:22,141   Epoch = 39 iter 10659 step
2022-06-12 02:13:22,141   Num examples = 1043
2022-06-12 02:13:22,141   Batch size = 32
2022-06-12 02:13:22,142 ***** Eval results *****
2022-06-12 02:13:22,142   att_loss = 0.4610430332945614
2022-06-12 02:13:22,142   global_step = 10659
2022-06-12 02:13:22,143   loss = 1.2304256839480827
2022-06-12 02:13:22,143   rep_loss = 0.7693826473825346
2022-06-12 02:13:22,143 ***** Save model *****
2022-06-12 02:13:27,798 ***** Running evaluation *****
2022-06-12 02:13:27,798   Epoch = 39 iter 10679 step
2022-06-12 02:13:27,799   Num examples = 1043
2022-06-12 02:13:27,799   Batch size = 32
2022-06-12 02:13:27,800 ***** Eval results *****
2022-06-12 02:13:27,800   att_loss = 0.4610011781516828
2022-06-12 02:13:27,800   global_step = 10679
2022-06-12 02:13:27,800   loss = 1.2310672639904165
2022-06-12 02:13:27,800   rep_loss = 0.7700660824775696
2022-06-12 02:13:27,800 ***** Save model *****
2022-06-12 02:13:33,463 ***** Running evaluation *****
2022-06-12 02:13:33,463   Epoch = 40 iter 10699 step
2022-06-12 02:13:33,463   Num examples = 1043
2022-06-12 02:13:33,464   Batch size = 32
2022-06-12 02:13:33,464 ***** Eval results *****
2022-06-12 02:13:33,465   att_loss = 0.4494075430066962
2022-06-12 02:13:33,465   global_step = 10699
2022-06-12 02:13:33,465   loss = 1.2104223150956004
2022-06-12 02:13:33,465   rep_loss = 0.7610147815001639
2022-06-12 02:13:33,465 ***** Save model *****
2022-06-12 02:13:39,102 ***** Running evaluation *****
2022-06-12 02:13:39,102   Epoch = 40 iter 10719 step
2022-06-12 02:13:39,102   Num examples = 1043
2022-06-12 02:13:39,102   Batch size = 32
2022-06-12 02:13:39,104 ***** Eval results *****
2022-06-12 02:13:39,104   att_loss = 0.44315041716282183
2022-06-12 02:13:39,104   global_step = 10719
2022-06-12 02:13:39,104   loss = 1.2061972954334357
2022-06-12 02:13:39,104   rep_loss = 0.7630468851480728
2022-06-12 02:13:39,104 ***** Save model *****
2022-06-12 02:13:44,825 ***** Running evaluation *****
2022-06-12 02:13:44,826   Epoch = 40 iter 10739 step
2022-06-12 02:13:44,826   Num examples = 1043
2022-06-12 02:13:44,826   Batch size = 32
2022-06-12 02:13:44,827 ***** Eval results *****
2022-06-12 02:13:44,827   att_loss = 0.44887755280834135
2022-06-12 02:13:44,827   global_step = 10739
2022-06-12 02:13:44,827   loss = 1.2141940916998912
2022-06-12 02:13:44,827   rep_loss = 0.7653165459632874
2022-06-12 02:13:44,828 ***** Save model *****
2022-06-12 02:13:50,466 ***** Running evaluation *****
2022-06-12 02:13:50,466   Epoch = 40 iter 10759 step
2022-06-12 02:13:50,466   Num examples = 1043
2022-06-12 02:13:50,466   Batch size = 32
2022-06-12 02:13:50,467 ***** Eval results *****
2022-06-12 02:13:50,467   att_loss = 0.45530025793027273
2022-06-12 02:13:50,468   global_step = 10759
2022-06-12 02:13:50,468   loss = 1.2219848315927047
2022-06-12 02:13:50,468   rep_loss = 0.7666845766803886
2022-06-12 02:13:50,468 ***** Save model *****
2022-06-12 02:13:56,137 ***** Running evaluation *****
2022-06-12 02:13:56,138   Epoch = 40 iter 10779 step
2022-06-12 02:13:56,138   Num examples = 1043
2022-06-12 02:13:56,138   Batch size = 32
2022-06-12 02:13:56,139 ***** Eval results *****
2022-06-12 02:13:56,139   att_loss = 0.4522069207947664
2022-06-12 02:13:56,139   global_step = 10779
2022-06-12 02:13:56,139   loss = 1.2161395573856855
2022-06-12 02:13:56,139   rep_loss = 0.7639326399022882
2022-06-12 02:13:56,139 ***** Save model *****
2022-06-12 02:14:01,759 ***** Running evaluation *****
2022-06-12 02:14:01,759   Epoch = 40 iter 10799 step
2022-06-12 02:14:01,759   Num examples = 1043
2022-06-12 02:14:01,759   Batch size = 32
2022-06-12 02:14:01,760 ***** Eval results *****
2022-06-12 02:14:01,760   att_loss = 0.4525830036952716
2022-06-12 02:14:01,761   global_step = 10799
2022-06-12 02:14:01,761   loss = 1.2164511991148235
2022-06-12 02:14:01,761   rep_loss = 0.7638681986752678
2022-06-12 02:14:01,761 ***** Save model *****
2022-06-12 02:14:07,413 ***** Running evaluation *****
2022-06-12 02:14:07,414   Epoch = 40 iter 10819 step
2022-06-12 02:14:07,414   Num examples = 1043
2022-06-12 02:14:07,414   Batch size = 32
2022-06-12 02:14:07,415 ***** Eval results *****
2022-06-12 02:14:07,415   att_loss = 0.45516248293917816
2022-06-12 02:14:07,415   global_step = 10819
2022-06-12 02:14:07,415   loss = 1.2204448953806926
2022-06-12 02:14:07,415   rep_loss = 0.7652824147999715
2022-06-12 02:14:07,415 ***** Save model *****
2022-06-12 02:14:13,055 ***** Running evaluation *****
2022-06-12 02:14:13,056   Epoch = 40 iter 10839 step
2022-06-12 02:14:13,056   Num examples = 1043
2022-06-12 02:14:13,056   Batch size = 32
2022-06-12 02:14:13,057 ***** Eval results *****
2022-06-12 02:14:13,057   att_loss = 0.4544461944942954
2022-06-12 02:14:13,057   global_step = 10839
2022-06-12 02:14:13,057   loss = 1.2192111900017697
2022-06-12 02:14:13,057   rep_loss = 0.764764998319014
2022-06-12 02:14:13,058 ***** Save model *****
2022-06-12 02:14:18,695 ***** Running evaluation *****
2022-06-12 02:14:18,696   Epoch = 40 iter 10859 step
2022-06-12 02:14:18,696   Num examples = 1043
2022-06-12 02:14:18,696   Batch size = 32
2022-06-12 02:14:18,697 ***** Eval results *****
2022-06-12 02:14:18,697   att_loss = 0.45527492804900227
2022-06-12 02:14:18,697   global_step = 10859
2022-06-12 02:14:18,697   loss = 1.2209541970791098
2022-06-12 02:14:18,697   rep_loss = 0.7656792718604957
2022-06-12 02:14:18,698 ***** Save model *****
2022-06-12 02:14:24,352 ***** Running evaluation *****
2022-06-12 02:14:24,353   Epoch = 40 iter 10879 step
2022-06-12 02:14:24,353   Num examples = 1043
2022-06-12 02:14:24,353   Batch size = 32
2022-06-12 02:14:24,354 ***** Eval results *****
2022-06-12 02:14:24,354   att_loss = 0.4557238507210909
2022-06-12 02:14:24,354   global_step = 10879
2022-06-12 02:14:24,354   loss = 1.2219294567204
2022-06-12 02:14:24,354   rep_loss = 0.7662056079461946
2022-06-12 02:14:24,354 ***** Save model *****
2022-06-12 02:14:30,031 ***** Running evaluation *****
2022-06-12 02:14:30,032   Epoch = 40 iter 10899 step
2022-06-12 02:14:30,032   Num examples = 1043
2022-06-12 02:14:30,032   Batch size = 32
2022-06-12 02:14:30,033 ***** Eval results *****
2022-06-12 02:14:30,033   att_loss = 0.458677708150045
2022-06-12 02:14:30,033   global_step = 10899
2022-06-12 02:14:30,034   loss = 1.2256191342932994
2022-06-12 02:14:30,034   rep_loss = 0.7669414284566766
2022-06-12 02:14:30,034 ***** Save model *****
2022-06-12 02:14:35,688 ***** Running evaluation *****
2022-06-12 02:14:35,689   Epoch = 40 iter 10919 step
2022-06-12 02:14:35,689   Num examples = 1043
2022-06-12 02:14:35,689   Batch size = 32
2022-06-12 02:14:35,690 ***** Eval results *****
2022-06-12 02:14:35,690   att_loss = 0.4581992035131574
2022-06-12 02:14:35,690   global_step = 10919
2022-06-12 02:14:35,690   loss = 1.2245064110935482
2022-06-12 02:14:35,691   rep_loss = 0.7663072095755253
2022-06-12 02:14:35,691 ***** Save model *****
2022-06-12 02:14:41,334 ***** Running evaluation *****
2022-06-12 02:14:41,335   Epoch = 40 iter 10939 step
2022-06-12 02:14:41,335   Num examples = 1043
2022-06-12 02:14:41,335   Batch size = 32
2022-06-12 02:14:41,336 ***** Eval results *****
2022-06-12 02:14:41,336   att_loss = 0.4591903740604872
2022-06-12 02:14:41,336   global_step = 10939
2022-06-12 02:14:41,336   loss = 1.2260515100707419
2022-06-12 02:14:41,336   rep_loss = 0.7668611372759904
2022-06-12 02:14:41,337 ***** Save model *****
2022-06-12 02:14:47,024 ***** Running evaluation *****
2022-06-12 02:14:47,024   Epoch = 41 iter 10959 step
2022-06-12 02:14:47,024   Num examples = 1043
2022-06-12 02:14:47,024   Batch size = 32
2022-06-12 02:14:47,025 ***** Eval results *****
2022-06-12 02:14:47,025   att_loss = 0.45910054941972095
2022-06-12 02:14:47,025   global_step = 10959
2022-06-12 02:14:47,025   loss = 1.2227975527445476
2022-06-12 02:14:47,025   rep_loss = 0.7636970033248266
2022-06-12 02:14:47,026 ***** Save model *****
2022-06-12 02:14:52,669 ***** Running evaluation *****
2022-06-12 02:14:52,669   Epoch = 41 iter 10979 step
2022-06-12 02:14:52,669   Num examples = 1043
2022-06-12 02:14:52,669   Batch size = 32
2022-06-12 02:14:52,670 ***** Eval results *****
2022-06-12 02:14:52,670   att_loss = 0.46174242720007896
2022-06-12 02:14:52,670   global_step = 10979
2022-06-12 02:14:52,670   loss = 1.2255339473485947
2022-06-12 02:14:52,670   rep_loss = 0.763791523873806
2022-06-12 02:14:52,671 ***** Save model *****
2022-06-12 02:14:57,134 ***** Running evaluation *****
2022-06-12 02:14:57,135   Epoch = 5 iter 19499 step
2022-06-12 02:14:57,135   Num examples = 5463
2022-06-12 02:14:57,135   Batch size = 32
2022-06-12 02:14:57,136 ***** Eval results *****
2022-06-12 02:14:57,136   att_loss = 3.75931546364001
2022-06-12 02:14:57,136   global_step = 19499
2022-06-12 02:14:57,136   loss = 4.6570065121933695
2022-06-12 02:14:57,136   rep_loss = 0.8976910521478964
2022-06-12 02:14:57,137 ***** Save model *****
2022-06-12 02:14:58,325 ***** Running evaluation *****
2022-06-12 02:14:58,326   Epoch = 41 iter 10999 step
2022-06-12 02:14:58,326   Num examples = 1043
2022-06-12 02:14:58,326   Batch size = 32
2022-06-12 02:14:58,327 ***** Eval results *****
2022-06-12 02:14:58,327   att_loss = 0.4528185696556018
2022-06-12 02:14:58,327   global_step = 10999
2022-06-12 02:14:58,327   loss = 1.2145499701683338
2022-06-12 02:14:58,327   rep_loss = 0.7617314045245831
2022-06-12 02:14:58,327 ***** Save model *****
2022-06-12 02:15:03,985 ***** Running evaluation *****
2022-06-12 02:15:03,986   Epoch = 41 iter 11019 step
2022-06-12 02:15:03,986   Num examples = 1043
2022-06-12 02:15:03,986   Batch size = 32
2022-06-12 02:15:03,987 ***** Eval results *****
2022-06-12 02:15:03,987   att_loss = 0.45513804546660847
2022-06-12 02:15:03,987   global_step = 11019
2022-06-12 02:15:03,987   loss = 1.2188406785329182
2022-06-12 02:15:03,987   rep_loss = 0.7637026376194425
2022-06-12 02:15:03,987 ***** Save model *****
2022-06-12 02:15:09,644 ***** Running evaluation *****
2022-06-12 02:15:09,645   Epoch = 41 iter 11039 step
2022-06-12 02:15:09,645   Num examples = 1043
2022-06-12 02:15:09,645   Batch size = 32
2022-06-12 02:15:09,646 ***** Eval results *****
2022-06-12 02:15:09,646   att_loss = 0.4558214315253755
2022-06-12 02:15:09,646   global_step = 11039
2022-06-12 02:15:09,646   loss = 1.218832333450732
2022-06-12 02:15:09,646   rep_loss = 0.7630109054886777
2022-06-12 02:15:09,646 ***** Save model *****
2022-06-12 02:15:15,314 ***** Running evaluation *****
2022-06-12 02:15:15,314   Epoch = 41 iter 11059 step
2022-06-12 02:15:15,314   Num examples = 1043
2022-06-12 02:15:15,314   Batch size = 32
2022-06-12 02:15:15,315 ***** Eval results *****
2022-06-12 02:15:15,316   att_loss = 0.4543180157031332
2022-06-12 02:15:15,316   global_step = 11059
2022-06-12 02:15:15,316   loss = 1.2163972865257944
2022-06-12 02:15:15,316   rep_loss = 0.7620792729513985
2022-06-12 02:15:15,316 ***** Save model *****
2022-06-12 02:15:20,975 ***** Running evaluation *****
2022-06-12 02:15:20,976   Epoch = 41 iter 11079 step
2022-06-12 02:15:20,976   Num examples = 1043
2022-06-12 02:15:20,976   Batch size = 32
2022-06-12 02:15:20,977 ***** Eval results *****
2022-06-12 02:15:20,977   att_loss = 0.45406376999436004
2022-06-12 02:15:20,977   global_step = 11079
2022-06-12 02:15:20,977   loss = 1.2166450014620116
2022-06-12 02:15:20,977   rep_loss = 0.7625812314676516
2022-06-12 02:15:20,978 ***** Save model *****
2022-06-12 02:15:26,668 ***** Running evaluation *****
2022-06-12 02:15:26,669   Epoch = 41 iter 11099 step
2022-06-12 02:15:26,669   Num examples = 1043
2022-06-12 02:15:26,669   Batch size = 32
2022-06-12 02:15:26,671 ***** Eval results *****
2022-06-12 02:15:26,671   att_loss = 0.4528894342089954
2022-06-12 02:15:26,671   global_step = 11099
2022-06-12 02:15:26,671   loss = 1.2147245077710402
2022-06-12 02:15:26,671   rep_loss = 0.7618350731699091
2022-06-12 02:15:26,671 ***** Save model *****
2022-06-12 02:15:32,358 ***** Running evaluation *****
2022-06-12 02:15:32,358   Epoch = 41 iter 11119 step
2022-06-12 02:15:32,358   Num examples = 1043
2022-06-12 02:15:32,358   Batch size = 32
2022-06-12 02:15:32,359 ***** Eval results *****
2022-06-12 02:15:32,360   att_loss = 0.4523913770221
2022-06-12 02:15:32,360   global_step = 11119
2022-06-12 02:15:32,360   loss = 1.2138145087763321
2022-06-12 02:15:32,360   rep_loss = 0.7614231303680775
2022-06-12 02:15:32,360 ***** Save model *****
2022-06-12 02:15:37,984 ***** Running evaluation *****
2022-06-12 02:15:37,984   Epoch = 41 iter 11139 step
2022-06-12 02:15:37,984   Num examples = 1043
2022-06-12 02:15:37,984   Batch size = 32
2022-06-12 02:15:37,985 ***** Eval results *****
2022-06-12 02:15:37,985   att_loss = 0.4531438850487272
2022-06-12 02:15:37,986   global_step = 11139
2022-06-12 02:15:37,986   loss = 1.2146920710802078
2022-06-12 02:15:37,986   rep_loss = 0.7615481823061904
2022-06-12 02:15:37,986 ***** Save model *****
2022-06-12 02:15:43,586 ***** Running evaluation *****
2022-06-12 02:15:43,586   Epoch = 41 iter 11159 step
2022-06-12 02:15:43,586   Num examples = 1043
2022-06-12 02:15:43,586   Batch size = 32
2022-06-12 02:15:43,587 ***** Eval results *****
2022-06-12 02:15:43,587   att_loss = 0.4533802431147054
2022-06-12 02:15:43,587   global_step = 11159
2022-06-12 02:15:43,587   loss = 1.214809869257909
2022-06-12 02:15:43,587   rep_loss = 0.7614296219258938
2022-06-12 02:15:43,587 ***** Save model *****
2022-06-12 02:15:49,220 ***** Running evaluation *****
2022-06-12 02:15:49,220   Epoch = 41 iter 11179 step
2022-06-12 02:15:49,220   Num examples = 1043
2022-06-12 02:15:49,220   Batch size = 32
2022-06-12 02:15:49,221 ***** Eval results *****
2022-06-12 02:15:49,222   att_loss = 0.4570978369949193
2022-06-12 02:15:49,222   global_step = 11179
2022-06-12 02:15:49,222   loss = 1.2202484119555046
2022-06-12 02:15:49,222   rep_loss = 0.7631505714922115
2022-06-12 02:15:49,222 ***** Save model *****
2022-06-12 02:15:54,846 ***** Running evaluation *****
2022-06-12 02:15:54,847   Epoch = 41 iter 11199 step
2022-06-12 02:15:54,847   Num examples = 1043
2022-06-12 02:15:54,847   Batch size = 32
2022-06-12 02:15:54,848 ***** Eval results *****
2022-06-12 02:15:54,848   att_loss = 0.45637531566714484
2022-06-12 02:15:54,848   global_step = 11199
2022-06-12 02:15:54,848   loss = 1.2194718027871752
2022-06-12 02:15:54,848   rep_loss = 0.7630964839269244
2022-06-12 02:15:54,848 ***** Save model *****
2022-06-12 02:16:00,536 ***** Running evaluation *****
2022-06-12 02:16:00,536   Epoch = 42 iter 11219 step
2022-06-12 02:16:00,536   Num examples = 1043
2022-06-12 02:16:00,536   Batch size = 32
2022-06-12 02:16:00,538 ***** Eval results *****
2022-06-12 02:16:00,538   att_loss = 0.44410369396209715
2022-06-12 02:16:00,538   global_step = 11219
2022-06-12 02:16:00,538   loss = 1.2031025886535645
2022-06-12 02:16:00,538   rep_loss = 0.7589988827705383
2022-06-12 02:16:00,538 ***** Save model *****
2022-06-12 02:16:06,200 ***** Running evaluation *****
2022-06-12 02:16:06,201   Epoch = 42 iter 11239 step
2022-06-12 02:16:06,201   Num examples = 1043
2022-06-12 02:16:06,201   Batch size = 32
2022-06-12 02:16:06,202 ***** Eval results *****
2022-06-12 02:16:06,202   att_loss = 0.44052168607711795
2022-06-12 02:16:06,203   global_step = 11239
2022-06-12 02:16:06,203   loss = 1.2041407299041749
2022-06-12 02:16:06,203   rep_loss = 0.7636190390586853
2022-06-12 02:16:06,203 ***** Save model *****
2022-06-12 02:16:11,813 ***** Running evaluation *****
2022-06-12 02:16:11,814   Epoch = 42 iter 11259 step
2022-06-12 02:16:11,814   Num examples = 1043
2022-06-12 02:16:11,814   Batch size = 32
2022-06-12 02:16:11,815 ***** Eval results *****
2022-06-12 02:16:11,815   att_loss = 0.4484730886088477
2022-06-12 02:16:11,815   global_step = 11259
2022-06-12 02:16:11,815   loss = 1.2113426764806112
2022-06-12 02:16:11,815   rep_loss = 0.7628695832358466
2022-06-12 02:16:11,815 ***** Save model *****
2022-06-12 02:16:17,439 ***** Running evaluation *****
2022-06-12 02:16:17,440   Epoch = 42 iter 11279 step
2022-06-12 02:16:17,440   Num examples = 1043
2022-06-12 02:16:17,440   Batch size = 32
2022-06-12 02:16:17,441 ***** Eval results *****
2022-06-12 02:16:17,441   att_loss = 0.45110741807864263
2022-06-12 02:16:17,441   global_step = 11279
2022-06-12 02:16:17,441   loss = 1.2134414746211126
2022-06-12 02:16:17,441   rep_loss = 0.762334055166978
2022-06-12 02:16:17,441 ***** Save model *****
2022-06-12 02:16:23,083 ***** Running evaluation *****
2022-06-12 02:16:23,084   Epoch = 42 iter 11299 step
2022-06-12 02:16:23,084   Num examples = 1043
2022-06-12 02:16:23,084   Batch size = 32
2022-06-12 02:16:23,085 ***** Eval results *****
2022-06-12 02:16:23,085   att_loss = 0.45154246302211987
2022-06-12 02:16:23,085   global_step = 11299
2022-06-12 02:16:23,085   loss = 1.21309092465569
2022-06-12 02:16:23,085   rep_loss = 0.7615484595298767
2022-06-12 02:16:23,085 ***** Save model *****
2022-06-12 02:16:28,712 ***** Running evaluation *****
2022-06-12 02:16:28,712   Epoch = 42 iter 11319 step
2022-06-12 02:16:28,713   Num examples = 1043
2022-06-12 02:16:28,713   Batch size = 32
2022-06-12 02:16:28,714 ***** Eval results *****
2022-06-12 02:16:28,714   att_loss = 0.45301169270560854
2022-06-12 02:16:28,714   global_step = 11319
2022-06-12 02:16:28,714   loss = 1.2145392701739357
2022-06-12 02:16:28,714   rep_loss = 0.7615275769006639
2022-06-12 02:16:28,714 ***** Save model *****
2022-06-12 02:16:34,346 ***** Running evaluation *****
2022-06-12 02:16:34,346   Epoch = 42 iter 11339 step
2022-06-12 02:16:34,346   Num examples = 1043
2022-06-12 02:16:34,347   Batch size = 32
2022-06-12 02:16:34,347 ***** Eval results *****
2022-06-12 02:16:34,347   att_loss = 0.451115246295929
2022-06-12 02:16:34,348   global_step = 11339
2022-06-12 02:16:34,348   loss = 1.2122935380935669
2022-06-12 02:16:34,348   rep_loss = 0.7611782903671265
2022-06-12 02:16:34,348 ***** Save model *****
2022-06-12 02:16:39,966 ***** Running evaluation *****
2022-06-12 02:16:39,966   Epoch = 42 iter 11359 step
2022-06-12 02:16:39,966   Num examples = 1043
2022-06-12 02:16:39,966   Batch size = 32
2022-06-12 02:16:39,967 ***** Eval results *****
2022-06-12 02:16:39,967   att_loss = 0.4513382839745489
2022-06-12 02:16:39,967   global_step = 11359
2022-06-12 02:16:39,967   loss = 1.2125550541384467
2022-06-12 02:16:39,967   rep_loss = 0.7612167691362315
2022-06-12 02:16:39,968 ***** Save model *****
2022-06-12 02:16:45,618 ***** Running evaluation *****
2022-06-12 02:16:45,618   Epoch = 42 iter 11379 step
2022-06-12 02:16:45,618   Num examples = 1043
2022-06-12 02:16:45,618   Batch size = 32
2022-06-12 02:16:45,619 ***** Eval results *****
2022-06-12 02:16:45,619   att_loss = 0.4515010196151155
2022-06-12 02:16:45,620   global_step = 11379
2022-06-12 02:16:45,620   loss = 1.212399882258791
2022-06-12 02:16:45,620   rep_loss = 0.7608988624630553
2022-06-12 02:16:45,620 ***** Save model *****
2022-06-12 02:16:51,290 ***** Running evaluation *****
2022-06-12 02:16:51,291   Epoch = 42 iter 11399 step
2022-06-12 02:16:51,291   Num examples = 1043
2022-06-12 02:16:51,291   Batch size = 32
2022-06-12 02:16:51,292 ***** Eval results *****
2022-06-12 02:16:51,292   att_loss = 0.4517616215589884
2022-06-12 02:16:51,292   global_step = 11399
2022-06-12 02:16:51,292   loss = 1.212440954027949
2022-06-12 02:16:51,292   rep_loss = 0.7606793332744289
2022-06-12 02:16:51,293 ***** Save model *****
2022-06-12 02:16:56,961 ***** Running evaluation *****
2022-06-12 02:16:56,962   Epoch = 42 iter 11419 step
2022-06-12 02:16:56,962   Num examples = 1043
2022-06-12 02:16:56,962   Batch size = 32
2022-06-12 02:16:56,963 ***** Eval results *****
2022-06-12 02:16:56,963   att_loss = 0.45311335281627935
2022-06-12 02:16:56,963   global_step = 11419
2022-06-12 02:16:56,963   loss = 1.2139597875315968
2022-06-12 02:16:56,963   rep_loss = 0.7608464354422034
2022-06-12 02:16:56,963 ***** Save model *****
2022-06-12 02:17:02,643 ***** Running evaluation *****
2022-06-12 02:17:02,644   Epoch = 42 iter 11439 step
2022-06-12 02:17:02,644   Num examples = 1043
2022-06-12 02:17:02,644   Batch size = 32
2022-06-12 02:17:02,645 ***** Eval results *****
2022-06-12 02:17:02,645   att_loss = 0.45268901414341395
2022-06-12 02:17:02,645   global_step = 11439
2022-06-12 02:17:02,645   loss = 1.2137199465433757
2022-06-12 02:17:02,645   rep_loss = 0.7610309341218736
2022-06-12 02:17:02,646 ***** Save model *****
2022-06-12 02:17:04,875 ***** Running evaluation *****
2022-06-12 02:17:04,875   Epoch = 6 iter 19999 step
2022-06-12 02:17:04,875   Num examples = 5463
2022-06-12 02:17:04,875   Batch size = 32
2022-06-12 02:17:04,877 ***** Eval results *****
2022-06-12 02:17:04,877   att_loss = 3.6809630539278575
2022-06-12 02:17:04,877   global_step = 19999
2022-06-12 02:17:04,877   loss = 4.569729999161823
2022-06-12 02:17:04,877   rep_loss = 0.8887669485361622
2022-06-12 02:17:04,878 ***** Save model *****
2022-06-12 02:17:08,304 ***** Running evaluation *****
2022-06-12 02:17:08,304   Epoch = 42 iter 11459 step
2022-06-12 02:17:08,305   Num examples = 1043
2022-06-12 02:17:08,305   Batch size = 32
2022-06-12 02:17:08,306 ***** Eval results *****
2022-06-12 02:17:08,306   att_loss = 0.454139767252669
2022-06-12 02:17:08,306   global_step = 11459
2022-06-12 02:17:08,306   loss = 1.215920724674147
2022-06-12 02:17:08,306   rep_loss = 0.7617809597326785
2022-06-12 02:17:08,307 ***** Save model *****
2022-06-12 02:17:13,945 ***** Running evaluation *****
2022-06-12 02:17:13,945   Epoch = 42 iter 11479 step
2022-06-12 02:17:13,945   Num examples = 1043
2022-06-12 02:17:13,945   Batch size = 32
2022-06-12 02:17:13,946 ***** Eval results *****
2022-06-12 02:17:13,946   att_loss = 0.4547881631356365
2022-06-12 02:17:13,946   global_step = 11479
2022-06-12 02:17:13,946   loss = 1.2169055722794442
2022-06-12 02:17:13,946   rep_loss = 0.7621174106058085
2022-06-12 02:17:13,946 ***** Save model *****
2022-06-12 02:17:19,639 ***** Running evaluation *****
2022-06-12 02:17:19,639   Epoch = 43 iter 11499 step
2022-06-12 02:17:19,639   Num examples = 1043
2022-06-12 02:17:19,639   Batch size = 32
2022-06-12 02:17:19,640 ***** Eval results *****
2022-06-12 02:17:19,641   att_loss = 0.4775626344813241
2022-06-12 02:17:19,641   global_step = 11499
2022-06-12 02:17:19,641   loss = 1.2464654511875577
2022-06-12 02:17:19,641   rep_loss = 0.768902815050549
2022-06-12 02:17:19,641 ***** Save model *****
2022-06-12 02:17:25,284 ***** Running evaluation *****
2022-06-12 02:17:25,284   Epoch = 43 iter 11519 step
2022-06-12 02:17:25,284   Num examples = 1043
2022-06-12 02:17:25,284   Batch size = 32
2022-06-12 02:17:25,285 ***** Eval results *****
2022-06-12 02:17:25,285   att_loss = 0.45677124277541514
2022-06-12 02:17:25,285   global_step = 11519
2022-06-12 02:17:25,285   loss = 1.2188447588368465
2022-06-12 02:17:25,285   rep_loss = 0.7620735090029868
2022-06-12 02:17:25,285 ***** Save model *****
2022-06-12 02:17:30,894 ***** Running evaluation *****
2022-06-12 02:17:30,895   Epoch = 43 iter 11539 step
2022-06-12 02:17:30,895   Num examples = 1043
2022-06-12 02:17:30,895   Batch size = 32
2022-06-12 02:17:30,896 ***** Eval results *****
2022-06-12 02:17:30,896   att_loss = 0.46213837841461447
2022-06-12 02:17:30,896   global_step = 11539
2022-06-12 02:17:30,896   loss = 1.2266349854140446
2022-06-12 02:17:30,896   rep_loss = 0.7644966080270964
2022-06-12 02:17:30,896 ***** Save model *****
2022-06-12 02:17:36,539 ***** Running evaluation *****
2022-06-12 02:17:36,539   Epoch = 43 iter 11559 step
2022-06-12 02:17:36,539   Num examples = 1043
2022-06-12 02:17:36,539   Batch size = 32
2022-06-12 02:17:36,540 ***** Eval results *****
2022-06-12 02:17:36,540   att_loss = 0.46220130912768537
2022-06-12 02:17:36,540   global_step = 11559
2022-06-12 02:17:36,540   loss = 1.2269666974361126
2022-06-12 02:17:36,540   rep_loss = 0.7647653871621841
2022-06-12 02:17:36,541 ***** Save model *****
2022-06-12 02:17:42,190 ***** Running evaluation *****
2022-06-12 02:17:42,191   Epoch = 43 iter 11579 step
2022-06-12 02:17:42,191   Num examples = 1043
2022-06-12 02:17:42,191   Batch size = 32
2022-06-12 02:17:42,192 ***** Eval results *****
2022-06-12 02:17:42,192   att_loss = 0.4609401551436405
2022-06-12 02:17:42,192   global_step = 11579
2022-06-12 02:17:42,192   loss = 1.2253475116223704
2022-06-12 02:17:42,192   rep_loss = 0.7644073598238886
2022-06-12 02:17:42,192 ***** Save model *****
2022-06-12 02:17:47,843 ***** Running evaluation *****
2022-06-12 02:17:47,843   Epoch = 43 iter 11599 step
2022-06-12 02:17:47,843   Num examples = 1043
2022-06-12 02:17:47,844   Batch size = 32
2022-06-12 02:17:47,845 ***** Eval results *****
2022-06-12 02:17:47,845   att_loss = 0.46332402097976816
2022-06-12 02:17:47,845   global_step = 11599
2022-06-12 02:17:47,845   loss = 1.2283007017636702
2022-06-12 02:17:47,845   rep_loss = 0.7649766822992745
2022-06-12 02:17:47,845 ***** Save model *****
2022-06-12 02:17:53,510 ***** Running evaluation *****
2022-06-12 02:17:53,511   Epoch = 43 iter 11619 step
2022-06-12 02:17:53,511   Num examples = 1043
2022-06-12 02:17:53,511   Batch size = 32
2022-06-12 02:17:53,512 ***** Eval results *****
2022-06-12 02:17:53,512   att_loss = 0.4641214343516723
2022-06-12 02:17:53,512   global_step = 11619
2022-06-12 02:17:53,512   loss = 1.2291297973066135
2022-06-12 02:17:53,513   rep_loss = 0.7650083640347356
2022-06-12 02:17:53,513 ***** Save model *****
2022-06-12 02:17:59,205 ***** Running evaluation *****
2022-06-12 02:17:59,205   Epoch = 43 iter 11639 step
2022-06-12 02:17:59,205   Num examples = 1043
2022-06-12 02:17:59,205   Batch size = 32
2022-06-12 02:17:59,206 ***** Eval results *****
2022-06-12 02:17:59,207   att_loss = 0.46154167757758613
2022-06-12 02:17:59,207   global_step = 11639
2022-06-12 02:17:59,207   loss = 1.225857367243948
2022-06-12 02:17:59,207   rep_loss = 0.7643156923070739
2022-06-12 02:17:59,207 ***** Save model *****
2022-06-12 02:18:04,842 ***** Running evaluation *****
2022-06-12 02:18:04,843   Epoch = 43 iter 11659 step
2022-06-12 02:18:04,843   Num examples = 1043
2022-06-12 02:18:04,843   Batch size = 32
2022-06-12 02:18:04,844 ***** Eval results *****
2022-06-12 02:18:04,844   att_loss = 0.4603470297676794
2022-06-12 02:18:04,844   global_step = 11659
2022-06-12 02:18:04,844   loss = 1.22318447038029
2022-06-12 02:18:04,844   rep_loss = 0.7628374437937576
2022-06-12 02:18:04,844 ***** Save model *****
2022-06-12 02:18:10,497 ***** Running evaluation *****
2022-06-12 02:18:10,498   Epoch = 43 iter 11679 step
2022-06-12 02:18:10,498   Num examples = 1043
2022-06-12 02:18:10,498   Batch size = 32
2022-06-12 02:18:10,499 ***** Eval results *****
2022-06-12 02:18:10,499   att_loss = 0.45943846982536896
2022-06-12 02:18:10,499   global_step = 11679
2022-06-12 02:18:10,499   loss = 1.2225189166839676
2022-06-12 02:18:10,499   rep_loss = 0.7630804503204847
2022-06-12 02:18:10,499 ***** Save model *****
2022-06-12 02:18:16,132 ***** Running evaluation *****
2022-06-12 02:18:16,133   Epoch = 43 iter 11699 step
2022-06-12 02:18:16,133   Num examples = 1043
2022-06-12 02:18:16,133   Batch size = 32
2022-06-12 02:18:16,134 ***** Eval results *****
2022-06-12 02:18:16,135   att_loss = 0.45903904749712815
2022-06-12 02:18:16,135   global_step = 11699
2022-06-12 02:18:16,135   loss = 1.2220643447079789
2022-06-12 02:18:16,135   rep_loss = 0.763025301038672
2022-06-12 02:18:16,135 ***** Save model *****
2022-06-12 02:18:21,763 ***** Running evaluation *****
2022-06-12 02:18:21,764   Epoch = 43 iter 11719 step
2022-06-12 02:18:21,764   Num examples = 1043
2022-06-12 02:18:21,764   Batch size = 32
2022-06-12 02:18:21,765 ***** Eval results *****
2022-06-12 02:18:21,765   att_loss = 0.4572738604385312
2022-06-12 02:18:21,765   global_step = 11719
2022-06-12 02:18:21,765   loss = 1.2197490124141468
2022-06-12 02:18:21,765   rep_loss = 0.7624751557322109
2022-06-12 02:18:21,765 ***** Save model *****
2022-06-12 02:18:27,417 ***** Running evaluation *****
2022-06-12 02:18:27,418   Epoch = 43 iter 11739 step
2022-06-12 02:18:27,418   Num examples = 1043
2022-06-12 02:18:27,418   Batch size = 32
2022-06-12 02:18:27,419 ***** Eval results *****
2022-06-12 02:18:27,419   att_loss = 0.45680799341016964
2022-06-12 02:18:27,419   global_step = 11739
2022-06-12 02:18:27,419   loss = 1.2187837006509765
2022-06-12 02:18:27,419   rep_loss = 0.7619757102441418
2022-06-12 02:18:27,420 ***** Save model *****
2022-06-12 02:18:33,058 ***** Running evaluation *****
2022-06-12 02:18:33,058   Epoch = 44 iter 11759 step
2022-06-12 02:18:33,058   Num examples = 1043
2022-06-12 02:18:33,058   Batch size = 32
2022-06-12 02:18:33,059 ***** Eval results *****
2022-06-12 02:18:33,059   att_loss = 0.4628070186484944
2022-06-12 02:18:33,059   global_step = 11759
2022-06-12 02:18:33,059   loss = 1.2184174060821533
2022-06-12 02:18:33,059   rep_loss = 0.7556103738871488
2022-06-12 02:18:33,059 ***** Save model *****
2022-06-12 02:18:38,657 ***** Running evaluation *****
2022-06-12 02:18:38,658   Epoch = 44 iter 11779 step
2022-06-12 02:18:38,658   Num examples = 1043
2022-06-12 02:18:38,658   Batch size = 32
2022-06-12 02:18:38,659 ***** Eval results *****
2022-06-12 02:18:38,659   att_loss = 0.45581732546129533
2022-06-12 02:18:38,659   global_step = 11779
2022-06-12 02:18:38,659   loss = 1.2146964111635763
2022-06-12 02:18:38,659   rep_loss = 0.7588790770499937
2022-06-12 02:18:38,659 ***** Save model *****
2022-06-12 02:18:44,314 ***** Running evaluation *****
2022-06-12 02:18:44,315   Epoch = 44 iter 11799 step
2022-06-12 02:18:44,315   Num examples = 1043
2022-06-12 02:18:44,315   Batch size = 32
2022-06-12 02:18:44,316 ***** Eval results *****
2022-06-12 02:18:44,316   att_loss = 0.44944782876500894
2022-06-12 02:18:44,316   global_step = 11799
2022-06-12 02:18:44,316   loss = 1.2085688721899892
2022-06-12 02:18:44,316   rep_loss = 0.7591210381657469
2022-06-12 02:18:44,316 ***** Save model *****
2022-06-12 02:18:49,994 ***** Running evaluation *****
2022-06-12 02:18:49,995   Epoch = 44 iter 11819 step
2022-06-12 02:18:49,995   Num examples = 1043
2022-06-12 02:18:49,995   Batch size = 32
2022-06-12 02:18:49,996 ***** Eval results *****
2022-06-12 02:18:49,996   att_loss = 0.4534336139618511
2022-06-12 02:18:49,996   global_step = 11819
2022-06-12 02:18:49,996   loss = 1.2133543944694627
2022-06-12 02:18:49,996   rep_loss = 0.7599207767298524
2022-06-12 02:18:49,996 ***** Save model *****
2022-06-12 02:18:55,627 ***** Running evaluation *****
2022-06-12 02:18:55,627   Epoch = 44 iter 11839 step
2022-06-12 02:18:55,627   Num examples = 1043
2022-06-12 02:18:55,627   Batch size = 32
2022-06-12 02:18:55,628 ***** Eval results *****
2022-06-12 02:18:55,629   att_loss = 0.4487382003239223
2022-06-12 02:18:55,629   global_step = 11839
2022-06-12 02:18:55,629   loss = 1.2066301199106069
2022-06-12 02:18:55,629   rep_loss = 0.7578919189316886
2022-06-12 02:18:55,629 ***** Save model *****
2022-06-12 02:19:01,296 ***** Running evaluation *****
2022-06-12 02:19:01,296   Epoch = 44 iter 11859 step
2022-06-12 02:19:01,296   Num examples = 1043
2022-06-12 02:19:01,296   Batch size = 32
2022-06-12 02:19:01,297 ***** Eval results *****
2022-06-12 02:19:01,297   att_loss = 0.44838283029762477
2022-06-12 02:19:01,298   global_step = 11859
2022-06-12 02:19:01,298   loss = 1.206216305225819
2022-06-12 02:19:01,298   rep_loss = 0.7578334760021519
2022-06-12 02:19:01,298 ***** Save model *****
2022-06-12 02:19:06,955 ***** Running evaluation *****
2022-06-12 02:19:06,955   Epoch = 44 iter 11879 step
2022-06-12 02:19:06,955   Num examples = 1043
2022-06-12 02:19:06,955   Batch size = 32
2022-06-12 02:19:06,957 ***** Eval results *****
2022-06-12 02:19:06,957   att_loss = 0.4521765497349601
2022-06-12 02:19:06,957   global_step = 11879
2022-06-12 02:19:06,957   loss = 1.2116093171461848
2022-06-12 02:19:06,957   rep_loss = 0.7594327721886962
2022-06-12 02:19:06,957 ***** Save model *****
2022-06-12 02:19:12,651 ***** Running evaluation *****
2022-06-12 02:19:12,651   Epoch = 44 iter 11899 step
2022-06-12 02:19:12,651   Num examples = 1043
2022-06-12 02:19:12,651   Batch size = 32
2022-06-12 02:19:12,653 ***** Eval results *****
2022-06-12 02:19:12,653   att_loss = 0.45222569084325376
2022-06-12 02:19:12,653   global_step = 11899
2022-06-12 02:19:12,653   loss = 1.2115284283429582
2022-06-12 02:19:12,653   rep_loss = 0.7593027420391311
2022-06-12 02:19:12,653 ***** Save model *****
2022-06-12 02:19:12,793 ***** Running evaluation *****
2022-06-12 02:19:12,793   Epoch = 6 iter 20499 step
2022-06-12 02:19:12,793   Num examples = 5463
2022-06-12 02:19:12,793   Batch size = 32
2022-06-12 02:19:12,794 ***** Eval results *****
2022-06-12 02:19:12,794   att_loss = 3.684412533120965
2022-06-12 02:19:12,795   global_step = 20499
2022-06-12 02:19:12,795   loss = 4.5729340211024265
2022-06-12 02:19:12,795   rep_loss = 0.8885214825125106
2022-06-12 02:19:12,795 ***** Save model *****
2022-06-12 02:19:18,290 ***** Running evaluation *****
2022-06-12 02:19:18,290   Epoch = 44 iter 11919 step
2022-06-12 02:19:18,290   Num examples = 1043
2022-06-12 02:19:18,291   Batch size = 32
2022-06-12 02:19:18,291 ***** Eval results *****
2022-06-12 02:19:18,291   att_loss = 0.45140138756462006
2022-06-12 02:19:18,292   global_step = 11919
2022-06-12 02:19:18,292   loss = 1.2099636535198368
2022-06-12 02:19:18,292   rep_loss = 0.7585622699637162
2022-06-12 02:19:18,292 ***** Save model *****
2022-06-12 02:19:23,939 ***** Running evaluation *****
2022-06-12 02:19:23,940   Epoch = 44 iter 11939 step
2022-06-12 02:19:23,940   Num examples = 1043
2022-06-12 02:19:23,940   Batch size = 32
2022-06-12 02:19:23,941 ***** Eval results *****
2022-06-12 02:19:23,941   att_loss = 0.45251315270418896
2022-06-12 02:19:23,942   global_step = 11939
2022-06-12 02:19:23,942   loss = 1.2106273754729027
2022-06-12 02:19:23,942   rep_loss = 0.7581142280738391
2022-06-12 02:19:23,942 ***** Save model *****
2022-06-12 02:19:29,591 ***** Running evaluation *****
2022-06-12 02:19:29,591   Epoch = 44 iter 11959 step
2022-06-12 02:19:29,591   Num examples = 1043
2022-06-12 02:19:29,592   Batch size = 32
2022-06-12 02:19:29,593 ***** Eval results *****
2022-06-12 02:19:29,593   att_loss = 0.4519587132320585
2022-06-12 02:19:29,593   global_step = 11959
2022-06-12 02:19:29,593   loss = 1.2100609151108006
2022-06-12 02:19:29,593   rep_loss = 0.7581022059747958
2022-06-12 02:19:29,593 ***** Save model *****
2022-06-12 02:19:35,271 ***** Running evaluation *****
2022-06-12 02:19:35,271   Epoch = 44 iter 11979 step
2022-06-12 02:19:35,271   Num examples = 1043
2022-06-12 02:19:35,271   Batch size = 32
2022-06-12 02:19:35,272 ***** Eval results *****
2022-06-12 02:19:35,272   att_loss = 0.4534560560664057
2022-06-12 02:19:35,272   global_step = 11979
2022-06-12 02:19:35,272   loss = 1.21194893405551
2022-06-12 02:19:35,272   rep_loss = 0.7584928810854494
2022-06-12 02:19:35,273 ***** Save model *****
2022-06-12 02:19:40,937 ***** Running evaluation *****
2022-06-12 02:19:40,937   Epoch = 44 iter 11999 step
2022-06-12 02:19:40,937   Num examples = 1043
2022-06-12 02:19:40,937   Batch size = 32
2022-06-12 02:19:40,939 ***** Eval results *****
2022-06-12 02:19:40,939   att_loss = 0.45334332207759537
2022-06-12 02:19:40,939   global_step = 11999
2022-06-12 02:19:40,939   loss = 1.2117127313081961
2022-06-12 02:19:40,939   rep_loss = 0.7583694113678191
2022-06-12 02:19:40,939 ***** Save model *****
2022-06-12 02:19:46,584 ***** Running evaluation *****
2022-06-12 02:19:46,585   Epoch = 45 iter 12019 step
2022-06-12 02:19:46,585   Num examples = 1043
2022-06-12 02:19:46,585   Batch size = 32
2022-06-12 02:19:46,586 ***** Eval results *****
2022-06-12 02:19:46,586   att_loss = 0.46634354442358017
2022-06-12 02:19:46,586   global_step = 12019
2022-06-12 02:19:46,586   loss = 1.2231117188930511
2022-06-12 02:19:46,586   rep_loss = 0.7567681819200516
2022-06-12 02:19:46,587 ***** Save model *****
2022-06-12 02:19:52,224 ***** Running evaluation *****
2022-06-12 02:19:52,225   Epoch = 45 iter 12039 step
2022-06-12 02:19:52,225   Num examples = 1043
2022-06-12 02:19:52,225   Batch size = 32
2022-06-12 02:19:52,226 ***** Eval results *****
2022-06-12 02:19:52,227   att_loss = 0.4403511683146159
2022-06-12 02:19:52,227   global_step = 12039
2022-06-12 02:19:52,227   loss = 1.1935332715511322
2022-06-12 02:19:52,227   rep_loss = 0.7531821156541506
2022-06-12 02:19:52,227 ***** Save model *****
2022-06-12 02:19:57,932 ***** Running evaluation *****
2022-06-12 02:19:57,932   Epoch = 45 iter 12059 step
2022-06-12 02:19:57,933   Num examples = 1043
2022-06-12 02:19:57,933   Batch size = 32
2022-06-12 02:19:57,934 ***** Eval results *****
2022-06-12 02:19:57,934   att_loss = 0.44520294530825183
2022-06-12 02:19:57,934   global_step = 12059
2022-06-12 02:19:57,934   loss = 1.199804874983701
2022-06-12 02:19:57,934   rep_loss = 0.754601932384751
2022-06-12 02:19:57,934 ***** Save model *****
2022-06-12 02:20:03,594 ***** Running evaluation *****
2022-06-12 02:20:03,594   Epoch = 45 iter 12079 step
2022-06-12 02:20:03,594   Num examples = 1043
2022-06-12 02:20:03,594   Batch size = 32
2022-06-12 02:20:03,595 ***** Eval results *****
2022-06-12 02:20:03,595   att_loss = 0.449360188562423
2022-06-12 02:20:03,595   global_step = 12079
2022-06-12 02:20:03,595   loss = 1.204792132601142
2022-06-12 02:20:03,595   rep_loss = 0.7554319426417351
2022-06-12 02:20:03,596 ***** Save model *****
2022-06-12 02:20:09,283 ***** Running evaluation *****
2022-06-12 02:20:09,284   Epoch = 45 iter 12099 step
2022-06-12 02:20:09,284   Num examples = 1043
2022-06-12 02:20:09,284   Batch size = 32
2022-06-12 02:20:09,286 ***** Eval results *****
2022-06-12 02:20:09,286   att_loss = 0.4482878407552129
2022-06-12 02:20:09,286   global_step = 12099
2022-06-12 02:20:09,286   loss = 1.203200611330214
2022-06-12 02:20:09,286   rep_loss = 0.7549127688010534
2022-06-12 02:20:09,287 ***** Save model *****
2022-06-12 02:20:14,949 ***** Running evaluation *****
2022-06-12 02:20:14,949   Epoch = 45 iter 12119 step
2022-06-12 02:20:14,949   Num examples = 1043
2022-06-12 02:20:14,949   Batch size = 32
2022-06-12 02:20:14,950 ***** Eval results *****
2022-06-12 02:20:14,950   att_loss = 0.45054185247192013
2022-06-12 02:20:14,950   global_step = 12119
2022-06-12 02:20:14,950   loss = 1.2067897973152308
2022-06-12 02:20:14,950   rep_loss = 0.7562479439836282
2022-06-12 02:20:14,951 ***** Save model *****
2022-06-12 02:20:20,585 ***** Running evaluation *****
2022-06-12 02:20:20,586   Epoch = 45 iter 12139 step
2022-06-12 02:20:20,586   Num examples = 1043
2022-06-12 02:20:20,586   Batch size = 32
2022-06-12 02:20:20,587 ***** Eval results *****
2022-06-12 02:20:20,587   att_loss = 0.4512814408348453
2022-06-12 02:20:20,587   global_step = 12139
2022-06-12 02:20:20,587   loss = 1.2075437374653355
2022-06-12 02:20:20,587   rep_loss = 0.7562622947077597
2022-06-12 02:20:20,587 ***** Save model *****
2022-06-12 02:20:26,233 ***** Running evaluation *****
2022-06-12 02:20:26,234   Epoch = 45 iter 12159 step
2022-06-12 02:20:26,234   Num examples = 1043
2022-06-12 02:20:26,234   Batch size = 32
2022-06-12 02:20:26,235 ***** Eval results *****
2022-06-12 02:20:26,235   att_loss = 0.4495903748191065
2022-06-12 02:20:26,235   global_step = 12159
2022-06-12 02:20:26,235   loss = 1.2052691645092435
2022-06-12 02:20:26,235   rep_loss = 0.7556787890692552
2022-06-12 02:20:26,235 ***** Save model *****
2022-06-12 02:20:31,946 ***** Running evaluation *****
2022-06-12 02:20:31,947   Epoch = 45 iter 12179 step
2022-06-12 02:20:31,947   Num examples = 1043
2022-06-12 02:20:31,947   Batch size = 32
2022-06-12 02:20:31,948 ***** Eval results *****
2022-06-12 02:20:31,948   att_loss = 0.450994581892723
2022-06-12 02:20:31,948   global_step = 12179
2022-06-12 02:20:31,948   loss = 1.2068466471462715
2022-06-12 02:20:31,948   rep_loss = 0.7558520661621559
2022-06-12 02:20:31,948 ***** Save model *****
2022-06-12 02:20:37,593 ***** Running evaluation *****
2022-06-12 02:20:37,593   Epoch = 45 iter 12199 step
2022-06-12 02:20:37,593   Num examples = 1043
2022-06-12 02:20:37,593   Batch size = 32
2022-06-12 02:20:37,594 ***** Eval results *****
2022-06-12 02:20:37,594   att_loss = 0.4518885505588158
2022-06-12 02:20:37,594   global_step = 12199
2022-06-12 02:20:37,595   loss = 1.2074788763471271
2022-06-12 02:20:37,595   rep_loss = 0.755590326436188
2022-06-12 02:20:37,595 ***** Save model *****
2022-06-12 02:20:43,246 ***** Running evaluation *****
2022-06-12 02:20:43,247   Epoch = 45 iter 12219 step
2022-06-12 02:20:43,247   Num examples = 1043
2022-06-12 02:20:43,247   Batch size = 32
2022-06-12 02:20:43,248 ***** Eval results *****
2022-06-12 02:20:43,248   att_loss = 0.4493677318096161
2022-06-12 02:20:43,248   global_step = 12219
2022-06-12 02:20:43,248   loss = 1.2042215238599216
2022-06-12 02:20:43,248   rep_loss = 0.7548537920503056
2022-06-12 02:20:43,248 ***** Save model *****
2022-06-12 02:20:48,917 ***** Running evaluation *****
2022-06-12 02:20:48,918   Epoch = 45 iter 12239 step
2022-06-12 02:20:48,918   Num examples = 1043
2022-06-12 02:20:48,918   Batch size = 32
2022-06-12 02:20:48,919 ***** Eval results *****
2022-06-12 02:20:48,919   att_loss = 0.45003399771771263
2022-06-12 02:20:48,919   global_step = 12239
2022-06-12 02:20:48,919   loss = 1.204724303313664
2022-06-12 02:20:48,919   rep_loss = 0.754690304930721
2022-06-12 02:20:48,919 ***** Save model *****
2022-06-12 02:20:54,568 ***** Running evaluation *****
2022-06-12 02:20:54,568   Epoch = 45 iter 12259 step
2022-06-12 02:20:54,568   Num examples = 1043
2022-06-12 02:20:54,568   Batch size = 32
2022-06-12 02:20:54,569 ***** Eval results *****
2022-06-12 02:20:54,569   att_loss = 0.4502651169896126
2022-06-12 02:20:54,569   global_step = 12259
2022-06-12 02:20:54,569   loss = 1.2048049549587438
2022-06-12 02:20:54,569   rep_loss = 0.7545398378469905
2022-06-12 02:20:54,570 ***** Save model *****
2022-06-12 02:21:00,254 ***** Running evaluation *****
2022-06-12 02:21:00,255   Epoch = 45 iter 12279 step
2022-06-12 02:21:00,255   Num examples = 1043
2022-06-12 02:21:00,255   Batch size = 32
2022-06-12 02:21:00,256 ***** Eval results *****
2022-06-12 02:21:00,256   att_loss = 0.4497559589870048
2022-06-12 02:21:00,256   global_step = 12279
2022-06-12 02:21:00,256   loss = 1.2041435611970497
2022-06-12 02:21:00,256   rep_loss = 0.7543876026615952
2022-06-12 02:21:00,257 ***** Save model *****
2022-06-12 02:21:05,930 ***** Running evaluation *****
2022-06-12 02:21:05,931   Epoch = 46 iter 12299 step
2022-06-12 02:21:05,931   Num examples = 1043
2022-06-12 02:21:05,931   Batch size = 32
2022-06-12 02:21:05,932 ***** Eval results *****
2022-06-12 02:21:05,932   att_loss = 0.44392672531745014
2022-06-12 02:21:05,932   global_step = 12299
2022-06-12 02:21:05,932   loss = 1.1980209841447718
2022-06-12 02:21:05,933   rep_loss = 0.7540942535680883
2022-06-12 02:21:05,933 ***** Save model *****
2022-06-12 02:21:11,569 ***** Running evaluation *****
2022-06-12 02:21:11,569   Epoch = 46 iter 12319 step
2022-06-12 02:21:11,569   Num examples = 1043
2022-06-12 02:21:11,569   Batch size = 32
2022-06-12 02:21:11,570 ***** Eval results *****
2022-06-12 02:21:11,570   att_loss = 0.44343862662444244
2022-06-12 02:21:11,570   global_step = 12319
2022-06-12 02:21:11,570   loss = 1.1994148235063296
2022-06-12 02:21:11,571   rep_loss = 0.7559761920490781
2022-06-12 02:21:11,571 ***** Save model *****
2022-06-12 02:21:17,204 ***** Running evaluation *****
2022-06-12 02:21:17,205   Epoch = 46 iter 12339 step
2022-06-12 02:21:17,205   Num examples = 1043
2022-06-12 02:21:17,205   Batch size = 32
2022-06-12 02:21:17,206 ***** Eval results *****
2022-06-12 02:21:17,206   att_loss = 0.4549715843117028
2022-06-12 02:21:17,206   global_step = 12339
2022-06-12 02:21:17,206   loss = 1.2155946744115729
2022-06-12 02:21:17,206   rep_loss = 0.7606230900998701
2022-06-12 02:21:17,207 ***** Save model *****
2022-06-12 02:21:20,696 ***** Running evaluation *****
2022-06-12 02:21:20,696   Epoch = 6 iter 20999 step
2022-06-12 02:21:20,697   Num examples = 5463
2022-06-12 02:21:20,697   Batch size = 32
2022-06-12 02:21:20,698 ***** Eval results *****
2022-06-12 02:21:20,698   att_loss = 3.6852288897820555
2022-06-12 02:21:20,699   global_step = 20999
2022-06-12 02:21:20,699   loss = 4.573769990877576
2022-06-12 02:21:20,699   rep_loss = 0.8885410967598409
2022-06-12 02:21:20,699 ***** Save model *****
2022-06-12 02:21:22,904 ***** Running evaluation *****
2022-06-12 02:21:22,904   Epoch = 46 iter 12359 step
2022-06-12 02:21:22,904   Num examples = 1043
2022-06-12 02:21:22,904   Batch size = 32
2022-06-12 02:21:22,905 ***** Eval results *****
2022-06-12 02:21:22,905   att_loss = 0.45305778376467815
2022-06-12 02:21:22,905   global_step = 12359
2022-06-12 02:21:22,905   loss = 1.2114806732574066
2022-06-12 02:21:22,905   rep_loss = 0.7584228894927285
2022-06-12 02:21:22,905 ***** Save model *****
2022-06-12 02:21:28,503 ***** Running evaluation *****
2022-06-12 02:21:28,503   Epoch = 46 iter 12379 step
2022-06-12 02:21:28,503   Num examples = 1043
2022-06-12 02:21:28,503   Batch size = 32
2022-06-12 02:21:28,504 ***** Eval results *****
2022-06-12 02:21:28,504   att_loss = 0.45173866109749705
2022-06-12 02:21:28,504   global_step = 12379
2022-06-12 02:21:28,504   loss = 1.2079342554525
2022-06-12 02:21:28,504   rep_loss = 0.7561955943550032
2022-06-12 02:21:28,505 ***** Save model *****
2022-06-12 02:21:34,167 ***** Running evaluation *****
2022-06-12 02:21:34,167   Epoch = 46 iter 12399 step
2022-06-12 02:21:34,167   Num examples = 1043
2022-06-12 02:21:34,167   Batch size = 32
2022-06-12 02:21:34,168 ***** Eval results *****
2022-06-12 02:21:34,168   att_loss = 0.44870146446757847
2022-06-12 02:21:34,169   global_step = 12399
2022-06-12 02:21:34,169   loss = 1.2034786970187457
2022-06-12 02:21:34,169   rep_loss = 0.7547772328058878
2022-06-12 02:21:34,169 ***** Save model *****
2022-06-12 02:21:39,818 ***** Running evaluation *****
2022-06-12 02:21:39,818   Epoch = 46 iter 12419 step
2022-06-12 02:21:39,818   Num examples = 1043
2022-06-12 02:21:39,818   Batch size = 32
2022-06-12 02:21:39,819 ***** Eval results *****
2022-06-12 02:21:39,819   att_loss = 0.44719810446683506
2022-06-12 02:21:39,819   global_step = 12419
2022-06-12 02:21:39,820   loss = 1.2010610260232522
2022-06-12 02:21:39,820   rep_loss = 0.7538629213388819
2022-06-12 02:21:39,820 ***** Save model *****
2022-06-12 02:21:45,455 ***** Running evaluation *****
2022-06-12 02:21:45,455   Epoch = 46 iter 12439 step
2022-06-12 02:21:45,455   Num examples = 1043
2022-06-12 02:21:45,455   Batch size = 32
2022-06-12 02:21:45,456 ***** Eval results *****
2022-06-12 02:21:45,457   att_loss = 0.4450452156886933
2022-06-12 02:21:45,457   global_step = 12439
2022-06-12 02:21:45,457   loss = 1.1986339540238593
2022-06-12 02:21:45,457   rep_loss = 0.7535887379555186
2022-06-12 02:21:45,457 ***** Save model *****
2022-06-12 02:21:51,116 ***** Running evaluation *****
2022-06-12 02:21:51,116   Epoch = 46 iter 12459 step
2022-06-12 02:21:51,116   Num examples = 1043
2022-06-12 02:21:51,116   Batch size = 32
2022-06-12 02:21:51,118 ***** Eval results *****
2022-06-12 02:21:51,118   att_loss = 0.44785279355480173
2022-06-12 02:21:51,118   global_step = 12459
2022-06-12 02:21:51,118   loss = 1.2020214324617116
2022-06-12 02:21:51,118   rep_loss = 0.754168637728287
2022-06-12 02:21:51,118 ***** Save model *****
2022-06-12 02:21:56,746 ***** Running evaluation *****
2022-06-12 02:21:56,746   Epoch = 46 iter 12479 step
2022-06-12 02:21:56,746   Num examples = 1043
2022-06-12 02:21:56,747   Batch size = 32
2022-06-12 02:21:56,748 ***** Eval results *****
2022-06-12 02:21:56,748   att_loss = 0.4497275567296798
2022-06-12 02:21:56,748   global_step = 12479
2022-06-12 02:21:56,748   loss = 1.2040509657206269
2022-06-12 02:21:56,748   rep_loss = 0.7543234083858238
2022-06-12 02:21:56,748 ***** Save model *****
2022-06-12 02:22:02,409 ***** Running evaluation *****
2022-06-12 02:22:02,409   Epoch = 46 iter 12499 step
2022-06-12 02:22:02,409   Num examples = 1043
2022-06-12 02:22:02,409   Batch size = 32
2022-06-12 02:22:02,410 ***** Eval results *****
2022-06-12 02:22:02,410   att_loss = 0.4493803064669332
2022-06-12 02:22:02,410   global_step = 12499
2022-06-12 02:22:02,410   loss = 1.203784819022851
2022-06-12 02:22:02,410   rep_loss = 0.7544045099464979
2022-06-12 02:22:02,411 ***** Save model *****
2022-06-12 02:22:08,042 ***** Running evaluation *****
2022-06-12 02:22:08,042   Epoch = 46 iter 12519 step
2022-06-12 02:22:08,042   Num examples = 1043
2022-06-12 02:22:08,042   Batch size = 32
2022-06-12 02:22:08,043 ***** Eval results *****
2022-06-12 02:22:08,043   att_loss = 0.4480589728818161
2022-06-12 02:22:08,043   global_step = 12519
2022-06-12 02:22:08,043   loss = 1.2017349039954979
2022-06-12 02:22:08,043   rep_loss = 0.7536759291017106
2022-06-12 02:22:08,043 ***** Save model *****
2022-06-12 02:22:13,664 ***** Running evaluation *****
2022-06-12 02:22:13,665   Epoch = 46 iter 12539 step
2022-06-12 02:22:13,665   Num examples = 1043
2022-06-12 02:22:13,665   Batch size = 32
2022-06-12 02:22:13,666 ***** Eval results *****
2022-06-12 02:22:13,666   att_loss = 0.44860450964029663
2022-06-12 02:22:13,666   global_step = 12539
2022-06-12 02:22:13,666   loss = 1.20227348711704
2022-06-12 02:22:13,666   rep_loss = 0.753668975969233
2022-06-12 02:22:13,666 ***** Save model *****
2022-06-12 02:22:19,310 ***** Running evaluation *****
2022-06-12 02:22:19,310   Epoch = 47 iter 12559 step
2022-06-12 02:22:19,310   Num examples = 1043
2022-06-12 02:22:19,311   Batch size = 32
2022-06-12 02:22:19,311 ***** Eval results *****
2022-06-12 02:22:19,312   att_loss = 0.4698469877243042
2022-06-12 02:22:19,312   global_step = 12559
2022-06-12 02:22:19,312   loss = 1.2192270874977111
2022-06-12 02:22:19,312   rep_loss = 0.7493800938129425
2022-06-12 02:22:19,312 ***** Save model *****
2022-06-12 02:22:25,022 ***** Running evaluation *****
2022-06-12 02:22:25,022   Epoch = 47 iter 12579 step
2022-06-12 02:22:25,023   Num examples = 1043
2022-06-12 02:22:25,023   Batch size = 32
2022-06-12 02:22:25,024 ***** Eval results *****
2022-06-12 02:22:25,024   att_loss = 0.45835553606351215
2022-06-12 02:22:25,024   global_step = 12579
2022-06-12 02:22:25,024   loss = 1.211644705136617
2022-06-12 02:22:25,024   rep_loss = 0.7532891770203908
2022-06-12 02:22:25,024 ***** Save model *****
2022-06-12 02:22:30,694 ***** Running evaluation *****
2022-06-12 02:22:30,694   Epoch = 47 iter 12599 step
2022-06-12 02:22:30,694   Num examples = 1043
2022-06-12 02:22:30,694   Batch size = 32
2022-06-12 02:22:30,695 ***** Eval results *****
2022-06-12 02:22:30,696   att_loss = 0.44981327652931213
2022-06-12 02:22:30,696   global_step = 12599
2022-06-12 02:22:30,696   loss = 1.2017683577537537
2022-06-12 02:22:30,696   rep_loss = 0.7519550895690919
2022-06-12 02:22:30,696 ***** Save model *****
2022-06-12 02:22:36,368 ***** Running evaluation *****
2022-06-12 02:22:36,369   Epoch = 47 iter 12619 step
2022-06-12 02:22:36,369   Num examples = 1043
2022-06-12 02:22:36,369   Batch size = 32
2022-06-12 02:22:36,370 ***** Eval results *****
2022-06-12 02:22:36,370   att_loss = 0.45084362626075747
2022-06-12 02:22:36,370   global_step = 12619
2022-06-12 02:22:36,370   loss = 1.2030134626797266
2022-06-12 02:22:36,370   rep_loss = 0.7521698440824236
2022-06-12 02:22:36,370 ***** Save model *****
2022-06-12 02:22:41,999 ***** Running evaluation *****
2022-06-12 02:22:42,000   Epoch = 47 iter 12639 step
2022-06-12 02:22:42,000   Num examples = 1043
2022-06-12 02:22:42,000   Batch size = 32
2022-06-12 02:22:42,001 ***** Eval results *****
2022-06-12 02:22:42,001   att_loss = 0.45393510825104183
2022-06-12 02:22:42,001   global_step = 12639
2022-06-12 02:22:42,002   loss = 1.2069038642777337
2022-06-12 02:22:42,002   rep_loss = 0.752968759669198
2022-06-12 02:22:42,002 ***** Save model *****
2022-06-12 02:22:47,608 ***** Running evaluation *****
2022-06-12 02:22:47,608   Epoch = 47 iter 12659 step
2022-06-12 02:22:47,609   Num examples = 1043
2022-06-12 02:22:47,609   Batch size = 32
2022-06-12 02:22:47,610 ***** Eval results *****
2022-06-12 02:22:47,610   att_loss = 0.4514097674326463
2022-06-12 02:22:47,610   global_step = 12659
2022-06-12 02:22:47,610   loss = 1.2042386824434455
2022-06-12 02:22:47,610   rep_loss = 0.7528289188038219
2022-06-12 02:22:47,610 ***** Save model *****
2022-06-12 02:22:53,251 ***** Running evaluation *****
2022-06-12 02:22:53,251   Epoch = 47 iter 12679 step
2022-06-12 02:22:53,251   Num examples = 1043
2022-06-12 02:22:53,251   Batch size = 32
2022-06-12 02:22:53,252 ***** Eval results *****
2022-06-12 02:22:53,252   att_loss = 0.45084435252042915
2022-06-12 02:22:53,252   global_step = 12679
2022-06-12 02:22:53,253   loss = 1.2042069123341488
2022-06-12 02:22:53,253   rep_loss = 0.753362563940195
2022-06-12 02:22:53,253 ***** Save model *****
2022-06-12 02:22:58,879 ***** Running evaluation *****
2022-06-12 02:22:58,879   Epoch = 47 iter 12699 step
2022-06-12 02:22:58,879   Num examples = 1043
2022-06-12 02:22:58,879   Batch size = 32
2022-06-12 02:22:58,880 ***** Eval results *****
2022-06-12 02:22:58,880   att_loss = 0.45196076929569245
2022-06-12 02:22:58,880   global_step = 12699
2022-06-12 02:22:58,881   loss = 1.2061085359255472
2022-06-12 02:22:58,881   rep_loss = 0.7541477688153585
2022-06-12 02:22:58,881 ***** Save model *****
2022-06-12 02:23:04,541 ***** Running evaluation *****
2022-06-12 02:23:04,541   Epoch = 47 iter 12719 step
2022-06-12 02:23:04,542   Num examples = 1043
2022-06-12 02:23:04,542   Batch size = 32
2022-06-12 02:23:04,542 ***** Eval results *****
2022-06-12 02:23:04,543   att_loss = 0.4487707017099156
2022-06-12 02:23:04,543   global_step = 12719
2022-06-12 02:23:04,543   loss = 1.2019696144496694
2022-06-12 02:23:04,543   rep_loss = 0.7531989139669082
2022-06-12 02:23:04,543 ***** Save model *****
2022-06-12 02:23:10,206 ***** Running evaluation *****
2022-06-12 02:23:10,207   Epoch = 47 iter 12739 step
2022-06-12 02:23:10,207   Num examples = 1043
2022-06-12 02:23:10,207   Batch size = 32
2022-06-12 02:23:10,208 ***** Eval results *****
2022-06-12 02:23:10,208   att_loss = 0.45014752510346867
2022-06-12 02:23:10,208   global_step = 12739
2022-06-12 02:23:10,208   loss = 1.2040030849607368
2022-06-12 02:23:10,208   rep_loss = 0.753855562210083
2022-06-12 02:23:10,209 ***** Save model *****
2022-06-12 02:23:15,852 ***** Running evaluation *****
2022-06-12 02:23:15,852   Epoch = 47 iter 12759 step
2022-06-12 02:23:15,852   Num examples = 1043
2022-06-12 02:23:15,852   Batch size = 32
2022-06-12 02:23:15,853 ***** Eval results *****
2022-06-12 02:23:15,854   att_loss = 0.45122096425011043
2022-06-12 02:23:15,854   global_step = 12759
2022-06-12 02:23:15,854   loss = 1.2047269338653201
2022-06-12 02:23:15,854   rep_loss = 0.7535059704667046
2022-06-12 02:23:15,854 ***** Save model *****
2022-06-12 02:23:21,497 ***** Running evaluation *****
2022-06-12 02:23:21,498   Epoch = 47 iter 12779 step
2022-06-12 02:23:21,498   Num examples = 1043
2022-06-12 02:23:21,498   Batch size = 32
2022-06-12 02:23:21,499 ***** Eval results *****
2022-06-12 02:23:21,499   att_loss = 0.450193615452103
2022-06-12 02:23:21,499   global_step = 12779
2022-06-12 02:23:21,499   loss = 1.2036783244298852
2022-06-12 02:23:21,499   rep_loss = 0.7534847093665081
2022-06-12 02:23:21,499 ***** Save model *****
2022-06-12 02:23:27,134 ***** Running evaluation *****
2022-06-12 02:23:27,134   Epoch = 47 iter 12799 step
2022-06-12 02:23:27,134   Num examples = 1043
2022-06-12 02:23:27,134   Batch size = 32
2022-06-12 02:23:27,135 ***** Eval results *****
2022-06-12 02:23:27,135   att_loss = 0.45031027841567994
2022-06-12 02:23:27,135   global_step = 12799
2022-06-12 02:23:27,135   loss = 1.203606120109558
2022-06-12 02:23:27,136   rep_loss = 0.753295841217041
2022-06-12 02:23:27,136 ***** Save model *****
2022-06-12 02:23:28,519 ***** Running evaluation *****
2022-06-12 02:23:28,520   Epoch = 6 iter 21499 step
2022-06-12 02:23:28,520   Num examples = 5463
2022-06-12 02:23:28,520   Batch size = 32
2022-06-12 02:23:28,521 ***** Eval results *****
2022-06-12 02:23:28,521   att_loss = 3.681129107867307
2022-06-12 02:23:28,521   global_step = 21499
2022-06-12 02:23:28,521   loss = 4.569175753139924
2022-06-12 02:23:28,521   rep_loss = 0.888046641140968
2022-06-12 02:23:28,521 ***** Save model *****
2022-06-12 02:23:32,776 ***** Running evaluation *****
2022-06-12 02:23:32,776   Epoch = 48 iter 12819 step
2022-06-12 02:23:32,776   Num examples = 1043
2022-06-12 02:23:32,776   Batch size = 32
2022-06-12 02:23:32,777 ***** Eval results *****
2022-06-12 02:23:32,778   att_loss = 0.45588470498720807
2022-06-12 02:23:32,778   global_step = 12819
2022-06-12 02:23:32,778   loss = 1.2070990403493245
2022-06-12 02:23:32,778   rep_loss = 0.7512143055597941
2022-06-12 02:23:32,778 ***** Save model *****
2022-06-12 02:23:38,400 ***** Running evaluation *****
2022-06-12 02:23:38,400   Epoch = 48 iter 12839 step
2022-06-12 02:23:38,401   Num examples = 1043
2022-06-12 02:23:38,401   Batch size = 32
2022-06-12 02:23:38,402 ***** Eval results *****
2022-06-12 02:23:38,402   att_loss = 0.45394956806431647
2022-06-12 02:23:38,402   global_step = 12839
2022-06-12 02:23:38,402   loss = 1.2063498652499656
2022-06-12 02:23:38,402   rep_loss = 0.7524002945941427
2022-06-12 02:23:38,402 ***** Save model *****
2022-06-12 02:23:44,063 ***** Running evaluation *****
2022-06-12 02:23:44,063   Epoch = 48 iter 12859 step
2022-06-12 02:23:44,063   Num examples = 1043
2022-06-12 02:23:44,063   Batch size = 32
2022-06-12 02:23:44,064 ***** Eval results *****
2022-06-12 02:23:44,064   att_loss = 0.4525514245033264
2022-06-12 02:23:44,064   global_step = 12859
2022-06-12 02:23:44,065   loss = 1.2078394141308098
2022-06-12 02:23:44,065   rep_loss = 0.7552879799244016
2022-06-12 02:23:44,065 ***** Save model *****
2022-06-12 02:23:49,738 ***** Running evaluation *****
2022-06-12 02:23:49,738   Epoch = 48 iter 12879 step
2022-06-12 02:23:49,738   Num examples = 1043
2022-06-12 02:23:49,738   Batch size = 32
2022-06-12 02:23:49,739 ***** Eval results *****
2022-06-12 02:23:49,739   att_loss = 0.4492063049286131
2022-06-12 02:23:49,739   global_step = 12879
2022-06-12 02:23:49,739   loss = 1.201771859138731
2022-06-12 02:23:49,740   rep_loss = 0.7525655466412741
2022-06-12 02:23:49,740 ***** Save model *****
2022-06-12 02:23:55,393 ***** Running evaluation *****
2022-06-12 02:23:55,393   Epoch = 48 iter 12899 step
2022-06-12 02:23:55,393   Num examples = 1043
2022-06-12 02:23:55,393   Batch size = 32
2022-06-12 02:23:55,394 ***** Eval results *****
2022-06-12 02:23:55,395   att_loss = 0.4465071810297219
2022-06-12 02:23:55,395   global_step = 12899
2022-06-12 02:23:55,395   loss = 1.1982896040721112
2022-06-12 02:23:55,395   rep_loss = 0.7517824194517481
2022-06-12 02:23:55,395 ***** Save model *****
2022-06-12 02:24:01,049 ***** Running evaluation *****
2022-06-12 02:24:01,050   Epoch = 48 iter 12919 step
2022-06-12 02:24:01,050   Num examples = 1043
2022-06-12 02:24:01,050   Batch size = 32
2022-06-12 02:24:01,051 ***** Eval results *****
2022-06-12 02:24:01,051   att_loss = 0.4470595954691322
2022-06-12 02:24:01,051   global_step = 12919
2022-06-12 02:24:01,051   loss = 1.198819087547006
2022-06-12 02:24:01,051   rep_loss = 0.7517594891844444
2022-06-12 02:24:01,051 ***** Save model *****
2022-06-12 02:24:06,667 ***** Running evaluation *****
2022-06-12 02:24:06,667   Epoch = 48 iter 12939 step
2022-06-12 02:24:06,667   Num examples = 1043
2022-06-12 02:24:06,667   Batch size = 32
2022-06-12 02:24:06,669 ***** Eval results *****
2022-06-12 02:24:06,669   att_loss = 0.45051659413469514
2022-06-12 02:24:06,669   global_step = 12939
2022-06-12 02:24:06,669   loss = 1.203480151610646
2022-06-12 02:24:06,669   rep_loss = 0.7529635555375882
2022-06-12 02:24:06,669 ***** Save model *****
2022-06-12 02:24:12,300 ***** Running evaluation *****
2022-06-12 02:24:12,300   Epoch = 48 iter 12959 step
2022-06-12 02:24:12,300   Num examples = 1043
2022-06-12 02:24:12,300   Batch size = 32
2022-06-12 02:24:12,301 ***** Eval results *****
2022-06-12 02:24:12,301   att_loss = 0.4494699360607387
2022-06-12 02:24:12,302   global_step = 12959
2022-06-12 02:24:12,302   loss = 1.201083088254595
2022-06-12 02:24:12,302   rep_loss = 0.7516131501097779
2022-06-12 02:24:12,302 ***** Save model *****
2022-06-12 02:24:17,960 ***** Running evaluation *****
2022-06-12 02:24:17,960   Epoch = 48 iter 12979 step
2022-06-12 02:24:17,960   Num examples = 1043
2022-06-12 02:24:17,960   Batch size = 32
2022-06-12 02:24:17,961 ***** Eval results *****
2022-06-12 02:24:17,962   att_loss = 0.44763714568746604
2022-06-12 02:24:17,962   global_step = 12979
2022-06-12 02:24:17,962   loss = 1.1989191170850415
2022-06-12 02:24:17,962   rep_loss = 0.7512819686550304
2022-06-12 02:24:17,962 ***** Save model *****
2022-06-12 02:24:23,629 ***** Running evaluation *****
2022-06-12 02:24:23,630   Epoch = 48 iter 12999 step
2022-06-12 02:24:23,630   Num examples = 1043
2022-06-12 02:24:23,630   Batch size = 32
2022-06-12 02:24:23,631 ***** Eval results *****
2022-06-12 02:24:23,631   att_loss = 0.4466705481862761
2022-06-12 02:24:23,631   global_step = 12999
2022-06-12 02:24:23,631   loss = 1.1978912060378029
2022-06-12 02:24:23,631   rep_loss = 0.7512206542687337
2022-06-12 02:24:23,631 ***** Save model *****
2022-06-12 02:24:29,281 ***** Running evaluation *****
2022-06-12 02:24:29,281   Epoch = 48 iter 13019 step
2022-06-12 02:24:29,281   Num examples = 1043
2022-06-12 02:24:29,281   Batch size = 32
2022-06-12 02:24:29,282 ***** Eval results *****
2022-06-12 02:24:29,282   att_loss = 0.44490633266312735
2022-06-12 02:24:29,282   global_step = 13019
2022-06-12 02:24:29,282   loss = 1.1960885195896542
2022-06-12 02:24:29,282   rep_loss = 0.7511821832562903
2022-06-12 02:24:29,283 ***** Save model *****
2022-06-12 02:24:34,962 ***** Running evaluation *****
2022-06-12 02:24:34,963   Epoch = 48 iter 13039 step
2022-06-12 02:24:34,963   Num examples = 1043
2022-06-12 02:24:34,963   Batch size = 32
2022-06-12 02:24:34,964 ***** Eval results *****
2022-06-12 02:24:34,964   att_loss = 0.4466505911318176
2022-06-12 02:24:34,964   global_step = 13039
2022-06-12 02:24:34,964   loss = 1.1980095503041563
2022-06-12 02:24:34,964   rep_loss = 0.751358955430343
2022-06-12 02:24:34,964 ***** Save model *****
2022-06-12 02:24:40,603 ***** Running evaluation *****
2022-06-12 02:24:40,604   Epoch = 48 iter 13059 step
2022-06-12 02:24:40,604   Num examples = 1043
2022-06-12 02:24:40,604   Batch size = 32
2022-06-12 02:24:40,605 ***** Eval results *****
2022-06-12 02:24:40,605   att_loss = 0.4471296592013826
2022-06-12 02:24:40,605   global_step = 13059
2022-06-12 02:24:40,605   loss = 1.1984674650945781
2022-06-12 02:24:40,605   rep_loss = 0.7513378027044697
2022-06-12 02:24:40,605 ***** Save model *****
2022-06-12 02:24:46,235 ***** Running evaluation *****
2022-06-12 02:24:46,235   Epoch = 48 iter 13079 step
2022-06-12 02:24:46,235   Num examples = 1043
2022-06-12 02:24:46,235   Batch size = 32
2022-06-12 02:24:46,236 ***** Eval results *****
2022-06-12 02:24:46,236   att_loss = 0.4462841612077938
2022-06-12 02:24:46,236   global_step = 13079
2022-06-12 02:24:46,236   loss = 1.1974111899676885
2022-06-12 02:24:46,236   rep_loss = 0.7511270261536079
2022-06-12 02:24:46,237 ***** Save model *****
2022-06-12 02:24:51,875 ***** Running evaluation *****
2022-06-12 02:24:51,875   Epoch = 49 iter 13099 step
2022-06-12 02:24:51,875   Num examples = 1043
2022-06-12 02:24:51,875   Batch size = 32
2022-06-12 02:24:51,876 ***** Eval results *****
2022-06-12 02:24:51,876   att_loss = 0.4297415893524885
2022-06-12 02:24:51,876   global_step = 13099
2022-06-12 02:24:51,877   loss = 1.1774298697710037
2022-06-12 02:24:51,877   rep_loss = 0.7476882785558701
2022-06-12 02:24:51,877 ***** Save model *****
2022-06-12 02:24:57,532 ***** Running evaluation *****
2022-06-12 02:24:57,532   Epoch = 49 iter 13119 step
2022-06-12 02:24:57,533   Num examples = 1043
2022-06-12 02:24:57,533   Batch size = 32
2022-06-12 02:24:57,533 ***** Eval results *****
2022-06-12 02:24:57,534   att_loss = 0.4436526579989327
2022-06-12 02:24:57,534   global_step = 13119
2022-06-12 02:24:57,534   loss = 1.1920420792367723
2022-06-12 02:24:57,534   rep_loss = 0.7483894129594167
2022-06-12 02:24:57,534 ***** Save model *****
2022-06-12 02:25:03,205 ***** Running evaluation *****
2022-06-12 02:25:03,205   Epoch = 49 iter 13139 step
2022-06-12 02:25:03,205   Num examples = 1043
2022-06-12 02:25:03,205   Batch size = 32
2022-06-12 02:25:03,206 ***** Eval results *****
2022-06-12 02:25:03,206   att_loss = 0.4461156635412148
2022-06-12 02:25:03,207   global_step = 13139
2022-06-12 02:25:03,207   loss = 1.1968828397137778
2022-06-12 02:25:03,207   rep_loss = 0.750767171382904
2022-06-12 02:25:03,207 ***** Save model *****
2022-06-12 02:25:08,868 ***** Running evaluation *****
2022-06-12 02:25:08,869   Epoch = 49 iter 13159 step
2022-06-12 02:25:08,869   Num examples = 1043
2022-06-12 02:25:08,869   Batch size = 32
2022-06-12 02:25:08,870 ***** Eval results *****
2022-06-12 02:25:08,870   att_loss = 0.44589970260858536
2022-06-12 02:25:08,870   global_step = 13159
2022-06-12 02:25:08,870   loss = 1.195328685798143
2022-06-12 02:25:08,870   rep_loss = 0.7494289804446069
2022-06-12 02:25:08,871 ***** Save model *****
2022-06-12 02:25:14,504 ***** Running evaluation *****
2022-06-12 02:25:14,504   Epoch = 49 iter 13179 step
2022-06-12 02:25:14,504   Num examples = 1043
2022-06-12 02:25:14,505   Batch size = 32
2022-06-12 02:25:14,506 ***** Eval results *****
2022-06-12 02:25:14,506   att_loss = 0.4472764739766717
2022-06-12 02:25:14,506   global_step = 13179
2022-06-12 02:25:14,506   loss = 1.196770745019118
2022-06-12 02:25:14,506   rep_loss = 0.7494942682484785
2022-06-12 02:25:14,506 ***** Save model *****
2022-06-12 02:25:20,161 ***** Running evaluation *****
2022-06-12 02:25:20,162   Epoch = 49 iter 13199 step
2022-06-12 02:25:20,162   Num examples = 1043
2022-06-12 02:25:20,162   Batch size = 32
2022-06-12 02:25:20,163 ***** Eval results *****
2022-06-12 02:25:20,163   att_loss = 0.44318365331353815
2022-06-12 02:25:20,163   global_step = 13199
2022-06-12 02:25:20,163   loss = 1.1913111312636013
2022-06-12 02:25:20,163   rep_loss = 0.7481274794915627
2022-06-12 02:25:20,163 ***** Save model *****
2022-06-12 02:25:25,788 ***** Running evaluation *****
2022-06-12 02:25:25,788   Epoch = 49 iter 13219 step
2022-06-12 02:25:25,788   Num examples = 1043
2022-06-12 02:25:25,788   Batch size = 32
2022-06-12 02:25:25,789 ***** Eval results *****
2022-06-12 02:25:25,789   att_loss = 0.4439631299499203
2022-06-12 02:25:25,789   global_step = 13219
2022-06-12 02:25:25,790   loss = 1.19224716109388
2022-06-12 02:25:25,790   rep_loss = 0.7482840300482863
2022-06-12 02:25:25,790 ***** Save model *****
2022-06-12 02:25:31,497 ***** Running evaluation *****
2022-06-12 02:25:31,498   Epoch = 49 iter 13239 step
2022-06-12 02:25:31,498   Num examples = 1043
2022-06-12 02:25:31,498   Batch size = 32
2022-06-12 02:25:31,499 ***** Eval results *****
2022-06-12 02:25:31,499   att_loss = 0.44586046918844563
2022-06-12 02:25:31,499   global_step = 13239
2022-06-12 02:25:31,499   loss = 1.1943600536921086
2022-06-12 02:25:31,499   rep_loss = 0.7484995829753387
2022-06-12 02:25:31,499 ***** Save model *****
2022-06-12 02:25:36,599 ***** Running evaluation *****
2022-06-12 02:25:36,600   Epoch = 6 iter 21999 step
2022-06-12 02:25:36,600   Num examples = 5463
2022-06-12 02:25:36,600   Batch size = 32
2022-06-12 02:25:36,601 ***** Eval results *****
2022-06-12 02:25:36,601   att_loss = 3.6785173229320933
2022-06-12 02:25:36,601   global_step = 21999
2022-06-12 02:25:36,601   loss = 4.56615641156479
2022-06-12 02:25:36,602   rep_loss = 0.8876390853002888
2022-06-12 02:25:36,602 ***** Save model *****
2022-06-12 02:25:37,158 ***** Running evaluation *****
2022-06-12 02:25:37,159   Epoch = 49 iter 13259 step
2022-06-12 02:25:37,159   Num examples = 1043
2022-06-12 02:25:37,159   Batch size = 32
2022-06-12 02:25:37,160 ***** Eval results *****
2022-06-12 02:25:37,160   att_loss = 0.446217429400845
2022-06-12 02:25:37,160   global_step = 13259
2022-06-12 02:25:37,160   loss = 1.1953872374512933
2022-06-12 02:25:37,160   rep_loss = 0.7491698072037913
2022-06-12 02:25:37,160 ***** Save model *****
2022-06-12 02:25:42,838 ***** Running evaluation *****
2022-06-12 02:25:42,838   Epoch = 49 iter 13279 step
2022-06-12 02:25:42,838   Num examples = 1043
2022-06-12 02:25:42,838   Batch size = 32
2022-06-12 02:25:42,839 ***** Eval results *****
2022-06-12 02:25:42,839   att_loss = 0.442857276420204
2022-06-12 02:25:42,839   global_step = 13279
2022-06-12 02:25:42,839   loss = 1.1905997967233464
2022-06-12 02:25:42,839   rep_loss = 0.7477425196949317
2022-06-12 02:25:42,840 ***** Save model *****
2022-06-12 02:25:48,492 ***** Running evaluation *****
2022-06-12 02:25:48,493   Epoch = 49 iter 13299 step
2022-06-12 02:25:48,493   Num examples = 1043
2022-06-12 02:25:48,493   Batch size = 32
2022-06-12 02:25:48,494 ***** Eval results *****
2022-06-12 02:25:48,494   att_loss = 0.44303095782244645
2022-06-12 02:25:48,494   global_step = 13299
2022-06-12 02:25:48,494   loss = 1.1908900715686657
2022-06-12 02:25:48,494   rep_loss = 0.7478591145740615
2022-06-12 02:25:48,494 ***** Save model *****
2022-06-12 02:25:54,164 ***** Running evaluation *****
2022-06-12 02:25:54,164   Epoch = 49 iter 13319 step
2022-06-12 02:25:54,164   Num examples = 1043
2022-06-12 02:25:54,165   Batch size = 32
2022-06-12 02:25:54,166 ***** Eval results *****
2022-06-12 02:25:54,167   att_loss = 0.442837193608284
2022-06-12 02:25:54,167   global_step = 13319
2022-06-12 02:25:54,167   loss = 1.1900826359199266
2022-06-12 02:25:54,167   rep_loss = 0.7472454423116426
2022-06-12 02:25:54,167 ***** Save model *****
2022-06-12 02:25:59,811 ***** Running evaluation *****
2022-06-12 02:25:59,811   Epoch = 49 iter 13339 step
2022-06-12 02:25:59,811   Num examples = 1043
2022-06-12 02:25:59,811   Batch size = 32
2022-06-12 02:25:59,812 ***** Eval results *****
2022-06-12 02:25:59,813   att_loss = 0.4434452465502545
2022-06-12 02:25:59,813   global_step = 13339
2022-06-12 02:25:59,813   loss = 1.1908469735644758
2022-06-12 02:25:59,813   rep_loss = 0.7474017264321446
2022-06-12 02:25:59,813 ***** Save model *****
2022-06-12 02:26:03,139 Task finish! 
2022-06-12 02:26:03,140 Task cost 63.30274875 minutes, i.e. 1.0550458166666667 hours. 
2022-06-12 02:26:05,277 Task start! 
2022-06-12 02:26:05,299 device: cuda n_gpu: 1
2022-06-12 02:26:05,300 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/CoLA', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=64, max_seq_length=128, no_cuda=False, num_train_epochs=30, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/cola/on_original_data', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/cola/on_original_data', task_name='cola', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/cola/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/cola/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 02:26:05,343 Writing example 0 of 8551
2022-06-12 02:26:05,343 *** Example ***
2022-06-12 02:26:05,343 guid: train-0
2022-06-12 02:26:05,343 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2022-06-12 02:26:05,343 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 02:26:05,344 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 02:26:05,344 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 02:26:05,344 label: 1
2022-06-12 02:26:05,344 label_id: 1
2022-06-12 02:26:06,758 Writing example 0 of 1043
2022-06-12 02:26:06,759 *** Example ***
2022-06-12 02:26:06,759 guid: dev-0
2022-06-12 02:26:06,759 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2022-06-12 02:26:06,759 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 02:26:06,759 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 02:26:06,759 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 02:26:06,759 label: 1
2022-06-12 02:26:06,759 label_id: 1
2022-06-12 02:26:06,927 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 02:26:12,349 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/cola/on_original_data/pytorch_model.bin
2022-06-12 02:26:12,974 loading model...
2022-06-12 02:26:13,229 done!
2022-06-12 02:26:13,230 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.dense_fit.weight', 'bert.dense_fit.bias']
2022-06-12 02:26:16,529 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 02:26:17,628 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/cola/on_original_data/pytorch_model.bin
2022-06-12 02:26:17,763 loading model...
2022-06-12 02:26:17,792 done!
2022-06-12 02:26:19,185 ***** Running training *****
2022-06-12 02:26:19,201   Num examples = 8551
2022-06-12 02:26:19,207   Batch size = 32
2022-06-12 02:26:19,207   Num steps = 8010
2022-06-12 02:26:19,207 n: bert.embeddings.word_embeddings.weight
2022-06-12 02:26:19,213 n: bert.embeddings.position_embeddings.weight
2022-06-12 02:26:19,228 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 02:26:19,244 n: bert.embeddings.LayerNorm.weight
2022-06-12 02:26:19,251 n: bert.embeddings.LayerNorm.bias
2022-06-12 02:26:19,252 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 02:26:19,252 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 02:26:19,252 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 02:26:19,252 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 02:26:19,252 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 02:26:19,252 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 02:26:19,252 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 02:26:19,253 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 02:26:19,253 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 02:26:19,253 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 02:26:19,253 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 02:26:19,253 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 02:26:19,253 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 02:26:19,253 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 02:26:19,254 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 02:26:19,254 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 02:26:19,254 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 02:26:19,254 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 02:26:19,254 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 02:26:19,254 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 02:26:19,254 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 02:26:19,255 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 02:26:19,256 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 02:26:19,256 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 02:26:19,256 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 02:26:19,256 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 02:26:19,256 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 02:26:19,256 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 02:26:19,256 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 02:26:19,256 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 02:26:19,256 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 02:26:19,256 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 02:26:19,257 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 02:26:19,257 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 02:26:19,257 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 02:26:19,257 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 02:26:19,257 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 02:26:19,257 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 02:26:19,257 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 02:26:19,257 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 02:26:19,258 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 02:26:19,259 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 02:26:19,259 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 02:26:19,259 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 02:26:19,259 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 02:26:19,259 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 02:26:19,259 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 02:26:19,259 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 02:26:19,259 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 02:26:19,260 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 02:26:19,260 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 02:26:19,260 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 02:26:19,260 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 02:26:19,260 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 02:26:19,260 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 02:26:19,260 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 02:26:19,260 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 02:26:19,261 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 02:26:19,261 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 02:26:19,261 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 02:26:19,261 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 02:26:19,261 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 02:26:19,261 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 02:26:19,261 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 02:26:19,261 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 02:26:19,262 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 02:26:19,263 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 02:26:19,263 n: bert.pooler.dense.weight
2022-06-12 02:26:19,263 n: bert.pooler.dense.bias
2022-06-12 02:26:19,263 n: classifier.weight
2022-06-12 02:26:19,263 n: classifier.bias
2022-06-12 02:26:19,263 n: fit_denses.0.weight
2022-06-12 02:26:19,263 n: fit_denses.0.bias
2022-06-12 02:26:19,263 n: fit_denses.1.weight
2022-06-12 02:26:19,263 n: fit_denses.1.bias
2022-06-12 02:26:19,263 n: fit_denses.2.weight
2022-06-12 02:26:19,263 n: fit_denses.2.bias
2022-06-12 02:26:19,263 n: fit_denses.3.weight
2022-06-12 02:26:19,263 n: fit_denses.3.bias
2022-06-12 02:26:19,263 n: fit_denses.4.weight
2022-06-12 02:26:19,263 n: fit_denses.4.bias
2022-06-12 02:26:19,263 n: fit_denses.5.weight
2022-06-12 02:26:19,263 n: fit_denses.5.bias
2022-06-12 02:26:19,263 n: fit_denses.6.weight
2022-06-12 02:26:19,263 n: fit_denses.6.bias
2022-06-12 02:26:19,263 Total parameters: 72468738
2022-06-12 02:26:23,929 ***** Running evaluation *****
2022-06-12 02:26:23,930   Epoch = 0 iter 19 step
2022-06-12 02:26:23,930   Num examples = 1043
2022-06-12 02:26:23,930   Batch size = 32
2022-06-12 02:26:24,832 ***** Eval results *****
2022-06-12 02:26:24,833   cls_loss = 0.3376821342267488
2022-06-12 02:26:24,833   eval_loss = 0.6729266950578401
2022-06-12 02:26:24,833   global_step = 19
2022-06-12 02:26:24,833   loss = 0.3376821342267488
2022-06-12 02:26:24,833   mcc = 0.24189979918651575
2022-06-12 02:26:24,833 ***** Save model *****
2022-06-12 02:26:30,143 ***** Running evaluation *****
2022-06-12 02:26:30,143   Epoch = 0 iter 39 step
2022-06-12 02:26:30,143   Num examples = 1043
2022-06-12 02:26:30,143   Batch size = 32
2022-06-12 02:26:31,048 ***** Eval results *****
2022-06-12 02:26:31,048   cls_loss = 0.31466552462333286
2022-06-12 02:26:31,048   eval_loss = 0.6167597499760714
2022-06-12 02:26:31,048   global_step = 39
2022-06-12 02:26:31,048   loss = 0.31466552462333286
2022-06-12 02:26:31,048   mcc = 0.2197981688854891
2022-06-12 02:26:35,928 ***** Running evaluation *****
2022-06-12 02:26:35,929   Epoch = 0 iter 59 step
2022-06-12 02:26:35,929   Num examples = 1043
2022-06-12 02:26:35,929   Batch size = 32
2022-06-12 02:26:36,824 ***** Eval results *****
2022-06-12 02:26:36,825   cls_loss = 0.27874481299165954
2022-06-12 02:26:36,825   eval_loss = 0.6129227768291127
2022-06-12 02:26:36,825   global_step = 59
2022-06-12 02:26:36,825   loss = 0.27874481299165954
2022-06-12 02:26:36,825   mcc = 0.2349193442362663
2022-06-12 02:26:41,690 ***** Running evaluation *****
2022-06-12 02:26:41,691   Epoch = 0 iter 79 step
2022-06-12 02:26:41,691   Num examples = 1043
2022-06-12 02:26:41,691   Batch size = 32
2022-06-12 02:26:42,588 ***** Eval results *****
2022-06-12 02:26:42,589   cls_loss = 0.2436472459495822
2022-06-12 02:26:42,589   eval_loss = 0.6854134572274757
2022-06-12 02:26:42,589   global_step = 79
2022-06-12 02:26:42,589   loss = 0.2436472459495822
2022-06-12 02:26:42,589   mcc = 0.219443785956581
2022-06-12 02:26:47,465 ***** Running evaluation *****
2022-06-12 02:26:47,466   Epoch = 0 iter 99 step
2022-06-12 02:26:47,466   Num examples = 1043
2022-06-12 02:26:47,466   Batch size = 32
2022-06-12 02:26:48,364 ***** Eval results *****
2022-06-12 02:26:48,364   cls_loss = 0.21604152016266429
2022-06-12 02:26:48,364   eval_loss = 0.7774875913605546
2022-06-12 02:26:48,365   global_step = 99
2022-06-12 02:26:48,365   loss = 0.21604152016266429
2022-06-12 02:26:48,365   mcc = 0.23852407549208424
2022-06-12 02:26:53,241 ***** Running evaluation *****
2022-06-12 02:26:53,242   Epoch = 0 iter 119 step
2022-06-12 02:26:53,242   Num examples = 1043
2022-06-12 02:26:53,242   Batch size = 32
2022-06-12 02:26:54,139 ***** Eval results *****
2022-06-12 02:26:54,139   cls_loss = 0.19597951034788325
2022-06-12 02:26:54,139   eval_loss = 0.8633452021714413
2022-06-12 02:26:54,140   global_step = 119
2022-06-12 02:26:54,140   loss = 0.19597951034788325
2022-06-12 02:26:54,140   mcc = 0.23132304745386884
2022-06-12 02:26:58,995 ***** Running evaluation *****
2022-06-12 02:26:58,995   Epoch = 0 iter 139 step
2022-06-12 02:26:58,995   Num examples = 1043
2022-06-12 02:26:58,995   Batch size = 32
2022-06-12 02:26:59,892 ***** Eval results *****
2022-06-12 02:26:59,892   cls_loss = 0.18139327172752764
2022-06-12 02:26:59,892   eval_loss = 0.8800165842879902
2022-06-12 02:26:59,892   global_step = 139
2022-06-12 02:26:59,892   loss = 0.18139327172752764
2022-06-12 02:26:59,893   mcc = 0.2510121036697612
2022-06-12 02:26:59,893 ***** Save model *****
2022-06-12 02:27:05,107 ***** Running evaluation *****
2022-06-12 02:27:05,107   Epoch = 0 iter 159 step
2022-06-12 02:27:05,107   Num examples = 1043
2022-06-12 02:27:05,107   Batch size = 32
2022-06-12 02:27:06,007 ***** Eval results *****
2022-06-12 02:27:06,007   cls_loss = 0.17018138767225938
2022-06-12 02:27:06,007   eval_loss = 0.9118612616351156
2022-06-12 02:27:06,007   global_step = 159
2022-06-12 02:27:06,007   loss = 0.17018138767225938
2022-06-12 02:27:06,007   mcc = 0.22455773485458155
2022-06-12 02:27:10,881 ***** Running evaluation *****
2022-06-12 02:27:10,882   Epoch = 0 iter 179 step
2022-06-12 02:27:10,882   Num examples = 1043
2022-06-12 02:27:10,882   Batch size = 32
2022-06-12 02:27:11,783 ***** Eval results *****
2022-06-12 02:27:11,783   cls_loss = 0.16113501520796195
2022-06-12 02:27:11,783   eval_loss = 1.023756245772044
2022-06-12 02:27:11,784   global_step = 179
2022-06-12 02:27:11,784   loss = 0.16113501520796195
2022-06-12 02:27:11,784   mcc = 0.24044801103787392
2022-06-12 02:27:16,641 ***** Running evaluation *****
2022-06-12 02:27:16,641   Epoch = 0 iter 199 step
2022-06-12 02:27:16,642   Num examples = 1043
2022-06-12 02:27:16,642   Batch size = 32
2022-06-12 02:27:17,540 ***** Eval results *****
2022-06-12 02:27:17,540   cls_loss = 0.15484782237203876
2022-06-12 02:27:17,540   eval_loss = 0.8719347616036733
2022-06-12 02:27:17,540   global_step = 199
2022-06-12 02:27:17,540   loss = 0.15484782237203876
2022-06-12 02:27:17,540   mcc = 0.26043937109499965
2022-06-12 02:27:17,541 ***** Save model *****
2022-06-12 02:27:22,771 ***** Running evaluation *****
2022-06-12 02:27:22,772   Epoch = 0 iter 219 step
2022-06-12 02:27:22,772   Num examples = 1043
2022-06-12 02:27:22,772   Batch size = 32
2022-06-12 02:27:23,673 ***** Eval results *****
2022-06-12 02:27:23,673   cls_loss = 0.14970811677577833
2022-06-12 02:27:23,673   eval_loss = 0.993119351791613
2022-06-12 02:27:23,674   global_step = 219
2022-06-12 02:27:23,674   loss = 0.14970811677577833
2022-06-12 02:27:23,674   mcc = 0.24768562492907278
2022-06-12 02:27:28,556 ***** Running evaluation *****
2022-06-12 02:27:28,557   Epoch = 0 iter 239 step
2022-06-12 02:27:28,557   Num examples = 1043
2022-06-12 02:27:28,557   Batch size = 32
2022-06-12 02:27:29,457 ***** Eval results *****
2022-06-12 02:27:29,457   cls_loss = 0.14484911645805487
2022-06-12 02:27:29,457   eval_loss = 0.9449087616169092
2022-06-12 02:27:29,457   global_step = 239
2022-06-12 02:27:29,457   loss = 0.14484911645805487
2022-06-12 02:27:29,457   mcc = 0.2402481538264528
2022-06-12 02:27:34,326 ***** Running evaluation *****
2022-06-12 02:27:34,327   Epoch = 0 iter 259 step
2022-06-12 02:27:34,327   Num examples = 1043
2022-06-12 02:27:34,327   Batch size = 32
2022-06-12 02:27:35,226 ***** Eval results *****
2022-06-12 02:27:35,226   cls_loss = 0.14055339586435597
2022-06-12 02:27:35,226   eval_loss = 0.9183059632778168
2022-06-12 02:27:35,227   global_step = 259
2022-06-12 02:27:35,227   loss = 0.14055339586435597
2022-06-12 02:27:35,227   mcc = 0.2386118260973925
2022-06-12 02:27:40,084 ***** Running evaluation *****
2022-06-12 02:27:40,084   Epoch = 1 iter 279 step
2022-06-12 02:27:40,084   Num examples = 1043
2022-06-12 02:27:40,084   Batch size = 32
2022-06-12 02:27:40,983 ***** Eval results *****
2022-06-12 02:27:40,983   cls_loss = 0.09514677400390308
2022-06-12 02:27:40,984   eval_loss = 0.910400699485432
2022-06-12 02:27:40,984   global_step = 279
2022-06-12 02:27:40,984   loss = 0.09514677400390308
2022-06-12 02:27:40,984   mcc = 0.24760683644544895
2022-06-12 02:27:44,416 ***** Running evaluation *****
2022-06-12 02:27:44,417   Epoch = 6 iter 22499 step
2022-06-12 02:27:44,417   Num examples = 5463
2022-06-12 02:27:44,417   Batch size = 32
2022-06-12 02:27:44,418 ***** Eval results *****
2022-06-12 02:27:44,418   att_loss = 3.6783807771016734
2022-06-12 02:27:44,418   global_step = 22499
2022-06-12 02:27:44,418   loss = 4.565585397749004
2022-06-12 02:27:44,418   rep_loss = 0.8872046167514659
2022-06-12 02:27:44,418 ***** Save model *****
2022-06-12 02:27:45,839 ***** Running evaluation *****
2022-06-12 02:27:45,839   Epoch = 1 iter 299 step
2022-06-12 02:27:45,839   Num examples = 1043
2022-06-12 02:27:45,839   Batch size = 32
2022-06-12 02:27:46,739 ***** Eval results *****
2022-06-12 02:27:46,739   cls_loss = 0.09875579341314733
2022-06-12 02:27:46,739   eval_loss = 0.9707672130880933
2022-06-12 02:27:46,739   global_step = 299
2022-06-12 02:27:46,739   loss = 0.09875579341314733
2022-06-12 02:27:46,739   mcc = 0.24217809031805668
2022-06-12 02:27:51,603 ***** Running evaluation *****
2022-06-12 02:27:51,603   Epoch = 1 iter 319 step
2022-06-12 02:27:51,604   Num examples = 1043
2022-06-12 02:27:51,604   Batch size = 32
2022-06-12 02:27:52,503 ***** Eval results *****
2022-06-12 02:27:52,503   cls_loss = 0.09300244241379775
2022-06-12 02:27:52,503   eval_loss = 1.0044861038525899
2022-06-12 02:27:52,503   global_step = 319
2022-06-12 02:27:52,503   loss = 0.09300244241379775
2022-06-12 02:27:52,503   mcc = 0.24641684731685584
2022-06-12 02:27:57,377 ***** Running evaluation *****
2022-06-12 02:27:57,377   Epoch = 1 iter 339 step
2022-06-12 02:27:57,377   Num examples = 1043
2022-06-12 02:27:57,377   Batch size = 32
2022-06-12 02:27:58,276 ***** Eval results *****
2022-06-12 02:27:58,276   cls_loss = 0.09459562589310938
2022-06-12 02:27:58,276   eval_loss = 0.9255166478229292
2022-06-12 02:27:58,276   global_step = 339
2022-06-12 02:27:58,276   loss = 0.09459562589310938
2022-06-12 02:27:58,276   mcc = 0.21273327075504264
2022-06-12 02:28:03,150 ***** Running evaluation *****
2022-06-12 02:28:03,150   Epoch = 1 iter 359 step
2022-06-12 02:28:03,151   Num examples = 1043
2022-06-12 02:28:03,151   Batch size = 32
2022-06-12 02:28:04,050 ***** Eval results *****
2022-06-12 02:28:04,051   cls_loss = 0.0943299584576617
2022-06-12 02:28:04,051   eval_loss = 0.9483070165822001
2022-06-12 02:28:04,051   global_step = 359
2022-06-12 02:28:04,051   loss = 0.0943299584576617
2022-06-12 02:28:04,051   mcc = 0.24928905262221102
2022-06-12 02:28:08,918 ***** Running evaluation *****
2022-06-12 02:28:08,919   Epoch = 1 iter 379 step
2022-06-12 02:28:08,919   Num examples = 1043
2022-06-12 02:28:08,919   Batch size = 32
2022-06-12 02:28:09,818 ***** Eval results *****
2022-06-12 02:28:09,818   cls_loss = 0.09389942472002336
2022-06-12 02:28:09,818   eval_loss = 0.9815309544404348
2022-06-12 02:28:09,818   global_step = 379
2022-06-12 02:28:09,818   loss = 0.09389942472002336
2022-06-12 02:28:09,818   mcc = 0.24097172200238098
2022-06-12 02:28:14,735 ***** Running evaluation *****
2022-06-12 02:28:14,735   Epoch = 1 iter 399 step
2022-06-12 02:28:14,735   Num examples = 1043
2022-06-12 02:28:14,736   Batch size = 32
2022-06-12 02:28:15,639 ***** Eval results *****
2022-06-12 02:28:15,640   cls_loss = 0.09269328522637035
2022-06-12 02:28:15,640   eval_loss = 0.9493022118553971
2022-06-12 02:28:15,640   global_step = 399
2022-06-12 02:28:15,640   loss = 0.09269328522637035
2022-06-12 02:28:15,640   mcc = 0.25038743653153106
2022-06-12 02:28:20,536 ***** Running evaluation *****
2022-06-12 02:28:20,536   Epoch = 1 iter 419 step
2022-06-12 02:28:20,536   Num examples = 1043
2022-06-12 02:28:20,536   Batch size = 32
2022-06-12 02:28:21,439 ***** Eval results *****
2022-06-12 02:28:21,439   cls_loss = 0.09166555927674237
2022-06-12 02:28:21,439   eval_loss = 0.9677514984752192
2022-06-12 02:28:21,440   global_step = 419
2022-06-12 02:28:21,440   loss = 0.09166555927674237
2022-06-12 02:28:21,440   mcc = 0.22109826660542278
2022-06-12 02:28:26,343 ***** Running evaluation *****
2022-06-12 02:28:26,343   Epoch = 1 iter 439 step
2022-06-12 02:28:26,343   Num examples = 1043
2022-06-12 02:28:26,343   Batch size = 32
2022-06-12 02:28:27,250 ***** Eval results *****
2022-06-12 02:28:27,250   cls_loss = 0.09106670842007843
2022-06-12 02:28:27,250   eval_loss = 0.9820107099684802
2022-06-12 02:28:27,250   global_step = 439
2022-06-12 02:28:27,251   loss = 0.09106670842007843
2022-06-12 02:28:27,251   mcc = 0.23796523010251527
2022-06-12 02:28:32,131 ***** Running evaluation *****
2022-06-12 02:28:32,132   Epoch = 1 iter 459 step
2022-06-12 02:28:32,132   Num examples = 1043
2022-06-12 02:28:32,132   Batch size = 32
2022-06-12 02:28:33,033 ***** Eval results *****
2022-06-12 02:28:33,033   cls_loss = 0.09095798981919263
2022-06-12 02:28:33,033   eval_loss = 0.8968871904141975
2022-06-12 02:28:33,033   global_step = 459
2022-06-12 02:28:33,033   loss = 0.09095798981919263
2022-06-12 02:28:33,033   mcc = 0.2418049162564047
2022-06-12 02:28:37,916 ***** Running evaluation *****
2022-06-12 02:28:37,916   Epoch = 1 iter 479 step
2022-06-12 02:28:37,916   Num examples = 1043
2022-06-12 02:28:37,917   Batch size = 32
2022-06-12 02:28:38,819 ***** Eval results *****
2022-06-12 02:28:38,819   cls_loss = 0.09109739662271063
2022-06-12 02:28:38,819   eval_loss = 0.9517214122143659
2022-06-12 02:28:38,819   global_step = 479
2022-06-12 02:28:38,819   loss = 0.09109739662271063
2022-06-12 02:28:38,819   mcc = 0.24841689010415244
2022-06-12 02:28:43,690 ***** Running evaluation *****
2022-06-12 02:28:43,691   Epoch = 1 iter 499 step
2022-06-12 02:28:43,691   Num examples = 1043
2022-06-12 02:28:43,691   Batch size = 32
2022-06-12 02:28:44,592 ***** Eval results *****
2022-06-12 02:28:44,593   cls_loss = 0.09205763759733789
2022-06-12 02:28:44,593   eval_loss = 0.9751604488401702
2022-06-12 02:28:44,593   global_step = 499
2022-06-12 02:28:44,593   loss = 0.09205763759733789
2022-06-12 02:28:44,593   mcc = 0.19626188306526265
2022-06-12 02:28:49,477 ***** Running evaluation *****
2022-06-12 02:28:49,477   Epoch = 1 iter 519 step
2022-06-12 02:28:49,478   Num examples = 1043
2022-06-12 02:28:49,478   Batch size = 32
2022-06-12 02:28:50,379 ***** Eval results *****
2022-06-12 02:28:50,379   cls_loss = 0.09232752892883524
2022-06-12 02:28:50,379   eval_loss = 0.9645586207960591
2022-06-12 02:28:50,380   global_step = 519
2022-06-12 02:28:50,380   loss = 0.09232752892883524
2022-06-12 02:28:50,380   mcc = 0.2232120080883875
2022-06-12 02:28:55,303 ***** Running evaluation *****
2022-06-12 02:28:55,304   Epoch = 2 iter 539 step
2022-06-12 02:28:55,304   Num examples = 1043
2022-06-12 02:28:55,304   Batch size = 32
2022-06-12 02:28:56,205 ***** Eval results *****
2022-06-12 02:28:56,206   cls_loss = 0.08546162694692612
2022-06-12 02:28:56,206   eval_loss = 0.8922581952629667
2022-06-12 02:28:56,206   global_step = 539
2022-06-12 02:28:56,206   loss = 0.08546162694692612
2022-06-12 02:28:56,206   mcc = 0.22109826660542278
2022-06-12 02:29:01,088 ***** Running evaluation *****
2022-06-12 02:29:01,088   Epoch = 2 iter 559 step
2022-06-12 02:29:01,088   Num examples = 1043
2022-06-12 02:29:01,089   Batch size = 32
2022-06-12 02:29:01,991 ***** Eval results *****
2022-06-12 02:29:01,992   cls_loss = 0.09053104937076568
2022-06-12 02:29:01,992   eval_loss = 0.9511200704357841
2022-06-12 02:29:01,992   global_step = 559
2022-06-12 02:29:01,992   loss = 0.09053104937076568
2022-06-12 02:29:01,992   mcc = 0.22023249509791615
2022-06-12 02:29:06,880 ***** Running evaluation *****
2022-06-12 02:29:06,881   Epoch = 2 iter 579 step
2022-06-12 02:29:06,881   Num examples = 1043
2022-06-12 02:29:06,881   Batch size = 32
2022-06-12 02:29:07,782 ***** Eval results *****
2022-06-12 02:29:07,782   cls_loss = 0.0906691677040524
2022-06-12 02:29:07,782   eval_loss = 1.0080436143008145
2022-06-12 02:29:07,782   global_step = 579
2022-06-12 02:29:07,782   loss = 0.0906691677040524
2022-06-12 02:29:07,782   mcc = 0.22668951469998705
2022-06-12 02:29:12,668 ***** Running evaluation *****
2022-06-12 02:29:12,668   Epoch = 2 iter 599 step
2022-06-12 02:29:12,668   Num examples = 1043
2022-06-12 02:29:12,668   Batch size = 32
2022-06-12 02:29:13,569 ***** Eval results *****
2022-06-12 02:29:13,570   cls_loss = 0.09350756246310014
2022-06-12 02:29:13,570   eval_loss = 0.8674311488866806
2022-06-12 02:29:13,570   global_step = 599
2022-06-12 02:29:13,570   loss = 0.09350756246310014
2022-06-12 02:29:13,570   mcc = 0.2688217002795981
2022-06-12 02:29:13,570 ***** Save model *****
2022-06-12 02:29:18,854 ***** Running evaluation *****
2022-06-12 02:29:18,855   Epoch = 2 iter 619 step
2022-06-12 02:29:18,855   Num examples = 1043
2022-06-12 02:29:18,855   Batch size = 32
2022-06-12 02:29:19,755 ***** Eval results *****
2022-06-12 02:29:19,755   cls_loss = 0.09440595034290762
2022-06-12 02:29:19,756   eval_loss = 0.9175854054364291
2022-06-12 02:29:19,756   global_step = 619
2022-06-12 02:29:19,756   loss = 0.09440595034290762
2022-06-12 02:29:19,756   mcc = 0.2428581504486696
2022-06-12 02:29:24,651 ***** Running evaluation *****
2022-06-12 02:29:24,651   Epoch = 2 iter 639 step
2022-06-12 02:29:24,651   Num examples = 1043
2022-06-12 02:29:24,652   Batch size = 32
2022-06-12 02:29:25,554 ***** Eval results *****
2022-06-12 02:29:25,555   cls_loss = 0.09462509403626124
2022-06-12 02:29:25,555   eval_loss = 0.9615899938525576
2022-06-12 02:29:25,555   global_step = 639
2022-06-12 02:29:25,555   loss = 0.09462509403626124
2022-06-12 02:29:25,555   mcc = 0.2517596224880464
2022-06-12 02:29:30,451 ***** Running evaluation *****
2022-06-12 02:29:30,451   Epoch = 2 iter 659 step
2022-06-12 02:29:30,451   Num examples = 1043
2022-06-12 02:29:30,451   Batch size = 32
2022-06-12 02:29:31,352 ***** Eval results *****
2022-06-12 02:29:31,352   cls_loss = 0.09523410695791244
2022-06-12 02:29:31,352   eval_loss = 0.8736716346307234
2022-06-12 02:29:31,352   global_step = 659
2022-06-12 02:29:31,352   loss = 0.09523410695791244
2022-06-12 02:29:31,352   mcc = 0.24400187292688222
2022-06-12 02:29:36,242 ***** Running evaluation *****
2022-06-12 02:29:36,242   Epoch = 2 iter 679 step
2022-06-12 02:29:36,242   Num examples = 1043
2022-06-12 02:29:36,243   Batch size = 32
2022-06-12 02:29:37,143 ***** Eval results *****
2022-06-12 02:29:37,144   cls_loss = 0.09521908790900789
2022-06-12 02:29:37,144   eval_loss = 0.9368478113954718
2022-06-12 02:29:37,144   global_step = 679
2022-06-12 02:29:37,144   loss = 0.09521908790900789
2022-06-12 02:29:37,144   mcc = 0.23791661651885057
2022-06-12 02:29:42,036 ***** Running evaluation *****
2022-06-12 02:29:42,036   Epoch = 2 iter 699 step
2022-06-12 02:29:42,036   Num examples = 1043
2022-06-12 02:29:42,036   Batch size = 32
2022-06-12 02:29:42,936 ***** Eval results *****
2022-06-12 02:29:42,936   cls_loss = 0.09557828022675081
2022-06-12 02:29:42,936   eval_loss = 0.9627218354832042
2022-06-12 02:29:42,936   global_step = 699
2022-06-12 02:29:42,937   loss = 0.09557828022675081
2022-06-12 02:29:42,937   mcc = 0.21776622523773148
2022-06-12 02:29:47,824 ***** Running evaluation *****
2022-06-12 02:29:47,825   Epoch = 2 iter 719 step
2022-06-12 02:29:47,825   Num examples = 1043
2022-06-12 02:29:47,825   Batch size = 32
2022-06-12 02:29:48,724 ***** Eval results *****
2022-06-12 02:29:48,724   cls_loss = 0.09544968798353866
2022-06-12 02:29:48,724   eval_loss = 0.8968067214344487
2022-06-12 02:29:48,724   global_step = 719
2022-06-12 02:29:48,725   loss = 0.09544968798353866
2022-06-12 02:29:48,725   mcc = 0.24613785475222064
2022-06-12 02:29:52,223 ***** Running evaluation *****
2022-06-12 02:29:52,223   Epoch = 7 iter 22999 step
2022-06-12 02:29:52,223   Num examples = 5463
2022-06-12 02:29:52,223   Batch size = 32
2022-06-12 02:29:52,224 ***** Eval results *****
2022-06-12 02:29:52,224   att_loss = 3.600999092513865
2022-06-12 02:29:52,224   global_step = 22999
2022-06-12 02:29:52,224   loss = 4.482955333861438
2022-06-12 02:29:52,225   rep_loss = 0.8819562352516435
2022-06-12 02:29:52,225 ***** Save model *****
2022-06-12 02:29:53,600 ***** Running evaluation *****
2022-06-12 02:29:53,600   Epoch = 2 iter 739 step
2022-06-12 02:29:53,600   Num examples = 1043
2022-06-12 02:29:53,600   Batch size = 32
2022-06-12 02:29:54,502 ***** Eval results *****
2022-06-12 02:29:54,502   cls_loss = 0.09648901758397498
2022-06-12 02:29:54,502   eval_loss = 0.8766010864214464
2022-06-12 02:29:54,502   global_step = 739
2022-06-12 02:29:54,503   loss = 0.09648901758397498
2022-06-12 02:29:54,503   mcc = 0.23074468675376475
2022-06-12 02:29:59,393 ***** Running evaluation *****
2022-06-12 02:29:59,393   Epoch = 2 iter 759 step
2022-06-12 02:29:59,393   Num examples = 1043
2022-06-12 02:29:59,393   Batch size = 32
2022-06-12 02:30:00,294 ***** Eval results *****
2022-06-12 02:30:00,294   cls_loss = 0.09642547504769432
2022-06-12 02:30:00,294   eval_loss = 0.9392174699089744
2022-06-12 02:30:00,295   global_step = 759
2022-06-12 02:30:00,295   loss = 0.09642547504769432
2022-06-12 02:30:00,295   mcc = 0.230955087667665
2022-06-12 02:30:05,184 ***** Running evaluation *****
2022-06-12 02:30:05,185   Epoch = 2 iter 779 step
2022-06-12 02:30:05,185   Num examples = 1043
2022-06-12 02:30:05,185   Batch size = 32
2022-06-12 02:30:06,086 ***** Eval results *****
2022-06-12 02:30:06,086   cls_loss = 0.09704449407908382
2022-06-12 02:30:06,086   eval_loss = 0.9077658653259277
2022-06-12 02:30:06,086   global_step = 779
2022-06-12 02:30:06,086   loss = 0.09704449407908382
2022-06-12 02:30:06,086   mcc = 0.2451849319983816
2022-06-12 02:30:10,974 ***** Running evaluation *****
2022-06-12 02:30:10,974   Epoch = 2 iter 799 step
2022-06-12 02:30:10,974   Num examples = 1043
2022-06-12 02:30:10,975   Batch size = 32
2022-06-12 02:30:11,873 ***** Eval results *****
2022-06-12 02:30:11,873   cls_loss = 0.09722686879477412
2022-06-12 02:30:11,874   eval_loss = 0.8942189270799811
2022-06-12 02:30:11,874   global_step = 799
2022-06-12 02:30:11,874   loss = 0.09722686879477412
2022-06-12 02:30:11,874   mcc = 0.22782619503138338
2022-06-12 02:30:16,769 ***** Running evaluation *****
2022-06-12 02:30:16,770   Epoch = 3 iter 819 step
2022-06-12 02:30:16,770   Num examples = 1043
2022-06-12 02:30:16,770   Batch size = 32
2022-06-12 02:30:17,669 ***** Eval results *****
2022-06-12 02:30:17,669   cls_loss = 0.09070226301749547
2022-06-12 02:30:17,670   eval_loss = 0.9673196239904924
2022-06-12 02:30:17,670   global_step = 819
2022-06-12 02:30:17,670   loss = 0.09070226301749547
2022-06-12 02:30:17,670   mcc = 0.19700449767696263
2022-06-12 02:30:22,533 ***** Running evaluation *****
2022-06-12 02:30:22,534   Epoch = 3 iter 839 step
2022-06-12 02:30:22,534   Num examples = 1043
2022-06-12 02:30:22,534   Batch size = 32
2022-06-12 02:30:23,435 ***** Eval results *****
2022-06-12 02:30:23,435   cls_loss = 0.09247526918586932
2022-06-12 02:30:23,435   eval_loss = 0.8904428319497542
2022-06-12 02:30:23,435   global_step = 839
2022-06-12 02:30:23,435   loss = 0.09247526918586932
2022-06-12 02:30:23,435   mcc = 0.17560878217535503
2022-06-12 02:30:28,345 ***** Running evaluation *****
2022-06-12 02:30:28,345   Epoch = 3 iter 859 step
2022-06-12 02:30:28,346   Num examples = 1043
2022-06-12 02:30:28,346   Batch size = 32
2022-06-12 02:30:29,250 ***** Eval results *****
2022-06-12 02:30:29,251   cls_loss = 0.09467896009827483
2022-06-12 02:30:29,251   eval_loss = 0.9723380975651018
2022-06-12 02:30:29,251   global_step = 859
2022-06-12 02:30:29,251   loss = 0.09467896009827483
2022-06-12 02:30:29,251   mcc = 0.22596991300153274
2022-06-12 02:30:34,167 ***** Running evaluation *****
2022-06-12 02:30:34,167   Epoch = 3 iter 879 step
2022-06-12 02:30:34,167   Num examples = 1043
2022-06-12 02:30:34,167   Batch size = 32
2022-06-12 02:30:35,068 ***** Eval results *****
2022-06-12 02:30:35,069   cls_loss = 0.09700333795104271
2022-06-12 02:30:35,069   eval_loss = 0.8579239610469702
2022-06-12 02:30:35,069   global_step = 879
2022-06-12 02:30:35,069   loss = 0.09700333795104271
2022-06-12 02:30:35,069   mcc = 0.230955087667665
2022-06-12 02:30:39,971 ***** Running evaluation *****
2022-06-12 02:30:39,971   Epoch = 3 iter 899 step
2022-06-12 02:30:39,971   Num examples = 1043
2022-06-12 02:30:39,971   Batch size = 32
2022-06-12 02:30:40,870 ***** Eval results *****
2022-06-12 02:30:40,871   cls_loss = 0.09808261008286963
2022-06-12 02:30:40,871   eval_loss = 0.8676581743991736
2022-06-12 02:30:40,871   global_step = 899
2022-06-12 02:30:40,871   loss = 0.09808261008286963
2022-06-12 02:30:40,871   mcc = 0.22544483431155077
2022-06-12 02:30:45,772 ***** Running evaluation *****
2022-06-12 02:30:45,773   Epoch = 3 iter 919 step
2022-06-12 02:30:45,773   Num examples = 1043
2022-06-12 02:30:45,773   Batch size = 32
2022-06-12 02:30:46,675 ***** Eval results *****
2022-06-12 02:30:46,676   cls_loss = 0.09695672995205652
2022-06-12 02:30:46,676   eval_loss = 0.9654140824621374
2022-06-12 02:30:46,676   global_step = 919
2022-06-12 02:30:46,676   loss = 0.09695672995205652
2022-06-12 02:30:46,676   mcc = 0.24750538306817763
2022-06-12 02:30:51,582 ***** Running evaluation *****
2022-06-12 02:30:51,582   Epoch = 3 iter 939 step
2022-06-12 02:30:51,582   Num examples = 1043
2022-06-12 02:30:51,583   Batch size = 32
2022-06-12 02:30:52,483 ***** Eval results *****
2022-06-12 02:30:52,483   cls_loss = 0.09655553507416145
2022-06-12 02:30:52,483   eval_loss = 0.8526898947629061
2022-06-12 02:30:52,484   global_step = 939
2022-06-12 02:30:52,484   loss = 0.09655553507416145
2022-06-12 02:30:52,484   mcc = 0.24020610683347723
2022-06-12 02:30:57,375 ***** Running evaluation *****
2022-06-12 02:30:57,375   Epoch = 3 iter 959 step
2022-06-12 02:30:57,375   Num examples = 1043
2022-06-12 02:30:57,375   Batch size = 32
2022-06-12 02:30:58,276 ***** Eval results *****
2022-06-12 02:30:58,276   cls_loss = 0.09685323576006709
2022-06-12 02:30:58,276   eval_loss = 0.9074615384593154
2022-06-12 02:30:58,276   global_step = 959
2022-06-12 02:30:58,276   loss = 0.09685323576006709
2022-06-12 02:30:58,276   mcc = 0.21895958523931583
2022-06-12 02:31:03,166 ***** Running evaluation *****
2022-06-12 02:31:03,167   Epoch = 3 iter 979 step
2022-06-12 02:31:03,167   Num examples = 1043
2022-06-12 02:31:03,167   Batch size = 32
2022-06-12 02:31:04,068 ***** Eval results *****
2022-06-12 02:31:04,068   cls_loss = 0.09795277624317769
2022-06-12 02:31:04,068   eval_loss = 0.9824985076080669
2022-06-12 02:31:04,068   global_step = 979
2022-06-12 02:31:04,068   loss = 0.09795277624317769
2022-06-12 02:31:04,068   mcc = 0.23555896561332282
2022-06-12 02:31:08,964 ***** Running evaluation *****
2022-06-12 02:31:08,965   Epoch = 3 iter 999 step
2022-06-12 02:31:08,965   Num examples = 1043
2022-06-12 02:31:08,965   Batch size = 32
2022-06-12 02:31:09,866 ***** Eval results *****
2022-06-12 02:31:09,866   cls_loss = 0.09772644816624998
2022-06-12 02:31:09,866   eval_loss = 0.9080872084155227
2022-06-12 02:31:09,866   global_step = 999
2022-06-12 02:31:09,866   loss = 0.09772644816624998
2022-06-12 02:31:09,866   mcc = 0.2385760513496379
2022-06-12 02:31:14,758 ***** Running evaluation *****
2022-06-12 02:31:14,758   Epoch = 3 iter 1019 step
2022-06-12 02:31:14,758   Num examples = 1043
2022-06-12 02:31:14,758   Batch size = 32
2022-06-12 02:31:15,660 ***** Eval results *****
2022-06-12 02:31:15,660   cls_loss = 0.09724623802194901
2022-06-12 02:31:15,660   eval_loss = 0.9553957744078203
2022-06-12 02:31:15,660   global_step = 1019
2022-06-12 02:31:15,660   loss = 0.09724623802194901
2022-06-12 02:31:15,660   mcc = 0.20717399848949097
2022-06-12 02:31:20,538 ***** Running evaluation *****
2022-06-12 02:31:20,538   Epoch = 3 iter 1039 step
2022-06-12 02:31:20,538   Num examples = 1043
2022-06-12 02:31:20,538   Batch size = 32
2022-06-12 02:31:21,438 ***** Eval results *****
2022-06-12 02:31:21,439   cls_loss = 0.09677000224840741
2022-06-12 02:31:21,439   eval_loss = 0.9142458529183359
2022-06-12 02:31:21,439   global_step = 1039
2022-06-12 02:31:21,439   loss = 0.09677000224840741
2022-06-12 02:31:21,439   mcc = 0.19151032940432003
2022-06-12 02:31:26,302 ***** Running evaluation *****
2022-06-12 02:31:26,303   Epoch = 3 iter 1059 step
2022-06-12 02:31:26,303   Num examples = 1043
2022-06-12 02:31:26,303   Batch size = 32
2022-06-12 02:31:27,202 ***** Eval results *****
2022-06-12 02:31:27,202   cls_loss = 0.09683753070674202
2022-06-12 02:31:27,202   eval_loss = 0.9864214568427114
2022-06-12 02:31:27,202   global_step = 1059
2022-06-12 02:31:27,202   loss = 0.09683753070674202
2022-06-12 02:31:27,202   mcc = 0.2325371987023683
2022-06-12 02:31:32,069 ***** Running evaluation *****
2022-06-12 02:31:32,069   Epoch = 4 iter 1079 step
2022-06-12 02:31:32,069   Num examples = 1043
2022-06-12 02:31:32,069   Batch size = 32
2022-06-12 02:31:32,968 ***** Eval results *****
2022-06-12 02:31:32,968   cls_loss = 0.08848826722665266
2022-06-12 02:31:32,969   eval_loss = 0.9177945247202208
2022-06-12 02:31:32,969   global_step = 1079
2022-06-12 02:31:32,969   loss = 0.08848826722665266
2022-06-12 02:31:32,969   mcc = 0.21331815793411338
2022-06-12 02:31:37,830 ***** Running evaluation *****
2022-06-12 02:31:37,830   Epoch = 4 iter 1099 step
2022-06-12 02:31:37,830   Num examples = 1043
2022-06-12 02:31:37,830   Batch size = 32
2022-06-12 02:31:38,731 ***** Eval results *****
2022-06-12 02:31:38,731   cls_loss = 0.08805402632682555
2022-06-12 02:31:38,731   eval_loss = 0.9257166936542048
2022-06-12 02:31:38,731   global_step = 1099
2022-06-12 02:31:38,731   loss = 0.08805402632682555
2022-06-12 02:31:38,731   mcc = 0.21175397700287898
2022-06-12 02:31:43,603 ***** Running evaluation *****
2022-06-12 02:31:43,604   Epoch = 4 iter 1119 step
2022-06-12 02:31:43,604   Num examples = 1043
2022-06-12 02:31:43,604   Batch size = 32
2022-06-12 02:31:44,506 ***** Eval results *****
2022-06-12 02:31:44,506   cls_loss = 0.0878593286170679
2022-06-12 02:31:44,506   eval_loss = 0.9360196057594183
2022-06-12 02:31:44,506   global_step = 1119
2022-06-12 02:31:44,506   loss = 0.0878593286170679
2022-06-12 02:31:44,506   mcc = 0.23208938358593978
2022-06-12 02:31:49,382 ***** Running evaluation *****
2022-06-12 02:31:49,382   Epoch = 4 iter 1139 step
2022-06-12 02:31:49,382   Num examples = 1043
2022-06-12 02:31:49,383   Batch size = 32
2022-06-12 02:31:50,282 ***** Eval results *****
2022-06-12 02:31:50,282   cls_loss = 0.08871354068249045
2022-06-12 02:31:50,282   eval_loss = 0.9890810206080928
2022-06-12 02:31:50,282   global_step = 1139
2022-06-12 02:31:50,282   loss = 0.08871354068249045
2022-06-12 02:31:50,283   mcc = 0.19566798479945066
2022-06-12 02:31:55,141 ***** Running evaluation *****
2022-06-12 02:31:55,142   Epoch = 4 iter 1159 step
2022-06-12 02:31:55,142   Num examples = 1043
2022-06-12 02:31:55,142   Batch size = 32
2022-06-12 02:31:56,041 ***** Eval results *****
2022-06-12 02:31:56,041   cls_loss = 0.08898168065390744
2022-06-12 02:31:56,041   eval_loss = 0.8691189271031003
2022-06-12 02:31:56,042   global_step = 1159
2022-06-12 02:31:56,042   loss = 0.08898168065390744
2022-06-12 02:31:56,042   mcc = 0.17336432249677297
2022-06-12 02:32:00,076 ***** Running evaluation *****
2022-06-12 02:32:00,076   Epoch = 7 iter 23499 step
2022-06-12 02:32:00,076   Num examples = 5463
2022-06-12 02:32:00,076   Batch size = 32
2022-06-12 02:32:00,078 ***** Eval results *****
2022-06-12 02:32:00,078   att_loss = 3.6335181322227528
2022-06-12 02:32:00,078   global_step = 23499
2022-06-12 02:32:00,078   loss = 4.515274242073501
2022-06-12 02:32:00,078   rep_loss = 0.8817561108644317
2022-06-12 02:32:00,078 ***** Save model *****
2022-06-12 02:32:00,914 ***** Running evaluation *****
2022-06-12 02:32:00,915   Epoch = 4 iter 1179 step
2022-06-12 02:32:00,915   Num examples = 1043
2022-06-12 02:32:00,915   Batch size = 32
2022-06-12 02:32:01,814 ***** Eval results *****
2022-06-12 02:32:01,814   cls_loss = 0.08888056035245862
2022-06-12 02:32:01,814   eval_loss = 0.9580721241055112
2022-06-12 02:32:01,814   global_step = 1179
2022-06-12 02:32:01,814   loss = 0.08888056035245862
2022-06-12 02:32:01,814   mcc = 0.20286423875683054
2022-06-12 02:32:06,707 ***** Running evaluation *****
2022-06-12 02:32:06,707   Epoch = 4 iter 1199 step
2022-06-12 02:32:06,707   Num examples = 1043
2022-06-12 02:32:06,707   Batch size = 32
2022-06-12 02:32:07,608 ***** Eval results *****
2022-06-12 02:32:07,608   cls_loss = 0.08934167254972093
2022-06-12 02:32:07,608   eval_loss = 0.9563143822279844
2022-06-12 02:32:07,608   global_step = 1199
2022-06-12 02:32:07,608   loss = 0.08934167254972093
2022-06-12 02:32:07,608   mcc = 0.24007288633988105
2022-06-12 02:32:12,480 ***** Running evaluation *****
2022-06-12 02:32:12,480   Epoch = 4 iter 1219 step
2022-06-12 02:32:12,480   Num examples = 1043
2022-06-12 02:32:12,480   Batch size = 32
2022-06-12 02:32:13,380 ***** Eval results *****
2022-06-12 02:32:13,380   cls_loss = 0.09025189078209415
2022-06-12 02:32:13,380   eval_loss = 0.8470836241136898
2022-06-12 02:32:13,380   global_step = 1219
2022-06-12 02:32:13,380   loss = 0.09025189078209415
2022-06-12 02:32:13,380   mcc = 0.22970607239704025
2022-06-12 02:32:18,263 ***** Running evaluation *****
2022-06-12 02:32:18,263   Epoch = 4 iter 1239 step
2022-06-12 02:32:18,263   Num examples = 1043
2022-06-12 02:32:18,263   Batch size = 32
2022-06-12 02:32:19,164 ***** Eval results *****
2022-06-12 02:32:19,164   cls_loss = 0.08974860168514195
2022-06-12 02:32:19,164   eval_loss = 0.9403179829770868
2022-06-12 02:32:19,164   global_step = 1239
2022-06-12 02:32:19,164   loss = 0.08974860168514195
2022-06-12 02:32:19,165   mcc = 0.2303348525612399
2022-06-12 02:32:24,046 ***** Running evaluation *****
2022-06-12 02:32:24,046   Epoch = 4 iter 1259 step
2022-06-12 02:32:24,046   Num examples = 1043
2022-06-12 02:32:24,046   Batch size = 32
2022-06-12 02:32:24,947 ***** Eval results *****
2022-06-12 02:32:24,947   cls_loss = 0.09077467621152938
2022-06-12 02:32:24,947   eval_loss = 0.9555937539447438
2022-06-12 02:32:24,947   global_step = 1259
2022-06-12 02:32:24,947   loss = 0.09077467621152938
2022-06-12 02:32:24,947   mcc = 0.218852388860082
2022-06-12 02:32:29,823 ***** Running evaluation *****
2022-06-12 02:32:29,823   Epoch = 4 iter 1279 step
2022-06-12 02:32:29,823   Num examples = 1043
2022-06-12 02:32:29,823   Batch size = 32
2022-06-12 02:32:30,722 ***** Eval results *****
2022-06-12 02:32:30,722   cls_loss = 0.09086213239702569
2022-06-12 02:32:30,722   eval_loss = 0.8593946894009908
2022-06-12 02:32:30,722   global_step = 1279
2022-06-12 02:32:30,723   loss = 0.09086213239702569
2022-06-12 02:32:30,723   mcc = 0.23424835433825666
2022-06-12 02:32:35,601 ***** Running evaluation *****
2022-06-12 02:32:35,601   Epoch = 4 iter 1299 step
2022-06-12 02:32:35,601   Num examples = 1043
2022-06-12 02:32:35,601   Batch size = 32
2022-06-12 02:32:36,501 ***** Eval results *****
2022-06-12 02:32:36,501   cls_loss = 0.09172092139462888
2022-06-12 02:32:36,501   eval_loss = 0.9031912910215782
2022-06-12 02:32:36,501   global_step = 1299
2022-06-12 02:32:36,502   loss = 0.09172092139462888
2022-06-12 02:32:36,502   mcc = 0.22659231316465678
2022-06-12 02:32:41,370 ***** Running evaluation *****
2022-06-12 02:32:41,370   Epoch = 4 iter 1319 step
2022-06-12 02:32:41,370   Num examples = 1043
2022-06-12 02:32:41,370   Batch size = 32
2022-06-12 02:32:42,269 ***** Eval results *****
2022-06-12 02:32:42,269   cls_loss = 0.09172378521991441
2022-06-12 02:32:42,270   eval_loss = 0.8968837107672836
2022-06-12 02:32:42,270   global_step = 1319
2022-06-12 02:32:42,270   loss = 0.09172378521991441
2022-06-12 02:32:42,270   mcc = 0.21873291773454392
2022-06-12 02:32:47,140 ***** Running evaluation *****
2022-06-12 02:32:47,141   Epoch = 5 iter 1339 step
2022-06-12 02:32:47,141   Num examples = 1043
2022-06-12 02:32:47,141   Batch size = 32
2022-06-12 02:32:48,041 ***** Eval results *****
2022-06-12 02:32:48,042   cls_loss = 0.07583926990628242
2022-06-12 02:32:48,042   eval_loss = 0.8923005109483545
2022-06-12 02:32:48,042   global_step = 1339
2022-06-12 02:32:48,042   loss = 0.07583926990628242
2022-06-12 02:32:48,042   mcc = 0.2344201896061598
2022-06-12 02:32:52,905 ***** Running evaluation *****
2022-06-12 02:32:52,906   Epoch = 5 iter 1359 step
2022-06-12 02:32:52,906   Num examples = 1043
2022-06-12 02:32:52,906   Batch size = 32
2022-06-12 02:32:53,806 ***** Eval results *****
2022-06-12 02:32:53,806   cls_loss = 0.08827921748161316
2022-06-12 02:32:53,806   eval_loss = 0.9121798972288767
2022-06-12 02:32:53,806   global_step = 1359
2022-06-12 02:32:53,806   loss = 0.08827921748161316
2022-06-12 02:32:53,807   mcc = 0.2395059563705902
2022-06-12 02:32:58,695 ***** Running evaluation *****
2022-06-12 02:32:58,696   Epoch = 5 iter 1379 step
2022-06-12 02:32:58,696   Num examples = 1043
2022-06-12 02:32:58,696   Batch size = 32
2022-06-12 02:32:59,595 ***** Eval results *****
2022-06-12 02:32:59,596   cls_loss = 0.08644339340654286
2022-06-12 02:32:59,596   eval_loss = 0.9864057672746254
2022-06-12 02:32:59,596   global_step = 1379
2022-06-12 02:32:59,596   loss = 0.08644339340654286
2022-06-12 02:32:59,596   mcc = 0.25603376903103564
2022-06-12 02:33:04,484 ***** Running evaluation *****
2022-06-12 02:33:04,484   Epoch = 5 iter 1399 step
2022-06-12 02:33:04,485   Num examples = 1043
2022-06-12 02:33:04,485   Batch size = 32
2022-06-12 02:33:05,385 ***** Eval results *****
2022-06-12 02:33:05,386   cls_loss = 0.08658502285834402
2022-06-12 02:33:05,386   eval_loss = 0.8634602427482605
2022-06-12 02:33:05,386   global_step = 1399
2022-06-12 02:33:05,386   loss = 0.08658502285834402
2022-06-12 02:33:05,386   mcc = 0.2555447116273561
2022-06-12 02:33:10,278 ***** Running evaluation *****
2022-06-12 02:33:10,279   Epoch = 5 iter 1419 step
2022-06-12 02:33:10,279   Num examples = 1043
2022-06-12 02:33:10,279   Batch size = 32
2022-06-12 02:33:11,180 ***** Eval results *****
2022-06-12 02:33:11,180   cls_loss = 0.08616913535765239
2022-06-12 02:33:11,180   eval_loss = 0.8908866074952212
2022-06-12 02:33:11,180   global_step = 1419
2022-06-12 02:33:11,180   loss = 0.08616913535765239
2022-06-12 02:33:11,180   mcc = 0.25167241205712393
2022-06-12 02:33:16,070 ***** Running evaluation *****
2022-06-12 02:33:16,071   Epoch = 5 iter 1439 step
2022-06-12 02:33:16,071   Num examples = 1043
2022-06-12 02:33:16,071   Batch size = 32
2022-06-12 02:33:16,972 ***** Eval results *****
2022-06-12 02:33:16,972   cls_loss = 0.08667984561851391
2022-06-12 02:33:16,972   eval_loss = 0.8840328680746483
2022-06-12 02:33:16,972   global_step = 1439
2022-06-12 02:33:16,972   loss = 0.08667984561851391
2022-06-12 02:33:16,972   mcc = 0.2312885421948531
2022-06-12 02:33:21,877 ***** Running evaluation *****
2022-06-12 02:33:21,877   Epoch = 5 iter 1459 step
2022-06-12 02:33:21,877   Num examples = 1043
2022-06-12 02:33:21,877   Batch size = 32
2022-06-12 02:33:22,777 ***** Eval results *****
2022-06-12 02:33:22,777   cls_loss = 0.08719314148108806
2022-06-12 02:33:22,777   eval_loss = 0.8835879717812394
2022-06-12 02:33:22,777   global_step = 1459
2022-06-12 02:33:22,777   loss = 0.08719314148108806
2022-06-12 02:33:22,777   mcc = 0.2208303521528171
2022-06-12 02:33:27,676 ***** Running evaluation *****
2022-06-12 02:33:27,677   Epoch = 5 iter 1479 step
2022-06-12 02:33:27,677   Num examples = 1043
2022-06-12 02:33:27,678   Batch size = 32
2022-06-12 02:33:28,579 ***** Eval results *****
2022-06-12 02:33:28,579   cls_loss = 0.0876563751242227
2022-06-12 02:33:28,579   eval_loss = 0.886130255280119
2022-06-12 02:33:28,579   global_step = 1479
2022-06-12 02:33:28,579   loss = 0.0876563751242227
2022-06-12 02:33:28,579   mcc = 0.24810019947327508
2022-06-12 02:33:33,483 ***** Running evaluation *****
2022-06-12 02:33:33,484   Epoch = 5 iter 1499 step
2022-06-12 02:33:33,484   Num examples = 1043
2022-06-12 02:33:33,484   Batch size = 32
2022-06-12 02:33:34,384 ***** Eval results *****
2022-06-12 02:33:34,384   cls_loss = 0.08789326436817646
2022-06-12 02:33:34,384   eval_loss = 0.9171873358162966
2022-06-12 02:33:34,384   global_step = 1499
2022-06-12 02:33:34,384   loss = 0.08789326436817646
2022-06-12 02:33:34,384   mcc = 0.2525531829182206
2022-06-12 02:33:39,296 ***** Running evaluation *****
2022-06-12 02:33:39,296   Epoch = 5 iter 1519 step
2022-06-12 02:33:39,296   Num examples = 1043
2022-06-12 02:33:39,296   Batch size = 32
2022-06-12 02:33:40,197 ***** Eval results *****
2022-06-12 02:33:40,198   cls_loss = 0.08770384283169456
2022-06-12 02:33:40,198   eval_loss = 0.9314522102023616
2022-06-12 02:33:40,198   global_step = 1519
2022-06-12 02:33:40,198   loss = 0.08770384283169456
2022-06-12 02:33:40,198   mcc = 0.23370929409401253
2022-06-12 02:33:45,094 ***** Running evaluation *****
2022-06-12 02:33:45,094   Epoch = 5 iter 1539 step
2022-06-12 02:33:45,095   Num examples = 1043
2022-06-12 02:33:45,095   Batch size = 32
2022-06-12 02:33:45,994 ***** Eval results *****
2022-06-12 02:33:45,995   cls_loss = 0.08761132424514667
2022-06-12 02:33:45,995   eval_loss = 0.954539645801891
2022-06-12 02:33:45,995   global_step = 1539
2022-06-12 02:33:45,995   loss = 0.08761132424514667
2022-06-12 02:33:45,995   mcc = 0.22500271827545165
2022-06-12 02:33:50,894 ***** Running evaluation *****
2022-06-12 02:33:50,894   Epoch = 5 iter 1559 step
2022-06-12 02:33:50,894   Num examples = 1043
2022-06-12 02:33:50,894   Batch size = 32
2022-06-12 02:33:51,796 ***** Eval results *****
2022-06-12 02:33:51,796   cls_loss = 0.08791876171848603
2022-06-12 02:33:51,796   eval_loss = 0.8964787017215382
2022-06-12 02:33:51,796   global_step = 1559
2022-06-12 02:33:51,796   loss = 0.08791876171848603
2022-06-12 02:33:51,796   mcc = 0.2344201896061598
2022-06-12 02:33:56,697 ***** Running evaluation *****
2022-06-12 02:33:56,698   Epoch = 5 iter 1579 step
2022-06-12 02:33:56,698   Num examples = 1043
2022-06-12 02:33:56,698   Batch size = 32
2022-06-12 02:33:57,599 ***** Eval results *****
2022-06-12 02:33:57,599   cls_loss = 0.08747281833383881
2022-06-12 02:33:57,599   eval_loss = 0.9842838717229438
2022-06-12 02:33:57,599   global_step = 1579
2022-06-12 02:33:57,600   loss = 0.08747281833383881
2022-06-12 02:33:57,600   mcc = 0.22080942932671346
2022-06-12 02:34:02,499 ***** Running evaluation *****
2022-06-12 02:34:02,500   Epoch = 5 iter 1599 step
2022-06-12 02:34:02,500   Num examples = 1043
2022-06-12 02:34:02,500   Batch size = 32
2022-06-12 02:34:03,400 ***** Eval results *****
2022-06-12 02:34:03,400   cls_loss = 0.08724576155796196
2022-06-12 02:34:03,400   eval_loss = 0.9483243290222052
2022-06-12 02:34:03,400   global_step = 1599
2022-06-12 02:34:03,400   loss = 0.08724576155796196
2022-06-12 02:34:03,400   mcc = 0.23183139845925813
2022-06-12 02:34:07,870 ***** Running evaluation *****
2022-06-12 02:34:07,870   Epoch = 7 iter 23999 step
2022-06-12 02:34:07,870   Num examples = 5463
2022-06-12 02:34:07,870   Batch size = 32
2022-06-12 02:34:07,872 ***** Eval results *****
2022-06-12 02:34:07,872   att_loss = 3.612660087864189
2022-06-12 02:34:07,872   global_step = 23999
2022-06-12 02:34:07,872   loss = 4.492334713392398
2022-06-12 02:34:07,872   rep_loss = 0.8796746236107805
2022-06-12 02:34:07,872 ***** Save model *****
2022-06-12 02:34:08,312 ***** Running evaluation *****
2022-06-12 02:34:08,313   Epoch = 6 iter 1619 step
2022-06-12 02:34:08,313   Num examples = 1043
2022-06-12 02:34:08,313   Batch size = 32
2022-06-12 02:34:09,215 ***** Eval results *****
2022-06-12 02:34:09,215   cls_loss = 0.08681535370209638
2022-06-12 02:34:09,215   eval_loss = 0.9348178415587454
2022-06-12 02:34:09,215   global_step = 1619
2022-06-12 02:34:09,215   loss = 0.08681535370209638
2022-06-12 02:34:09,215   mcc = 0.24165095231055853
2022-06-12 02:34:14,125 ***** Running evaluation *****
2022-06-12 02:34:14,125   Epoch = 6 iter 1639 step
2022-06-12 02:34:14,125   Num examples = 1043
2022-06-12 02:34:14,126   Batch size = 32
2022-06-12 02:34:15,028 ***** Eval results *****
2022-06-12 02:34:15,028   cls_loss = 0.08932269465278934
2022-06-12 02:34:15,028   eval_loss = 0.8870271335948597
2022-06-12 02:34:15,028   global_step = 1639
2022-06-12 02:34:15,028   loss = 0.08932269465278934
2022-06-12 02:34:15,028   mcc = 0.2633225935720042
2022-06-12 02:34:19,963 ***** Running evaluation *****
2022-06-12 02:34:19,963   Epoch = 6 iter 1659 step
2022-06-12 02:34:19,963   Num examples = 1043
2022-06-12 02:34:19,963   Batch size = 32
2022-06-12 02:34:20,871 ***** Eval results *****
2022-06-12 02:34:20,871   cls_loss = 0.0908170067950299
2022-06-12 02:34:20,871   eval_loss = 0.9143140844323419
2022-06-12 02:34:20,871   global_step = 1659
2022-06-12 02:34:20,871   loss = 0.0908170067950299
2022-06-12 02:34:20,871   mcc = 0.24946318523126898
2022-06-12 02:34:25,799 ***** Running evaluation *****
2022-06-12 02:34:25,799   Epoch = 6 iter 1679 step
2022-06-12 02:34:25,799   Num examples = 1043
2022-06-12 02:34:25,799   Batch size = 32
2022-06-12 02:34:26,702 ***** Eval results *****
2022-06-12 02:34:26,702   cls_loss = 0.08930870203615783
2022-06-12 02:34:26,702   eval_loss = 0.9293812657847549
2022-06-12 02:34:26,702   global_step = 1679
2022-06-12 02:34:26,702   loss = 0.08930870203615783
2022-06-12 02:34:26,702   mcc = 0.2825114601341239
2022-06-12 02:34:26,703 ***** Save model *****
2022-06-12 02:34:32,006 ***** Running evaluation *****
2022-06-12 02:34:32,007   Epoch = 6 iter 1699 step
2022-06-12 02:34:32,007   Num examples = 1043
2022-06-12 02:34:32,007   Batch size = 32
2022-06-12 02:34:32,909 ***** Eval results *****
2022-06-12 02:34:32,909   cls_loss = 0.08925858731429602
2022-06-12 02:34:32,910   eval_loss = 0.9446308513482412
2022-06-12 02:34:32,910   global_step = 1699
2022-06-12 02:34:32,910   loss = 0.08925858731429602
2022-06-12 02:34:32,910   mcc = 0.2567855647711933
2022-06-12 02:34:37,800 ***** Running evaluation *****
2022-06-12 02:34:37,800   Epoch = 6 iter 1719 step
2022-06-12 02:34:37,800   Num examples = 1043
2022-06-12 02:34:37,800   Batch size = 32
2022-06-12 02:34:38,701 ***** Eval results *****
2022-06-12 02:34:38,701   cls_loss = 0.08803191379858898
2022-06-12 02:34:38,701   eval_loss = 0.9260347377170216
2022-06-12 02:34:38,702   global_step = 1719
2022-06-12 02:34:38,702   loss = 0.08803191379858898
2022-06-12 02:34:38,702   mcc = 0.2623744406802895
2022-06-12 02:34:43,595 ***** Running evaluation *****
2022-06-12 02:34:43,595   Epoch = 6 iter 1739 step
2022-06-12 02:34:43,596   Num examples = 1043
2022-06-12 02:34:43,596   Batch size = 32
2022-06-12 02:34:44,502 ***** Eval results *****
2022-06-12 02:34:44,503   cls_loss = 0.08740139094582439
2022-06-12 02:34:44,503   eval_loss = 0.9312590712850745
2022-06-12 02:34:44,503   global_step = 1739
2022-06-12 02:34:44,503   loss = 0.08740139094582439
2022-06-12 02:34:44,503   mcc = 0.24912683446931658
2022-06-12 02:34:49,408 ***** Running evaluation *****
2022-06-12 02:34:49,408   Epoch = 6 iter 1759 step
2022-06-12 02:34:49,408   Num examples = 1043
2022-06-12 02:34:49,409   Batch size = 32
2022-06-12 02:34:50,312 ***** Eval results *****
2022-06-12 02:34:50,313   cls_loss = 0.08761093931592953
2022-06-12 02:34:50,313   eval_loss = 0.8962612630742969
2022-06-12 02:34:50,313   global_step = 1759
2022-06-12 02:34:50,313   loss = 0.08761093931592953
2022-06-12 02:34:50,313   mcc = 0.24832830066414616
2022-06-12 02:34:55,209 ***** Running evaluation *****
2022-06-12 02:34:55,209   Epoch = 6 iter 1779 step
2022-06-12 02:34:55,209   Num examples = 1043
2022-06-12 02:34:55,210   Batch size = 32
2022-06-12 02:34:56,111 ***** Eval results *****
2022-06-12 02:34:56,111   cls_loss = 0.08845355795264917
2022-06-12 02:34:56,111   eval_loss = 0.8959552708900336
2022-06-12 02:34:56,111   global_step = 1779
2022-06-12 02:34:56,111   loss = 0.08845355795264917
2022-06-12 02:34:56,111   mcc = 0.2198619682090848
2022-06-12 02:35:01,005 ***** Running evaluation *****
2022-06-12 02:35:01,006   Epoch = 6 iter 1799 step
2022-06-12 02:35:01,006   Num examples = 1043
2022-06-12 02:35:01,006   Batch size = 32
2022-06-12 02:35:01,907 ***** Eval results *****
2022-06-12 02:35:01,907   cls_loss = 0.08834600093098462
2022-06-12 02:35:01,907   eval_loss = 0.9792619735905619
2022-06-12 02:35:01,907   global_step = 1799
2022-06-12 02:35:01,907   loss = 0.08834600093098462
2022-06-12 02:35:01,907   mcc = 0.23558559817945146
2022-06-12 02:35:06,805 ***** Running evaluation *****
2022-06-12 02:35:06,805   Epoch = 6 iter 1819 step
2022-06-12 02:35:06,805   Num examples = 1043
2022-06-12 02:35:06,805   Batch size = 32
2022-06-12 02:35:07,705 ***** Eval results *****
2022-06-12 02:35:07,706   cls_loss = 0.08836411245270259
2022-06-12 02:35:07,706   eval_loss = 0.8939592549295137
2022-06-12 02:35:07,706   global_step = 1819
2022-06-12 02:35:07,706   loss = 0.08836411245270259
2022-06-12 02:35:07,706   mcc = 0.20100910423052637
2022-06-12 02:35:12,599 ***** Running evaluation *****
2022-06-12 02:35:12,599   Epoch = 6 iter 1839 step
2022-06-12 02:35:12,599   Num examples = 1043
2022-06-12 02:35:12,599   Batch size = 32
2022-06-12 02:35:13,498 ***** Eval results *****
2022-06-12 02:35:13,499   cls_loss = 0.0884500495177784
2022-06-12 02:35:13,499   eval_loss = 0.9269109630223477
2022-06-12 02:35:13,499   global_step = 1839
2022-06-12 02:35:13,499   loss = 0.0884500495177784
2022-06-12 02:35:13,499   mcc = 0.2130558362502192
2022-06-12 02:35:18,429 ***** Running evaluation *****
2022-06-12 02:35:18,430   Epoch = 6 iter 1859 step
2022-06-12 02:35:18,430   Num examples = 1043
2022-06-12 02:35:18,430   Batch size = 32
2022-06-12 02:35:19,333 ***** Eval results *****
2022-06-12 02:35:19,334   cls_loss = 0.0881923466688928
2022-06-12 02:35:19,334   eval_loss = 0.9361159178343686
2022-06-12 02:35:19,334   global_step = 1859
2022-06-12 02:35:19,334   loss = 0.0881923466688928
2022-06-12 02:35:19,334   mcc = 0.20796275588624588
2022-06-12 02:35:24,246 ***** Running evaluation *****
2022-06-12 02:35:24,247   Epoch = 7 iter 1879 step
2022-06-12 02:35:24,247   Num examples = 1043
2022-06-12 02:35:24,247   Batch size = 32
2022-06-12 02:35:25,149 ***** Eval results *****
2022-06-12 02:35:25,149   cls_loss = 0.09411234110593795
2022-06-12 02:35:25,149   eval_loss = 0.9474816647442904
2022-06-12 02:35:25,149   global_step = 1879
2022-06-12 02:35:25,149   loss = 0.09411234110593795
2022-06-12 02:35:25,150   mcc = 0.23538175122983318
2022-06-12 02:35:30,040 ***** Running evaluation *****
2022-06-12 02:35:30,041   Epoch = 7 iter 1899 step
2022-06-12 02:35:30,041   Num examples = 1043
2022-06-12 02:35:30,041   Batch size = 32
2022-06-12 02:35:30,942 ***** Eval results *****
2022-06-12 02:35:30,942   cls_loss = 0.08933283686637879
2022-06-12 02:35:30,942   eval_loss = 0.9097260489608302
2022-06-12 02:35:30,942   global_step = 1899
2022-06-12 02:35:30,942   loss = 0.08933283686637879
2022-06-12 02:35:30,942   mcc = 0.24572636204735215
2022-06-12 02:35:35,853 ***** Running evaluation *****
2022-06-12 02:35:35,853   Epoch = 7 iter 1919 step
2022-06-12 02:35:35,853   Num examples = 1043
2022-06-12 02:35:35,853   Batch size = 32
2022-06-12 02:35:36,754 ***** Eval results *****
2022-06-12 02:35:36,754   cls_loss = 0.08835717782378197
2022-06-12 02:35:36,754   eval_loss = 0.9384891824288801
2022-06-12 02:35:36,754   global_step = 1919
2022-06-12 02:35:36,754   loss = 0.08835717782378197
2022-06-12 02:35:36,754   mcc = 0.22859262486027096
2022-06-12 02:35:41,647 ***** Running evaluation *****
2022-06-12 02:35:41,647   Epoch = 7 iter 1939 step
2022-06-12 02:35:41,647   Num examples = 1043
2022-06-12 02:35:41,647   Batch size = 32
2022-06-12 02:35:42,549 ***** Eval results *****
2022-06-12 02:35:42,549   cls_loss = 0.08625985692654337
2022-06-12 02:35:42,549   eval_loss = 0.9406592195684259
2022-06-12 02:35:42,549   global_step = 1939
2022-06-12 02:35:42,549   loss = 0.08625985692654337
2022-06-12 02:35:42,550   mcc = 0.23237405239945857
2022-06-12 02:35:47,423 ***** Running evaluation *****
2022-06-12 02:35:47,423   Epoch = 7 iter 1959 step
2022-06-12 02:35:47,423   Num examples = 1043
2022-06-12 02:35:47,423   Batch size = 32
2022-06-12 02:35:48,324 ***** Eval results *****
2022-06-12 02:35:48,324   cls_loss = 0.08566805844505627
2022-06-12 02:35:48,325   eval_loss = 1.0307427337675383
2022-06-12 02:35:48,325   global_step = 1959
2022-06-12 02:35:48,325   loss = 0.08566805844505627
2022-06-12 02:35:48,325   mcc = 0.25201460821809
2022-06-12 02:35:53,211 ***** Running evaluation *****
2022-06-12 02:35:53,211   Epoch = 7 iter 1979 step
2022-06-12 02:35:53,211   Num examples = 1043
2022-06-12 02:35:53,211   Batch size = 32
2022-06-12 02:35:54,111 ***** Eval results *****
2022-06-12 02:35:54,112   cls_loss = 0.08499988713725047
2022-06-12 02:35:54,112   eval_loss = 0.9311732116973761
2022-06-12 02:35:54,112   global_step = 1979
2022-06-12 02:35:54,112   loss = 0.08499988713725047
2022-06-12 02:35:54,112   mcc = 0.247112855315933
2022-06-12 02:35:58,993 ***** Running evaluation *****
2022-06-12 02:35:58,994   Epoch = 7 iter 1999 step
2022-06-12 02:35:58,994   Num examples = 1043
2022-06-12 02:35:58,994   Batch size = 32
2022-06-12 02:35:59,894 ***** Eval results *****
2022-06-12 02:35:59,894   cls_loss = 0.08485546028957917
2022-06-12 02:35:59,894   eval_loss = 0.9514647424221039
2022-06-12 02:35:59,894   global_step = 1999
2022-06-12 02:35:59,894   loss = 0.08485546028957917
2022-06-12 02:35:59,894   mcc = 0.25028757907382854
2022-06-12 02:36:04,755 ***** Running evaluation *****
2022-06-12 02:36:04,755   Epoch = 7 iter 2019 step
2022-06-12 02:36:04,755   Num examples = 1043
2022-06-12 02:36:04,755   Batch size = 32
2022-06-12 02:36:05,655 ***** Eval results *****
2022-06-12 02:36:05,655   cls_loss = 0.0847456996391217
2022-06-12 02:36:05,655   eval_loss = 0.9490461394642339
2022-06-12 02:36:05,655   global_step = 2019
2022-06-12 02:36:05,655   loss = 0.0847456996391217
2022-06-12 02:36:05,656   mcc = 0.2225029861463785
2022-06-12 02:36:10,517 ***** Running evaluation *****
2022-06-12 02:36:10,517   Epoch = 7 iter 2039 step
2022-06-12 02:36:10,517   Num examples = 1043
2022-06-12 02:36:10,517   Batch size = 32
2022-06-12 02:36:11,418 ***** Eval results *****
2022-06-12 02:36:11,418   cls_loss = 0.08420643585131449
2022-06-12 02:36:11,418   eval_loss = 0.9679736991723379
2022-06-12 02:36:11,418   global_step = 2039
2022-06-12 02:36:11,418   loss = 0.08420643585131449
2022-06-12 02:36:11,418   mcc = 0.23376817585619566
2022-06-12 02:36:15,763 ***** Running evaluation *****
2022-06-12 02:36:15,764   Epoch = 7 iter 24499 step
2022-06-12 02:36:15,764   Num examples = 5463
2022-06-12 02:36:15,764   Batch size = 32
2022-06-12 02:36:15,765 ***** Eval results *****
2022-06-12 02:36:15,766   att_loss = 3.6149019828671474
2022-06-12 02:36:15,766   global_step = 24499
2022-06-12 02:36:15,766   loss = 4.494557836794433
2022-06-12 02:36:15,766   rep_loss = 0.8796558535144071
2022-06-12 02:36:15,766 ***** Save model *****
2022-06-12 02:36:16,295 ***** Running evaluation *****
2022-06-12 02:36:16,295   Epoch = 7 iter 2059 step
2022-06-12 02:36:16,295   Num examples = 1043
2022-06-12 02:36:16,295   Batch size = 32
2022-06-12 02:36:17,195 ***** Eval results *****
2022-06-12 02:36:17,195   cls_loss = 0.08425684635968585
2022-06-12 02:36:17,195   eval_loss = 0.9236988226572672
2022-06-12 02:36:17,195   global_step = 2059
2022-06-12 02:36:17,195   loss = 0.08425684635968585
2022-06-12 02:36:17,195   mcc = 0.23376817585619566
2022-06-12 02:36:22,085 ***** Running evaluation *****
2022-06-12 02:36:22,085   Epoch = 7 iter 2079 step
2022-06-12 02:36:22,085   Num examples = 1043
2022-06-12 02:36:22,085   Batch size = 32
2022-06-12 02:36:22,987 ***** Eval results *****
2022-06-12 02:36:22,987   cls_loss = 0.08491546875309376
2022-06-12 02:36:22,987   eval_loss = 0.9265243808428446
2022-06-12 02:36:22,987   global_step = 2079
2022-06-12 02:36:22,987   loss = 0.08491546875309376
2022-06-12 02:36:22,987   mcc = 0.21304649203410367
2022-06-12 02:36:27,875 ***** Running evaluation *****
2022-06-12 02:36:27,875   Epoch = 7 iter 2099 step
2022-06-12 02:36:27,875   Num examples = 1043
2022-06-12 02:36:27,875   Batch size = 32
2022-06-12 02:36:28,775 ***** Eval results *****
2022-06-12 02:36:28,775   cls_loss = 0.08498230723907119
2022-06-12 02:36:28,775   eval_loss = 0.8739688974438291
2022-06-12 02:36:28,776   global_step = 2099
2022-06-12 02:36:28,776   loss = 0.08498230723907119
2022-06-12 02:36:28,776   mcc = 0.25219946756859773
2022-06-12 02:36:33,680 ***** Running evaluation *****
2022-06-12 02:36:33,680   Epoch = 7 iter 2119 step
2022-06-12 02:36:33,680   Num examples = 1043
2022-06-12 02:36:33,681   Batch size = 32
2022-06-12 02:36:34,580 ***** Eval results *****
2022-06-12 02:36:34,580   cls_loss = 0.0851550123244524
2022-06-12 02:36:34,581   eval_loss = 0.8884866508570585
2022-06-12 02:36:34,581   global_step = 2119
2022-06-12 02:36:34,581   loss = 0.0851550123244524
2022-06-12 02:36:34,581   mcc = 0.2287344718744512
2022-06-12 02:36:39,465 ***** Running evaluation *****
2022-06-12 02:36:39,465   Epoch = 8 iter 2139 step
2022-06-12 02:36:39,466   Num examples = 1043
2022-06-12 02:36:39,466   Batch size = 32
2022-06-12 02:36:40,366 ***** Eval results *****
2022-06-12 02:36:40,366   cls_loss = 0.08483227094014485
2022-06-12 02:36:40,367   eval_loss = 0.9529122123212526
2022-06-12 02:36:40,367   global_step = 2139
2022-06-12 02:36:40,367   loss = 0.08483227094014485
2022-06-12 02:36:40,367   mcc = 0.24233224124733924
2022-06-12 02:36:45,244 ***** Running evaluation *****
2022-06-12 02:36:45,244   Epoch = 8 iter 2159 step
2022-06-12 02:36:45,244   Num examples = 1043
2022-06-12 02:36:45,244   Batch size = 32
2022-06-12 02:36:46,143 ***** Eval results *****
2022-06-12 02:36:46,143   cls_loss = 0.08987366829229437
2022-06-12 02:36:46,143   eval_loss = 0.894612886688926
2022-06-12 02:36:46,143   global_step = 2159
2022-06-12 02:36:46,144   loss = 0.08987366829229437
2022-06-12 02:36:46,144   mcc = 0.22019296235062286
2022-06-12 02:36:51,013 ***** Running evaluation *****
2022-06-12 02:36:51,014   Epoch = 8 iter 2179 step
2022-06-12 02:36:51,014   Num examples = 1043
2022-06-12 02:36:51,014   Batch size = 32
2022-06-12 02:36:51,914 ***** Eval results *****
2022-06-12 02:36:51,914   cls_loss = 0.08934746665316959
2022-06-12 02:36:51,914   eval_loss = 0.9294634479464907
2022-06-12 02:36:51,914   global_step = 2179
2022-06-12 02:36:51,914   loss = 0.08934746665316959
2022-06-12 02:36:51,914   mcc = 0.22774812412608503
2022-06-12 02:36:56,787 ***** Running evaluation *****
2022-06-12 02:36:56,787   Epoch = 8 iter 2199 step
2022-06-12 02:36:56,787   Num examples = 1043
2022-06-12 02:36:56,787   Batch size = 32
2022-06-12 02:36:57,686 ***** Eval results *****
2022-06-12 02:36:57,687   cls_loss = 0.08874643050015919
2022-06-12 02:36:57,687   eval_loss = 0.9402613883668726
2022-06-12 02:36:57,687   global_step = 2199
2022-06-12 02:36:57,687   loss = 0.08874643050015919
2022-06-12 02:36:57,687   mcc = 0.2192571954856336
2022-06-12 02:37:02,562 ***** Running evaluation *****
2022-06-12 02:37:02,562   Epoch = 8 iter 2219 step
2022-06-12 02:37:02,562   Num examples = 1043
2022-06-12 02:37:02,563   Batch size = 32
2022-06-12 02:37:03,462 ***** Eval results *****
2022-06-12 02:37:03,462   cls_loss = 0.08745474574795689
2022-06-12 02:37:03,462   eval_loss = 0.9613598955400062
2022-06-12 02:37:03,462   global_step = 2219
2022-06-12 02:37:03,462   loss = 0.08745474574795689
2022-06-12 02:37:03,462   mcc = 0.22933300518272287
2022-06-12 02:37:08,356 ***** Running evaluation *****
2022-06-12 02:37:08,356   Epoch = 8 iter 2239 step
2022-06-12 02:37:08,356   Num examples = 1043
2022-06-12 02:37:08,356   Batch size = 32
2022-06-12 02:37:09,255 ***** Eval results *****
2022-06-12 02:37:09,255   cls_loss = 0.08694532593187776
2022-06-12 02:37:09,255   eval_loss = 1.008485195311633
2022-06-12 02:37:09,255   global_step = 2239
2022-06-12 02:37:09,255   loss = 0.08694532593187776
2022-06-12 02:37:09,255   mcc = 0.22758244388383697
2022-06-12 02:37:14,123 ***** Running evaluation *****
2022-06-12 02:37:14,123   Epoch = 8 iter 2259 step
2022-06-12 02:37:14,124   Num examples = 1043
2022-06-12 02:37:14,124   Batch size = 32
2022-06-12 02:37:15,026 ***** Eval results *****
2022-06-12 02:37:15,026   cls_loss = 0.08567010326598717
2022-06-12 02:37:15,026   eval_loss = 0.9717716953971169
2022-06-12 02:37:15,026   global_step = 2259
2022-06-12 02:37:15,026   loss = 0.08567010326598717
2022-06-12 02:37:15,027   mcc = 0.22597505851104907
2022-06-12 02:37:19,914 ***** Running evaluation *****
2022-06-12 02:37:19,914   Epoch = 8 iter 2279 step
2022-06-12 02:37:19,914   Num examples = 1043
2022-06-12 02:37:19,915   Batch size = 32
2022-06-12 02:37:20,815 ***** Eval results *****
2022-06-12 02:37:20,815   cls_loss = 0.08657687485634864
2022-06-12 02:37:20,815   eval_loss = 0.9552346919522141
2022-06-12 02:37:20,815   global_step = 2279
2022-06-12 02:37:20,815   loss = 0.08657687485634864
2022-06-12 02:37:20,815   mcc = 0.23985260898664917
2022-06-12 02:37:25,705 ***** Running evaluation *****
2022-06-12 02:37:25,706   Epoch = 8 iter 2299 step
2022-06-12 02:37:25,706   Num examples = 1043
2022-06-12 02:37:25,706   Batch size = 32
2022-06-12 02:37:26,606 ***** Eval results *****
2022-06-12 02:37:26,607   cls_loss = 0.08685213446251454
2022-06-12 02:37:26,607   eval_loss = 0.922632482918826
2022-06-12 02:37:26,607   global_step = 2299
2022-06-12 02:37:26,607   loss = 0.08685213446251454
2022-06-12 02:37:26,607   mcc = 0.23007774232359784
2022-06-12 02:37:31,496 ***** Running evaluation *****
2022-06-12 02:37:31,496   Epoch = 8 iter 2319 step
2022-06-12 02:37:31,496   Num examples = 1043
2022-06-12 02:37:31,496   Batch size = 32
2022-06-12 02:37:32,395 ***** Eval results *****
2022-06-12 02:37:32,395   cls_loss = 0.08668769936743981
2022-06-12 02:37:32,395   eval_loss = 0.9405529959635301
2022-06-12 02:37:32,395   global_step = 2319
2022-06-12 02:37:32,395   loss = 0.08668769936743981
2022-06-12 02:37:32,396   mcc = 0.23760535359506268
2022-06-12 02:37:37,263 ***** Running evaluation *****
2022-06-12 02:37:37,263   Epoch = 8 iter 2339 step
2022-06-12 02:37:37,264   Num examples = 1043
2022-06-12 02:37:37,264   Batch size = 32
2022-06-12 02:37:38,163 ***** Eval results *****
2022-06-12 02:37:38,163   cls_loss = 0.08691749457508473
2022-06-12 02:37:38,163   eval_loss = 0.8952284500454412
2022-06-12 02:37:38,163   global_step = 2339
2022-06-12 02:37:38,163   loss = 0.08691749457508473
2022-06-12 02:37:38,164   mcc = 0.23194676355085636
2022-06-12 02:37:43,035 ***** Running evaluation *****
2022-06-12 02:37:43,035   Epoch = 8 iter 2359 step
2022-06-12 02:37:43,035   Num examples = 1043
2022-06-12 02:37:43,035   Batch size = 32
2022-06-12 02:37:43,934 ***** Eval results *****
2022-06-12 02:37:43,934   cls_loss = 0.08672135867880064
2022-06-12 02:37:43,935   eval_loss = 0.9487330624551484
2022-06-12 02:37:43,935   global_step = 2359
2022-06-12 02:37:43,935   loss = 0.08672135867880064
2022-06-12 02:37:43,935   mcc = 0.2392794015641758
2022-06-12 02:37:48,818 ***** Running evaluation *****
2022-06-12 02:37:48,818   Epoch = 8 iter 2379 step
2022-06-12 02:37:48,818   Num examples = 1043
2022-06-12 02:37:48,818   Batch size = 32
2022-06-12 02:37:49,718 ***** Eval results *****
2022-06-12 02:37:49,718   cls_loss = 0.08641504358362269
2022-06-12 02:37:49,718   eval_loss = 0.9635907980528745
2022-06-12 02:37:49,718   global_step = 2379
2022-06-12 02:37:49,718   loss = 0.08641504358362269
2022-06-12 02:37:49,718   mcc = 0.22424582075928595
2022-06-12 02:37:54,593 ***** Running evaluation *****
2022-06-12 02:37:54,594   Epoch = 8 iter 2399 step
2022-06-12 02:37:54,594   Num examples = 1043
2022-06-12 02:37:54,594   Batch size = 32
2022-06-12 02:37:55,493 ***** Eval results *****
2022-06-12 02:37:55,493   cls_loss = 0.08627257164666861
2022-06-12 02:37:55,493   eval_loss = 0.8780925454515399
2022-06-12 02:37:55,493   global_step = 2399
2022-06-12 02:37:55,493   loss = 0.08627257164666861
2022-06-12 02:37:55,493   mcc = 0.20408881471593668
2022-06-12 02:38:00,378 ***** Running evaluation *****
2022-06-12 02:38:00,378   Epoch = 9 iter 2419 step
2022-06-12 02:38:00,378   Num examples = 1043
2022-06-12 02:38:00,378   Batch size = 32
2022-06-12 02:38:01,278 ***** Eval results *****
2022-06-12 02:38:01,278   cls_loss = 0.08499749889597297
2022-06-12 02:38:01,278   eval_loss = 1.0078923684177976
2022-06-12 02:38:01,278   global_step = 2419
2022-06-12 02:38:01,278   loss = 0.08499749889597297
2022-06-12 02:38:01,278   mcc = 0.19006456933307134
2022-06-12 02:38:06,147 ***** Running evaluation *****
2022-06-12 02:38:06,147   Epoch = 9 iter 2439 step
2022-06-12 02:38:06,147   Num examples = 1043
2022-06-12 02:38:06,148   Batch size = 32
2022-06-12 02:38:07,047 ***** Eval results *****
2022-06-12 02:38:07,047   cls_loss = 0.0865474860701296
2022-06-12 02:38:07,047   eval_loss = 0.9135732605601802
2022-06-12 02:38:07,047   global_step = 2439
2022-06-12 02:38:07,047   loss = 0.0865474860701296
2022-06-12 02:38:07,047   mcc = 0.19836291897338604
2022-06-12 02:38:11,939 ***** Running evaluation *****
2022-06-12 02:38:11,940   Epoch = 9 iter 2459 step
2022-06-12 02:38:11,940   Num examples = 1043
2022-06-12 02:38:11,940   Batch size = 32
2022-06-12 02:38:12,841 ***** Eval results *****
2022-06-12 02:38:12,841   cls_loss = 0.08548803401312657
2022-06-12 02:38:12,841   eval_loss = 0.9844963378978498
2022-06-12 02:38:12,841   global_step = 2459
2022-06-12 02:38:12,841   loss = 0.08548803401312657
2022-06-12 02:38:12,841   mcc = 0.23038252487022867
2022-06-12 02:38:17,729 ***** Running evaluation *****
2022-06-12 02:38:17,729   Epoch = 9 iter 2479 step
2022-06-12 02:38:17,729   Num examples = 1043
2022-06-12 02:38:17,729   Batch size = 32
2022-06-12 02:38:18,630 ***** Eval results *****
2022-06-12 02:38:18,630   cls_loss = 0.08715163445786427
2022-06-12 02:38:18,631   eval_loss = 0.9276436937577797
2022-06-12 02:38:18,631   global_step = 2479
2022-06-12 02:38:18,631   loss = 0.08715163445786427
2022-06-12 02:38:18,631   mcc = 0.19594802300869782
2022-06-12 02:38:23,074 ***** Running evaluation *****
2022-06-12 02:38:23,074   Epoch = 7 iter 24999 step
2022-06-12 02:38:23,074   Num examples = 5463
2022-06-12 02:38:23,074   Batch size = 32
2022-06-12 02:38:23,075 ***** Eval results *****
2022-06-12 02:38:23,075   att_loss = 3.6157870928674822
2022-06-12 02:38:23,075   global_step = 24999
2022-06-12 02:38:23,075   loss = 4.494943918510415
2022-06-12 02:38:23,076   rep_loss = 0.8791568275840803
2022-06-12 02:38:23,076 ***** Save model *****
2022-06-12 02:38:23,502 ***** Running evaluation *****
2022-06-12 02:38:23,502   Epoch = 9 iter 2499 step
2022-06-12 02:38:23,502   Num examples = 1043
2022-06-12 02:38:23,503   Batch size = 32
2022-06-12 02:38:24,402 ***** Eval results *****
2022-06-12 02:38:24,403   cls_loss = 0.08740453577289979
2022-06-12 02:38:24,403   eval_loss = 0.9485614624890414
2022-06-12 02:38:24,403   global_step = 2499
2022-06-12 02:38:24,403   loss = 0.08740453577289979
2022-06-12 02:38:24,403   mcc = 0.21903837559176814
2022-06-12 02:38:29,275 ***** Running evaluation *****
2022-06-12 02:38:29,275   Epoch = 9 iter 2519 step
2022-06-12 02:38:29,275   Num examples = 1043
2022-06-12 02:38:29,275   Batch size = 32
2022-06-12 02:38:30,174 ***** Eval results *****
2022-06-12 02:38:30,174   cls_loss = 0.08778503607830097
2022-06-12 02:38:30,175   eval_loss = 0.9235765870773431
2022-06-12 02:38:30,175   global_step = 2519
2022-06-12 02:38:30,175   loss = 0.08778503607830097
2022-06-12 02:38:30,175   mcc = 0.2294316731044511
2022-06-12 02:38:35,037 ***** Running evaluation *****
2022-06-12 02:38:35,037   Epoch = 9 iter 2539 step
2022-06-12 02:38:35,037   Num examples = 1043
2022-06-12 02:38:35,037   Batch size = 32
2022-06-12 02:38:35,937 ***** Eval results *****
2022-06-12 02:38:35,938   cls_loss = 0.08845672581125708
2022-06-12 02:38:35,938   eval_loss = 1.0059459001728983
2022-06-12 02:38:35,938   global_step = 2539
2022-06-12 02:38:35,938   loss = 0.08845672581125708
2022-06-12 02:38:35,938   mcc = 0.2290236458829187
2022-06-12 02:38:40,803 ***** Running evaluation *****
2022-06-12 02:38:40,803   Epoch = 9 iter 2559 step
2022-06-12 02:38:40,803   Num examples = 1043
2022-06-12 02:38:40,803   Batch size = 32
2022-06-12 02:38:41,703 ***** Eval results *****
2022-06-12 02:38:41,703   cls_loss = 0.08871107534147225
2022-06-12 02:38:41,703   eval_loss = 0.8694755380803888
2022-06-12 02:38:41,703   global_step = 2559
2022-06-12 02:38:41,703   loss = 0.08871107534147225
2022-06-12 02:38:41,703   mcc = 0.20965463814321775
2022-06-12 02:38:46,607 ***** Running evaluation *****
2022-06-12 02:38:46,608   Epoch = 9 iter 2579 step
2022-06-12 02:38:46,608   Num examples = 1043
2022-06-12 02:38:46,608   Batch size = 32
2022-06-12 02:38:47,510 ***** Eval results *****
2022-06-12 02:38:47,510   cls_loss = 0.08872978114099665
2022-06-12 02:38:47,510   eval_loss = 1.020976017821919
2022-06-12 02:38:47,510   global_step = 2579
2022-06-12 02:38:47,510   loss = 0.08872978114099665
2022-06-12 02:38:47,510   mcc = 0.21536460552609665
2022-06-12 02:38:52,422 ***** Running evaluation *****
2022-06-12 02:38:52,422   Epoch = 9 iter 2599 step
2022-06-12 02:38:52,422   Num examples = 1043
2022-06-12 02:38:52,422   Batch size = 32
2022-06-12 02:38:53,323 ***** Eval results *****
2022-06-12 02:38:53,323   cls_loss = 0.0878404739065742
2022-06-12 02:38:53,323   eval_loss = 0.9663448613701444
2022-06-12 02:38:53,323   global_step = 2599
2022-06-12 02:38:53,323   loss = 0.0878404739065742
2022-06-12 02:38:53,323   mcc = 0.21462820614637154
2022-06-12 02:38:58,234 ***** Running evaluation *****
2022-06-12 02:38:58,235   Epoch = 9 iter 2619 step
2022-06-12 02:38:58,235   Num examples = 1043
2022-06-12 02:38:58,235   Batch size = 32
2022-06-12 02:38:59,135 ***** Eval results *****
2022-06-12 02:38:59,135   cls_loss = 0.08802750884313826
2022-06-12 02:38:59,136   eval_loss = 0.9080302209565134
2022-06-12 02:38:59,136   global_step = 2619
2022-06-12 02:38:59,136   loss = 0.08802750884313826
2022-06-12 02:38:59,136   mcc = 0.20508319858966723
2022-06-12 02:39:04,003 ***** Running evaluation *****
2022-06-12 02:39:04,004   Epoch = 9 iter 2639 step
2022-06-12 02:39:04,004   Num examples = 1043
2022-06-12 02:39:04,004   Batch size = 32
2022-06-12 02:39:04,904 ***** Eval results *****
2022-06-12 02:39:04,904   cls_loss = 0.0880000209120118
2022-06-12 02:39:04,904   eval_loss = 0.9511068192395297
2022-06-12 02:39:04,904   global_step = 2639
2022-06-12 02:39:04,904   loss = 0.0880000209120118
2022-06-12 02:39:04,904   mcc = 0.21462966773581318
2022-06-12 02:39:09,785 ***** Running evaluation *****
2022-06-12 02:39:09,785   Epoch = 9 iter 2659 step
2022-06-12 02:39:09,785   Num examples = 1043
2022-06-12 02:39:09,785   Batch size = 32
2022-06-12 02:39:10,685 ***** Eval results *****
2022-06-12 02:39:10,685   cls_loss = 0.08758462518744636
2022-06-12 02:39:10,685   eval_loss = 0.9811448870283185
2022-06-12 02:39:10,685   global_step = 2659
2022-06-12 02:39:10,685   loss = 0.08758462518744636
2022-06-12 02:39:10,685   mcc = 0.20739825653942073
2022-06-12 02:39:15,583 ***** Running evaluation *****
2022-06-12 02:39:15,584   Epoch = 10 iter 2679 step
2022-06-12 02:39:15,584   Num examples = 1043
2022-06-12 02:39:15,584   Batch size = 32
2022-06-12 02:39:16,484 ***** Eval results *****
2022-06-12 02:39:16,485   cls_loss = 0.07673465294970407
2022-06-12 02:39:16,485   eval_loss = 0.9983137258977601
2022-06-12 02:39:16,485   global_step = 2679
2022-06-12 02:39:16,485   loss = 0.07673465294970407
2022-06-12 02:39:16,485   mcc = 0.21026739681607276
2022-06-12 02:39:21,392 ***** Running evaluation *****
2022-06-12 02:39:21,392   Epoch = 10 iter 2699 step
2022-06-12 02:39:21,393   Num examples = 1043
2022-06-12 02:39:21,393   Batch size = 32
2022-06-12 02:39:22,295 ***** Eval results *****
2022-06-12 02:39:22,295   cls_loss = 0.08286891338126413
2022-06-12 02:39:22,296   eval_loss = 0.9877451342163663
2022-06-12 02:39:22,296   global_step = 2699
2022-06-12 02:39:22,296   loss = 0.08286891338126413
2022-06-12 02:39:22,296   mcc = 0.1946612918225643
2022-06-12 02:39:27,191 ***** Running evaluation *****
2022-06-12 02:39:27,192   Epoch = 10 iter 2719 step
2022-06-12 02:39:27,192   Num examples = 1043
2022-06-12 02:39:27,192   Batch size = 32
2022-06-12 02:39:28,090 ***** Eval results *****
2022-06-12 02:39:28,091   cls_loss = 0.08546616760443668
2022-06-12 02:39:28,091   eval_loss = 0.930913123217496
2022-06-12 02:39:28,091   global_step = 2719
2022-06-12 02:39:28,091   loss = 0.08546616760443668
2022-06-12 02:39:28,091   mcc = 0.19955176173637962
2022-06-12 02:39:32,982 ***** Running evaluation *****
2022-06-12 02:39:32,983   Epoch = 10 iter 2739 step
2022-06-12 02:39:32,983   Num examples = 1043
2022-06-12 02:39:32,983   Batch size = 32
2022-06-12 02:39:33,882 ***** Eval results *****
2022-06-12 02:39:33,882   cls_loss = 0.08464871724878532
2022-06-12 02:39:33,882   eval_loss = 0.9715070281967972
2022-06-12 02:39:33,882   global_step = 2739
2022-06-12 02:39:33,882   loss = 0.08464871724878532
2022-06-12 02:39:33,882   mcc = 0.21065625313813668
2022-06-12 02:39:38,770 ***** Running evaluation *****
2022-06-12 02:39:38,771   Epoch = 10 iter 2759 step
2022-06-12 02:39:38,771   Num examples = 1043
2022-06-12 02:39:38,771   Batch size = 32
2022-06-12 02:39:39,672 ***** Eval results *****
2022-06-12 02:39:39,672   cls_loss = 0.08462304931678129
2022-06-12 02:39:39,672   eval_loss = 0.9743855785239827
2022-06-12 02:39:39,672   global_step = 2759
2022-06-12 02:39:39,672   loss = 0.08462304931678129
2022-06-12 02:39:39,672   mcc = 0.20391647063386514
2022-06-12 02:39:44,560 ***** Running evaluation *****
2022-06-12 02:39:44,560   Epoch = 10 iter 2779 step
2022-06-12 02:39:44,560   Num examples = 1043
2022-06-12 02:39:44,561   Batch size = 32
2022-06-12 02:39:45,460 ***** Eval results *****
2022-06-12 02:39:45,460   cls_loss = 0.08461492289917184
2022-06-12 02:39:45,460   eval_loss = 0.9700676175681028
2022-06-12 02:39:45,460   global_step = 2779
2022-06-12 02:39:45,460   loss = 0.08461492289917184
2022-06-12 02:39:45,460   mcc = 0.20737809972235108
2022-06-12 02:39:50,368 ***** Running evaluation *****
2022-06-12 02:39:50,368   Epoch = 10 iter 2799 step
2022-06-12 02:39:50,368   Num examples = 1043
2022-06-12 02:39:50,368   Batch size = 32
2022-06-12 02:39:51,268 ***** Eval results *****
2022-06-12 02:39:51,268   cls_loss = 0.0842735287408496
2022-06-12 02:39:51,268   eval_loss = 0.9357090050523932
2022-06-12 02:39:51,268   global_step = 2799
2022-06-12 02:39:51,268   loss = 0.0842735287408496
2022-06-12 02:39:51,268   mcc = 0.17494608975255396
2022-06-12 02:39:56,130 ***** Running evaluation *****
2022-06-12 02:39:56,130   Epoch = 10 iter 2819 step
2022-06-12 02:39:56,130   Num examples = 1043
2022-06-12 02:39:56,130   Batch size = 32
2022-06-12 02:39:57,030 ***** Eval results *****
2022-06-12 02:39:57,031   cls_loss = 0.0839428803164687
2022-06-12 02:39:57,031   eval_loss = 0.97533129381411
2022-06-12 02:39:57,031   global_step = 2819
2022-06-12 02:39:57,031   loss = 0.0839428803164687
2022-06-12 02:39:57,031   mcc = 0.1979221279738887
2022-06-12 02:40:01,916 ***** Running evaluation *****
2022-06-12 02:40:01,916   Epoch = 10 iter 2839 step
2022-06-12 02:40:01,917   Num examples = 1043
2022-06-12 02:40:01,917   Batch size = 32
2022-06-12 02:40:02,817 ***** Eval results *****
2022-06-12 02:40:02,817   cls_loss = 0.08408996797877656
2022-06-12 02:40:02,818   eval_loss = 0.975273199153669
2022-06-12 02:40:02,818   global_step = 2839
2022-06-12 02:40:02,818   loss = 0.08408996797877656
2022-06-12 02:40:02,818   mcc = 0.21511270242767316
2022-06-12 02:40:07,715 ***** Running evaluation *****
2022-06-12 02:40:07,716   Epoch = 10 iter 2859 step
2022-06-12 02:40:07,716   Num examples = 1043
2022-06-12 02:40:07,716   Batch size = 32
2022-06-12 02:40:08,620 ***** Eval results *****
2022-06-12 02:40:08,620   cls_loss = 0.08446637939208398
2022-06-12 02:40:08,620   eval_loss = 0.9708681223970471
2022-06-12 02:40:08,620   global_step = 2859
2022-06-12 02:40:08,621   loss = 0.08446637939208398
2022-06-12 02:40:08,621   mcc = 0.21148178606518211
2022-06-12 02:40:13,515 ***** Running evaluation *****
2022-06-12 02:40:13,516   Epoch = 10 iter 2879 step
2022-06-12 02:40:13,516   Num examples = 1043
2022-06-12 02:40:13,516   Batch size = 32
2022-06-12 02:40:14,417 ***** Eval results *****
2022-06-12 02:40:14,417   cls_loss = 0.08481183441346912
2022-06-12 02:40:14,417   eval_loss = 0.9341288805007935
2022-06-12 02:40:14,417   global_step = 2879
2022-06-12 02:40:14,417   loss = 0.08481183441346912
2022-06-12 02:40:14,417   mcc = 0.21477688175782397
2022-06-12 02:40:19,317 ***** Running evaluation *****
2022-06-12 02:40:19,317   Epoch = 10 iter 2899 step
2022-06-12 02:40:19,317   Num examples = 1043
2022-06-12 02:40:19,317   Batch size = 32
2022-06-12 02:40:20,217 ***** Eval results *****
2022-06-12 02:40:20,217   cls_loss = 0.08589586394322492
2022-06-12 02:40:20,217   eval_loss = 0.8865508997079098
2022-06-12 02:40:20,218   global_step = 2899
2022-06-12 02:40:20,218   loss = 0.08589586394322492
2022-06-12 02:40:20,218   mcc = 0.21524151168039987
2022-06-12 02:40:25,099 ***** Running evaluation *****
2022-06-12 02:40:25,100   Epoch = 10 iter 2919 step
2022-06-12 02:40:25,100   Num examples = 1043
2022-06-12 02:40:25,100   Batch size = 32
2022-06-12 02:40:26,000 ***** Eval results *****
2022-06-12 02:40:26,000   cls_loss = 0.08557070701955313
2022-06-12 02:40:26,000   eval_loss = 0.9428580114335725
2022-06-12 02:40:26,000   global_step = 2919
2022-06-12 02:40:26,000   loss = 0.08557070701955313
2022-06-12 02:40:26,000   mcc = 0.19548014788377568
2022-06-12 02:40:30,876 ***** Running evaluation *****
2022-06-12 02:40:30,876   Epoch = 11 iter 2939 step
2022-06-12 02:40:30,876   Num examples = 1043
2022-06-12 02:40:30,876   Batch size = 32
2022-06-12 02:40:30,928 ***** Running evaluation *****
2022-06-12 02:40:30,929   Epoch = 7 iter 25499 step
2022-06-12 02:40:30,929   Num examples = 5463
2022-06-12 02:40:30,929   Batch size = 32
2022-06-12 02:40:30,930 ***** Eval results *****
2022-06-12 02:40:30,930   att_loss = 3.611878864547753
2022-06-12 02:40:30,930   global_step = 25499
2022-06-12 02:40:30,930   loss = 4.4906158127593105
2022-06-12 02:40:30,930   rep_loss = 0.8787369506298298
2022-06-12 02:40:30,930 ***** Save model *****
2022-06-12 02:40:31,776 ***** Eval results *****
2022-06-12 02:40:31,776   cls_loss = 0.08708121255040169
2022-06-12 02:40:31,776   eval_loss = 0.9238558050357935
2022-06-12 02:40:31,776   global_step = 2939
2022-06-12 02:40:31,776   loss = 0.08708121255040169
2022-06-12 02:40:31,776   mcc = 0.21185656837636874
2022-06-12 02:40:36,686 ***** Running evaluation *****
2022-06-12 02:40:36,687   Epoch = 11 iter 2959 step
2022-06-12 02:40:36,687   Num examples = 1043
2022-06-12 02:40:36,687   Batch size = 32
2022-06-12 02:40:37,588 ***** Eval results *****
2022-06-12 02:40:37,588   cls_loss = 0.08521744033152406
2022-06-12 02:40:37,588   eval_loss = 0.9950951608744535
2022-06-12 02:40:37,588   global_step = 2959
2022-06-12 02:40:37,588   loss = 0.08521744033152406
2022-06-12 02:40:37,588   mcc = 0.2374962983697775
2022-06-12 02:40:42,483 ***** Running evaluation *****
2022-06-12 02:40:42,483   Epoch = 11 iter 2979 step
2022-06-12 02:40:42,483   Num examples = 1043
2022-06-12 02:40:42,484   Batch size = 32
2022-06-12 02:40:43,384 ***** Eval results *****
2022-06-12 02:40:43,384   cls_loss = 0.08872210057008834
2022-06-12 02:40:43,384   eval_loss = 0.8703529581879125
2022-06-12 02:40:43,384   global_step = 2979
2022-06-12 02:40:43,384   loss = 0.08872210057008834
2022-06-12 02:40:43,385   mcc = 0.23410056206034538
2022-06-12 02:40:48,248 ***** Running evaluation *****
2022-06-12 02:40:48,248   Epoch = 11 iter 2999 step
2022-06-12 02:40:48,248   Num examples = 1043
2022-06-12 02:40:48,249   Batch size = 32
2022-06-12 02:40:49,150 ***** Eval results *****
2022-06-12 02:40:49,150   cls_loss = 0.0874531187357441
2022-06-12 02:40:49,150   eval_loss = 0.9807863461248802
2022-06-12 02:40:49,150   global_step = 2999
2022-06-12 02:40:49,151   loss = 0.0874531187357441
2022-06-12 02:40:49,151   mcc = 0.2216778441320201
2022-06-12 02:40:54,028 ***** Running evaluation *****
2022-06-12 02:40:54,028   Epoch = 11 iter 3019 step
2022-06-12 02:40:54,028   Num examples = 1043
2022-06-12 02:40:54,028   Batch size = 32
2022-06-12 02:40:54,928 ***** Eval results *****
2022-06-12 02:40:54,928   cls_loss = 0.08674651956776293
2022-06-12 02:40:54,928   eval_loss = 0.8997817761970289
2022-06-12 02:40:54,928   global_step = 3019
2022-06-12 02:40:54,928   loss = 0.08674651956776293
2022-06-12 02:40:54,928   mcc = 0.2308312966231533
2022-06-12 02:40:59,809 ***** Running evaluation *****
2022-06-12 02:40:59,809   Epoch = 11 iter 3039 step
2022-06-12 02:40:59,810   Num examples = 1043
2022-06-12 02:40:59,810   Batch size = 32
2022-06-12 02:41:00,712 ***** Eval results *****
2022-06-12 02:41:00,712   cls_loss = 0.08725484672422502
2022-06-12 02:41:00,712   eval_loss = 0.9248714726982694
2022-06-12 02:41:00,712   global_step = 3039
2022-06-12 02:41:00,712   loss = 0.08725484672422502
2022-06-12 02:41:00,713   mcc = 0.21428651086075598
2022-06-12 02:41:05,612 ***** Running evaluation *****
2022-06-12 02:41:05,612   Epoch = 11 iter 3059 step
2022-06-12 02:41:05,613   Num examples = 1043
2022-06-12 02:41:05,613   Batch size = 32
2022-06-12 02:41:06,515 ***** Eval results *****
2022-06-12 02:41:06,515   cls_loss = 0.08612340352818613
2022-06-12 02:41:06,515   eval_loss = 0.9742178880807125
2022-06-12 02:41:06,515   global_step = 3059
2022-06-12 02:41:06,515   loss = 0.08612340352818613
2022-06-12 02:41:06,515   mcc = 0.2427003686035201
2022-06-12 02:41:11,415 ***** Running evaluation *****
2022-06-12 02:41:11,415   Epoch = 11 iter 3079 step
2022-06-12 02:41:11,415   Num examples = 1043
2022-06-12 02:41:11,415   Batch size = 32
2022-06-12 02:41:12,318 ***** Eval results *****
2022-06-12 02:41:12,318   cls_loss = 0.08529310164527154
2022-06-12 02:41:12,318   eval_loss = 0.9430130349867272
2022-06-12 02:41:12,318   global_step = 3079
2022-06-12 02:41:12,318   loss = 0.08529310164527154
2022-06-12 02:41:12,318   mcc = 0.22896296148887765
2022-06-12 02:41:17,232 ***** Running evaluation *****
2022-06-12 02:41:17,232   Epoch = 11 iter 3099 step
2022-06-12 02:41:17,232   Num examples = 1043
2022-06-12 02:41:17,232   Batch size = 32
2022-06-12 02:41:18,132 ***** Eval results *****
2022-06-12 02:41:18,133   cls_loss = 0.08598734185467531
2022-06-12 02:41:18,133   eval_loss = 0.885518676403797
2022-06-12 02:41:18,133   global_step = 3099
2022-06-12 02:41:18,133   loss = 0.08598734185467531
2022-06-12 02:41:18,133   mcc = 0.2427227762618374
2022-06-12 02:41:23,033 ***** Running evaluation *****
2022-06-12 02:41:23,034   Epoch = 11 iter 3119 step
2022-06-12 02:41:23,034   Num examples = 1043
2022-06-12 02:41:23,034   Batch size = 32
2022-06-12 02:41:23,935 ***** Eval results *****
2022-06-12 02:41:23,936   cls_loss = 0.08671303367712996
2022-06-12 02:41:23,936   eval_loss = 0.8933557940251899
2022-06-12 02:41:23,936   global_step = 3119
2022-06-12 02:41:23,936   loss = 0.08671303367712996
2022-06-12 02:41:23,936   mcc = 0.2286901411285955
2022-06-12 02:41:28,828 ***** Running evaluation *****
2022-06-12 02:41:28,828   Epoch = 11 iter 3139 step
2022-06-12 02:41:28,828   Num examples = 1043
2022-06-12 02:41:28,828   Batch size = 32
2022-06-12 02:41:29,729 ***** Eval results *****
2022-06-12 02:41:29,729   cls_loss = 0.0863124406573796
2022-06-12 02:41:29,729   eval_loss = 0.940579989642808
2022-06-12 02:41:29,729   global_step = 3139
2022-06-12 02:41:29,730   loss = 0.0863124406573796
2022-06-12 02:41:29,730   mcc = 0.2221172362135175
2022-06-12 02:41:34,649 ***** Running evaluation *****
2022-06-12 02:41:34,649   Epoch = 11 iter 3159 step
2022-06-12 02:41:34,649   Num examples = 1043
2022-06-12 02:41:34,649   Batch size = 32
2022-06-12 02:41:35,549 ***** Eval results *****
2022-06-12 02:41:35,549   cls_loss = 0.08662301902701189
2022-06-12 02:41:35,549   eval_loss = 0.9144208079034631
2022-06-12 02:41:35,549   global_step = 3159
2022-06-12 02:41:35,549   loss = 0.08662301902701189
2022-06-12 02:41:35,549   mcc = 0.23315224346442234
2022-06-12 02:41:40,417 ***** Running evaluation *****
2022-06-12 02:41:40,418   Epoch = 11 iter 3179 step
2022-06-12 02:41:40,418   Num examples = 1043
2022-06-12 02:41:40,418   Batch size = 32
2022-06-12 02:41:41,317 ***** Eval results *****
2022-06-12 02:41:41,317   cls_loss = 0.08668763566115671
2022-06-12 02:41:41,318   eval_loss = 0.9214665934895024
2022-06-12 02:41:41,318   global_step = 3179
2022-06-12 02:41:41,318   loss = 0.08668763566115671
2022-06-12 02:41:41,318   mcc = 0.22544483431155077
2022-06-12 02:41:46,205 ***** Running evaluation *****
2022-06-12 02:41:46,205   Epoch = 11 iter 3199 step
2022-06-12 02:41:46,205   Num examples = 1043
2022-06-12 02:41:46,205   Batch size = 32
2022-06-12 02:41:47,105 ***** Eval results *****
2022-06-12 02:41:47,105   cls_loss = 0.08640000465830774
2022-06-12 02:41:47,105   eval_loss = 0.9196337100231287
2022-06-12 02:41:47,105   global_step = 3199
2022-06-12 02:41:47,105   loss = 0.08640000465830774
2022-06-12 02:41:47,105   mcc = 0.22541730182403236
2022-06-12 02:41:51,991 ***** Running evaluation *****
2022-06-12 02:41:51,992   Epoch = 12 iter 3219 step
2022-06-12 02:41:51,992   Num examples = 1043
2022-06-12 02:41:51,992   Batch size = 32
2022-06-12 02:41:52,891 ***** Eval results *****
2022-06-12 02:41:52,891   cls_loss = 0.08549863398075104
2022-06-12 02:41:52,892   eval_loss = 0.9391945040587223
2022-06-12 02:41:52,892   global_step = 3219
2022-06-12 02:41:52,892   loss = 0.08549863398075104
2022-06-12 02:41:52,892   mcc = 0.22142593907719943
2022-06-12 02:41:57,784 ***** Running evaluation *****
2022-06-12 02:41:57,785   Epoch = 12 iter 3239 step
2022-06-12 02:41:57,785   Num examples = 1043
2022-06-12 02:41:57,785   Batch size = 32
2022-06-12 02:41:58,684 ***** Eval results *****
2022-06-12 02:41:58,684   cls_loss = 0.08610594740935734
2022-06-12 02:41:58,684   eval_loss = 0.9424442242492329
2022-06-12 02:41:58,684   global_step = 3239
2022-06-12 02:41:58,684   loss = 0.08610594740935734
2022-06-12 02:41:58,685   mcc = 0.2597914084042546
2022-06-12 02:42:03,570 ***** Running evaluation *****
2022-06-12 02:42:03,571   Epoch = 12 iter 3259 step
2022-06-12 02:42:03,571   Num examples = 1043
2022-06-12 02:42:03,571   Batch size = 32
2022-06-12 02:42:04,471 ***** Eval results *****
2022-06-12 02:42:04,471   cls_loss = 0.08544905565001748
2022-06-12 02:42:04,471   eval_loss = 0.9542708387880614
2022-06-12 02:42:04,472   global_step = 3259
2022-06-12 02:42:04,472   loss = 0.08544905565001748
2022-06-12 02:42:04,472   mcc = 0.2309239093878603
2022-06-12 02:42:09,366 ***** Running evaluation *****
2022-06-12 02:42:09,366   Epoch = 12 iter 3279 step
2022-06-12 02:42:09,366   Num examples = 1043
2022-06-12 02:42:09,366   Batch size = 32
2022-06-12 02:42:10,265 ***** Eval results *****
2022-06-12 02:42:10,265   cls_loss = 0.08435854613780976
2022-06-12 02:42:10,266   eval_loss = 0.9481947033694296
2022-06-12 02:42:10,266   global_step = 3279
2022-06-12 02:42:10,266   loss = 0.08435854613780976
2022-06-12 02:42:10,266   mcc = 0.20624907655056632
2022-06-12 02:42:15,160 ***** Running evaluation *****
2022-06-12 02:42:15,160   Epoch = 12 iter 3299 step
2022-06-12 02:42:15,160   Num examples = 1043
2022-06-12 02:42:15,160   Batch size = 32
2022-06-12 02:42:16,061 ***** Eval results *****
2022-06-12 02:42:16,061   cls_loss = 0.08415891512444144
2022-06-12 02:42:16,061   eval_loss = 0.9204873650362997
2022-06-12 02:42:16,061   global_step = 3299
2022-06-12 02:42:16,061   loss = 0.08415891512444144
2022-06-12 02:42:16,061   mcc = 0.22381100124268438
2022-06-12 02:42:20,927 ***** Running evaluation *****
2022-06-12 02:42:20,927   Epoch = 12 iter 3319 step
2022-06-12 02:42:20,928   Num examples = 1043
2022-06-12 02:42:20,928   Batch size = 32
2022-06-12 02:42:21,827 ***** Eval results *****
2022-06-12 02:42:21,828   cls_loss = 0.08501840991818387
2022-06-12 02:42:21,828   eval_loss = 0.9190009579514012
2022-06-12 02:42:21,828   global_step = 3319
2022-06-12 02:42:21,828   loss = 0.08501840991818387
2022-06-12 02:42:21,828   mcc = 0.2324635152768777
2022-06-12 02:42:26,702 ***** Running evaluation *****
2022-06-12 02:42:26,702   Epoch = 12 iter 3339 step
2022-06-12 02:42:26,702   Num examples = 1043
2022-06-12 02:42:26,702   Batch size = 32
2022-06-12 02:42:27,602 ***** Eval results *****
2022-06-12 02:42:27,602   cls_loss = 0.08446215766447562
2022-06-12 02:42:27,602   eval_loss = 0.9637525912487146
2022-06-12 02:42:27,602   global_step = 3339
2022-06-12 02:42:27,603   loss = 0.08446215766447562
2022-06-12 02:42:27,603   mcc = 0.23105093846035507
2022-06-12 02:42:32,487 ***** Running evaluation *****
2022-06-12 02:42:32,487   Epoch = 12 iter 3359 step
2022-06-12 02:42:32,487   Num examples = 1043
2022-06-12 02:42:32,487   Batch size = 32
2022-06-12 02:42:33,386 ***** Eval results *****
2022-06-12 02:42:33,387   cls_loss = 0.08382166638489692
2022-06-12 02:42:33,387   eval_loss = 0.9709600851391301
2022-06-12 02:42:33,387   global_step = 3359
2022-06-12 02:42:33,387   loss = 0.08382166638489692
2022-06-12 02:42:33,387   mcc = 0.24724884036850667
2022-06-12 02:42:38,283 ***** Running evaluation *****
2022-06-12 02:42:38,283   Epoch = 12 iter 3379 step
2022-06-12 02:42:38,283   Num examples = 1043
2022-06-12 02:42:38,283   Batch size = 32
2022-06-12 02:42:38,530 ***** Running evaluation *****
2022-06-12 02:42:38,531   Epoch = 7 iter 25999 step
2022-06-12 02:42:38,531   Num examples = 5463
2022-06-12 02:42:38,531   Batch size = 32
2022-06-12 02:42:38,532 ***** Eval results *****
2022-06-12 02:42:38,533   att_loss = 3.6144066310917156
2022-06-12 02:42:38,533   global_step = 25999
2022-06-12 02:42:38,533   loss = 4.4929241575107675
2022-06-12 02:42:38,533   rep_loss = 0.8785175281755356
2022-06-12 02:42:38,533 ***** Save model *****
2022-06-12 02:42:39,182 ***** Eval results *****
2022-06-12 02:42:39,182   cls_loss = 0.08396740449326379
2022-06-12 02:42:39,183   eval_loss = 0.9386392226724913
2022-06-12 02:42:39,183   global_step = 3379
2022-06-12 02:42:39,183   loss = 0.08396740449326379
2022-06-12 02:42:39,183   mcc = 0.2253396117220268
2022-06-12 02:42:44,087 ***** Running evaluation *****
2022-06-12 02:42:44,087   Epoch = 12 iter 3399 step
2022-06-12 02:42:44,087   Num examples = 1043
2022-06-12 02:42:44,087   Batch size = 32
2022-06-12 02:42:44,986 ***** Eval results *****
2022-06-12 02:42:44,986   cls_loss = 0.08380038646551279
2022-06-12 02:42:44,986   eval_loss = 0.9293705017277689
2022-06-12 02:42:44,986   global_step = 3399
2022-06-12 02:42:44,987   loss = 0.08380038646551279
2022-06-12 02:42:44,987   mcc = 0.21077041180199932
2022-06-12 02:42:49,851 ***** Running evaluation *****
2022-06-12 02:42:49,852   Epoch = 12 iter 3419 step
2022-06-12 02:42:49,852   Num examples = 1043
2022-06-12 02:42:49,852   Batch size = 32
2022-06-12 02:42:50,751 ***** Eval results *****
2022-06-12 02:42:50,751   cls_loss = 0.08380585018285486
2022-06-12 02:42:50,751   eval_loss = 0.9483885963757833
2022-06-12 02:42:50,752   global_step = 3419
2022-06-12 02:42:50,752   loss = 0.08380585018285486
2022-06-12 02:42:50,752   mcc = 0.20100910423052637
2022-06-12 02:42:55,625 ***** Running evaluation *****
2022-06-12 02:42:55,625   Epoch = 12 iter 3439 step
2022-06-12 02:42:55,625   Num examples = 1043
2022-06-12 02:42:55,625   Batch size = 32
2022-06-12 02:42:56,529 ***** Eval results *****
2022-06-12 02:42:56,529   cls_loss = 0.08394244436887985
2022-06-12 02:42:56,529   eval_loss = 0.9915761152903239
2022-06-12 02:42:56,529   global_step = 3439
2022-06-12 02:42:56,529   loss = 0.08394244436887985
2022-06-12 02:42:56,529   mcc = 0.22815052890294182
2022-06-12 02:43:01,410 ***** Running evaluation *****
2022-06-12 02:43:01,410   Epoch = 12 iter 3459 step
2022-06-12 02:43:01,410   Num examples = 1043
2022-06-12 02:43:01,410   Batch size = 32
2022-06-12 02:43:02,309 ***** Eval results *****
2022-06-12 02:43:02,309   cls_loss = 0.08318967632218903
2022-06-12 02:43:02,309   eval_loss = 0.9921321155446948
2022-06-12 02:43:02,309   global_step = 3459
2022-06-12 02:43:02,309   loss = 0.08318967632218903
2022-06-12 02:43:02,309   mcc = 0.2340718748381182
2022-06-12 02:43:07,207 ***** Running evaluation *****
2022-06-12 02:43:07,208   Epoch = 13 iter 3479 step
2022-06-12 02:43:07,208   Num examples = 1043
2022-06-12 02:43:07,208   Batch size = 32
2022-06-12 02:43:08,106 ***** Eval results *****
2022-06-12 02:43:08,106   cls_loss = 0.07677311170846224
2022-06-12 02:43:08,106   eval_loss = 0.9732058572046685
2022-06-12 02:43:08,106   global_step = 3479
2022-06-12 02:43:08,107   loss = 0.07677311170846224
2022-06-12 02:43:08,107   mcc = 0.19056427447337368
2022-06-12 02:43:12,985 ***** Running evaluation *****
2022-06-12 02:43:12,985   Epoch = 13 iter 3499 step
2022-06-12 02:43:12,985   Num examples = 1043
2022-06-12 02:43:12,985   Batch size = 32
2022-06-12 02:43:13,885 ***** Eval results *****
2022-06-12 02:43:13,886   cls_loss = 0.08859053546828884
2022-06-12 02:43:13,886   eval_loss = 0.8796664169340422
2022-06-12 02:43:13,886   global_step = 3499
2022-06-12 02:43:13,886   loss = 0.08859053546828884
2022-06-12 02:43:13,886   mcc = 0.21425672604895138
2022-06-12 02:43:18,785 ***** Running evaluation *****
2022-06-12 02:43:18,785   Epoch = 13 iter 3519 step
2022-06-12 02:43:18,785   Num examples = 1043
2022-06-12 02:43:18,785   Batch size = 32
2022-06-12 02:43:19,685 ***** Eval results *****
2022-06-12 02:43:19,686   cls_loss = 0.08850338015084465
2022-06-12 02:43:19,686   eval_loss = 0.9678645007538073
2022-06-12 02:43:19,686   global_step = 3519
2022-06-12 02:43:19,686   loss = 0.08850338015084465
2022-06-12 02:43:19,686   mcc = 0.25951349199782353
2022-06-12 02:43:24,568 ***** Running evaluation *****
2022-06-12 02:43:24,568   Epoch = 13 iter 3539 step
2022-06-12 02:43:24,568   Num examples = 1043
2022-06-12 02:43:24,568   Batch size = 32
2022-06-12 02:43:25,467 ***** Eval results *****
2022-06-12 02:43:25,467   cls_loss = 0.08577480682117097
2022-06-12 02:43:25,468   eval_loss = 0.9309047072222738
2022-06-12 02:43:25,468   global_step = 3539
2022-06-12 02:43:25,468   loss = 0.08577480682117097
2022-06-12 02:43:25,468   mcc = 0.25125577104871455
2022-06-12 02:43:30,354 ***** Running evaluation *****
2022-06-12 02:43:30,354   Epoch = 13 iter 3559 step
2022-06-12 02:43:30,354   Num examples = 1043
2022-06-12 02:43:30,354   Batch size = 32
2022-06-12 02:43:31,255 ***** Eval results *****
2022-06-12 02:43:31,255   cls_loss = 0.0847559015859257
2022-06-12 02:43:31,255   eval_loss = 0.9170876985246484
2022-06-12 02:43:31,256   global_step = 3559
2022-06-12 02:43:31,256   loss = 0.0847559015859257
2022-06-12 02:43:31,256   mcc = 0.24189979918651575
2022-06-12 02:43:36,186 ***** Running evaluation *****
2022-06-12 02:43:36,187   Epoch = 13 iter 3579 step
2022-06-12 02:43:36,187   Num examples = 1043
2022-06-12 02:43:36,187   Batch size = 32
2022-06-12 02:43:37,090 ***** Eval results *****
2022-06-12 02:43:37,090   cls_loss = 0.08439679781871813
2022-06-12 02:43:37,090   eval_loss = 0.9699296418464545
2022-06-12 02:43:37,090   global_step = 3579
2022-06-12 02:43:37,090   loss = 0.08439679781871813
2022-06-12 02:43:37,090   mcc = 0.23128964842265648
2022-06-12 02:43:42,002 ***** Running evaluation *****
2022-06-12 02:43:42,002   Epoch = 13 iter 3599 step
2022-06-12 02:43:42,002   Num examples = 1043
2022-06-12 02:43:42,002   Batch size = 32
2022-06-12 02:43:42,910 ***** Eval results *****
2022-06-12 02:43:42,910   cls_loss = 0.08449378627119586
2022-06-12 02:43:42,910   eval_loss = 0.9206404776284189
2022-06-12 02:43:42,910   global_step = 3599
2022-06-12 02:43:42,910   loss = 0.08449378627119586
2022-06-12 02:43:42,910   mcc = 0.23557742249995717
2022-06-12 02:43:47,816 ***** Running evaluation *****
2022-06-12 02:43:47,817   Epoch = 13 iter 3619 step
2022-06-12 02:43:47,817   Num examples = 1043
2022-06-12 02:43:47,817   Batch size = 32
2022-06-12 02:43:48,719 ***** Eval results *****
2022-06-12 02:43:48,719   cls_loss = 0.08471764452956818
2022-06-12 02:43:48,719   eval_loss = 0.9036676116061934
2022-06-12 02:43:48,719   global_step = 3619
2022-06-12 02:43:48,719   loss = 0.08471764452956818
2022-06-12 02:43:48,719   mcc = 0.23481097845044813
2022-06-12 02:43:53,640 ***** Running evaluation *****
2022-06-12 02:43:53,641   Epoch = 13 iter 3639 step
2022-06-12 02:43:53,641   Num examples = 1043
2022-06-12 02:43:53,641   Batch size = 32
2022-06-12 02:43:54,543 ***** Eval results *****
2022-06-12 02:43:54,543   cls_loss = 0.0845648737269498
2022-06-12 02:43:54,543   eval_loss = 0.9313101560780497
2022-06-12 02:43:54,543   global_step = 3639
2022-06-12 02:43:54,544   loss = 0.0845648737269498
2022-06-12 02:43:54,544   mcc = 0.2153216199980261
2022-06-12 02:43:59,430 ***** Running evaluation *****
2022-06-12 02:43:59,430   Epoch = 13 iter 3659 step
2022-06-12 02:43:59,430   Num examples = 1043
2022-06-12 02:43:59,430   Batch size = 32
2022-06-12 02:44:00,330 ***** Eval results *****
2022-06-12 02:44:00,330   cls_loss = 0.08439715480075237
2022-06-12 02:44:00,330   eval_loss = 0.934303877028552
2022-06-12 02:44:00,330   global_step = 3659
2022-06-12 02:44:00,330   loss = 0.08439715480075237
2022-06-12 02:44:00,330   mcc = 0.2498593831811951
2022-06-12 02:44:05,234 ***** Running evaluation *****
2022-06-12 02:44:05,234   Epoch = 13 iter 3679 step
2022-06-12 02:44:05,234   Num examples = 1043
2022-06-12 02:44:05,234   Batch size = 32
2022-06-12 02:44:06,138 ***** Eval results *****
2022-06-12 02:44:06,138   cls_loss = 0.08492744588651337
2022-06-12 02:44:06,139   eval_loss = 0.9029046315135378
2022-06-12 02:44:06,139   global_step = 3679
2022-06-12 02:44:06,139   loss = 0.08492744588651337
2022-06-12 02:44:06,139   mcc = 0.267456279131036
2022-06-12 02:44:11,062 ***** Running evaluation *****
2022-06-12 02:44:11,062   Epoch = 13 iter 3699 step
2022-06-12 02:44:11,062   Num examples = 1043
2022-06-12 02:44:11,062   Batch size = 32
2022-06-12 02:44:11,962 ***** Eval results *****
2022-06-12 02:44:11,962   cls_loss = 0.08530685903602525
2022-06-12 02:44:11,963   eval_loss = 0.902631430011807
2022-06-12 02:44:11,963   global_step = 3699
2022-06-12 02:44:11,963   loss = 0.08530685903602525
2022-06-12 02:44:11,963   mcc = 0.2412025240252886
2022-06-12 02:44:16,866 ***** Running evaluation *****
2022-06-12 02:44:16,866   Epoch = 13 iter 3719 step
2022-06-12 02:44:16,866   Num examples = 1043
2022-06-12 02:44:16,866   Batch size = 32
2022-06-12 02:44:17,771 ***** Eval results *****
2022-06-12 02:44:17,771   cls_loss = 0.08529607080403835
2022-06-12 02:44:17,772   eval_loss = 0.9248042901357015
2022-06-12 02:44:17,772   global_step = 3719
2022-06-12 02:44:17,772   loss = 0.08529607080403835
2022-06-12 02:44:17,772   mcc = 0.22975833614823102
2022-06-12 02:44:22,670 ***** Running evaluation *****
2022-06-12 02:44:22,670   Epoch = 14 iter 3739 step
2022-06-12 02:44:22,670   Num examples = 1043
2022-06-12 02:44:22,670   Batch size = 32
2022-06-12 02:44:23,573 ***** Eval results *****
2022-06-12 02:44:23,573   cls_loss = 0.0745118260383606
2022-06-12 02:44:23,573   eval_loss = 0.9352128424427726
2022-06-12 02:44:23,573   global_step = 3739
2022-06-12 02:44:23,573   loss = 0.0745118260383606
2022-06-12 02:44:23,573   mcc = 0.2320339352206038
2022-06-12 02:44:28,463 ***** Running evaluation *****
2022-06-12 02:44:28,464   Epoch = 14 iter 3759 step
2022-06-12 02:44:28,464   Num examples = 1043
2022-06-12 02:44:28,464   Batch size = 32
2022-06-12 02:44:29,370 ***** Eval results *****
2022-06-12 02:44:29,371   cls_loss = 0.08004592678376607
2022-06-12 02:44:29,371   eval_loss = 0.9430272082487742
2022-06-12 02:44:29,371   global_step = 3759
2022-06-12 02:44:29,371   loss = 0.08004592678376607
2022-06-12 02:44:29,371   mcc = 0.22331970375585078
2022-06-12 02:44:34,262 ***** Running evaluation *****
2022-06-12 02:44:34,263   Epoch = 14 iter 3779 step
2022-06-12 02:44:34,263   Num examples = 1043
2022-06-12 02:44:34,263   Batch size = 32
2022-06-12 02:44:35,166 ***** Eval results *****
2022-06-12 02:44:35,166   cls_loss = 0.08313817948829837
2022-06-12 02:44:35,166   eval_loss = 0.9143594810456941
2022-06-12 02:44:35,166   global_step = 3779
2022-06-12 02:44:35,166   loss = 0.08313817948829837
2022-06-12 02:44:35,166   mcc = 0.22306616606673543
2022-06-12 02:44:40,045 ***** Running evaluation *****
2022-06-12 02:44:40,045   Epoch = 14 iter 3799 step
2022-06-12 02:44:40,045   Num examples = 1043
2022-06-12 02:44:40,045   Batch size = 32
2022-06-12 02:44:40,946 ***** Eval results *****
2022-06-12 02:44:40,946   cls_loss = 0.0841052138903102
2022-06-12 02:44:40,946   eval_loss = 0.9433758836804014
2022-06-12 02:44:40,946   global_step = 3799
2022-06-12 02:44:40,946   loss = 0.0841052138903102
2022-06-12 02:44:40,946   mcc = 0.22317225695623735
2022-06-12 02:44:45,841 ***** Running evaluation *****
2022-06-12 02:44:45,841   Epoch = 14 iter 3819 step
2022-06-12 02:44:45,841   Num examples = 1043
2022-06-12 02:44:45,841   Batch size = 32
2022-06-12 02:44:46,228 ***** Running evaluation *****
2022-06-12 02:44:46,229   Epoch = 8 iter 26499 step
2022-06-12 02:44:46,229   Num examples = 5463
2022-06-12 02:44:46,229   Batch size = 32
2022-06-12 02:44:46,230 ***** Eval results *****
2022-06-12 02:44:46,230   att_loss = 3.5778789611089796
2022-06-12 02:44:46,231   global_step = 26499
2022-06-12 02:44:46,231   loss = 4.4510637154654855
2022-06-12 02:44:46,231   rep_loss = 0.8731847456523351
2022-06-12 02:44:46,231 ***** Save model *****
2022-06-12 02:44:46,744 ***** Eval results *****
2022-06-12 02:44:46,744   cls_loss = 0.08368823024225824
2022-06-12 02:44:46,744   eval_loss = 0.9651665723685062
2022-06-12 02:44:46,745   global_step = 3819
2022-06-12 02:44:46,745   loss = 0.08368823024225824
2022-06-12 02:44:46,745   mcc = 0.23873407559405013
2022-06-12 02:44:51,642 ***** Running evaluation *****
2022-06-12 02:44:51,642   Epoch = 14 iter 3839 step
2022-06-12 02:44:51,643   Num examples = 1043
2022-06-12 02:44:51,643   Batch size = 32
2022-06-12 02:44:52,543 ***** Eval results *****
2022-06-12 02:44:52,544   cls_loss = 0.08518469562329868
2022-06-12 02:44:52,544   eval_loss = 0.9155809391628612
2022-06-12 02:44:52,544   global_step = 3839
2022-06-12 02:44:52,544   loss = 0.08518469562329868
2022-06-12 02:44:52,544   mcc = 0.22230601856829857
2022-06-12 02:44:57,447 ***** Running evaluation *****
2022-06-12 02:44:57,447   Epoch = 14 iter 3859 step
2022-06-12 02:44:57,447   Num examples = 1043
2022-06-12 02:44:57,447   Batch size = 32
2022-06-12 02:44:58,350 ***** Eval results *****
2022-06-12 02:44:58,350   cls_loss = 0.08382163067494543
2022-06-12 02:44:58,350   eval_loss = 0.9329643619782997
2022-06-12 02:44:58,350   global_step = 3859
2022-06-12 02:44:58,350   loss = 0.08382163067494543
2022-06-12 02:44:58,350   mcc = 0.21770396781193882
2022-06-12 02:45:03,227 ***** Running evaluation *****
2022-06-12 02:45:03,227   Epoch = 14 iter 3879 step
2022-06-12 02:45:03,227   Num examples = 1043
2022-06-12 02:45:03,227   Batch size = 32
2022-06-12 02:45:04,127 ***** Eval results *****
2022-06-12 02:45:04,127   cls_loss = 0.08440329044634569
2022-06-12 02:45:04,127   eval_loss = 0.9154934774745594
2022-06-12 02:45:04,128   global_step = 3879
2022-06-12 02:45:04,128   loss = 0.08440329044634569
2022-06-12 02:45:04,128   mcc = 0.2623744406802895
2022-06-12 02:45:09,006 ***** Running evaluation *****
2022-06-12 02:45:09,006   Epoch = 14 iter 3899 step
2022-06-12 02:45:09,006   Num examples = 1043
2022-06-12 02:45:09,006   Batch size = 32
2022-06-12 02:45:09,906 ***** Eval results *****
2022-06-12 02:45:09,906   cls_loss = 0.08460653832425243
2022-06-12 02:45:09,906   eval_loss = 0.9488372540835178
2022-06-12 02:45:09,906   global_step = 3899
2022-06-12 02:45:09,907   loss = 0.08460653832425243
2022-06-12 02:45:09,907   mcc = 0.2062147479205027
2022-06-12 02:45:14,791 ***** Running evaluation *****
2022-06-12 02:45:14,791   Epoch = 14 iter 3919 step
2022-06-12 02:45:14,791   Num examples = 1043
2022-06-12 02:45:14,791   Batch size = 32
2022-06-12 02:45:15,691 ***** Eval results *****
2022-06-12 02:45:15,691   cls_loss = 0.08448903139124918
2022-06-12 02:45:15,691   eval_loss = 0.9178445745598186
2022-06-12 02:45:15,691   global_step = 3919
2022-06-12 02:45:15,691   loss = 0.08448903139124918
2022-06-12 02:45:15,691   mcc = 0.219443785956581
2022-06-12 02:45:20,572 ***** Running evaluation *****
2022-06-12 02:45:20,573   Epoch = 14 iter 3939 step
2022-06-12 02:45:20,573   Num examples = 1043
2022-06-12 02:45:20,573   Batch size = 32
2022-06-12 02:45:21,476 ***** Eval results *****
2022-06-12 02:45:21,476   cls_loss = 0.08446657624262482
2022-06-12 02:45:21,476   eval_loss = 0.9338182843092716
2022-06-12 02:45:21,476   global_step = 3939
2022-06-12 02:45:21,476   loss = 0.08446657624262482
2022-06-12 02:45:21,476   mcc = 0.22573819311612775
2022-06-12 02:45:26,407 ***** Running evaluation *****
2022-06-12 02:45:26,407   Epoch = 14 iter 3959 step
2022-06-12 02:45:26,407   Num examples = 1043
2022-06-12 02:45:26,407   Batch size = 32
2022-06-12 02:45:27,308 ***** Eval results *****
2022-06-12 02:45:27,308   cls_loss = 0.08496861149687573
2022-06-12 02:45:27,308   eval_loss = 0.926608987829902
2022-06-12 02:45:27,308   global_step = 3959
2022-06-12 02:45:27,308   loss = 0.08496861149687573
2022-06-12 02:45:27,308   mcc = 0.2637395433561201
2022-06-12 02:45:32,220 ***** Running evaluation *****
2022-06-12 02:45:32,220   Epoch = 14 iter 3979 step
2022-06-12 02:45:32,220   Num examples = 1043
2022-06-12 02:45:32,220   Batch size = 32
2022-06-12 02:45:33,120 ***** Eval results *****
2022-06-12 02:45:33,120   cls_loss = 0.08506435075488823
2022-06-12 02:45:33,120   eval_loss = 0.8975237129312573
2022-06-12 02:45:33,121   global_step = 3979
2022-06-12 02:45:33,121   loss = 0.08506435075488823
2022-06-12 02:45:33,121   mcc = 0.2298072292026734
2022-06-12 02:45:38,013 ***** Running evaluation *****
2022-06-12 02:45:38,014   Epoch = 14 iter 3999 step
2022-06-12 02:45:38,014   Num examples = 1043
2022-06-12 02:45:38,014   Batch size = 32
2022-06-12 02:45:38,915 ***** Eval results *****
2022-06-12 02:45:38,915   cls_loss = 0.08525370672288068
2022-06-12 02:45:38,915   eval_loss = 0.8993610459746737
2022-06-12 02:45:38,915   global_step = 3999
2022-06-12 02:45:38,915   loss = 0.08525370672288068
2022-06-12 02:45:38,915   mcc = 0.2329152641457069
2022-06-12 02:45:43,817 ***** Running evaluation *****
2022-06-12 02:45:43,818   Epoch = 15 iter 4019 step
2022-06-12 02:45:43,818   Num examples = 1043
2022-06-12 02:45:43,818   Batch size = 32
2022-06-12 02:45:44,718 ***** Eval results *****
2022-06-12 02:45:44,718   cls_loss = 0.0811928931091513
2022-06-12 02:45:44,718   eval_loss = 0.9488666608478084
2022-06-12 02:45:44,719   global_step = 4019
2022-06-12 02:45:44,719   loss = 0.0811928931091513
2022-06-12 02:45:44,719   mcc = 0.22509426868235644
2022-06-12 02:45:49,617 ***** Running evaluation *****
2022-06-12 02:45:49,618   Epoch = 15 iter 4039 step
2022-06-12 02:45:49,618   Num examples = 1043
2022-06-12 02:45:49,618   Batch size = 32
2022-06-12 02:45:50,519 ***** Eval results *****
2022-06-12 02:45:50,520   cls_loss = 0.08081711445222883
2022-06-12 02:45:50,520   eval_loss = 0.919487375201601
2022-06-12 02:45:50,520   global_step = 4039
2022-06-12 02:45:50,520   loss = 0.08081711445222883
2022-06-12 02:45:50,520   mcc = 0.21457875270570104
2022-06-12 02:45:55,434 ***** Running evaluation *****
2022-06-12 02:45:55,435   Epoch = 15 iter 4059 step
2022-06-12 02:45:55,435   Num examples = 1043
2022-06-12 02:45:55,435   Batch size = 32
2022-06-12 02:45:56,337 ***** Eval results *****
2022-06-12 02:45:56,337   cls_loss = 0.08215724297419742
2022-06-12 02:45:56,337   eval_loss = 0.9212254159378283
2022-06-12 02:45:56,337   global_step = 4059
2022-06-12 02:45:56,337   loss = 0.08215724297419742
2022-06-12 02:45:56,337   mcc = 0.23426533656843046
2022-06-12 02:46:01,267 ***** Running evaluation *****
2022-06-12 02:46:01,267   Epoch = 15 iter 4079 step
2022-06-12 02:46:01,267   Num examples = 1043
2022-06-12 02:46:01,267   Batch size = 32
2022-06-12 02:46:02,166 ***** Eval results *****
2022-06-12 02:46:02,166   cls_loss = 0.08238733051395095
2022-06-12 02:46:02,166   eval_loss = 0.9436490165464806
2022-06-12 02:46:02,167   global_step = 4079
2022-06-12 02:46:02,167   loss = 0.08238733051395095
2022-06-12 02:46:02,167   mcc = 0.23557589395964054
2022-06-12 02:46:07,051 ***** Running evaluation *****
2022-06-12 02:46:07,051   Epoch = 15 iter 4099 step
2022-06-12 02:46:07,051   Num examples = 1043
2022-06-12 02:46:07,051   Batch size = 32
2022-06-12 02:46:07,951 ***** Eval results *****
2022-06-12 02:46:07,952   cls_loss = 0.0825555581757997
2022-06-12 02:46:07,952   eval_loss = 0.9253155254956448
2022-06-12 02:46:07,952   global_step = 4099
2022-06-12 02:46:07,952   loss = 0.0825555581757997
2022-06-12 02:46:07,952   mcc = 0.23985260898664917
2022-06-12 02:46:12,842 ***** Running evaluation *****
2022-06-12 02:46:12,842   Epoch = 15 iter 4119 step
2022-06-12 02:46:12,842   Num examples = 1043
2022-06-12 02:46:12,842   Batch size = 32
2022-06-12 02:46:13,743 ***** Eval results *****
2022-06-12 02:46:13,743   cls_loss = 0.08323540639851176
2022-06-12 02:46:13,743   eval_loss = 0.9091596802075704
2022-06-12 02:46:13,743   global_step = 4119
2022-06-12 02:46:13,743   loss = 0.08323540639851176
2022-06-12 02:46:13,743   mcc = 0.2221431154499795
2022-06-12 02:46:18,637 ***** Running evaluation *****
2022-06-12 02:46:18,637   Epoch = 15 iter 4139 step
2022-06-12 02:46:18,637   Num examples = 1043
2022-06-12 02:46:18,637   Batch size = 32
2022-06-12 02:46:19,539 ***** Eval results *****
2022-06-12 02:46:19,539   cls_loss = 0.08310904073070234
2022-06-12 02:46:19,539   eval_loss = 0.9802555667631554
2022-06-12 02:46:19,539   global_step = 4139
2022-06-12 02:46:19,539   loss = 0.08310904073070234
2022-06-12 02:46:19,539   mcc = 0.20796275588624588
2022-06-12 02:46:24,427 ***** Running evaluation *****
2022-06-12 02:46:24,428   Epoch = 15 iter 4159 step
2022-06-12 02:46:24,428   Num examples = 1043
2022-06-12 02:46:24,428   Batch size = 32
2022-06-12 02:46:25,330 ***** Eval results *****
2022-06-12 02:46:25,330   cls_loss = 0.08357305972316822
2022-06-12 02:46:25,330   eval_loss = 0.9232021055438302
2022-06-12 02:46:25,330   global_step = 4159
2022-06-12 02:46:25,331   loss = 0.08357305972316822
2022-06-12 02:46:25,331   mcc = 0.2006830801099795
2022-06-12 02:46:30,217 ***** Running evaluation *****
2022-06-12 02:46:30,217   Epoch = 15 iter 4179 step
2022-06-12 02:46:30,217   Num examples = 1043
2022-06-12 02:46:30,217   Batch size = 32
2022-06-12 02:46:31,117 ***** Eval results *****
2022-06-12 02:46:31,117   cls_loss = 0.08432939513747034
2022-06-12 02:46:31,117   eval_loss = 0.8892239034175873
2022-06-12 02:46:31,117   global_step = 4179
2022-06-12 02:46:31,117   loss = 0.08432939513747034
2022-06-12 02:46:31,118   mcc = 0.19689620786923703
2022-06-12 02:46:36,019 ***** Running evaluation *****
2022-06-12 02:46:36,020   Epoch = 15 iter 4199 step
2022-06-12 02:46:36,020   Num examples = 1043
2022-06-12 02:46:36,020   Batch size = 32
2022-06-12 02:46:36,920 ***** Eval results *****
2022-06-12 02:46:36,920   cls_loss = 0.08505809650823627
2022-06-12 02:46:36,920   eval_loss = 0.9032460776242343
2022-06-12 02:46:36,920   global_step = 4199
2022-06-12 02:46:36,920   loss = 0.08505809650823627
2022-06-12 02:46:36,920   mcc = 0.22500271827545165
2022-06-12 02:46:41,808 ***** Running evaluation *****
2022-06-12 02:46:41,808   Epoch = 15 iter 4219 step
2022-06-12 02:46:41,808   Num examples = 1043
2022-06-12 02:46:41,808   Batch size = 32
2022-06-12 02:46:42,708 ***** Eval results *****
2022-06-12 02:46:42,708   cls_loss = 0.08460103377490957
2022-06-12 02:46:42,708   eval_loss = 0.9461799805814569
2022-06-12 02:46:42,708   global_step = 4219
2022-06-12 02:46:42,708   loss = 0.08460103377490957
2022-06-12 02:46:42,708   mcc = 0.21717977726137364
2022-06-12 02:46:47,590 ***** Running evaluation *****
2022-06-12 02:46:47,590   Epoch = 15 iter 4239 step
2022-06-12 02:46:47,590   Num examples = 1043
2022-06-12 02:46:47,590   Batch size = 32
2022-06-12 02:46:48,491 ***** Eval results *****
2022-06-12 02:46:48,492   cls_loss = 0.08462111300064458
2022-06-12 02:46:48,492   eval_loss = 0.9398598589680411
2022-06-12 02:46:48,492   global_step = 4239
2022-06-12 02:46:48,492   loss = 0.08462111300064458
2022-06-12 02:46:48,492   mcc = 0.19393617084009424
2022-06-12 02:46:53,348 ***** Running evaluation *****
2022-06-12 02:46:53,349   Epoch = 15 iter 4259 step
2022-06-12 02:46:53,349   Num examples = 1043
2022-06-12 02:46:53,349   Batch size = 32
2022-06-12 02:46:54,189 ***** Running evaluation *****
2022-06-12 02:46:54,190   Epoch = 8 iter 26999 step
2022-06-12 02:46:54,190   Num examples = 5463
2022-06-12 02:46:54,190   Batch size = 32
2022-06-12 02:46:54,191 ***** Eval results *****
2022-06-12 02:46:54,191   att_loss = 3.5598410117845596
2022-06-12 02:46:54,191   global_step = 26999
2022-06-12 02:46:54,192   loss = 4.432205546267925
2022-06-12 02:46:54,192   rep_loss = 0.8723645347759036
2022-06-12 02:46:54,192 ***** Save model *****
2022-06-12 02:46:54,249 ***** Eval results *****
2022-06-12 02:46:54,249   cls_loss = 0.08436589912460075
2022-06-12 02:46:54,249   eval_loss = 0.9683357838428381
2022-06-12 02:46:54,249   global_step = 4259
2022-06-12 02:46:54,249   loss = 0.08436589912460075
2022-06-12 02:46:54,249   mcc = 0.19099803035164228
2022-06-12 02:46:59,123 ***** Running evaluation *****
2022-06-12 02:46:59,123   Epoch = 16 iter 4279 step
2022-06-12 02:46:59,123   Num examples = 1043
2022-06-12 02:46:59,123   Batch size = 32
2022-06-12 02:47:00,022 ***** Eval results *****
2022-06-12 02:47:00,023   cls_loss = 0.09072781141315188
2022-06-12 02:47:00,023   eval_loss = 0.9760996383247953
2022-06-12 02:47:00,023   global_step = 4279
2022-06-12 02:47:00,023   loss = 0.09072781141315188
2022-06-12 02:47:00,023   mcc = 0.22037445237760042
2022-06-12 02:47:04,895 ***** Running evaluation *****
2022-06-12 02:47:04,895   Epoch = 16 iter 4299 step
2022-06-12 02:47:04,895   Num examples = 1043
2022-06-12 02:47:04,895   Batch size = 32
2022-06-12 02:47:05,796 ***** Eval results *****
2022-06-12 02:47:05,796   cls_loss = 0.08337874958912532
2022-06-12 02:47:05,796   eval_loss = 0.9628075731523109
2022-06-12 02:47:05,796   global_step = 4299
2022-06-12 02:47:05,796   loss = 0.08337874958912532
2022-06-12 02:47:05,796   mcc = 0.22424193394048839
2022-06-12 02:47:10,679 ***** Running evaluation *****
2022-06-12 02:47:10,679   Epoch = 16 iter 4319 step
2022-06-12 02:47:10,679   Num examples = 1043
2022-06-12 02:47:10,679   Batch size = 32
2022-06-12 02:47:11,578 ***** Eval results *****
2022-06-12 02:47:11,578   cls_loss = 0.08362359743803105
2022-06-12 02:47:11,578   eval_loss = 0.9427220089869066
2022-06-12 02:47:11,579   global_step = 4319
2022-06-12 02:47:11,579   loss = 0.08362359743803105
2022-06-12 02:47:11,579   mcc = 0.226260630798264
2022-06-12 02:47:16,467 ***** Running evaluation *****
2022-06-12 02:47:16,467   Epoch = 16 iter 4339 step
2022-06-12 02:47:16,467   Num examples = 1043
2022-06-12 02:47:16,467   Batch size = 32
2022-06-12 02:47:17,371 ***** Eval results *****
2022-06-12 02:47:17,372   cls_loss = 0.08420205583323294
2022-06-12 02:47:17,372   eval_loss = 0.9389246530605085
2022-06-12 02:47:17,372   global_step = 4339
2022-06-12 02:47:17,372   loss = 0.08420205583323294
2022-06-12 02:47:17,372   mcc = 0.2037351935559132
2022-06-12 02:47:22,233 ***** Running evaluation *****
2022-06-12 02:47:22,233   Epoch = 16 iter 4359 step
2022-06-12 02:47:22,233   Num examples = 1043
2022-06-12 02:47:22,233   Batch size = 32
2022-06-12 02:47:23,134 ***** Eval results *****
2022-06-12 02:47:23,135   cls_loss = 0.08435057471880968
2022-06-12 02:47:23,135   eval_loss = 0.9555961932196761
2022-06-12 02:47:23,135   global_step = 4359
2022-06-12 02:47:23,135   loss = 0.08435057471880968
2022-06-12 02:47:23,135   mcc = 0.22668951469998705
2022-06-12 02:47:28,022 ***** Running evaluation *****
2022-06-12 02:47:28,023   Epoch = 16 iter 4379 step
2022-06-12 02:47:28,023   Num examples = 1043
2022-06-12 02:47:28,023   Batch size = 32
2022-06-12 02:47:28,923 ***** Eval results *****
2022-06-12 02:47:28,924   cls_loss = 0.08449707222040569
2022-06-12 02:47:28,924   eval_loss = 0.939891924460729
2022-06-12 02:47:28,924   global_step = 4379
2022-06-12 02:47:28,924   loss = 0.08449707222040569
2022-06-12 02:47:28,924   mcc = 0.2235131095078494
2022-06-12 02:47:33,818 ***** Running evaluation *****
2022-06-12 02:47:33,818   Epoch = 16 iter 4399 step
2022-06-12 02:47:33,818   Num examples = 1043
2022-06-12 02:47:33,818   Batch size = 32
2022-06-12 02:47:34,716 ***** Eval results *****
2022-06-12 02:47:34,716   cls_loss = 0.08473284695092148
2022-06-12 02:47:34,716   eval_loss = 0.9305135580626401
2022-06-12 02:47:34,717   global_step = 4399
2022-06-12 02:47:34,717   loss = 0.08473284695092148
2022-06-12 02:47:34,717   mcc = 0.21646226983912564
2022-06-12 02:47:39,604 ***** Running evaluation *****
2022-06-12 02:47:39,604   Epoch = 16 iter 4419 step
2022-06-12 02:47:39,605   Num examples = 1043
2022-06-12 02:47:39,605   Batch size = 32
2022-06-12 02:47:40,504 ***** Eval results *****
2022-06-12 02:47:40,504   cls_loss = 0.08421824076751462
2022-06-12 02:47:40,504   eval_loss = 0.9655735023093946
2022-06-12 02:47:40,504   global_step = 4419
2022-06-12 02:47:40,504   loss = 0.08421824076751462
2022-06-12 02:47:40,504   mcc = 0.22500271827545165
2022-06-12 02:47:45,412 ***** Running evaluation *****
2022-06-12 02:47:45,412   Epoch = 16 iter 4439 step
2022-06-12 02:47:45,412   Num examples = 1043
2022-06-12 02:47:45,412   Batch size = 32
2022-06-12 02:47:46,315 ***** Eval results *****
2022-06-12 02:47:46,315   cls_loss = 0.08432653934477333
2022-06-12 02:47:46,315   eval_loss = 0.9282373135740106
2022-06-12 02:47:46,315   global_step = 4439
2022-06-12 02:47:46,315   loss = 0.08432653934477333
2022-06-12 02:47:46,315   mcc = 0.2324990430050044
2022-06-12 02:47:51,219 ***** Running evaluation *****
2022-06-12 02:47:51,219   Epoch = 16 iter 4459 step
2022-06-12 02:47:51,220   Num examples = 1043
2022-06-12 02:47:51,220   Batch size = 32
2022-06-12 02:47:52,124 ***** Eval results *****
2022-06-12 02:47:52,124   cls_loss = 0.08445246724840154
2022-06-12 02:47:52,124   eval_loss = 0.8880490985783663
2022-06-12 02:47:52,124   global_step = 4459
2022-06-12 02:47:52,124   loss = 0.08445246724840154
2022-06-12 02:47:52,125   mcc = 0.22659221689796533
2022-06-12 02:47:57,024 ***** Running evaluation *****
2022-06-12 02:47:57,025   Epoch = 16 iter 4479 step
2022-06-12 02:47:57,025   Num examples = 1043
2022-06-12 02:47:57,025   Batch size = 32
2022-06-12 02:47:57,926 ***** Eval results *****
2022-06-12 02:47:57,926   cls_loss = 0.08420641957849696
2022-06-12 02:47:57,926   eval_loss = 0.9501873625047279
2022-06-12 02:47:57,926   global_step = 4479
2022-06-12 02:47:57,926   loss = 0.08420641957849696
2022-06-12 02:47:57,927   mcc = 0.2529047291936044
2022-06-12 02:48:02,828 ***** Running evaluation *****
2022-06-12 02:48:02,829   Epoch = 16 iter 4499 step
2022-06-12 02:48:02,829   Num examples = 1043
2022-06-12 02:48:02,829   Batch size = 32
2022-06-12 02:48:03,730 ***** Eval results *****
2022-06-12 02:48:03,730   cls_loss = 0.08380378743362847
2022-06-12 02:48:03,730   eval_loss = 0.9186533707560915
2022-06-12 02:48:03,731   global_step = 4499
2022-06-12 02:48:03,731   loss = 0.08380378743362847
2022-06-12 02:48:03,731   mcc = 0.23578691527626477
2022-06-12 02:48:08,640 ***** Running evaluation *****
2022-06-12 02:48:08,640   Epoch = 16 iter 4519 step
2022-06-12 02:48:08,640   Num examples = 1043
2022-06-12 02:48:08,640   Batch size = 32
2022-06-12 02:48:09,539 ***** Eval results *****
2022-06-12 02:48:09,539   cls_loss = 0.08363055600690456
2022-06-12 02:48:09,539   eval_loss = 0.9292605708945881
2022-06-12 02:48:09,540   global_step = 4519
2022-06-12 02:48:09,540   loss = 0.08363055600690456
2022-06-12 02:48:09,540   mcc = 0.2244163040507826
2022-06-12 02:48:14,477 ***** Running evaluation *****
2022-06-12 02:48:14,478   Epoch = 16 iter 4539 step
2022-06-12 02:48:14,478   Num examples = 1043
2022-06-12 02:48:14,478   Batch size = 32
2022-06-12 02:48:15,389 ***** Eval results *****
2022-06-12 02:48:15,389   cls_loss = 0.08363371504268396
2022-06-12 02:48:15,389   eval_loss = 0.9464544924822721
2022-06-12 02:48:15,390   global_step = 4539
2022-06-12 02:48:15,390   loss = 0.08363371504268396
2022-06-12 02:48:15,390   mcc = 0.22423725901852415
2022-06-12 02:48:20,293 ***** Running evaluation *****
2022-06-12 02:48:20,293   Epoch = 17 iter 4559 step
2022-06-12 02:48:20,293   Num examples = 1043
2022-06-12 02:48:20,293   Batch size = 32
2022-06-12 02:48:21,195 ***** Eval results *****
2022-06-12 02:48:21,196   cls_loss = 0.08433790355920792
2022-06-12 02:48:21,196   eval_loss = 0.9483831145546653
2022-06-12 02:48:21,196   global_step = 4559
2022-06-12 02:48:21,196   loss = 0.08433790355920792
2022-06-12 02:48:21,196   mcc = 0.22917949300884982
2022-06-12 02:48:26,135 ***** Running evaluation *****
2022-06-12 02:48:26,135   Epoch = 17 iter 4579 step
2022-06-12 02:48:26,135   Num examples = 1043
2022-06-12 02:48:26,135   Batch size = 32
2022-06-12 02:48:27,037 ***** Eval results *****
2022-06-12 02:48:27,037   cls_loss = 0.08137602787464857
2022-06-12 02:48:27,037   eval_loss = 0.9696351289749146
2022-06-12 02:48:27,037   global_step = 4579
2022-06-12 02:48:27,037   loss = 0.08137602787464857
2022-06-12 02:48:27,037   mcc = 0.22643934692336393
2022-06-12 02:48:31,972 ***** Running evaluation *****
2022-06-12 02:48:31,972   Epoch = 17 iter 4599 step
2022-06-12 02:48:31,972   Num examples = 1043
2022-06-12 02:48:31,972   Batch size = 32
2022-06-12 02:48:32,875 ***** Eval results *****
2022-06-12 02:48:32,875   cls_loss = 0.08231675003965695
2022-06-12 02:48:32,875   eval_loss = 0.9328874548276266
2022-06-12 02:48:32,875   global_step = 4599
2022-06-12 02:48:32,876   loss = 0.08231675003965695
2022-06-12 02:48:32,876   mcc = 0.2142375721312879
2022-06-12 02:48:37,865 ***** Running evaluation *****
2022-06-12 02:48:37,865   Epoch = 17 iter 4619 step
2022-06-12 02:48:37,865   Num examples = 1043
2022-06-12 02:48:37,865   Batch size = 32
2022-06-12 02:48:38,766 ***** Eval results *****
2022-06-12 02:48:38,766   cls_loss = 0.08251184867694975
2022-06-12 02:48:38,766   eval_loss = 0.9598447653380308
2022-06-12 02:48:38,766   global_step = 4619
2022-06-12 02:48:38,766   loss = 0.08251184867694975
2022-06-12 02:48:38,767   mcc = 0.24685877128827513
2022-06-12 02:48:43,673 ***** Running evaluation *****
2022-06-12 02:48:43,673   Epoch = 17 iter 4639 step
2022-06-12 02:48:43,673   Num examples = 1043
2022-06-12 02:48:43,673   Batch size = 32
2022-06-12 02:48:44,573 ***** Eval results *****
2022-06-12 02:48:44,573   cls_loss = 0.08238903813064098
2022-06-12 02:48:44,573   eval_loss = 0.9515355680928086
2022-06-12 02:48:44,573   global_step = 4639
2022-06-12 02:48:44,573   loss = 0.08238903813064098
2022-06-12 02:48:44,573   mcc = 0.2470622994215344
2022-06-12 02:48:49,490 ***** Running evaluation *****
2022-06-12 02:48:49,491   Epoch = 17 iter 4659 step
2022-06-12 02:48:49,491   Num examples = 1043
2022-06-12 02:48:49,491   Batch size = 32
2022-06-12 02:48:50,392 ***** Eval results *****
2022-06-12 02:48:50,392   cls_loss = 0.08214805349707603
2022-06-12 02:48:50,393   eval_loss = 0.959219338315906
2022-06-12 02:48:50,393   global_step = 4659
2022-06-12 02:48:50,393   loss = 0.08214805349707603
2022-06-12 02:48:50,393   mcc = 0.2460437933366867
2022-06-12 02:48:55,305 ***** Running evaluation *****
2022-06-12 02:48:55,305   Epoch = 17 iter 4679 step
2022-06-12 02:48:55,305   Num examples = 1043
2022-06-12 02:48:55,305   Batch size = 32
2022-06-12 02:48:56,206 ***** Eval results *****
2022-06-12 02:48:56,206   cls_loss = 0.08164828265351909
2022-06-12 02:48:56,206   eval_loss = 0.9535011603976741
2022-06-12 02:48:56,206   global_step = 4679
2022-06-12 02:48:56,206   loss = 0.08164828265351909
2022-06-12 02:48:56,206   mcc = 0.24725780951448623
2022-06-12 02:49:01,068 ***** Running evaluation *****
2022-06-12 02:49:01,069   Epoch = 17 iter 4699 step
2022-06-12 02:49:01,069   Num examples = 1043
2022-06-12 02:49:01,069   Batch size = 32
2022-06-12 02:49:01,814 ***** Running evaluation *****
2022-06-12 02:49:01,815   Epoch = 8 iter 27499 step
2022-06-12 02:49:01,815   Num examples = 5463
2022-06-12 02:49:01,815   Batch size = 32
2022-06-12 02:49:01,816 ***** Eval results *****
2022-06-12 02:49:01,817   att_loss = 3.5549297479622264
2022-06-12 02:49:01,817   global_step = 27499
2022-06-12 02:49:01,817   loss = 4.426722867407273
2022-06-12 02:49:01,817   rep_loss = 0.8717931203969078
2022-06-12 02:49:01,817 ***** Save model *****
2022-06-12 02:49:01,968 ***** Eval results *****
2022-06-12 02:49:01,968   cls_loss = 0.08210070440545678
2022-06-12 02:49:01,968   eval_loss = 0.9108805990580356
2022-06-12 02:49:01,968   global_step = 4699
2022-06-12 02:49:01,969   loss = 0.08210070440545678
2022-06-12 02:49:01,969   mcc = 0.23968325937560422
2022-06-12 02:49:06,832 ***** Running evaluation *****
2022-06-12 02:49:06,832   Epoch = 17 iter 4719 step
2022-06-12 02:49:06,832   Num examples = 1043
2022-06-12 02:49:06,832   Batch size = 32
2022-06-12 02:49:07,734 ***** Eval results *****
2022-06-12 02:49:07,734   cls_loss = 0.0822742186486721
2022-06-12 02:49:07,734   eval_loss = 0.9662022662885261
2022-06-12 02:49:07,734   global_step = 4719
2022-06-12 02:49:07,734   loss = 0.0822742186486721
2022-06-12 02:49:07,734   mcc = 0.2496817428764705
2022-06-12 02:49:12,639 ***** Running evaluation *****
2022-06-12 02:49:12,639   Epoch = 17 iter 4739 step
2022-06-12 02:49:12,639   Num examples = 1043
2022-06-12 02:49:12,639   Batch size = 32
2022-06-12 02:49:13,539 ***** Eval results *****
2022-06-12 02:49:13,539   cls_loss = 0.08212371222674847
2022-06-12 02:49:13,539   eval_loss = 0.9568113892367391
2022-06-12 02:49:13,539   global_step = 4739
2022-06-12 02:49:13,539   loss = 0.08212371222674847
2022-06-12 02:49:13,539   mcc = 0.2701843547294651
2022-06-12 02:49:18,457 ***** Running evaluation *****
2022-06-12 02:49:18,457   Epoch = 17 iter 4759 step
2022-06-12 02:49:18,457   Num examples = 1043
2022-06-12 02:49:18,457   Batch size = 32
2022-06-12 02:49:19,359 ***** Eval results *****
2022-06-12 02:49:19,359   cls_loss = 0.08274194560945033
2022-06-12 02:49:19,359   eval_loss = 0.9416573472095259
2022-06-12 02:49:19,359   global_step = 4759
2022-06-12 02:49:19,359   loss = 0.08274194560945033
2022-06-12 02:49:19,359   mcc = 0.27716229620528465
2022-06-12 02:49:24,264 ***** Running evaluation *****
2022-06-12 02:49:24,265   Epoch = 17 iter 4779 step
2022-06-12 02:49:24,265   Num examples = 1043
2022-06-12 02:49:24,265   Batch size = 32
2022-06-12 02:49:25,168 ***** Eval results *****
2022-06-12 02:49:25,168   cls_loss = 0.08306390397871534
2022-06-12 02:49:25,169   eval_loss = 0.9548776556145061
2022-06-12 02:49:25,169   global_step = 4779
2022-06-12 02:49:25,169   loss = 0.08306390397871534
2022-06-12 02:49:25,169   mcc = 0.2136402714038066
2022-06-12 02:49:30,065 ***** Running evaluation *****
2022-06-12 02:49:30,065   Epoch = 17 iter 4799 step
2022-06-12 02:49:30,065   Num examples = 1043
2022-06-12 02:49:30,065   Batch size = 32
2022-06-12 02:49:30,965 ***** Eval results *****
2022-06-12 02:49:30,965   cls_loss = 0.08273293020633551
2022-06-12 02:49:30,965   eval_loss = 0.9610148984374423
2022-06-12 02:49:30,965   global_step = 4799
2022-06-12 02:49:30,965   loss = 0.08273293020633551
2022-06-12 02:49:30,968   mcc = 0.25367187185079343
2022-06-12 02:49:35,882 ***** Running evaluation *****
2022-06-12 02:49:35,882   Epoch = 18 iter 4819 step
2022-06-12 02:49:35,882   Num examples = 1043
2022-06-12 02:49:35,882   Batch size = 32
2022-06-12 02:49:36,786 ***** Eval results *****
2022-06-12 02:49:36,786   cls_loss = 0.08416942449716422
2022-06-12 02:49:36,786   eval_loss = 0.920260404998606
2022-06-12 02:49:36,786   global_step = 4819
2022-06-12 02:49:36,786   loss = 0.08416942449716422
2022-06-12 02:49:36,786   mcc = 0.27265721492278644
2022-06-12 02:49:41,696 ***** Running evaluation *****
2022-06-12 02:49:41,696   Epoch = 18 iter 4839 step
2022-06-12 02:49:41,696   Num examples = 1043
2022-06-12 02:49:41,696   Batch size = 32
2022-06-12 02:49:42,599 ***** Eval results *****
2022-06-12 02:49:42,599   cls_loss = 0.08441974651632887
2022-06-12 02:49:42,600   eval_loss = 0.9204260381785306
2022-06-12 02:49:42,600   global_step = 4839
2022-06-12 02:49:42,600   loss = 0.08441974651632887
2022-06-12 02:49:42,600   mcc = 0.23761610142406409
2022-06-12 02:49:47,508 ***** Running evaluation *****
2022-06-12 02:49:47,508   Epoch = 18 iter 4859 step
2022-06-12 02:49:47,508   Num examples = 1043
2022-06-12 02:49:47,508   Batch size = 32
2022-06-12 02:49:48,411 ***** Eval results *****
2022-06-12 02:49:48,411   cls_loss = 0.08383876577300846
2022-06-12 02:49:48,411   eval_loss = 0.9693392849329746
2022-06-12 02:49:48,411   global_step = 4859
2022-06-12 02:49:48,411   loss = 0.08383876577300846
2022-06-12 02:49:48,411   mcc = 0.21759597327077843
2022-06-12 02:49:53,284 ***** Running evaluation *****
2022-06-12 02:49:53,285   Epoch = 18 iter 4879 step
2022-06-12 02:49:53,285   Num examples = 1043
2022-06-12 02:49:53,285   Batch size = 32
2022-06-12 02:49:54,187 ***** Eval results *****
2022-06-12 02:49:54,187   cls_loss = 0.08370768197187006
2022-06-12 02:49:54,187   eval_loss = 0.9349208275477091
2022-06-12 02:49:54,188   global_step = 4879
2022-06-12 02:49:54,188   loss = 0.08370768197187006
2022-06-12 02:49:54,188   mcc = 0.26003944383054384
2022-06-12 02:49:59,079 ***** Running evaluation *****
2022-06-12 02:49:59,080   Epoch = 18 iter 4899 step
2022-06-12 02:49:59,080   Num examples = 1043
2022-06-12 02:49:59,080   Batch size = 32
2022-06-12 02:49:59,979 ***** Eval results *****
2022-06-12 02:49:59,979   cls_loss = 0.08337568916300292
2022-06-12 02:49:59,979   eval_loss = 0.9451182349161669
2022-06-12 02:49:59,979   global_step = 4899
2022-06-12 02:49:59,979   loss = 0.08337568916300292
2022-06-12 02:49:59,979   mcc = 0.26209378093381486
2022-06-12 02:50:04,857 ***** Running evaluation *****
2022-06-12 02:50:04,857   Epoch = 18 iter 4919 step
2022-06-12 02:50:04,857   Num examples = 1043
2022-06-12 02:50:04,857   Batch size = 32
2022-06-12 02:50:05,757 ***** Eval results *****
2022-06-12 02:50:05,757   cls_loss = 0.08353563582738943
2022-06-12 02:50:05,757   eval_loss = 0.943043875874895
2022-06-12 02:50:05,757   global_step = 4919
2022-06-12 02:50:05,757   loss = 0.08353563582738943
2022-06-12 02:50:05,757   mcc = 0.22327972019684564
2022-06-12 02:50:10,652 ***** Running evaluation *****
2022-06-12 02:50:10,652   Epoch = 18 iter 4939 step
2022-06-12 02:50:10,652   Num examples = 1043
2022-06-12 02:50:10,652   Batch size = 32
2022-06-12 02:50:11,553 ***** Eval results *****
2022-06-12 02:50:11,553   cls_loss = 0.08360982075669712
2022-06-12 02:50:11,553   eval_loss = 0.9377520210815199
2022-06-12 02:50:11,553   global_step = 4939
2022-06-12 02:50:11,553   loss = 0.08360982075669712
2022-06-12 02:50:11,554   mcc = 0.22865388596427977
2022-06-12 02:50:16,425 ***** Running evaluation *****
2022-06-12 02:50:16,425   Epoch = 18 iter 4959 step
2022-06-12 02:50:16,425   Num examples = 1043
2022-06-12 02:50:16,425   Batch size = 32
2022-06-12 02:50:17,325 ***** Eval results *****
2022-06-12 02:50:17,325   cls_loss = 0.08412328649774875
2022-06-12 02:50:17,325   eval_loss = 0.931759754816691
2022-06-12 02:50:17,325   global_step = 4959
2022-06-12 02:50:17,325   loss = 0.08412328649774875
2022-06-12 02:50:17,325   mcc = 0.22688533709443207
2022-06-12 02:50:22,198 ***** Running evaluation *****
2022-06-12 02:50:22,198   Epoch = 18 iter 4979 step
2022-06-12 02:50:22,198   Num examples = 1043
2022-06-12 02:50:22,198   Batch size = 32
2022-06-12 02:50:23,099 ***** Eval results *****
2022-06-12 02:50:23,099   cls_loss = 0.08394838990159117
2022-06-12 02:50:23,100   eval_loss = 0.9018344960429452
2022-06-12 02:50:23,100   global_step = 4979
2022-06-12 02:50:23,100   loss = 0.08394838990159117
2022-06-12 02:50:23,100   mcc = 0.24059226299737854
2022-06-12 02:50:27,971 ***** Running evaluation *****
2022-06-12 02:50:27,971   Epoch = 18 iter 4999 step
2022-06-12 02:50:27,971   Num examples = 1043
2022-06-12 02:50:27,971   Batch size = 32
2022-06-12 02:50:28,873 ***** Eval results *****
2022-06-12 02:50:28,873   cls_loss = 0.08401260331967952
2022-06-12 02:50:28,873   eval_loss = 0.9353540494586482
2022-06-12 02:50:28,873   global_step = 4999
2022-06-12 02:50:28,873   loss = 0.08401260331967952
2022-06-12 02:50:28,873   mcc = 0.25785725704832546
2022-06-12 02:50:33,736 ***** Running evaluation *****
2022-06-12 02:50:33,736   Epoch = 18 iter 5019 step
2022-06-12 02:50:33,736   Num examples = 1043
2022-06-12 02:50:33,736   Batch size = 32
2022-06-12 02:50:34,636 ***** Eval results *****
2022-06-12 02:50:34,636   cls_loss = 0.08425869442907298
2022-06-12 02:50:34,636   eval_loss = 0.9406079514460131
2022-06-12 02:50:34,636   global_step = 5019
2022-06-12 02:50:34,636   loss = 0.08425869442907298
2022-06-12 02:50:34,636   mcc = 0.2668990606188593
2022-06-12 02:50:39,511 ***** Running evaluation *****
2022-06-12 02:50:39,512   Epoch = 18 iter 5039 step
2022-06-12 02:50:39,512   Num examples = 1043
2022-06-12 02:50:39,512   Batch size = 32
2022-06-12 02:50:40,411 ***** Eval results *****
2022-06-12 02:50:40,412   cls_loss = 0.08434016206361705
2022-06-12 02:50:40,412   eval_loss = 0.9032745280049064
2022-06-12 02:50:40,412   global_step = 5039
2022-06-12 02:50:40,412   loss = 0.08434016206361705
2022-06-12 02:50:40,412   mcc = 0.2655180877484442
2022-06-12 02:50:45,296 ***** Running evaluation *****
2022-06-12 02:50:45,296   Epoch = 18 iter 5059 step
2022-06-12 02:50:45,296   Num examples = 1043
2022-06-12 02:50:45,296   Batch size = 32
2022-06-12 02:50:46,196 ***** Eval results *****
2022-06-12 02:50:46,196   cls_loss = 0.08428787634424541
2022-06-12 02:50:46,196   eval_loss = 0.9604289540738771
2022-06-12 02:50:46,197   global_step = 5059
2022-06-12 02:50:46,197   loss = 0.08428787634424541
2022-06-12 02:50:46,197   mcc = 0.2635098894314153
2022-06-12 02:50:51,074 ***** Running evaluation *****
2022-06-12 02:50:51,074   Epoch = 19 iter 5079 step
2022-06-12 02:50:51,074   Num examples = 1043
2022-06-12 02:50:51,075   Batch size = 32
2022-06-12 02:50:51,973 ***** Eval results *****
2022-06-12 02:50:51,974   cls_loss = 0.0967996579905351
2022-06-12 02:50:51,974   eval_loss = 0.9071310487660494
2022-06-12 02:50:51,974   global_step = 5079
2022-06-12 02:50:51,974   loss = 0.0967996579905351
2022-06-12 02:50:51,974   mcc = 0.2607931460871485
2022-06-12 02:50:56,832 ***** Running evaluation *****
2022-06-12 02:50:56,832   Epoch = 19 iter 5099 step
2022-06-12 02:50:56,832   Num examples = 1043
2022-06-12 02:50:56,832   Batch size = 32
2022-06-12 02:50:57,731 ***** Eval results *****
2022-06-12 02:50:57,731   cls_loss = 0.08692359121946189
2022-06-12 02:50:57,731   eval_loss = 0.9507717941746567
2022-06-12 02:50:57,731   global_step = 5099
2022-06-12 02:50:57,731   loss = 0.08692359121946189
2022-06-12 02:50:57,732   mcc = 0.25806700779996966
2022-06-12 02:51:02,611 ***** Running evaluation *****
2022-06-12 02:51:02,611   Epoch = 19 iter 5119 step
2022-06-12 02:51:02,611   Num examples = 1043
2022-06-12 02:51:02,611   Batch size = 32
2022-06-12 02:51:03,510 ***** Eval results *****
2022-06-12 02:51:03,510   cls_loss = 0.08480921841186026
2022-06-12 02:51:03,510   eval_loss = 0.9404608787912311
2022-06-12 02:51:03,510   global_step = 5119
2022-06-12 02:51:03,510   loss = 0.08480921841186026
2022-06-12 02:51:03,510   mcc = 0.2587560378236877
2022-06-12 02:51:08,415 ***** Running evaluation *****
2022-06-12 02:51:08,415   Epoch = 19 iter 5139 step
2022-06-12 02:51:08,415   Num examples = 1043
2022-06-12 02:51:08,415   Batch size = 32
2022-06-12 02:51:09,316 ***** Eval results *****
2022-06-12 02:51:09,316   cls_loss = 0.08406075282078801
2022-06-12 02:51:09,316   eval_loss = 0.9330867023179026
2022-06-12 02:51:09,316   global_step = 5139
2022-06-12 02:51:09,316   loss = 0.08406075282078801
2022-06-12 02:51:09,317   mcc = 0.2617913001253491
2022-06-12 02:51:09,687 ***** Running evaluation *****
2022-06-12 02:51:09,687   Epoch = 8 iter 27999 step
2022-06-12 02:51:09,687   Num examples = 5463
2022-06-12 02:51:09,688   Batch size = 32
2022-06-12 02:51:09,689 ***** Eval results *****
2022-06-12 02:51:09,689   att_loss = 3.5685144547916967
2022-06-12 02:51:09,689   global_step = 27999
2022-06-12 02:51:09,689   loss = 4.440622698439711
2022-06-12 02:51:09,689   rep_loss = 0.8721082478515373
2022-06-12 02:51:09,690 ***** Save model *****
2022-06-12 02:51:14,207 ***** Running evaluation *****
2022-06-12 02:51:14,207   Epoch = 19 iter 5159 step
2022-06-12 02:51:14,207   Num examples = 1043
2022-06-12 02:51:14,207   Batch size = 32
2022-06-12 02:51:15,105 ***** Eval results *****
2022-06-12 02:51:15,106   cls_loss = 0.08347250070682792
2022-06-12 02:51:15,106   eval_loss = 0.9632329751144756
2022-06-12 02:51:15,106   global_step = 5159
2022-06-12 02:51:15,106   loss = 0.08347250070682792
2022-06-12 02:51:15,106   mcc = 0.24947143738126262
2022-06-12 02:51:19,996 ***** Running evaluation *****
2022-06-12 02:51:19,996   Epoch = 19 iter 5179 step
2022-06-12 02:51:19,997   Num examples = 1043
2022-06-12 02:51:19,997   Batch size = 32
2022-06-12 02:51:20,896 ***** Eval results *****
2022-06-12 02:51:20,896   cls_loss = 0.08490956567649571
2022-06-12 02:51:20,896   eval_loss = 0.9023826086159908
2022-06-12 02:51:20,896   global_step = 5179
2022-06-12 02:51:20,896   loss = 0.08490956567649571
2022-06-12 02:51:20,896   mcc = 0.22467564035409449
2022-06-12 02:51:25,772 ***** Running evaluation *****
2022-06-12 02:51:25,772   Epoch = 19 iter 5199 step
2022-06-12 02:51:25,772   Num examples = 1043
2022-06-12 02:51:25,772   Batch size = 32
2022-06-12 02:51:26,671 ***** Eval results *****
2022-06-12 02:51:26,671   cls_loss = 0.08485000817075608
2022-06-12 02:51:26,672   eval_loss = 0.9683591064178583
2022-06-12 02:51:26,672   global_step = 5199
2022-06-12 02:51:26,672   loss = 0.08485000817075608
2022-06-12 02:51:26,672   mcc = 0.22659231316465678
2022-06-12 02:51:31,583 ***** Running evaluation *****
2022-06-12 02:51:31,584   Epoch = 19 iter 5219 step
2022-06-12 02:51:31,584   Num examples = 1043
2022-06-12 02:51:31,584   Batch size = 32
2022-06-12 02:51:32,484 ***** Eval results *****
2022-06-12 02:51:32,484   cls_loss = 0.08491577580571175
2022-06-12 02:51:32,484   eval_loss = 0.9545109272003174
2022-06-12 02:51:32,485   global_step = 5219
2022-06-12 02:51:32,485   loss = 0.08491577580571175
2022-06-12 02:51:32,485   mcc = 0.22158949612892298
2022-06-12 02:51:37,365 ***** Running evaluation *****
2022-06-12 02:51:37,365   Epoch = 19 iter 5239 step
2022-06-12 02:51:37,365   Num examples = 1043
2022-06-12 02:51:37,365   Batch size = 32
2022-06-12 02:51:38,266 ***** Eval results *****
2022-06-12 02:51:38,266   cls_loss = 0.08436214528888104
2022-06-12 02:51:38,266   eval_loss = 0.966559783075795
2022-06-12 02:51:38,266   global_step = 5239
2022-06-12 02:51:38,266   loss = 0.08436214528888104
2022-06-12 02:51:38,266   mcc = 0.21482085402868578
2022-06-12 02:51:43,152 ***** Running evaluation *****
2022-06-12 02:51:43,153   Epoch = 19 iter 5259 step
2022-06-12 02:51:43,153   Num examples = 1043
2022-06-12 02:51:43,153   Batch size = 32
2022-06-12 02:51:44,054 ***** Eval results *****
2022-06-12 02:51:44,055   cls_loss = 0.08399719144067457
2022-06-12 02:51:44,055   eval_loss = 1.0112462730118723
2022-06-12 02:51:44,055   global_step = 5259
2022-06-12 02:51:44,055   loss = 0.08399719144067457
2022-06-12 02:51:44,055   mcc = 0.23571231546729038
2022-06-12 02:51:48,950 ***** Running evaluation *****
2022-06-12 02:51:48,950   Epoch = 19 iter 5279 step
2022-06-12 02:51:48,950   Num examples = 1043
2022-06-12 02:51:48,950   Batch size = 32
2022-06-12 02:51:49,852 ***** Eval results *****
2022-06-12 02:51:49,852   cls_loss = 0.08424123861257313
2022-06-12 02:51:49,852   eval_loss = 0.9516117455381335
2022-06-12 02:51:49,852   global_step = 5279
2022-06-12 02:51:49,852   loss = 0.08424123861257313
2022-06-12 02:51:49,852   mcc = 0.24302848632901336
2022-06-12 02:51:54,745 ***** Running evaluation *****
2022-06-12 02:51:54,745   Epoch = 19 iter 5299 step
2022-06-12 02:51:54,746   Num examples = 1043
2022-06-12 02:51:54,746   Batch size = 32
2022-06-12 02:51:55,645 ***** Eval results *****
2022-06-12 02:51:55,645   cls_loss = 0.08409214006588522
2022-06-12 02:51:55,645   eval_loss = 0.9816353140455304
2022-06-12 02:51:55,645   global_step = 5299
2022-06-12 02:51:55,645   loss = 0.08409214006588522
2022-06-12 02:51:55,645   mcc = 0.22223259405702478
2022-06-12 02:52:00,555 ***** Running evaluation *****
2022-06-12 02:52:00,556   Epoch = 19 iter 5319 step
2022-06-12 02:52:00,556   Num examples = 1043
2022-06-12 02:52:00,556   Batch size = 32
2022-06-12 02:52:01,456 ***** Eval results *****
2022-06-12 02:52:01,456   cls_loss = 0.08371270838670614
2022-06-12 02:52:01,456   eval_loss = 0.9645303957390062
2022-06-12 02:52:01,456   global_step = 5319
2022-06-12 02:52:01,456   loss = 0.08371270838670614
2022-06-12 02:52:01,456   mcc = 0.23015570365110669
2022-06-12 02:52:06,348 ***** Running evaluation *****
2022-06-12 02:52:06,348   Epoch = 19 iter 5339 step
2022-06-12 02:52:06,348   Num examples = 1043
2022-06-12 02:52:06,348   Batch size = 32
2022-06-12 02:52:07,248 ***** Eval results *****
2022-06-12 02:52:07,249   cls_loss = 0.0835326913380085
2022-06-12 02:52:07,249   eval_loss = 0.9539485185435324
2022-06-12 02:52:07,249   global_step = 5339
2022-06-12 02:52:07,249   loss = 0.0835326913380085
2022-06-12 02:52:07,249   mcc = 0.23119604155133006
2022-06-12 02:52:12,130 ***** Running evaluation *****
2022-06-12 02:52:12,131   Epoch = 20 iter 5359 step
2022-06-12 02:52:12,131   Num examples = 1043
2022-06-12 02:52:12,131   Batch size = 32
2022-06-12 02:52:13,030 ***** Eval results *****
2022-06-12 02:52:13,030   cls_loss = 0.08151474006866154
2022-06-12 02:52:13,030   eval_loss = 0.9703421168255083
2022-06-12 02:52:13,030   global_step = 5359
2022-06-12 02:52:13,030   loss = 0.08151474006866154
2022-06-12 02:52:13,031   mcc = 0.23660709796833249
2022-06-12 02:52:17,900 ***** Running evaluation *****
2022-06-12 02:52:17,901   Epoch = 20 iter 5379 step
2022-06-12 02:52:17,901   Num examples = 1043
2022-06-12 02:52:17,901   Batch size = 32
2022-06-12 02:52:18,800 ***** Eval results *****
2022-06-12 02:52:18,800   cls_loss = 0.08388200650612514
2022-06-12 02:52:18,800   eval_loss = 0.94536220666134
2022-06-12 02:52:18,800   global_step = 5379
2022-06-12 02:52:18,800   loss = 0.08388200650612514
2022-06-12 02:52:18,800   mcc = 0.22944549228920866
2022-06-12 02:52:23,662 ***** Running evaluation *****
2022-06-12 02:52:23,662   Epoch = 20 iter 5399 step
2022-06-12 02:52:23,662   Num examples = 1043
2022-06-12 02:52:23,663   Batch size = 32
2022-06-12 02:52:24,564 ***** Eval results *****
2022-06-12 02:52:24,564   cls_loss = 0.08336175119472762
2022-06-12 02:52:24,564   eval_loss = 0.9901060931610338
2022-06-12 02:52:24,564   global_step = 5399
2022-06-12 02:52:24,564   loss = 0.08336175119472762
2022-06-12 02:52:24,564   mcc = 0.22366058640932485
2022-06-12 02:52:29,450 ***** Running evaluation *****
2022-06-12 02:52:29,451   Epoch = 20 iter 5419 step
2022-06-12 02:52:29,451   Num examples = 1043
2022-06-12 02:52:29,451   Batch size = 32
2022-06-12 02:52:30,354 ***** Eval results *****
2022-06-12 02:52:30,354   cls_loss = 0.08384428842912746
2022-06-12 02:52:30,354   eval_loss = 0.9112113804528208
2022-06-12 02:52:30,354   global_step = 5419
2022-06-12 02:52:30,354   loss = 0.08384428842912746
2022-06-12 02:52:30,354   mcc = 0.21890180965165226
2022-06-12 02:52:35,237 ***** Running evaluation *****
2022-06-12 02:52:35,238   Epoch = 20 iter 5439 step
2022-06-12 02:52:35,238   Num examples = 1043
2022-06-12 02:52:35,238   Batch size = 32
2022-06-12 02:52:36,137 ***** Eval results *****
2022-06-12 02:52:36,137   cls_loss = 0.08279015643127037
2022-06-12 02:52:36,137   eval_loss = 1.0126284270575552
2022-06-12 02:52:36,137   global_step = 5439
2022-06-12 02:52:36,137   loss = 0.08279015643127037
2022-06-12 02:52:36,137   mcc = 0.25309402846646617
2022-06-12 02:52:41,019 ***** Running evaluation *****
2022-06-12 02:52:41,019   Epoch = 20 iter 5459 step
2022-06-12 02:52:41,019   Num examples = 1043
2022-06-12 02:52:41,019   Batch size = 32
2022-06-12 02:52:41,921 ***** Eval results *****
2022-06-12 02:52:41,921   cls_loss = 0.08279583202440198
2022-06-12 02:52:41,921   eval_loss = 1.0033933971867417
2022-06-12 02:52:41,921   global_step = 5459
2022-06-12 02:52:41,921   loss = 0.08279583202440198
2022-06-12 02:52:41,921   mcc = 0.25602530509417626
2022-06-12 02:52:46,830 ***** Running evaluation *****
2022-06-12 02:52:46,831   Epoch = 20 iter 5479 step
2022-06-12 02:52:46,831   Num examples = 1043
2022-06-12 02:52:46,831   Batch size = 32
2022-06-12 02:52:47,734 ***** Eval results *****
2022-06-12 02:52:47,734   cls_loss = 0.08358899595068513
2022-06-12 02:52:47,734   eval_loss = 0.9363038241863251
2022-06-12 02:52:47,734   global_step = 5479
2022-06-12 02:52:47,734   loss = 0.08358899595068513
2022-06-12 02:52:47,734   mcc = 0.22000053228059496
2022-06-12 02:52:52,622 ***** Running evaluation *****
2022-06-12 02:52:52,622   Epoch = 20 iter 5499 step
2022-06-12 02:52:52,622   Num examples = 1043
2022-06-12 02:52:52,622   Batch size = 32
2022-06-12 02:52:53,521 ***** Eval results *****
2022-06-12 02:52:53,521   cls_loss = 0.08428579901561797
2022-06-12 02:52:53,521   eval_loss = 0.9101451301213467
2022-06-12 02:52:53,521   global_step = 5499
2022-06-12 02:52:53,521   loss = 0.08428579901561797
2022-06-12 02:52:53,521   mcc = 0.20636398529139036
2022-06-12 02:52:58,394 ***** Running evaluation *****
2022-06-12 02:52:58,394   Epoch = 20 iter 5519 step
2022-06-12 02:52:58,394   Num examples = 1043
2022-06-12 02:52:58,394   Batch size = 32
2022-06-12 02:52:59,294 ***** Eval results *****
2022-06-12 02:52:59,294   cls_loss = 0.08428354965075434
2022-06-12 02:52:59,294   eval_loss = 1.0081925762422157
2022-06-12 02:52:59,295   global_step = 5519
2022-06-12 02:52:59,295   loss = 0.08428354965075434
2022-06-12 02:52:59,295   mcc = 0.20504421314290283
2022-06-12 02:53:04,181 ***** Running evaluation *****
2022-06-12 02:53:04,181   Epoch = 20 iter 5539 step
2022-06-12 02:53:04,181   Num examples = 1043
2022-06-12 02:53:04,181   Batch size = 32
2022-06-12 02:53:05,080 ***** Eval results *****
2022-06-12 02:53:05,081   cls_loss = 0.08389068765556393
2022-06-12 02:53:05,081   eval_loss = 0.9662115763534199
2022-06-12 02:53:05,081   global_step = 5539
2022-06-12 02:53:05,081   loss = 0.08389068765556393
2022-06-12 02:53:05,081   mcc = 0.2275763597379058
2022-06-12 02:53:09,990 ***** Running evaluation *****
2022-06-12 02:53:09,990   Epoch = 20 iter 5559 step
2022-06-12 02:53:09,990   Num examples = 1043
2022-06-12 02:53:09,991   Batch size = 32
2022-06-12 02:53:10,890 ***** Eval results *****
2022-06-12 02:53:10,891   cls_loss = 0.08380286722150568
2022-06-12 02:53:10,891   eval_loss = 0.9597769370584777
2022-06-12 02:53:10,891   global_step = 5559
2022-06-12 02:53:10,891   loss = 0.08380286722150568
2022-06-12 02:53:10,891   mcc = 0.22534993222532376
2022-06-12 02:53:15,794 ***** Running evaluation *****
2022-06-12 02:53:15,794   Epoch = 20 iter 5579 step
2022-06-12 02:53:15,794   Num examples = 1043
2022-06-12 02:53:15,794   Batch size = 32
2022-06-12 02:53:16,696 ***** Eval results *****
2022-06-12 02:53:16,696   cls_loss = 0.08439301783569687
2022-06-12 02:53:16,696   eval_loss = 0.9064687300812114
2022-06-12 02:53:16,696   global_step = 5579
2022-06-12 02:53:16,696   loss = 0.08439301783569687
2022-06-12 02:53:16,696   mcc = 0.22153312215553364
2022-06-12 02:53:17,336 ***** Running evaluation *****
2022-06-12 02:53:17,336   Epoch = 8 iter 28499 step
2022-06-12 02:53:17,336   Num examples = 5463
2022-06-12 02:53:17,336   Batch size = 32
2022-06-12 02:53:17,337 ***** Eval results *****
2022-06-12 02:53:17,338   att_loss = 3.5706355793172286
2022-06-12 02:53:17,338   global_step = 28499
2022-06-12 02:53:17,338   loss = 4.442638666943907
2022-06-12 02:53:17,338   rep_loss = 0.8720030933425442
2022-06-12 02:53:17,338 ***** Save model *****
2022-06-12 02:53:21,596 ***** Running evaluation *****
2022-06-12 02:53:21,597   Epoch = 20 iter 5599 step
2022-06-12 02:53:21,597   Num examples = 1043
2022-06-12 02:53:21,597   Batch size = 32
2022-06-12 02:53:22,502 ***** Eval results *****
2022-06-12 02:53:22,502   cls_loss = 0.08393360677257929
2022-06-12 02:53:22,503   eval_loss = 0.9943842734351303
2022-06-12 02:53:22,503   global_step = 5599
2022-06-12 02:53:22,503   loss = 0.08393360677257929
2022-06-12 02:53:22,503   mcc = 0.20974198149720766
2022-06-12 02:53:27,388 ***** Running evaluation *****
2022-06-12 02:53:27,388   Epoch = 21 iter 5619 step
2022-06-12 02:53:27,388   Num examples = 1043
2022-06-12 02:53:27,388   Batch size = 32
2022-06-12 02:53:28,289 ***** Eval results *****
2022-06-12 02:53:28,289   cls_loss = 0.08427462975184123
2022-06-12 02:53:28,289   eval_loss = 0.9605289056445613
2022-06-12 02:53:28,289   global_step = 5619
2022-06-12 02:53:28,289   loss = 0.08427462975184123
2022-06-12 02:53:28,290   mcc = 0.21090483107996957
2022-06-12 02:53:33,172 ***** Running evaluation *****
2022-06-12 02:53:33,173   Epoch = 21 iter 5639 step
2022-06-12 02:53:33,173   Num examples = 1043
2022-06-12 02:53:33,173   Batch size = 32
2022-06-12 02:53:34,073 ***** Eval results *****
2022-06-12 02:53:34,074   cls_loss = 0.08178873592987657
2022-06-12 02:53:34,074   eval_loss = 0.9568073993379419
2022-06-12 02:53:34,074   global_step = 5639
2022-06-12 02:53:34,074   loss = 0.08178873592987657
2022-06-12 02:53:34,074   mcc = 0.22839800300267465
2022-06-12 02:53:38,959 ***** Running evaluation *****
2022-06-12 02:53:38,959   Epoch = 21 iter 5659 step
2022-06-12 02:53:38,959   Num examples = 1043
2022-06-12 02:53:38,959   Batch size = 32
2022-06-12 02:53:39,860 ***** Eval results *****
2022-06-12 02:53:39,860   cls_loss = 0.08137421510540523
2022-06-12 02:53:39,860   eval_loss = 0.9458947136546626
2022-06-12 02:53:39,861   global_step = 5659
2022-06-12 02:53:39,861   loss = 0.08137421510540523
2022-06-12 02:53:39,861   mcc = 0.23350480857139888
2022-06-12 02:53:44,779 ***** Running evaluation *****
2022-06-12 02:53:44,780   Epoch = 21 iter 5679 step
2022-06-12 02:53:44,780   Num examples = 1043
2022-06-12 02:53:44,780   Batch size = 32
2022-06-12 02:53:45,683 ***** Eval results *****
2022-06-12 02:53:45,683   cls_loss = 0.08183474683513244
2022-06-12 02:53:45,683   eval_loss = 0.936659599795486
2022-06-12 02:53:45,683   global_step = 5679
2022-06-12 02:53:45,683   loss = 0.08183474683513244
2022-06-12 02:53:45,684   mcc = 0.22236408412902245
2022-06-12 02:53:50,583 ***** Running evaluation *****
2022-06-12 02:53:50,583   Epoch = 21 iter 5699 step
2022-06-12 02:53:50,583   Num examples = 1043
2022-06-12 02:53:50,584   Batch size = 32
2022-06-12 02:53:51,484 ***** Eval results *****
2022-06-12 02:53:51,484   cls_loss = 0.08265115045334982
2022-06-12 02:53:51,485   eval_loss = 0.9316650403268409
2022-06-12 02:53:51,485   global_step = 5699
2022-06-12 02:53:51,485   loss = 0.08265115045334982
2022-06-12 02:53:51,485   mcc = 0.2145907404220213
2022-06-12 02:53:56,389 ***** Running evaluation *****
2022-06-12 02:53:56,389   Epoch = 21 iter 5719 step
2022-06-12 02:53:56,389   Num examples = 1043
2022-06-12 02:53:56,390   Batch size = 32
2022-06-12 02:53:57,292 ***** Eval results *****
2022-06-12 02:53:57,292   cls_loss = 0.08328136163098472
2022-06-12 02:53:57,292   eval_loss = 0.9502813662543441
2022-06-12 02:53:57,292   global_step = 5719
2022-06-12 02:53:57,292   loss = 0.08328136163098472
2022-06-12 02:53:57,292   mcc = 0.21756808664486996
2022-06-12 02:54:02,187 ***** Running evaluation *****
2022-06-12 02:54:02,187   Epoch = 21 iter 5739 step
2022-06-12 02:54:02,187   Num examples = 1043
2022-06-12 02:54:02,187   Batch size = 32
2022-06-12 02:54:03,087 ***** Eval results *****
2022-06-12 02:54:03,087   cls_loss = 0.08302724812970017
2022-06-12 02:54:03,087   eval_loss = 0.9436949175415617
2022-06-12 02:54:03,088   global_step = 5739
2022-06-12 02:54:03,088   loss = 0.08302724812970017
2022-06-12 02:54:03,088   mcc = 0.21206701692402774
2022-06-12 02:54:07,958 ***** Running evaluation *****
2022-06-12 02:54:07,958   Epoch = 21 iter 5759 step
2022-06-12 02:54:07,958   Num examples = 1043
2022-06-12 02:54:07,958   Batch size = 32
2022-06-12 02:54:08,859 ***** Eval results *****
2022-06-12 02:54:08,859   cls_loss = 0.08309928195453004
2022-06-12 02:54:08,859   eval_loss = 0.9700863027211392
2022-06-12 02:54:08,859   global_step = 5759
2022-06-12 02:54:08,860   loss = 0.08309928195453004
2022-06-12 02:54:08,860   mcc = 0.2153216199980261
2022-06-12 02:54:13,737 ***** Running evaluation *****
2022-06-12 02:54:13,738   Epoch = 21 iter 5779 step
2022-06-12 02:54:13,738   Num examples = 1043
2022-06-12 02:54:13,738   Batch size = 32
2022-06-12 02:54:14,638 ***** Eval results *****
2022-06-12 02:54:14,638   cls_loss = 0.08317031897604465
2022-06-12 02:54:14,638   eval_loss = 0.9312261135289164
2022-06-12 02:54:14,638   global_step = 5779
2022-06-12 02:54:14,639   loss = 0.08317031897604465
2022-06-12 02:54:14,639   mcc = 0.22879878385311733
2022-06-12 02:54:19,528 ***** Running evaluation *****
2022-06-12 02:54:19,528   Epoch = 21 iter 5799 step
2022-06-12 02:54:19,528   Num examples = 1043
2022-06-12 02:54:19,529   Batch size = 32
2022-06-12 02:54:20,429 ***** Eval results *****
2022-06-12 02:54:20,429   cls_loss = 0.08335100475233048
2022-06-12 02:54:20,429   eval_loss = 0.9409587094278047
2022-06-12 02:54:20,430   global_step = 5799
2022-06-12 02:54:20,430   loss = 0.08335100475233048
2022-06-12 02:54:20,430   mcc = 0.24321177040819328
2022-06-12 02:54:25,317 ***** Running evaluation *****
2022-06-12 02:54:25,318   Epoch = 21 iter 5819 step
2022-06-12 02:54:25,318   Num examples = 1043
2022-06-12 02:54:25,318   Batch size = 32
2022-06-12 02:54:26,220 ***** Eval results *****
2022-06-12 02:54:26,220   cls_loss = 0.08312367366732291
2022-06-12 02:54:26,220   eval_loss = 0.9604666070504622
2022-06-12 02:54:26,220   global_step = 5819
2022-06-12 02:54:26,220   loss = 0.08312367366732291
2022-06-12 02:54:26,220   mcc = 0.2514811386627169
2022-06-12 02:54:31,106 ***** Running evaluation *****
2022-06-12 02:54:31,106   Epoch = 21 iter 5839 step
2022-06-12 02:54:31,106   Num examples = 1043
2022-06-12 02:54:31,106   Batch size = 32
2022-06-12 02:54:32,007 ***** Eval results *****
2022-06-12 02:54:32,007   cls_loss = 0.08285607914600907
2022-06-12 02:54:32,007   eval_loss = 0.958849614316767
2022-06-12 02:54:32,008   global_step = 5839
2022-06-12 02:54:32,008   loss = 0.08285607914600907
2022-06-12 02:54:32,008   mcc = 0.24836002353943648
2022-06-12 02:54:36,889 ***** Running evaluation *****
2022-06-12 02:54:36,889   Epoch = 21 iter 5859 step
2022-06-12 02:54:36,889   Num examples = 1043
2022-06-12 02:54:36,890   Batch size = 32
2022-06-12 02:54:37,789 ***** Eval results *****
2022-06-12 02:54:37,789   cls_loss = 0.08303348731900019
2022-06-12 02:54:37,789   eval_loss = 0.9280088661294995
2022-06-12 02:54:37,789   global_step = 5859
2022-06-12 02:54:37,790   loss = 0.08303348731900019
2022-06-12 02:54:37,790   mcc = 0.23481097845044813
2022-06-12 02:54:42,666 ***** Running evaluation *****
2022-06-12 02:54:42,666   Epoch = 22 iter 5879 step
2022-06-12 02:54:42,666   Num examples = 1043
2022-06-12 02:54:42,666   Batch size = 32
2022-06-12 02:54:43,565 ***** Eval results *****
2022-06-12 02:54:43,566   cls_loss = 0.07931706756353378
2022-06-12 02:54:43,566   eval_loss = 0.9782208068804308
2022-06-12 02:54:43,566   global_step = 5879
2022-06-12 02:54:43,566   loss = 0.07931706756353378
2022-06-12 02:54:43,566   mcc = 0.24101422478034612
2022-06-12 02:54:48,439 ***** Running evaluation *****
2022-06-12 02:54:48,440   Epoch = 22 iter 5899 step
2022-06-12 02:54:48,440   Num examples = 1043
2022-06-12 02:54:48,440   Batch size = 32
2022-06-12 02:54:49,340 ***** Eval results *****
2022-06-12 02:54:49,341   cls_loss = 0.08264833092689514
2022-06-12 02:54:49,341   eval_loss = 0.9514287701158812
2022-06-12 02:54:49,341   global_step = 5899
2022-06-12 02:54:49,341   loss = 0.08264833092689514
2022-06-12 02:54:49,341   mcc = 0.2547689272999074
2022-06-12 02:54:54,217 ***** Running evaluation *****
2022-06-12 02:54:54,217   Epoch = 22 iter 5919 step
2022-06-12 02:54:54,217   Num examples = 1043
2022-06-12 02:54:54,217   Batch size = 32
2022-06-12 02:54:55,117 ***** Eval results *****
2022-06-12 02:54:55,117   cls_loss = 0.0824446752667427
2022-06-12 02:54:55,117   eval_loss = 0.9701362234173398
2022-06-12 02:54:55,117   global_step = 5919
2022-06-12 02:54:55,117   loss = 0.0824446752667427
2022-06-12 02:54:55,118   mcc = 0.2412025240252886
2022-06-12 02:54:59,998 ***** Running evaluation *****
2022-06-12 02:54:59,998   Epoch = 22 iter 5939 step
2022-06-12 02:54:59,998   Num examples = 1043
2022-06-12 02:54:59,998   Batch size = 32
2022-06-12 02:55:00,897 ***** Eval results *****
2022-06-12 02:55:00,897   cls_loss = 0.08241108827866041
2022-06-12 02:55:00,897   eval_loss = 0.9590787345712836
2022-06-12 02:55:00,897   global_step = 5939
2022-06-12 02:55:00,898   loss = 0.08241108827866041
2022-06-12 02:55:00,898   mcc = 0.22767308018729177
2022-06-12 02:55:05,785 ***** Running evaluation *****
2022-06-12 02:55:05,785   Epoch = 22 iter 5959 step
2022-06-12 02:55:05,785   Num examples = 1043
2022-06-12 02:55:05,785   Batch size = 32
2022-06-12 02:55:06,686 ***** Eval results *****
2022-06-12 02:55:06,686   cls_loss = 0.08226401069585015
2022-06-12 02:55:06,686   eval_loss = 0.9541344588453119
2022-06-12 02:55:06,686   global_step = 5959
2022-06-12 02:55:06,686   loss = 0.08226401069585015
2022-06-12 02:55:06,686   mcc = 0.22204744718966396
2022-06-12 02:55:11,556 ***** Running evaluation *****
2022-06-12 02:55:11,556   Epoch = 22 iter 5979 step
2022-06-12 02:55:11,556   Num examples = 1043
2022-06-12 02:55:11,556   Batch size = 32
2022-06-12 02:55:12,455 ***** Eval results *****
2022-06-12 02:55:12,455   cls_loss = 0.08301521639029184
2022-06-12 02:55:12,456   eval_loss = 0.9353155141527002
2022-06-12 02:55:12,456   global_step = 5979
2022-06-12 02:55:12,456   loss = 0.08301521639029184
2022-06-12 02:55:12,456   mcc = 0.23314940391638828
2022-06-12 02:55:17,330 ***** Running evaluation *****
2022-06-12 02:55:17,330   Epoch = 22 iter 5999 step
2022-06-12 02:55:17,330   Num examples = 1043
2022-06-12 02:55:17,330   Batch size = 32
2022-06-12 02:55:18,230 ***** Eval results *****
2022-06-12 02:55:18,230   cls_loss = 0.08341260886192321
2022-06-12 02:55:18,230   eval_loss = 0.9452125782316382
2022-06-12 02:55:18,230   global_step = 5999
2022-06-12 02:55:18,231   loss = 0.08341260886192321
2022-06-12 02:55:18,231   mcc = 0.24820330504878688
2022-06-12 02:55:23,121 ***** Running evaluation *****
2022-06-12 02:55:23,121   Epoch = 22 iter 6019 step
2022-06-12 02:55:23,121   Num examples = 1043
2022-06-12 02:55:23,121   Batch size = 32
2022-06-12 02:55:24,022 ***** Eval results *****
2022-06-12 02:55:24,023   cls_loss = 0.08263623267412186
2022-06-12 02:55:24,023   eval_loss = 0.975599956331831
2022-06-12 02:55:24,023   global_step = 6019
2022-06-12 02:55:24,023   loss = 0.08263623267412186
2022-06-12 02:55:24,023   mcc = 0.236498666486716
2022-06-12 02:55:25,234 ***** Running evaluation *****
2022-06-12 02:55:25,234   Epoch = 8 iter 28999 step
2022-06-12 02:55:25,234   Num examples = 5463
2022-06-12 02:55:25,234   Batch size = 32
2022-06-12 02:55:25,235 ***** Eval results *****
2022-06-12 02:55:25,235   att_loss = 3.5651632386880063
2022-06-12 02:55:25,236   global_step = 28999
2022-06-12 02:55:25,236   loss = 4.436642025884896
2022-06-12 02:55:25,236   rep_loss = 0.8714787903306328
2022-06-12 02:55:25,236 ***** Save model *****
2022-06-12 02:55:28,897 ***** Running evaluation *****
2022-06-12 02:55:28,898   Epoch = 22 iter 6039 step
2022-06-12 02:55:28,898   Num examples = 1043
2022-06-12 02:55:28,898   Batch size = 32
2022-06-12 02:55:29,798 ***** Eval results *****
2022-06-12 02:55:29,798   cls_loss = 0.08212438401850787
2022-06-12 02:55:29,798   eval_loss = 0.9728180919632767
2022-06-12 02:55:29,798   global_step = 6039
2022-06-12 02:55:29,798   loss = 0.08212438401850787
2022-06-12 02:55:29,799   mcc = 0.23873407559405013
2022-06-12 02:55:34,688 ***** Running evaluation *****
2022-06-12 02:55:34,689   Epoch = 22 iter 6059 step
2022-06-12 02:55:34,689   Num examples = 1043
2022-06-12 02:55:34,689   Batch size = 32
2022-06-12 02:55:35,589 ***** Eval results *****
2022-06-12 02:55:35,589   cls_loss = 0.08169512329874812
2022-06-12 02:55:35,589   eval_loss = 0.964117705821991
2022-06-12 02:55:35,589   global_step = 6059
2022-06-12 02:55:35,589   loss = 0.08169512329874812
2022-06-12 02:55:35,589   mcc = 0.2407911701021713
2022-06-12 02:55:40,462 ***** Running evaluation *****
2022-06-12 02:55:40,462   Epoch = 22 iter 6079 step
2022-06-12 02:55:40,462   Num examples = 1043
2022-06-12 02:55:40,462   Batch size = 32
2022-06-12 02:55:41,362 ***** Eval results *****
2022-06-12 02:55:41,362   cls_loss = 0.08154625438335465
2022-06-12 02:55:41,362   eval_loss = 0.9626133730917266
2022-06-12 02:55:41,362   global_step = 6079
2022-06-12 02:55:41,362   loss = 0.08154625438335465
2022-06-12 02:55:41,362   mcc = 0.2407911701021713
2022-06-12 02:55:46,234 ***** Running evaluation *****
2022-06-12 02:55:46,234   Epoch = 22 iter 6099 step
2022-06-12 02:55:46,234   Num examples = 1043
2022-06-12 02:55:46,234   Batch size = 32
2022-06-12 02:55:47,134 ***** Eval results *****
2022-06-12 02:55:47,134   cls_loss = 0.08128941459788216
2022-06-12 02:55:47,134   eval_loss = 0.9773418957536871
2022-06-12 02:55:47,134   global_step = 6099
2022-06-12 02:55:47,134   loss = 0.08128941459788216
2022-06-12 02:55:47,134   mcc = 0.2340718748381182
2022-06-12 02:55:52,005 ***** Running evaluation *****
2022-06-12 02:55:52,005   Epoch = 22 iter 6119 step
2022-06-12 02:55:52,005   Num examples = 1043
2022-06-12 02:55:52,005   Batch size = 32
2022-06-12 02:55:52,904 ***** Eval results *****
2022-06-12 02:55:52,904   cls_loss = 0.08136507731919386
2022-06-12 02:55:52,904   eval_loss = 0.9577268334952268
2022-06-12 02:55:52,904   global_step = 6119
2022-06-12 02:55:52,905   loss = 0.08136507731919386
2022-06-12 02:55:52,905   mcc = 0.23606321670657446
2022-06-12 02:55:57,782 ***** Running evaluation *****
2022-06-12 02:55:57,782   Epoch = 22 iter 6139 step
2022-06-12 02:55:57,782   Num examples = 1043
2022-06-12 02:55:57,782   Batch size = 32
2022-06-12 02:55:58,684 ***** Eval results *****
2022-06-12 02:55:58,684   cls_loss = 0.08136003228291026
2022-06-12 02:55:58,684   eval_loss = 0.988375596927874
2022-06-12 02:55:58,684   global_step = 6139
2022-06-12 02:55:58,684   loss = 0.08136003228291026
2022-06-12 02:55:58,684   mcc = 0.236498666486716
2022-06-12 02:56:03,560 ***** Running evaluation *****
2022-06-12 02:56:03,560   Epoch = 23 iter 6159 step
2022-06-12 02:56:03,560   Num examples = 1043
2022-06-12 02:56:03,560   Batch size = 32
2022-06-12 02:56:04,460 ***** Eval results *****
2022-06-12 02:56:04,460   cls_loss = 0.07877137367096212
2022-06-12 02:56:04,460   eval_loss = 0.953889655344414
2022-06-12 02:56:04,460   global_step = 6159
2022-06-12 02:56:04,460   loss = 0.07877137367096212
2022-06-12 02:56:04,460   mcc = 0.2374695309664015
2022-06-12 02:56:09,339 ***** Running evaluation *****
2022-06-12 02:56:09,339   Epoch = 23 iter 6179 step
2022-06-12 02:56:09,339   Num examples = 1043
2022-06-12 02:56:09,340   Batch size = 32
2022-06-12 02:56:10,239 ***** Eval results *****
2022-06-12 02:56:10,239   cls_loss = 0.08128387726059086
2022-06-12 02:56:10,239   eval_loss = 0.9617854690912998
2022-06-12 02:56:10,239   global_step = 6179
2022-06-12 02:56:10,239   loss = 0.08128387726059086
2022-06-12 02:56:10,239   mcc = 0.22980432037427062
2022-06-12 02:56:15,125 ***** Running evaluation *****
2022-06-12 02:56:15,125   Epoch = 23 iter 6199 step
2022-06-12 02:56:15,125   Num examples = 1043
2022-06-12 02:56:15,125   Batch size = 32
2022-06-12 02:56:16,026 ***** Eval results *****
2022-06-12 02:56:16,026   cls_loss = 0.08246327943072237
2022-06-12 02:56:16,026   eval_loss = 0.9723456745797937
2022-06-12 02:56:16,026   global_step = 6199
2022-06-12 02:56:16,026   loss = 0.08246327943072237
2022-06-12 02:56:16,026   mcc = 0.2297416003025044
2022-06-12 02:56:20,914 ***** Running evaluation *****
2022-06-12 02:56:20,914   Epoch = 23 iter 6219 step
2022-06-12 02:56:20,914   Num examples = 1043
2022-06-12 02:56:20,914   Batch size = 32
2022-06-12 02:56:21,814 ***** Eval results *****
2022-06-12 02:56:21,815   cls_loss = 0.08227292959315655
2022-06-12 02:56:21,815   eval_loss = 0.9829769432544708
2022-06-12 02:56:21,815   global_step = 6219
2022-06-12 02:56:21,815   loss = 0.08227292959315655
2022-06-12 02:56:21,815   mcc = 0.24101422478034612
2022-06-12 02:56:26,706 ***** Running evaluation *****
2022-06-12 02:56:26,707   Epoch = 23 iter 6239 step
2022-06-12 02:56:26,707   Num examples = 1043
2022-06-12 02:56:26,707   Batch size = 32
2022-06-12 02:56:27,607 ***** Eval results *****
2022-06-12 02:56:27,607   cls_loss = 0.08134483755091015
2022-06-12 02:56:27,607   eval_loss = 0.9982774031884742
2022-06-12 02:56:27,607   global_step = 6239
2022-06-12 02:56:27,607   loss = 0.08134483755091015
2022-06-12 02:56:27,607   mcc = 0.24374708325064798
2022-06-12 02:56:32,484 ***** Running evaluation *****
2022-06-12 02:56:32,485   Epoch = 23 iter 6259 step
2022-06-12 02:56:32,485   Num examples = 1043
2022-06-12 02:56:32,485   Batch size = 32
2022-06-12 02:56:33,386 ***** Eval results *****
2022-06-12 02:56:33,386   cls_loss = 0.08147950031620971
2022-06-12 02:56:33,386   eval_loss = 0.9807577819535227
2022-06-12 02:56:33,386   global_step = 6259
2022-06-12 02:56:33,386   loss = 0.08147950031620971
2022-06-12 02:56:33,386   mcc = 0.2344201896061598
2022-06-12 02:56:38,248 ***** Running evaluation *****
2022-06-12 02:56:38,249   Epoch = 23 iter 6279 step
2022-06-12 02:56:38,249   Num examples = 1043
2022-06-12 02:56:38,249   Batch size = 32
2022-06-12 02:56:39,148 ***** Eval results *****
2022-06-12 02:56:39,149   cls_loss = 0.08173759030583112
2022-06-12 02:56:39,149   eval_loss = 0.9620854529467496
2022-06-12 02:56:39,149   global_step = 6279
2022-06-12 02:56:39,149   loss = 0.08173759030583112
2022-06-12 02:56:39,149   mcc = 0.23208938358593978
2022-06-12 02:56:44,029 ***** Running evaluation *****
2022-06-12 02:56:44,029   Epoch = 23 iter 6299 step
2022-06-12 02:56:44,029   Num examples = 1043
2022-06-12 02:56:44,030   Batch size = 32
2022-06-12 02:56:44,929 ***** Eval results *****
2022-06-12 02:56:44,929   cls_loss = 0.08197372045980979
2022-06-12 02:56:44,929   eval_loss = 0.9602524954261202
2022-06-12 02:56:44,929   global_step = 6299
2022-06-12 02:56:44,929   loss = 0.08197372045980979
2022-06-12 02:56:44,929   mcc = 0.238427015501258
2022-06-12 02:56:49,809 ***** Running evaluation *****
2022-06-12 02:56:49,809   Epoch = 23 iter 6319 step
2022-06-12 02:56:49,809   Num examples = 1043
2022-06-12 02:56:49,809   Batch size = 32
2022-06-12 02:56:50,710 ***** Eval results *****
2022-06-12 02:56:50,710   cls_loss = 0.08156237108737566
2022-06-12 02:56:50,710   eval_loss = 0.9870718365365808
2022-06-12 02:56:50,710   global_step = 6319
2022-06-12 02:56:50,710   loss = 0.08156237108737566
2022-06-12 02:56:50,710   mcc = 0.2320339352206038
2022-06-12 02:56:55,585 ***** Running evaluation *****
2022-06-12 02:56:55,585   Epoch = 23 iter 6339 step
2022-06-12 02:56:55,585   Num examples = 1043
2022-06-12 02:56:55,585   Batch size = 32
2022-06-12 02:56:56,485 ***** Eval results *****
2022-06-12 02:56:56,486   cls_loss = 0.08162254221135318
2022-06-12 02:56:56,486   eval_loss = 0.952729172778852
2022-06-12 02:56:56,486   global_step = 6339
2022-06-12 02:56:56,486   loss = 0.08162254221135318
2022-06-12 02:56:56,486   mcc = 0.23195593663748776
2022-06-12 02:57:01,374 ***** Running evaluation *****
2022-06-12 02:57:01,374   Epoch = 23 iter 6359 step
2022-06-12 02:57:01,374   Num examples = 1043
2022-06-12 02:57:01,374   Batch size = 32
2022-06-12 02:57:02,276 ***** Eval results *****
2022-06-12 02:57:02,276   cls_loss = 0.08162427907211518
2022-06-12 02:57:02,276   eval_loss = 0.9857498109340668
2022-06-12 02:57:02,276   global_step = 6359
2022-06-12 02:57:02,276   loss = 0.08162427907211518
2022-06-12 02:57:02,276   mcc = 0.24066366100893638
2022-06-12 02:57:07,146 ***** Running evaluation *****
2022-06-12 02:57:07,146   Epoch = 23 iter 6379 step
2022-06-12 02:57:07,146   Num examples = 1043
2022-06-12 02:57:07,146   Batch size = 32
2022-06-12 02:57:08,047 ***** Eval results *****
2022-06-12 02:57:08,047   cls_loss = 0.0816898471636682
2022-06-12 02:57:08,047   eval_loss = 0.9880212021596504
2022-06-12 02:57:08,047   global_step = 6379
2022-06-12 02:57:08,048   loss = 0.0816898471636682
2022-06-12 02:57:08,048   mcc = 0.24101422478034612
2022-06-12 02:57:12,927 ***** Running evaluation *****
2022-06-12 02:57:12,927   Epoch = 23 iter 6399 step
2022-06-12 02:57:12,927   Num examples = 1043
2022-06-12 02:57:12,927   Batch size = 32
2022-06-12 02:57:13,828 ***** Eval results *****
2022-06-12 02:57:13,828   cls_loss = 0.08188666768603084
2022-06-12 02:57:13,828   eval_loss = 0.981920860933535
2022-06-12 02:57:13,828   global_step = 6399
2022-06-12 02:57:13,828   loss = 0.08188666768603084
2022-06-12 02:57:13,828   mcc = 0.23598465622400558
2022-06-12 02:57:18,713 ***** Running evaluation *****
2022-06-12 02:57:18,713   Epoch = 24 iter 6419 step
2022-06-12 02:57:18,713   Num examples = 1043
2022-06-12 02:57:18,713   Batch size = 32
2022-06-12 02:57:19,614 ***** Eval results *****
2022-06-12 02:57:19,614   cls_loss = 0.08536331084641544
2022-06-12 02:57:19,614   eval_loss = 0.9696669289560029
2022-06-12 02:57:19,614   global_step = 6419
2022-06-12 02:57:19,614   loss = 0.08536331084641544
2022-06-12 02:57:19,614   mcc = 0.2374695309664015
2022-06-12 02:57:24,504 ***** Running evaluation *****
2022-06-12 02:57:24,505   Epoch = 24 iter 6439 step
2022-06-12 02:57:24,505   Num examples = 1043
2022-06-12 02:57:24,505   Batch size = 32
2022-06-12 02:57:25,406 ***** Eval results *****
2022-06-12 02:57:25,406   cls_loss = 0.08263388276100159
2022-06-12 02:57:25,406   eval_loss = 0.9384099100575303
2022-06-12 02:57:25,406   global_step = 6439
2022-06-12 02:57:25,406   loss = 0.08263388276100159
2022-06-12 02:57:25,406   mcc = 0.230830124799434
2022-06-12 02:57:30,272 ***** Running evaluation *****
2022-06-12 02:57:30,273   Epoch = 24 iter 6459 step
2022-06-12 02:57:30,273   Num examples = 1043
2022-06-12 02:57:30,273   Batch size = 32
2022-06-12 02:57:31,172 ***** Eval results *****
2022-06-12 02:57:31,172   cls_loss = 0.08159408177815232
2022-06-12 02:57:31,172   eval_loss = 0.9872609882643728
2022-06-12 02:57:31,173   global_step = 6459
2022-06-12 02:57:31,173   loss = 0.08159408177815232
2022-06-12 02:57:31,173   mcc = 0.24947143738126262
2022-06-12 02:57:32,624 ***** Running evaluation *****
2022-06-12 02:57:32,624   Epoch = 9 iter 29499 step
2022-06-12 02:57:32,624   Num examples = 5463
2022-06-12 02:57:32,624   Batch size = 32
2022-06-12 02:57:32,625 ***** Eval results *****
2022-06-12 02:57:32,625   att_loss = 3.492064186504909
2022-06-12 02:57:32,626   global_step = 29499
2022-06-12 02:57:32,626   loss = 4.356793835049584
2022-06-12 02:57:32,626   rep_loss = 0.8647296499638331
2022-06-12 02:57:32,626 ***** Save model *****
2022-06-12 02:57:36,045 ***** Running evaluation *****
2022-06-12 02:57:36,045   Epoch = 24 iter 6479 step
2022-06-12 02:57:36,045   Num examples = 1043
2022-06-12 02:57:36,045   Batch size = 32
2022-06-12 02:57:36,946 ***** Eval results *****
2022-06-12 02:57:36,946   cls_loss = 0.08302394667981376
2022-06-12 02:57:36,946   eval_loss = 0.9699051479498545
2022-06-12 02:57:36,946   global_step = 6479
2022-06-12 02:57:36,947   loss = 0.08302394667981376
2022-06-12 02:57:36,947   mcc = 0.2567123797828337
2022-06-12 02:57:41,818 ***** Running evaluation *****
2022-06-12 02:57:41,819   Epoch = 24 iter 6499 step
2022-06-12 02:57:41,819   Num examples = 1043
2022-06-12 02:57:41,819   Batch size = 32
2022-06-12 02:57:42,719 ***** Eval results *****
2022-06-12 02:57:42,720   cls_loss = 0.0830878673510237
2022-06-12 02:57:42,720   eval_loss = 0.9697310012398344
2022-06-12 02:57:42,720   global_step = 6499
2022-06-12 02:57:42,720   loss = 0.0830878673510237
2022-06-12 02:57:42,720   mcc = 0.2536674097746432
2022-06-12 02:57:47,610 ***** Running evaluation *****
2022-06-12 02:57:47,611   Epoch = 24 iter 6519 step
2022-06-12 02:57:47,611   Num examples = 1043
2022-06-12 02:57:47,611   Batch size = 32
2022-06-12 02:57:48,510 ***** Eval results *****
2022-06-12 02:57:48,511   cls_loss = 0.08347949627283457
2022-06-12 02:57:48,511   eval_loss = 0.9702977312333656
2022-06-12 02:57:48,511   global_step = 6519
2022-06-12 02:57:48,511   loss = 0.08347949627283457
2022-06-12 02:57:48,511   mcc = 0.2567123797828337
2022-06-12 02:57:53,385 ***** Running evaluation *****
2022-06-12 02:57:53,386   Epoch = 24 iter 6539 step
2022-06-12 02:57:53,386   Num examples = 1043
2022-06-12 02:57:53,386   Batch size = 32
2022-06-12 02:57:54,288 ***** Eval results *****
2022-06-12 02:57:54,288   cls_loss = 0.08377633437173057
2022-06-12 02:57:54,288   eval_loss = 0.9835364240588564
2022-06-12 02:57:54,288   global_step = 6539
2022-06-12 02:57:54,288   loss = 0.08377633437173057
2022-06-12 02:57:54,288   mcc = 0.2577823893825732
2022-06-12 02:57:59,183 ***** Running evaluation *****
2022-06-12 02:57:59,183   Epoch = 24 iter 6559 step
2022-06-12 02:57:59,183   Num examples = 1043
2022-06-12 02:57:59,183   Batch size = 32
2022-06-12 02:58:00,084 ***** Eval results *****
2022-06-12 02:58:00,084   cls_loss = 0.08400053521063154
2022-06-12 02:58:00,084   eval_loss = 0.9590903728297262
2022-06-12 02:58:00,084   global_step = 6559
2022-06-12 02:58:00,084   loss = 0.08400053521063154
2022-06-12 02:58:00,084   mcc = 0.271038022702
2022-06-12 02:58:04,960 ***** Running evaluation *****
2022-06-12 02:58:04,960   Epoch = 24 iter 6579 step
2022-06-12 02:58:04,960   Num examples = 1043
2022-06-12 02:58:04,960   Batch size = 32
2022-06-12 02:58:05,861 ***** Eval results *****
2022-06-12 02:58:05,861   cls_loss = 0.08423980565098992
2022-06-12 02:58:05,861   eval_loss = 0.9356922687906207
2022-06-12 02:58:05,862   global_step = 6579
2022-06-12 02:58:05,862   loss = 0.08423980565098992
2022-06-12 02:58:05,862   mcc = 0.26166513707762096
2022-06-12 02:58:10,746 ***** Running evaluation *****
2022-06-12 02:58:10,747   Epoch = 24 iter 6599 step
2022-06-12 02:58:10,747   Num examples = 1043
2022-06-12 02:58:10,747   Batch size = 32
2022-06-12 02:58:11,649 ***** Eval results *****
2022-06-12 02:58:11,649   cls_loss = 0.08410974537358858
2022-06-12 02:58:11,649   eval_loss = 0.9442066333510659
2022-06-12 02:58:11,649   global_step = 6599
2022-06-12 02:58:11,649   loss = 0.08410974537358858
2022-06-12 02:58:11,649   mcc = 0.2605504590349186
2022-06-12 02:58:16,539 ***** Running evaluation *****
2022-06-12 02:58:16,539   Epoch = 24 iter 6619 step
2022-06-12 02:58:16,539   Num examples = 1043
2022-06-12 02:58:16,540   Batch size = 32
2022-06-12 02:58:17,441 ***** Eval results *****
2022-06-12 02:58:17,442   cls_loss = 0.0840465478705004
2022-06-12 02:58:17,442   eval_loss = 0.9563754498958588
2022-06-12 02:58:17,442   global_step = 6619
2022-06-12 02:58:17,442   loss = 0.0840465478705004
2022-06-12 02:58:17,442   mcc = 0.2567123797828337
2022-06-12 02:58:22,348 ***** Running evaluation *****
2022-06-12 02:58:22,349   Epoch = 24 iter 6639 step
2022-06-12 02:58:22,349   Num examples = 1043
2022-06-12 02:58:22,349   Batch size = 32
2022-06-12 02:58:23,256 ***** Eval results *****
2022-06-12 02:58:23,256   cls_loss = 0.08367306339147287
2022-06-12 02:58:23,256   eval_loss = 0.9797864718870684
2022-06-12 02:58:23,257   global_step = 6639
2022-06-12 02:58:23,257   loss = 0.08367306339147287
2022-06-12 02:58:23,257   mcc = 0.24295421433913034
2022-06-12 02:58:28,175 ***** Running evaluation *****
2022-06-12 02:58:28,175   Epoch = 24 iter 6659 step
2022-06-12 02:58:28,175   Num examples = 1043
2022-06-12 02:58:28,175   Batch size = 32
2022-06-12 02:58:29,079 ***** Eval results *****
2022-06-12 02:58:29,079   cls_loss = 0.0834174964473067
2022-06-12 02:58:29,080   eval_loss = 0.9689928889274597
2022-06-12 02:58:29,080   global_step = 6659
2022-06-12 02:58:29,080   loss = 0.0834174964473067
2022-06-12 02:58:29,080   mcc = 0.24572636204735215
2022-06-12 02:58:33,984 ***** Running evaluation *****
2022-06-12 02:58:33,985   Epoch = 25 iter 6679 step
2022-06-12 02:58:33,985   Num examples = 1043
2022-06-12 02:58:33,985   Batch size = 32
2022-06-12 02:58:34,886 ***** Eval results *****
2022-06-12 02:58:34,887   cls_loss = 0.0794491358101368
2022-06-12 02:58:34,887   eval_loss = 0.9626835720105604
2022-06-12 02:58:34,887   global_step = 6679
2022-06-12 02:58:34,887   loss = 0.0794491358101368
2022-06-12 02:58:34,887   mcc = 0.23376817585619566
2022-06-12 02:58:39,795 ***** Running evaluation *****
2022-06-12 02:58:39,795   Epoch = 25 iter 6699 step
2022-06-12 02:58:39,795   Num examples = 1043
2022-06-12 02:58:39,795   Batch size = 32
2022-06-12 02:58:40,696 ***** Eval results *****
2022-06-12 02:58:40,696   cls_loss = 0.08699190647651751
2022-06-12 02:58:40,696   eval_loss = 0.943308977466641
2022-06-12 02:58:40,696   global_step = 6699
2022-06-12 02:58:40,697   loss = 0.08699190647651751
2022-06-12 02:58:40,697   mcc = 0.2333042388550367
2022-06-12 02:58:45,581 ***** Running evaluation *****
2022-06-12 02:58:45,581   Epoch = 25 iter 6719 step
2022-06-12 02:58:45,582   Num examples = 1043
2022-06-12 02:58:45,582   Batch size = 32
2022-06-12 02:58:46,482 ***** Eval results *****
2022-06-12 02:58:46,483   cls_loss = 0.08303843709555539
2022-06-12 02:58:46,483   eval_loss = 0.9484118078694199
2022-06-12 02:58:46,483   global_step = 6719
2022-06-12 02:58:46,483   loss = 0.08303843709555539
2022-06-12 02:58:46,483   mcc = 0.22865388596427977
2022-06-12 02:58:51,356 ***** Running evaluation *****
2022-06-12 02:58:51,357   Epoch = 25 iter 6739 step
2022-06-12 02:58:51,357   Num examples = 1043
2022-06-12 02:58:51,357   Batch size = 32
2022-06-12 02:58:52,258 ***** Eval results *****
2022-06-12 02:58:52,258   cls_loss = 0.08320265205111355
2022-06-12 02:58:52,258   eval_loss = 0.9337348901864254
2022-06-12 02:58:52,258   global_step = 6739
2022-06-12 02:58:52,258   loss = 0.08320265205111355
2022-06-12 02:58:52,258   mcc = 0.23968325937560422
2022-06-12 02:58:57,140 ***** Running evaluation *****
2022-06-12 02:58:57,140   Epoch = 25 iter 6759 step
2022-06-12 02:58:57,140   Num examples = 1043
2022-06-12 02:58:57,140   Batch size = 32
2022-06-12 02:58:58,039 ***** Eval results *****
2022-06-12 02:58:58,040   cls_loss = 0.08278273649158932
2022-06-12 02:58:58,040   eval_loss = 0.9453029876405542
2022-06-12 02:58:58,040   global_step = 6759
2022-06-12 02:58:58,040   loss = 0.08278273649158932
2022-06-12 02:58:58,040   mcc = 0.24745430575165586
2022-06-12 02:59:02,930 ***** Running evaluation *****
2022-06-12 02:59:02,930   Epoch = 25 iter 6779 step
2022-06-12 02:59:02,930   Num examples = 1043
2022-06-12 02:59:02,930   Batch size = 32
2022-06-12 02:59:03,831 ***** Eval results *****
2022-06-12 02:59:03,831   cls_loss = 0.08343675152327006
2022-06-12 02:59:03,831   eval_loss = 0.9436460886940812
2022-06-12 02:59:03,831   global_step = 6779
2022-06-12 02:59:03,831   loss = 0.08343675152327006
2022-06-12 02:59:03,831   mcc = 0.23957712438913295
2022-06-12 02:59:08,707 ***** Running evaluation *****
2022-06-12 02:59:08,708   Epoch = 25 iter 6799 step
2022-06-12 02:59:08,708   Num examples = 1043
2022-06-12 02:59:08,708   Batch size = 32
2022-06-12 02:59:09,609 ***** Eval results *****
2022-06-12 02:59:09,609   cls_loss = 0.0832396814179036
2022-06-12 02:59:09,609   eval_loss = 0.9414444547710996
2022-06-12 02:59:09,609   global_step = 6799
2022-06-12 02:59:09,609   loss = 0.0832396814179036
2022-06-12 02:59:09,610   mcc = 0.24221283140723873
2022-06-12 02:59:14,488 ***** Running evaluation *****
2022-06-12 02:59:14,489   Epoch = 25 iter 6819 step
2022-06-12 02:59:14,489   Num examples = 1043
2022-06-12 02:59:14,489   Batch size = 32
2022-06-12 02:59:15,389 ***** Eval results *****
2022-06-12 02:59:15,390   cls_loss = 0.0830576443630788
2022-06-12 02:59:15,390   eval_loss = 0.9365372468124736
2022-06-12 02:59:15,390   global_step = 6819
2022-06-12 02:59:15,390   loss = 0.0830576443630788
2022-06-12 02:59:15,390   mcc = 0.24221283140723873
2022-06-12 02:59:20,278 ***** Running evaluation *****
2022-06-12 02:59:20,278   Epoch = 25 iter 6839 step
2022-06-12 02:59:20,278   Num examples = 1043
2022-06-12 02:59:20,278   Batch size = 32
2022-06-12 02:59:21,179 ***** Eval results *****
2022-06-12 02:59:21,179   cls_loss = 0.08290358705491555
2022-06-12 02:59:21,179   eval_loss = 0.9373620604023789
2022-06-12 02:59:21,179   global_step = 6839
2022-06-12 02:59:21,179   loss = 0.08290358705491555
2022-06-12 02:59:21,180   mcc = 0.24650757572414878
2022-06-12 02:59:26,061 ***** Running evaluation *****
2022-06-12 02:59:26,062   Epoch = 25 iter 6859 step
2022-06-12 02:59:26,062   Num examples = 1043
2022-06-12 02:59:26,062   Batch size = 32
2022-06-12 02:59:26,960 ***** Eval results *****
2022-06-12 02:59:26,961   cls_loss = 0.0827657266560456
2022-06-12 02:59:26,961   eval_loss = 0.9422369229071068
2022-06-12 02:59:26,961   global_step = 6859
2022-06-12 02:59:26,961   loss = 0.0827657266560456
2022-06-12 02:59:26,961   mcc = 0.24422913630893614
2022-06-12 02:59:31,845 ***** Running evaluation *****
2022-06-12 02:59:31,845   Epoch = 25 iter 6879 step
2022-06-12 02:59:31,846   Num examples = 1043
2022-06-12 02:59:31,846   Batch size = 32
2022-06-12 02:59:32,744 ***** Eval results *****
2022-06-12 02:59:32,745   cls_loss = 0.08255408543582056
2022-06-12 02:59:32,745   eval_loss = 0.9669315092491381
2022-06-12 02:59:32,745   global_step = 6879
2022-06-12 02:59:32,745   loss = 0.08255408543582056
2022-06-12 02:59:32,745   mcc = 0.24422913630893614
2022-06-12 02:59:37,625 ***** Running evaluation *****
2022-06-12 02:59:37,625   Epoch = 25 iter 6899 step
2022-06-12 02:59:37,625   Num examples = 1043
2022-06-12 02:59:37,625   Batch size = 32
2022-06-12 02:59:38,526 ***** Eval results *****
2022-06-12 02:59:38,526   cls_loss = 0.08226245176047087
2022-06-12 02:59:38,526   eval_loss = 0.9460730588797367
2022-06-12 02:59:38,526   global_step = 6899
2022-06-12 02:59:38,526   loss = 0.08226245176047087
2022-06-12 02:59:38,527   mcc = 0.230830124799434
2022-06-12 02:59:40,172 ***** Running evaluation *****
2022-06-12 02:59:40,172   Epoch = 9 iter 29999 step
2022-06-12 02:59:40,172   Num examples = 5463
2022-06-12 02:59:40,173   Batch size = 32
2022-06-12 02:59:40,174 ***** Eval results *****
2022-06-12 02:59:40,174   att_loss = 3.5225792929695103
2022-06-12 02:59:40,174   global_step = 29999
2022-06-12 02:59:40,174   loss = 4.389033728859961
2022-06-12 02:59:40,174   rep_loss = 0.866454442598723
2022-06-12 02:59:40,175 ***** Save model *****
2022-06-12 02:59:43,437 ***** Running evaluation *****
2022-06-12 02:59:43,437   Epoch = 25 iter 6919 step
2022-06-12 02:59:43,437   Num examples = 1043
2022-06-12 02:59:43,437   Batch size = 32
2022-06-12 02:59:44,338 ***** Eval results *****
2022-06-12 02:59:44,338   cls_loss = 0.08227065587263616
2022-06-12 02:59:44,339   eval_loss = 0.9319622923027385
2022-06-12 02:59:44,339   global_step = 6919
2022-06-12 02:59:44,339   loss = 0.08227065587263616
2022-06-12 02:59:44,339   mcc = 0.2562681175006562
2022-06-12 02:59:49,232 ***** Running evaluation *****
2022-06-12 02:59:49,232   Epoch = 25 iter 6939 step
2022-06-12 02:59:49,232   Num examples = 1043
2022-06-12 02:59:49,232   Batch size = 32
2022-06-12 02:59:50,134 ***** Eval results *****
2022-06-12 02:59:50,134   cls_loss = 0.08218008756750461
2022-06-12 02:59:50,134   eval_loss = 0.9382682731657317
2022-06-12 02:59:50,135   global_step = 6939
2022-06-12 02:59:50,135   loss = 0.08218008756750461
2022-06-12 02:59:50,135   mcc = 0.24591769729927312
2022-06-12 02:59:55,026 ***** Running evaluation *****
2022-06-12 02:59:55,027   Epoch = 26 iter 6959 step
2022-06-12 02:59:55,027   Num examples = 1043
2022-06-12 02:59:55,027   Batch size = 32
2022-06-12 02:59:55,931 ***** Eval results *****
2022-06-12 02:59:55,931   cls_loss = 0.08465130013578079
2022-06-12 02:59:55,931   eval_loss = 0.9332495530446371
2022-06-12 02:59:55,931   global_step = 6959
2022-06-12 02:59:55,931   loss = 0.08465130013578079
2022-06-12 02:59:55,931   mcc = 0.24630665394418813
2022-06-12 03:00:00,836 ***** Running evaluation *****
2022-06-12 03:00:00,836   Epoch = 26 iter 6979 step
2022-06-12 03:00:00,836   Num examples = 1043
2022-06-12 03:00:00,836   Batch size = 32
2022-06-12 03:00:01,737 ***** Eval results *****
2022-06-12 03:00:01,737   cls_loss = 0.08250881026725511
2022-06-12 03:00:01,737   eval_loss = 0.9437782565752665
2022-06-12 03:00:01,737   global_step = 6979
2022-06-12 03:00:01,738   loss = 0.08250881026725511
2022-06-12 03:00:01,738   mcc = 0.25020049256194704
2022-06-12 03:00:06,634 ***** Running evaluation *****
2022-06-12 03:00:06,634   Epoch = 26 iter 6999 step
2022-06-12 03:00:06,634   Num examples = 1043
2022-06-12 03:00:06,634   Batch size = 32
2022-06-12 03:00:07,536 ***** Eval results *****
2022-06-12 03:00:07,536   cls_loss = 0.08207079794323235
2022-06-12 03:00:07,536   eval_loss = 0.9381152535929824
2022-06-12 03:00:07,536   global_step = 6999
2022-06-12 03:00:07,536   loss = 0.08207079794323235
2022-06-12 03:00:07,536   mcc = 0.24684059492240898
2022-06-12 03:00:12,424 ***** Running evaluation *****
2022-06-12 03:00:12,424   Epoch = 26 iter 7019 step
2022-06-12 03:00:12,424   Num examples = 1043
2022-06-12 03:00:12,424   Batch size = 32
2022-06-12 03:00:13,329 ***** Eval results *****
2022-06-12 03:00:13,329   cls_loss = 0.08241375758276358
2022-06-12 03:00:13,329   eval_loss = 0.9133600935791478
2022-06-12 03:00:13,329   global_step = 7019
2022-06-12 03:00:13,330   loss = 0.08241375758276358
2022-06-12 03:00:13,330   mcc = 0.24964040460594325
2022-06-12 03:00:18,226 ***** Running evaluation *****
2022-06-12 03:00:18,226   Epoch = 26 iter 7039 step
2022-06-12 03:00:18,226   Num examples = 1043
2022-06-12 03:00:18,226   Batch size = 32
2022-06-12 03:00:19,128 ***** Eval results *****
2022-06-12 03:00:19,128   cls_loss = 0.08136384250576963
2022-06-12 03:00:19,128   eval_loss = 0.9349829012697394
2022-06-12 03:00:19,128   global_step = 7039
2022-06-12 03:00:19,128   loss = 0.08136384250576963
2022-06-12 03:00:19,128   mcc = 0.2487037210399385
2022-06-12 03:00:24,015 ***** Running evaluation *****
2022-06-12 03:00:24,016   Epoch = 26 iter 7059 step
2022-06-12 03:00:24,016   Num examples = 1043
2022-06-12 03:00:24,016   Batch size = 32
2022-06-12 03:00:24,918 ***** Eval results *****
2022-06-12 03:00:24,918   cls_loss = 0.08042380933323477
2022-06-12 03:00:24,918   eval_loss = 0.9539761796142116
2022-06-12 03:00:24,918   global_step = 7059
2022-06-12 03:00:24,918   loss = 0.08042380933323477
2022-06-12 03:00:24,918   mcc = 0.24964040460594325
2022-06-12 03:00:29,789 ***** Running evaluation *****
2022-06-12 03:00:29,790   Epoch = 26 iter 7079 step
2022-06-12 03:00:29,790   Num examples = 1043
2022-06-12 03:00:29,790   Batch size = 32
2022-06-12 03:00:30,689 ***** Eval results *****
2022-06-12 03:00:30,690   cls_loss = 0.0806107055531801
2022-06-12 03:00:30,690   eval_loss = 0.9392812522974882
2022-06-12 03:00:30,690   global_step = 7079
2022-06-12 03:00:30,690   loss = 0.0806107055531801
2022-06-12 03:00:30,690   mcc = 0.23084409143038281
2022-06-12 03:00:35,560 ***** Running evaluation *****
2022-06-12 03:00:35,561   Epoch = 26 iter 7099 step
2022-06-12 03:00:35,561   Num examples = 1043
2022-06-12 03:00:35,561   Batch size = 32
2022-06-12 03:00:36,462 ***** Eval results *****
2022-06-12 03:00:36,462   cls_loss = 0.08070850130289224
2022-06-12 03:00:36,462   eval_loss = 0.9392481355956106
2022-06-12 03:00:36,462   global_step = 7099
2022-06-12 03:00:36,462   loss = 0.08070850130289224
2022-06-12 03:00:36,462   mcc = 0.23519233669775863
2022-06-12 03:00:41,355 ***** Running evaluation *****
2022-06-12 03:00:41,356   Epoch = 26 iter 7119 step
2022-06-12 03:00:41,356   Num examples = 1043
2022-06-12 03:00:41,356   Batch size = 32
2022-06-12 03:00:42,255 ***** Eval results *****
2022-06-12 03:00:42,255   cls_loss = 0.08076833184324415
2022-06-12 03:00:42,256   eval_loss = 0.9409468246228767
2022-06-12 03:00:42,256   global_step = 7119
2022-06-12 03:00:42,256   loss = 0.08076833184324415
2022-06-12 03:00:42,256   mcc = 0.238427015501258
2022-06-12 03:00:47,157 ***** Running evaluation *****
2022-06-12 03:00:47,158   Epoch = 26 iter 7139 step
2022-06-12 03:00:47,158   Num examples = 1043
2022-06-12 03:00:47,158   Batch size = 32
2022-06-12 03:00:48,059 ***** Eval results *****
2022-06-12 03:00:48,059   cls_loss = 0.08070241712827973
2022-06-12 03:00:48,060   eval_loss = 0.9550394153956211
2022-06-12 03:00:48,060   global_step = 7139
2022-06-12 03:00:48,060   loss = 0.08070241712827973
2022-06-12 03:00:48,060   mcc = 0.2374695309664015
2022-06-12 03:00:52,936 ***** Running evaluation *****
2022-06-12 03:00:52,936   Epoch = 26 iter 7159 step
2022-06-12 03:00:52,936   Num examples = 1043
2022-06-12 03:00:52,937   Batch size = 32
2022-06-12 03:00:53,838 ***** Eval results *****
2022-06-12 03:00:53,838   cls_loss = 0.08116090170661425
2022-06-12 03:00:53,838   eval_loss = 0.9348119027686842
2022-06-12 03:00:53,838   global_step = 7159
2022-06-12 03:00:53,838   loss = 0.08116090170661425
2022-06-12 03:00:53,838   mcc = 0.24384786384423915
2022-06-12 03:00:58,716 ***** Running evaluation *****
2022-06-12 03:00:58,717   Epoch = 26 iter 7179 step
2022-06-12 03:00:58,717   Num examples = 1043
2022-06-12 03:00:58,717   Batch size = 32
2022-06-12 03:00:59,618 ***** Eval results *****
2022-06-12 03:00:59,618   cls_loss = 0.08165486694513997
2022-06-12 03:00:59,618   eval_loss = 0.9171771967049801
2022-06-12 03:00:59,618   global_step = 7179
2022-06-12 03:00:59,618   loss = 0.08165486694513997
2022-06-12 03:00:59,618   mcc = 0.24493514255898144
2022-06-12 03:01:04,494 ***** Running evaluation *****
2022-06-12 03:01:04,495   Epoch = 26 iter 7199 step
2022-06-12 03:01:04,495   Num examples = 1043
2022-06-12 03:01:04,495   Batch size = 32
2022-06-12 03:01:05,396 ***** Eval results *****
2022-06-12 03:01:05,396   cls_loss = 0.08158882385561902
2022-06-12 03:01:05,396   eval_loss = 0.9357304275035858
2022-06-12 03:01:05,396   global_step = 7199
2022-06-12 03:01:05,396   loss = 0.08158882385561902
2022-06-12 03:01:05,396   mcc = 0.24493514255898144
2022-06-12 03:01:10,274 ***** Running evaluation *****
2022-06-12 03:01:10,275   Epoch = 27 iter 7219 step
2022-06-12 03:01:10,275   Num examples = 1043
2022-06-12 03:01:10,275   Batch size = 32
2022-06-12 03:01:11,174 ***** Eval results *****
2022-06-12 03:01:11,174   cls_loss = 0.08485993146896362
2022-06-12 03:01:11,174   eval_loss = 0.9422731083450895
2022-06-12 03:01:11,174   global_step = 7219
2022-06-12 03:01:11,174   loss = 0.08485993146896362
2022-06-12 03:01:11,174   mcc = 0.23873407559405013
2022-06-12 03:01:16,047 ***** Running evaluation *****
2022-06-12 03:01:16,047   Epoch = 27 iter 7239 step
2022-06-12 03:01:16,047   Num examples = 1043
2022-06-12 03:01:16,047   Batch size = 32
2022-06-12 03:01:16,949 ***** Eval results *****
2022-06-12 03:01:16,949   cls_loss = 0.08588086689511935
2022-06-12 03:01:16,949   eval_loss = 0.9295578048084722
2022-06-12 03:01:16,950   global_step = 7239
2022-06-12 03:01:16,950   loss = 0.08588086689511935
2022-06-12 03:01:16,950   mcc = 0.24097172200238098
2022-06-12 03:01:21,820 ***** Running evaluation *****
2022-06-12 03:01:21,821   Epoch = 27 iter 7259 step
2022-06-12 03:01:21,821   Num examples = 1043
2022-06-12 03:01:21,821   Batch size = 32
2022-06-12 03:01:22,720 ***** Eval results *****
2022-06-12 03:01:22,720   cls_loss = 0.08404919132590294
2022-06-12 03:01:22,720   eval_loss = 0.9313564426971205
2022-06-12 03:01:22,721   global_step = 7259
2022-06-12 03:01:22,721   loss = 0.08404919132590294
2022-06-12 03:01:22,721   mcc = 0.2428581504486696
2022-06-12 03:01:27,600 ***** Running evaluation *****
2022-06-12 03:01:27,600   Epoch = 27 iter 7279 step
2022-06-12 03:01:27,600   Num examples = 1043
2022-06-12 03:01:27,600   Batch size = 32
2022-06-12 03:01:28,501 ***** Eval results *****
2022-06-12 03:01:28,501   cls_loss = 0.08345326227801186
2022-06-12 03:01:28,501   eval_loss = 0.9389639406493215
2022-06-12 03:01:28,501   global_step = 7279
2022-06-12 03:01:28,502   loss = 0.08345326227801186
2022-06-12 03:01:28,502   mcc = 0.23426533656843046
2022-06-12 03:01:33,394 ***** Running evaluation *****
2022-06-12 03:01:33,394   Epoch = 27 iter 7299 step
2022-06-12 03:01:33,394   Num examples = 1043
2022-06-12 03:01:33,395   Batch size = 32
2022-06-12 03:01:34,297 ***** Eval results *****
2022-06-12 03:01:34,297   cls_loss = 0.08227043524384499
2022-06-12 03:01:34,297   eval_loss = 0.943025532093915
2022-06-12 03:01:34,297   global_step = 7299
2022-06-12 03:01:34,297   loss = 0.08227043524384499
2022-06-12 03:01:34,297   mcc = 0.23636368380458236
2022-06-12 03:01:39,195 ***** Running evaluation *****
2022-06-12 03:01:39,196   Epoch = 27 iter 7319 step
2022-06-12 03:01:39,196   Num examples = 1043
2022-06-12 03:01:39,196   Batch size = 32
2022-06-12 03:01:40,097 ***** Eval results *****
2022-06-12 03:01:40,097   cls_loss = 0.08217639645392244
2022-06-12 03:01:40,098   eval_loss = 0.9360389971371853
2022-06-12 03:01:40,098   global_step = 7319
2022-06-12 03:01:40,098   loss = 0.08217639645392244
2022-06-12 03:01:40,098   mcc = 0.23426533656843046
2022-06-12 03:01:44,978 ***** Running evaluation *****
2022-06-12 03:01:44,978   Epoch = 27 iter 7339 step
2022-06-12 03:01:44,978   Num examples = 1043
2022-06-12 03:01:44,978   Batch size = 32
2022-06-12 03:01:45,878 ***** Eval results *****
2022-06-12 03:01:45,878   cls_loss = 0.08214833008555265
2022-06-12 03:01:45,878   eval_loss = 0.9360319484363903
2022-06-12 03:01:45,879   global_step = 7339
2022-06-12 03:01:45,879   loss = 0.08214833008555265
2022-06-12 03:01:45,879   mcc = 0.23426533656843046
2022-06-12 03:01:47,469 ***** Running evaluation *****
2022-06-12 03:01:47,469   Epoch = 9 iter 30499 step
2022-06-12 03:01:47,469   Num examples = 5463
2022-06-12 03:01:47,469   Batch size = 32
2022-06-12 03:01:47,470 ***** Eval results *****
2022-06-12 03:01:47,470   att_loss = 3.52139785491116
2022-06-12 03:01:47,471   global_step = 30499
2022-06-12 03:01:47,471   loss = 4.387632390130276
2022-06-12 03:01:47,471   rep_loss = 0.8662345346470941
2022-06-12 03:01:47,471 ***** Save model *****
2022-06-12 03:01:50,772 ***** Running evaluation *****
2022-06-12 03:01:50,772   Epoch = 27 iter 7359 step
2022-06-12 03:01:50,772   Num examples = 1043
2022-06-12 03:01:50,772   Batch size = 32
2022-06-12 03:01:51,673 ***** Eval results *****
2022-06-12 03:01:51,673   cls_loss = 0.0821797263622284
2022-06-12 03:01:51,673   eval_loss = 0.9537633738734506
2022-06-12 03:01:51,673   global_step = 7359
2022-06-12 03:01:51,674   loss = 0.0821797263622284
2022-06-12 03:01:51,674   mcc = 0.25079674398130253
2022-06-12 03:01:56,549 ***** Running evaluation *****
2022-06-12 03:01:56,549   Epoch = 27 iter 7379 step
2022-06-12 03:01:56,549   Num examples = 1043
2022-06-12 03:01:56,549   Batch size = 32
2022-06-12 03:01:57,452 ***** Eval results *****
2022-06-12 03:01:57,452   cls_loss = 0.0820933305165347
2022-06-12 03:01:57,452   eval_loss = 0.9407352976726763
2022-06-12 03:01:57,452   global_step = 7379
2022-06-12 03:01:57,452   loss = 0.0820933305165347
2022-06-12 03:01:57,452   mcc = 0.23873407559405013
2022-06-12 03:02:02,332 ***** Running evaluation *****
2022-06-12 03:02:02,333   Epoch = 27 iter 7399 step
2022-06-12 03:02:02,333   Num examples = 1043
2022-06-12 03:02:02,333   Batch size = 32
2022-06-12 03:02:03,234 ***** Eval results *****
2022-06-12 03:02:03,234   cls_loss = 0.08207299697555993
2022-06-12 03:02:03,234   eval_loss = 0.9367015361785889
2022-06-12 03:02:03,234   global_step = 7399
2022-06-12 03:02:03,234   loss = 0.08207299697555993
2022-06-12 03:02:03,234   mcc = 0.24167642914880966
2022-06-12 03:02:08,148 ***** Running evaluation *****
2022-06-12 03:02:08,149   Epoch = 27 iter 7419 step
2022-06-12 03:02:08,149   Num examples = 1043
2022-06-12 03:02:08,149   Batch size = 32
2022-06-12 03:02:09,051 ***** Eval results *****
2022-06-12 03:02:09,051   cls_loss = 0.08191491118854001
2022-06-12 03:02:09,051   eval_loss = 0.9403759177887079
2022-06-12 03:02:09,051   global_step = 7419
2022-06-12 03:02:09,052   loss = 0.08191491118854001
2022-06-12 03:02:09,052   mcc = 0.24167642914880966
2022-06-12 03:02:13,974 ***** Running evaluation *****
2022-06-12 03:02:13,975   Epoch = 27 iter 7439 step
2022-06-12 03:02:13,975   Num examples = 1043
2022-06-12 03:02:13,975   Batch size = 32
2022-06-12 03:02:14,874 ***** Eval results *****
2022-06-12 03:02:14,874   cls_loss = 0.0817319436079782
2022-06-12 03:02:14,874   eval_loss = 0.9387248447447112
2022-06-12 03:02:14,875   global_step = 7439
2022-06-12 03:02:14,875   loss = 0.0817319436079782
2022-06-12 03:02:14,875   mcc = 0.24592602987531476
2022-06-12 03:02:19,759 ***** Running evaluation *****
2022-06-12 03:02:19,760   Epoch = 27 iter 7459 step
2022-06-12 03:02:19,760   Num examples = 1043
2022-06-12 03:02:19,760   Batch size = 32
2022-06-12 03:02:20,660 ***** Eval results *****
2022-06-12 03:02:20,660   cls_loss = 0.08185249502956868
2022-06-12 03:02:20,660   eval_loss = 0.9352742686416163
2022-06-12 03:02:20,661   global_step = 7459
2022-06-12 03:02:20,661   loss = 0.08185249502956868
2022-06-12 03:02:20,661   mcc = 0.2427227762618374
2022-06-12 03:02:25,521 ***** Running evaluation *****
2022-06-12 03:02:25,521   Epoch = 28 iter 7479 step
2022-06-12 03:02:25,521   Num examples = 1043
2022-06-12 03:02:25,521   Batch size = 32
2022-06-12 03:02:26,423 ***** Eval results *****
2022-06-12 03:02:26,423   cls_loss = 0.07407554239034653
2022-06-12 03:02:26,423   eval_loss = 0.9371240364782738
2022-06-12 03:02:26,423   global_step = 7479
2022-06-12 03:02:26,423   loss = 0.07407554239034653
2022-06-12 03:02:26,423   mcc = 0.2427227762618374
2022-06-12 03:02:31,309 ***** Running evaluation *****
2022-06-12 03:02:31,309   Epoch = 28 iter 7499 step
2022-06-12 03:02:31,310   Num examples = 1043
2022-06-12 03:02:31,310   Batch size = 32
2022-06-12 03:02:32,210 ***** Eval results *****
2022-06-12 03:02:32,210   cls_loss = 0.07796890612529672
2022-06-12 03:02:32,210   eval_loss = 0.944402280178937
2022-06-12 03:02:32,210   global_step = 7499
2022-06-12 03:02:32,210   loss = 0.07796890612529672
2022-06-12 03:02:32,210   mcc = 0.23956771164798327
2022-06-12 03:02:37,092 ***** Running evaluation *****
2022-06-12 03:02:37,093   Epoch = 28 iter 7519 step
2022-06-12 03:02:37,093   Num examples = 1043
2022-06-12 03:02:37,093   Batch size = 32
2022-06-12 03:02:37,995 ***** Eval results *****
2022-06-12 03:02:37,996   cls_loss = 0.08255473662947499
2022-06-12 03:02:37,996   eval_loss = 0.9311159244089415
2022-06-12 03:02:37,996   global_step = 7519
2022-06-12 03:02:37,996   loss = 0.08255473662947499
2022-06-12 03:02:37,996   mcc = 0.24810019947327508
2022-06-12 03:02:42,873 ***** Running evaluation *****
2022-06-12 03:02:42,873   Epoch = 28 iter 7539 step
2022-06-12 03:02:42,873   Num examples = 1043
2022-06-12 03:02:42,873   Batch size = 32
2022-06-12 03:02:43,773 ***** Eval results *****
2022-06-12 03:02:43,773   cls_loss = 0.08170333374587316
2022-06-12 03:02:43,774   eval_loss = 0.9215112102754188
2022-06-12 03:02:43,774   global_step = 7539
2022-06-12 03:02:43,774   loss = 0.08170333374587316
2022-06-12 03:02:43,774   mcc = 0.24805372483837093
2022-06-12 03:02:48,651 ***** Running evaluation *****
2022-06-12 03:02:48,651   Epoch = 28 iter 7559 step
2022-06-12 03:02:48,651   Num examples = 1043
2022-06-12 03:02:48,652   Batch size = 32
2022-06-12 03:02:49,551 ***** Eval results *****
2022-06-12 03:02:49,551   cls_loss = 0.08127532046602433
2022-06-12 03:02:49,551   eval_loss = 0.9380117510304307
2022-06-12 03:02:49,552   global_step = 7559
2022-06-12 03:02:49,552   loss = 0.08127532046602433
2022-06-12 03:02:49,552   mcc = 0.24379582168639802
2022-06-12 03:02:54,422 ***** Running evaluation *****
2022-06-12 03:02:54,422   Epoch = 28 iter 7579 step
2022-06-12 03:02:54,423   Num examples = 1043
2022-06-12 03:02:54,423   Batch size = 32
2022-06-12 03:02:55,325 ***** Eval results *****
2022-06-12 03:02:55,326   cls_loss = 0.08152943567454236
2022-06-12 03:02:55,326   eval_loss = 0.9395813237537037
2022-06-12 03:02:55,326   global_step = 7579
2022-06-12 03:02:55,326   loss = 0.08152943567454236
2022-06-12 03:02:55,326   mcc = 0.23747252635161156
2022-06-12 03:03:00,225 ***** Running evaluation *****
2022-06-12 03:03:00,226   Epoch = 28 iter 7599 step
2022-06-12 03:03:00,226   Num examples = 1043
2022-06-12 03:03:00,226   Batch size = 32
2022-06-12 03:03:01,127 ***** Eval results *****
2022-06-12 03:03:01,128   cls_loss = 0.08110091977608883
2022-06-12 03:03:01,128   eval_loss = 0.9401311793110587
2022-06-12 03:03:01,128   global_step = 7599
2022-06-12 03:03:01,128   loss = 0.08110091977608883
2022-06-12 03:03:01,128   mcc = 0.24221283140723873
2022-06-12 03:03:06,036 ***** Running evaluation *****
2022-06-12 03:03:06,036   Epoch = 28 iter 7619 step
2022-06-12 03:03:06,036   Num examples = 1043
2022-06-12 03:03:06,036   Batch size = 32
2022-06-12 03:03:06,938 ***** Eval results *****
2022-06-12 03:03:06,939   cls_loss = 0.08087920624163601
2022-06-12 03:03:06,939   eval_loss = 0.9405480289097988
2022-06-12 03:03:06,939   global_step = 7619
2022-06-12 03:03:06,939   loss = 0.08087920624163601
2022-06-12 03:03:06,939   mcc = 0.23867508543744173
2022-06-12 03:03:11,859 ***** Running evaluation *****
2022-06-12 03:03:11,859   Epoch = 28 iter 7639 step
2022-06-12 03:03:11,859   Num examples = 1043
2022-06-12 03:03:11,860   Batch size = 32
2022-06-12 03:03:12,760 ***** Eval results *****
2022-06-12 03:03:12,760   cls_loss = 0.08103078903139004
2022-06-12 03:03:12,760   eval_loss = 0.9359748923417294
2022-06-12 03:03:12,760   global_step = 7639
2022-06-12 03:03:12,760   loss = 0.08103078903139004
2022-06-12 03:03:12,760   mcc = 0.23867508543744173
2022-06-12 03:03:17,637 ***** Running evaluation *****
2022-06-12 03:03:17,637   Epoch = 28 iter 7659 step
2022-06-12 03:03:17,637   Num examples = 1043
2022-06-12 03:03:17,637   Batch size = 32
2022-06-12 03:03:18,539 ***** Eval results *****
2022-06-12 03:03:18,539   cls_loss = 0.08144138555953412
2022-06-12 03:03:18,539   eval_loss = 0.9359185794989268
2022-06-12 03:03:18,540   global_step = 7659
2022-06-12 03:03:18,540   loss = 0.08144138555953412
2022-06-12 03:03:18,540   mcc = 0.24094079355032363
2022-06-12 03:03:23,443 ***** Running evaluation *****
2022-06-12 03:03:23,443   Epoch = 28 iter 7679 step
2022-06-12 03:03:23,443   Num examples = 1043
2022-06-12 03:03:23,444   Batch size = 32
2022-06-12 03:03:24,343 ***** Eval results *****
2022-06-12 03:03:24,343   cls_loss = 0.08129641698132008
2022-06-12 03:03:24,344   eval_loss = 0.9328122897581621
2022-06-12 03:03:24,344   global_step = 7679
2022-06-12 03:03:24,344   loss = 0.08129641698132008
2022-06-12 03:03:24,344   mcc = 0.24094079355032363
2022-06-12 03:03:29,229 ***** Running evaluation *****
2022-06-12 03:03:29,229   Epoch = 28 iter 7699 step
2022-06-12 03:03:29,229   Num examples = 1043
2022-06-12 03:03:29,229   Batch size = 32
2022-06-12 03:03:30,131 ***** Eval results *****
2022-06-12 03:03:30,131   cls_loss = 0.08095277306277121
2022-06-12 03:03:30,131   eval_loss = 0.9423576737895156
2022-06-12 03:03:30,131   global_step = 7699
2022-06-12 03:03:30,131   loss = 0.08095277306277121
2022-06-12 03:03:30,131   mcc = 0.24451972317243376
2022-06-12 03:03:35,045 ***** Running evaluation *****
2022-06-12 03:03:35,046   Epoch = 28 iter 7719 step
2022-06-12 03:03:35,046   Num examples = 1043
2022-06-12 03:03:35,046   Batch size = 32
2022-06-12 03:03:35,950 ***** Eval results *****
2022-06-12 03:03:35,950   cls_loss = 0.08088892252172952
2022-06-12 03:03:35,950   eval_loss = 0.9447963779622858
2022-06-12 03:03:35,950   global_step = 7719
2022-06-12 03:03:35,950   loss = 0.08088892252172952
2022-06-12 03:03:35,950   mcc = 0.24451972317243376
2022-06-12 03:03:40,856 ***** Running evaluation *****
2022-06-12 03:03:40,857   Epoch = 28 iter 7739 step
2022-06-12 03:03:40,857   Num examples = 1043
2022-06-12 03:03:40,857   Batch size = 32
2022-06-12 03:03:41,758 ***** Eval results *****
2022-06-12 03:03:41,758   cls_loss = 0.08119425057175042
2022-06-12 03:03:41,758   eval_loss = 0.9362391341816295
2022-06-12 03:03:41,758   global_step = 7739
2022-06-12 03:03:41,758   loss = 0.08119425057175042
2022-06-12 03:03:41,758   mcc = 0.23867508543744173
2022-06-12 03:03:46,664 ***** Running evaluation *****
2022-06-12 03:03:46,664   Epoch = 29 iter 7759 step
2022-06-12 03:03:46,664   Num examples = 1043
2022-06-12 03:03:46,664   Batch size = 32
2022-06-12 03:03:47,564 ***** Eval results *****
2022-06-12 03:03:47,564   cls_loss = 0.07984931627288461
2022-06-12 03:03:47,564   eval_loss = 0.934899142294219
2022-06-12 03:03:47,564   global_step = 7759
2022-06-12 03:03:47,565   loss = 0.07984931627288461
2022-06-12 03:03:47,565   mcc = 0.23852407549208424
2022-06-12 03:03:52,452 ***** Running evaluation *****
2022-06-12 03:03:52,453   Epoch = 29 iter 7779 step
2022-06-12 03:03:52,453   Num examples = 1043
2022-06-12 03:03:52,453   Batch size = 32
2022-06-12 03:03:53,354 ***** Eval results *****
2022-06-12 03:03:53,354   cls_loss = 0.08195777899689144
2022-06-12 03:03:53,354   eval_loss = 0.9422431443676804
2022-06-12 03:03:53,354   global_step = 7779
2022-06-12 03:03:53,354   loss = 0.08195777899689144
2022-06-12 03:03:53,354   mcc = 0.24607647129467225
2022-06-12 03:03:55,029 ***** Running evaluation *****
2022-06-12 03:03:55,029   Epoch = 9 iter 30999 step
2022-06-12 03:03:55,029   Num examples = 5463
2022-06-12 03:03:55,029   Batch size = 32
2022-06-12 03:03:55,030 ***** Eval results *****
2022-06-12 03:03:55,031   att_loss = 3.515382549951358
2022-06-12 03:03:55,031   global_step = 30999
2022-06-12 03:03:55,031   loss = 4.381128708212292
2022-06-12 03:03:55,031   rep_loss = 0.8657461540089816
2022-06-12 03:03:55,031 ***** Save model *****
2022-06-12 03:03:58,245 ***** Running evaluation *****
2022-06-12 03:03:58,246   Epoch = 29 iter 7799 step
2022-06-12 03:03:58,246   Num examples = 1043
2022-06-12 03:03:58,246   Batch size = 32
2022-06-12 03:03:59,148 ***** Eval results *****
2022-06-12 03:03:59,148   cls_loss = 0.08252160849847964
2022-06-12 03:03:59,148   eval_loss = 0.934332883719242
2022-06-12 03:03:59,148   global_step = 7799
2022-06-12 03:03:59,148   loss = 0.08252160849847964
2022-06-12 03:03:59,148   mcc = 0.24814426445809473
2022-06-12 03:04:04,016 ***** Running evaluation *****
2022-06-12 03:04:04,016   Epoch = 29 iter 7819 step
2022-06-12 03:04:04,016   Num examples = 1043
2022-06-12 03:04:04,016   Batch size = 32
2022-06-12 03:04:04,916 ***** Eval results *****
2022-06-12 03:04:04,916   cls_loss = 0.08138940434314702
2022-06-12 03:04:04,916   eval_loss = 0.9371345539887747
2022-06-12 03:04:04,916   global_step = 7819
2022-06-12 03:04:04,916   loss = 0.08138940434314702
2022-06-12 03:04:04,916   mcc = 0.250331824855595
2022-06-12 03:04:09,799 ***** Running evaluation *****
2022-06-12 03:04:09,800   Epoch = 29 iter 7839 step
2022-06-12 03:04:09,800   Num examples = 1043
2022-06-12 03:04:09,800   Batch size = 32
2022-06-12 03:04:10,701 ***** Eval results *****
2022-06-12 03:04:10,702   cls_loss = 0.0826412916649133
2022-06-12 03:04:10,702   eval_loss = 0.9352982640266418
2022-06-12 03:04:10,702   global_step = 7839
2022-06-12 03:04:10,702   loss = 0.0826412916649133
2022-06-12 03:04:10,702   mcc = 0.250331824855595
2022-06-12 03:04:15,583 ***** Running evaluation *****
2022-06-12 03:04:15,583   Epoch = 29 iter 7859 step
2022-06-12 03:04:15,583   Num examples = 1043
2022-06-12 03:04:15,583   Batch size = 32
2022-06-12 03:04:16,484 ***** Eval results *****
2022-06-12 03:04:16,484   cls_loss = 0.08315211971258295
2022-06-12 03:04:16,485   eval_loss = 0.9324859696807284
2022-06-12 03:04:16,485   global_step = 7859
2022-06-12 03:04:16,485   loss = 0.08315211971258295
2022-06-12 03:04:16,485   mcc = 0.24814426445809473
2022-06-12 03:04:21,377 ***** Running evaluation *****
2022-06-12 03:04:21,377   Epoch = 29 iter 7879 step
2022-06-12 03:04:21,377   Num examples = 1043
2022-06-12 03:04:21,377   Batch size = 32
2022-06-12 03:04:22,278 ***** Eval results *****
2022-06-12 03:04:22,278   cls_loss = 0.08251094593502142
2022-06-12 03:04:22,278   eval_loss = 0.9352661280921011
2022-06-12 03:04:22,278   global_step = 7879
2022-06-12 03:04:22,278   loss = 0.08251094593502142
2022-06-12 03:04:22,278   mcc = 0.24814426445809473
2022-06-12 03:04:27,148 ***** Running evaluation *****
2022-06-12 03:04:27,148   Epoch = 29 iter 7899 step
2022-06-12 03:04:27,148   Num examples = 1043
2022-06-12 03:04:27,148   Batch size = 32
2022-06-12 03:04:28,049 ***** Eval results *****
2022-06-12 03:04:28,049   cls_loss = 0.08256293842807794
2022-06-12 03:04:28,049   eval_loss = 0.9344189808224187
2022-06-12 03:04:28,049   global_step = 7899
2022-06-12 03:04:28,049   loss = 0.08256293842807794
2022-06-12 03:04:28,049   mcc = 0.2525312097299585
2022-06-12 03:04:32,930 ***** Running evaluation *****
2022-06-12 03:04:32,930   Epoch = 29 iter 7919 step
2022-06-12 03:04:32,930   Num examples = 1043
2022-06-12 03:04:32,930   Batch size = 32
2022-06-12 03:04:33,832 ***** Eval results *****
2022-06-12 03:04:33,832   cls_loss = 0.08233519575812599
2022-06-12 03:04:33,832   eval_loss = 0.9325165017084642
2022-06-12 03:04:33,832   global_step = 7919
2022-06-12 03:04:33,832   loss = 0.08233519575812599
2022-06-12 03:04:33,832   mcc = 0.24814426445809473
2022-06-12 03:04:38,707 ***** Running evaluation *****
2022-06-12 03:04:38,707   Epoch = 29 iter 7939 step
2022-06-12 03:04:38,707   Num examples = 1043
2022-06-12 03:04:38,707   Batch size = 32
2022-06-12 03:04:39,608 ***** Eval results *****
2022-06-12 03:04:39,608   cls_loss = 0.08178397785035932
2022-06-12 03:04:39,608   eval_loss = 0.9339529004963961
2022-06-12 03:04:39,608   global_step = 7939
2022-06-12 03:04:39,608   loss = 0.08178397785035932
2022-06-12 03:04:39,608   mcc = 0.2525312097299585
2022-06-12 03:04:44,485 ***** Running evaluation *****
2022-06-12 03:04:44,485   Epoch = 29 iter 7959 step
2022-06-12 03:04:44,485   Num examples = 1043
2022-06-12 03:04:44,485   Batch size = 32
2022-06-12 03:04:45,385 ***** Eval results *****
2022-06-12 03:04:45,385   cls_loss = 0.08201601828827902
2022-06-12 03:04:45,385   eval_loss = 0.9345028066273892
2022-06-12 03:04:45,385   global_step = 7959
2022-06-12 03:04:45,385   loss = 0.08201601828827902
2022-06-12 03:04:45,385   mcc = 0.2525312097299585
2022-06-12 03:04:50,253 ***** Running evaluation *****
2022-06-12 03:04:50,254   Epoch = 29 iter 7979 step
2022-06-12 03:04:50,254   Num examples = 1043
2022-06-12 03:04:50,254   Batch size = 32
2022-06-12 03:04:51,154 ***** Eval results *****
2022-06-12 03:04:51,155   cls_loss = 0.08209950277992224
2022-06-12 03:04:51,155   eval_loss = 0.9333660087802194
2022-06-12 03:04:51,155   global_step = 7979
2022-06-12 03:04:51,155   loss = 0.08209950277992224
2022-06-12 03:04:51,155   mcc = 0.2525312097299585
2022-06-12 03:04:56,027 ***** Running evaluation *****
2022-06-12 03:04:56,028   Epoch = 29 iter 7999 step
2022-06-12 03:04:56,028   Num examples = 1043
2022-06-12 03:04:56,028   Batch size = 32
2022-06-12 03:04:56,928 ***** Eval results *****
2022-06-12 03:04:56,928   cls_loss = 0.08213432919001207
2022-06-12 03:04:56,928   eval_loss = 0.9334710746100454
2022-06-12 03:04:56,928   global_step = 7999
2022-06-12 03:04:56,928   loss = 0.08213432919001207
2022-06-12 03:04:56,928   mcc = 0.2525312097299585
2022-06-12 03:04:59,613 **************S*************
task_name = cola
best_metirc = 0.2825114601341239
**************E*************

2022-06-12 03:04:59,643 Task finish! 
2022-06-12 03:04:59,644 Task cost 38.9061047 minutes, i.e. 0.6484350847222222 hours. 
2022-06-12 03:05:01,802 Task start! 
2022-06-12 03:05:01,825 device: cuda n_gpu: 1
2022-06-12 03:05:01,825 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/STS-B', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=20, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/sts-b/on_original_data', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='sts-b', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/sts-b/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/sts-b/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 03:05:01,873 Writing example 0 of 5749
2022-06-12 03:05:01,873 *** Example ***
2022-06-12 03:05:01,873 guid: train-0
2022-06-12 03:05:01,873 tokens: [CLS] a plane is taking off . [SEP] an air plane is taking off . [SEP]
2022-06-12 03:05:01,874 input_ids: 101 1037 4946 2003 2635 2125 1012 102 2019 2250 4946 2003 2635 2125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:05:01,874 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:05:01,874 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:05:01,874 label: 5.000
2022-06-12 03:05:01,874 label_id: 5.0
2022-06-12 03:05:04,339 Writing example 0 of 1500
2022-06-12 03:05:04,339 *** Example ***
2022-06-12 03:05:04,339 guid: dev-0
2022-06-12 03:05:04,339 tokens: [CLS] a man with a hard hat is dancing . [SEP] a man wearing a hard hat is dancing . [SEP]
2022-06-12 03:05:04,339 input_ids: 101 1037 2158 2007 1037 2524 6045 2003 5613 1012 102 1037 2158 4147 1037 2524 6045 2003 5613 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:05:04,340 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:05:04,340 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:05:04,340 label: 5.000
2022-06-12 03:05:04,340 label_id: 5.0
2022-06-12 03:05:05,015 Model config {
  "_num_labels": 1,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "sts-b",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 03:05:10,342 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/sts-b/on_original_data/pytorch_model.bin
2022-06-12 03:05:12,004 loading model...
2022-06-12 03:05:12,329 done!
2022-06-12 03:05:15,932 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 03:05:17,031 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-12 03:05:17,225 loading model...
2022-06-12 03:05:17,252 done!
2022-06-12 03:05:17,252 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-12 03:05:17,252 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-12 03:05:18,709 ***** Running training *****
2022-06-12 03:05:18,724   Num examples = 5749
2022-06-12 03:05:18,740   Batch size = 32
2022-06-12 03:05:18,756   Num steps = 3580
2022-06-12 03:05:18,772 n: bert.embeddings.word_embeddings.weight
2022-06-12 03:05:18,782 n: bert.embeddings.position_embeddings.weight
2022-06-12 03:05:18,783 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 03:05:18,783 n: bert.embeddings.LayerNorm.weight
2022-06-12 03:05:18,783 n: bert.embeddings.LayerNorm.bias
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 03:05:18,783 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 03:05:18,784 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 03:05:18,785 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 03:05:18,785 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 03:05:18,786 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 03:05:18,786 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 03:05:18,787 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 03:05:18,788 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 03:05:18,788 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 03:05:18,788 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 03:05:18,788 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 03:05:18,788 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 03:05:18,788 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 03:05:18,788 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 03:05:18,788 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 03:05:18,788 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 03:05:18,788 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 03:05:18,788 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 03:05:18,789 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 03:05:18,789 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 03:05:18,789 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 03:05:18,789 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 03:05:18,789 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 03:05:18,789 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 03:05:18,789 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 03:05:18,789 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 03:05:18,789 n: bert.pooler.dense.weight
2022-06-12 03:05:18,789 n: bert.pooler.dense.bias
2022-06-12 03:05:18,789 n: classifier.weight
2022-06-12 03:05:18,790 n: classifier.bias
2022-06-12 03:05:18,790 n: fit_denses.0.weight
2022-06-12 03:05:18,790 n: fit_denses.0.bias
2022-06-12 03:05:18,790 n: fit_denses.1.weight
2022-06-12 03:05:18,790 n: fit_denses.1.bias
2022-06-12 03:05:18,790 n: fit_denses.2.weight
2022-06-12 03:05:18,790 n: fit_denses.2.bias
2022-06-12 03:05:18,790 n: fit_denses.3.weight
2022-06-12 03:05:18,790 n: fit_denses.3.bias
2022-06-12 03:05:18,790 n: fit_denses.4.weight
2022-06-12 03:05:18,790 n: fit_denses.4.bias
2022-06-12 03:05:18,790 n: fit_denses.5.weight
2022-06-12 03:05:18,790 n: fit_denses.5.bias
2022-06-12 03:05:18,791 n: fit_denses.6.weight
2022-06-12 03:05:18,791 n: fit_denses.6.bias
2022-06-12 03:05:18,791 Total parameters: 72467969
2022-06-12 03:05:23,817 ***** Running evaluation *****
2022-06-12 03:05:23,817   Epoch = 0 iter 19 step
2022-06-12 03:05:23,817   Num examples = 1500
2022-06-12 03:05:23,818   Batch size = 32
2022-06-12 03:05:23,819 ***** Eval results *****
2022-06-12 03:05:23,819   att_loss = 6.206931114196777
2022-06-12 03:05:23,819   global_step = 19
2022-06-12 03:05:23,819   loss = 8.417478486111289
2022-06-12 03:05:23,819   rep_loss = 2.2105473279953003
2022-06-12 03:05:23,819 ***** Save model *****
2022-06-12 03:05:29,498 ***** Running evaluation *****
2022-06-12 03:05:29,499   Epoch = 0 iter 39 step
2022-06-12 03:05:29,499   Num examples = 1500
2022-06-12 03:05:29,499   Batch size = 32
2022-06-12 03:05:29,500 ***** Eval results *****
2022-06-12 03:05:29,500   att_loss = 5.243015912862925
2022-06-12 03:05:29,500   global_step = 39
2022-06-12 03:05:29,500   loss = 7.1176276206970215
2022-06-12 03:05:29,500   rep_loss = 1.874611692550855
2022-06-12 03:05:29,500 ***** Save model *****
2022-06-12 03:05:35,236 ***** Running evaluation *****
2022-06-12 03:05:35,236   Epoch = 0 iter 59 step
2022-06-12 03:05:35,236   Num examples = 1500
2022-06-12 03:05:35,236   Batch size = 32
2022-06-12 03:05:35,237 ***** Eval results *****
2022-06-12 03:05:35,237   att_loss = 4.919371592796455
2022-06-12 03:05:35,237   global_step = 59
2022-06-12 03:05:35,237   loss = 6.629542423506915
2022-06-12 03:05:35,237   rep_loss = 1.7101707822185452
2022-06-12 03:05:35,237 ***** Save model *****
2022-06-12 03:05:40,985 ***** Running evaluation *****
2022-06-12 03:05:40,986   Epoch = 0 iter 79 step
2022-06-12 03:05:40,986   Num examples = 1500
2022-06-12 03:05:40,986   Batch size = 32
2022-06-12 03:05:40,987 ***** Eval results *****
2022-06-12 03:05:40,987   att_loss = 4.646822165839279
2022-06-12 03:05:40,987   global_step = 79
2022-06-12 03:05:40,987   loss = 6.257344668424582
2022-06-12 03:05:40,988   rep_loss = 1.610522457315952
2022-06-12 03:05:40,988 ***** Save model *****
2022-06-12 03:05:46,708 ***** Running evaluation *****
2022-06-12 03:05:46,708   Epoch = 0 iter 99 step
2022-06-12 03:05:46,708   Num examples = 1500
2022-06-12 03:05:46,708   Batch size = 32
2022-06-12 03:05:46,709 ***** Eval results *****
2022-06-12 03:05:46,709   att_loss = 4.430548398181646
2022-06-12 03:05:46,710   global_step = 99
2022-06-12 03:05:46,710   loss = 5.964073484594172
2022-06-12 03:05:46,710   rep_loss = 1.533525050288499
2022-06-12 03:05:46,710 ***** Save model *****
2022-06-12 03:05:52,443 ***** Running evaluation *****
2022-06-12 03:05:52,443   Epoch = 0 iter 119 step
2022-06-12 03:05:52,443   Num examples = 1500
2022-06-12 03:05:52,443   Batch size = 32
2022-06-12 03:05:52,445 ***** Eval results *****
2022-06-12 03:05:52,445   att_loss = 4.277341329750895
2022-06-12 03:05:52,445   global_step = 119
2022-06-12 03:05:52,445   loss = 5.755380219772082
2022-06-12 03:05:52,445   rep_loss = 1.4780388609701847
2022-06-12 03:05:52,445 ***** Save model *****
2022-06-12 03:05:58,134 ***** Running evaluation *****
2022-06-12 03:05:58,135   Epoch = 0 iter 139 step
2022-06-12 03:05:58,135   Num examples = 1500
2022-06-12 03:05:58,135   Batch size = 32
2022-06-12 03:05:58,136 ***** Eval results *****
2022-06-12 03:05:58,137   att_loss = 4.16436490052038
2022-06-12 03:05:58,137   global_step = 139
2022-06-12 03:05:58,137   loss = 5.599403794720876
2022-06-12 03:05:58,137   rep_loss = 1.435038864183769
2022-06-12 03:05:58,137 ***** Save model *****
2022-06-12 03:06:02,703 ***** Running evaluation *****
2022-06-12 03:06:02,703   Epoch = 9 iter 31499 step
2022-06-12 03:06:02,703   Num examples = 5463
2022-06-12 03:06:02,703   Batch size = 32
2022-06-12 03:06:02,704 ***** Eval results *****
2022-06-12 03:06:02,704   att_loss = 3.512226392193009
2022-06-12 03:06:02,704   global_step = 31499
2022-06-12 03:06:02,705   loss = 4.3774142138054275
2022-06-12 03:06:02,705   rep_loss = 0.865187822050259
2022-06-12 03:06:02,705 ***** Save model *****
2022-06-12 03:06:03,878 ***** Running evaluation *****
2022-06-12 03:06:03,879   Epoch = 0 iter 159 step
2022-06-12 03:06:03,879   Num examples = 1500
2022-06-12 03:06:03,879   Batch size = 32
2022-06-12 03:06:03,880 ***** Eval results *****
2022-06-12 03:06:03,880   att_loss = 4.1001437909948
2022-06-12 03:06:03,880   global_step = 159
2022-06-12 03:06:03,880   loss = 5.502132378284286
2022-06-12 03:06:03,880   rep_loss = 1.4019885595489598
2022-06-12 03:06:03,880 ***** Save model *****
2022-06-12 03:06:09,558 ***** Running evaluation *****
2022-06-12 03:06:09,558   Epoch = 0 iter 179 step
2022-06-12 03:06:09,558   Num examples = 1500
2022-06-12 03:06:09,558   Batch size = 32
2022-06-12 03:06:09,560 ***** Eval results *****
2022-06-12 03:06:09,560   att_loss = 4.036341378142714
2022-06-12 03:06:09,560   global_step = 179
2022-06-12 03:06:09,560   loss = 5.411383751384373
2022-06-12 03:06:09,560   rep_loss = 1.3750423486006327
2022-06-12 03:06:09,560 ***** Save model *****
2022-06-12 03:06:15,253 ***** Running evaluation *****
2022-06-12 03:06:15,253   Epoch = 1 iter 199 step
2022-06-12 03:06:15,253   Num examples = 1500
2022-06-12 03:06:15,253   Batch size = 32
2022-06-12 03:06:15,254 ***** Eval results *****
2022-06-12 03:06:15,254   att_loss = 3.3029038310050964
2022-06-12 03:06:15,254   global_step = 199
2022-06-12 03:06:15,255   loss = 4.441621470451355
2022-06-12 03:06:15,255   rep_loss = 1.1387176632881164
2022-06-12 03:06:15,255 ***** Save model *****
2022-06-12 03:06:20,980 ***** Running evaluation *****
2022-06-12 03:06:20,981   Epoch = 1 iter 219 step
2022-06-12 03:06:20,981   Num examples = 1500
2022-06-12 03:06:20,981   Batch size = 32
2022-06-12 03:06:20,982 ***** Eval results *****
2022-06-12 03:06:20,982   att_loss = 3.3297214210033417
2022-06-12 03:06:20,982   global_step = 219
2022-06-12 03:06:20,982   loss = 4.460652905702591
2022-06-12 03:06:20,982   rep_loss = 1.1309314757585525
2022-06-12 03:06:20,982 ***** Save model *****
2022-06-12 03:06:26,654 ***** Running evaluation *****
2022-06-12 03:06:26,654   Epoch = 1 iter 239 step
2022-06-12 03:06:26,654   Num examples = 1500
2022-06-12 03:06:26,654   Batch size = 32
2022-06-12 03:06:26,655 ***** Eval results *****
2022-06-12 03:06:26,655   att_loss = 3.2833908637364706
2022-06-12 03:06:26,655   global_step = 239
2022-06-12 03:06:26,656   loss = 4.407635565598806
2022-06-12 03:06:26,656   rep_loss = 1.1242446939150492
2022-06-12 03:06:26,656 ***** Save model *****
2022-06-12 03:06:32,373 ***** Running evaluation *****
2022-06-12 03:06:32,373   Epoch = 1 iter 259 step
2022-06-12 03:06:32,373   Num examples = 1500
2022-06-12 03:06:32,373   Batch size = 32
2022-06-12 03:06:32,374 ***** Eval results *****
2022-06-12 03:06:32,375   att_loss = 3.2343846648931502
2022-06-12 03:06:32,375   global_step = 259
2022-06-12 03:06:32,375   loss = 4.351070728898049
2022-06-12 03:06:32,375   rep_loss = 1.1166860610246658
2022-06-12 03:06:32,375 ***** Save model *****
2022-06-12 03:06:38,076 ***** Running evaluation *****
2022-06-12 03:06:38,076   Epoch = 1 iter 279 step
2022-06-12 03:06:38,076   Num examples = 1500
2022-06-12 03:06:38,077   Batch size = 32
2022-06-12 03:06:38,078 ***** Eval results *****
2022-06-12 03:06:38,078   att_loss = 3.2171264481544495
2022-06-12 03:06:38,078   global_step = 279
2022-06-12 03:06:38,078   loss = 4.328905637264252
2022-06-12 03:06:38,078   rep_loss = 1.111779191493988
2022-06-12 03:06:38,078 ***** Save model *****
2022-06-12 03:06:43,785 ***** Running evaluation *****
2022-06-12 03:06:43,786   Epoch = 1 iter 299 step
2022-06-12 03:06:43,786   Num examples = 1500
2022-06-12 03:06:43,786   Batch size = 32
2022-06-12 03:06:43,787 ***** Eval results *****
2022-06-12 03:06:43,787   att_loss = 3.192065872748693
2022-06-12 03:06:43,787   global_step = 299
2022-06-12 03:06:43,787   loss = 4.296500360965728
2022-06-12 03:06:43,787   rep_loss = 1.1044344872236251
2022-06-12 03:06:43,787 ***** Save model *****
2022-06-12 03:06:49,491 ***** Running evaluation *****
2022-06-12 03:06:49,492   Epoch = 1 iter 319 step
2022-06-12 03:06:49,492   Num examples = 1500
2022-06-12 03:06:49,492   Batch size = 32
2022-06-12 03:06:49,493 ***** Eval results *****
2022-06-12 03:06:49,493   att_loss = 3.1784618854522706
2022-06-12 03:06:49,494   global_step = 319
2022-06-12 03:06:49,494   loss = 4.278693185533796
2022-06-12 03:06:49,494   rep_loss = 1.100231295824051
2022-06-12 03:06:49,494 ***** Save model *****
2022-06-12 03:06:55,194 ***** Running evaluation *****
2022-06-12 03:06:55,194   Epoch = 1 iter 339 step
2022-06-12 03:06:55,194   Num examples = 1500
2022-06-12 03:06:55,194   Batch size = 32
2022-06-12 03:06:55,196 ***** Eval results *****
2022-06-12 03:06:55,196   att_loss = 3.1584287136793137
2022-06-12 03:06:55,196   global_step = 339
2022-06-12 03:06:55,196   loss = 4.254343916475773
2022-06-12 03:06:55,196   rep_loss = 1.0959151990711689
2022-06-12 03:06:55,196 ***** Save model *****
2022-06-12 03:07:01,109 ***** Running evaluation *****
2022-06-12 03:07:01,109   Epoch = 2 iter 359 step
2022-06-12 03:07:01,109   Num examples = 1500
2022-06-12 03:07:01,109   Batch size = 32
2022-06-12 03:07:01,110 ***** Eval results *****
2022-06-12 03:07:01,110   att_loss = 2.4182183742523193
2022-06-12 03:07:01,110   global_step = 359
2022-06-12 03:07:01,110   loss = 3.406903028488159
2022-06-12 03:07:01,110   rep_loss = 0.9886846542358398
2022-06-12 03:07:01,110 ***** Save model *****
2022-06-12 03:07:06,741 ***** Running evaluation *****
2022-06-12 03:07:06,742   Epoch = 2 iter 379 step
2022-06-12 03:07:06,742   Num examples = 1500
2022-06-12 03:07:06,742   Batch size = 32
2022-06-12 03:07:06,743 ***** Eval results *****
2022-06-12 03:07:06,743   att_loss = 3.028895162400745
2022-06-12 03:07:06,743   global_step = 379
2022-06-12 03:07:06,743   loss = 4.0939698332831975
2022-06-12 03:07:06,743   rep_loss = 1.0650746339843387
2022-06-12 03:07:06,743 ***** Save model *****
2022-06-12 03:07:12,354 ***** Running evaluation *****
2022-06-12 03:07:12,355   Epoch = 2 iter 399 step
2022-06-12 03:07:12,355   Num examples = 1500
2022-06-12 03:07:12,355   Batch size = 32
2022-06-12 03:07:12,356 ***** Eval results *****
2022-06-12 03:07:12,356   att_loss = 2.944120081459604
2022-06-12 03:07:12,356   global_step = 399
2022-06-12 03:07:12,356   loss = 3.9968595737364234
2022-06-12 03:07:12,356   rep_loss = 1.0527394661089269
2022-06-12 03:07:12,356 ***** Save model *****
2022-06-12 03:07:17,982 ***** Running evaluation *****
2022-06-12 03:07:17,983   Epoch = 2 iter 419 step
2022-06-12 03:07:17,983   Num examples = 1500
2022-06-12 03:07:17,983   Batch size = 32
2022-06-12 03:07:17,984 ***** Eval results *****
2022-06-12 03:07:17,984   att_loss = 2.9231200921730918
2022-06-12 03:07:17,984   global_step = 419
2022-06-12 03:07:17,984   loss = 3.970850764727983
2022-06-12 03:07:17,984   rep_loss = 1.0477306491038838
2022-06-12 03:07:17,984 ***** Save model *****
2022-06-12 03:07:23,595 ***** Running evaluation *****
2022-06-12 03:07:23,596   Epoch = 2 iter 439 step
2022-06-12 03:07:23,596   Num examples = 1500
2022-06-12 03:07:23,596   Batch size = 32
2022-06-12 03:07:23,597 ***** Eval results *****
2022-06-12 03:07:23,597   att_loss = 2.933621695012222
2022-06-12 03:07:23,597   global_step = 439
2022-06-12 03:07:23,597   loss = 3.9793129438235435
2022-06-12 03:07:23,597   rep_loss = 1.0456912282072468
2022-06-12 03:07:23,597 ***** Save model *****
2022-06-12 03:07:29,174 ***** Running evaluation *****
2022-06-12 03:07:29,174   Epoch = 2 iter 459 step
2022-06-12 03:07:29,174   Num examples = 1500
2022-06-12 03:07:29,175   Batch size = 32
2022-06-12 03:07:29,175 ***** Eval results *****
2022-06-12 03:07:29,176   att_loss = 2.9217984369485683
2022-06-12 03:07:29,176   global_step = 459
2022-06-12 03:07:29,176   loss = 3.9630295663776964
2022-06-12 03:07:29,176   rep_loss = 1.041231109364198
2022-06-12 03:07:29,176 ***** Save model *****
2022-06-12 03:07:34,822 ***** Running evaluation *****
2022-06-12 03:07:34,823   Epoch = 2 iter 479 step
2022-06-12 03:07:34,823   Num examples = 1500
2022-06-12 03:07:34,823   Batch size = 32
2022-06-12 03:07:34,824 ***** Eval results *****
2022-06-12 03:07:34,824   att_loss = 2.8906958359332124
2022-06-12 03:07:34,824   global_step = 479
2022-06-12 03:07:34,824   loss = 3.9267088362008087
2022-06-12 03:07:34,824   rep_loss = 1.0360129790857804
2022-06-12 03:07:34,824 ***** Save model *****
2022-06-12 03:07:40,417 ***** Running evaluation *****
2022-06-12 03:07:40,418   Epoch = 2 iter 499 step
2022-06-12 03:07:40,418   Num examples = 1500
2022-06-12 03:07:40,418   Batch size = 32
2022-06-12 03:07:40,419 ***** Eval results *****
2022-06-12 03:07:40,419   att_loss = 2.8994442790958055
2022-06-12 03:07:40,419   global_step = 499
2022-06-12 03:07:40,419   loss = 3.934114324285629
2022-06-12 03:07:40,419   rep_loss = 1.034670023630697
2022-06-12 03:07:40,419 ***** Save model *****
2022-06-12 03:07:46,038 ***** Running evaluation *****
2022-06-12 03:07:46,039   Epoch = 2 iter 519 step
2022-06-12 03:07:46,039   Num examples = 1500
2022-06-12 03:07:46,039   Batch size = 32
2022-06-12 03:07:46,040 ***** Eval results *****
2022-06-12 03:07:46,040   att_loss = 2.8989751649939497
2022-06-12 03:07:46,041   global_step = 519
2022-06-12 03:07:46,041   loss = 3.9330131681809513
2022-06-12 03:07:46,041   rep_loss = 1.0340379857868882
2022-06-12 03:07:46,041 ***** Save model *****
2022-06-12 03:07:51,678 ***** Running evaluation *****
2022-06-12 03:07:51,678   Epoch = 3 iter 539 step
2022-06-12 03:07:51,678   Num examples = 1500
2022-06-12 03:07:51,678   Batch size = 32
2022-06-12 03:07:51,679 ***** Eval results *****
2022-06-12 03:07:51,679   att_loss = 2.329825282096863
2022-06-12 03:07:51,679   global_step = 539
2022-06-12 03:07:51,679   loss = 3.297587275505066
2022-06-12 03:07:51,680   rep_loss = 0.9677619338035583
2022-06-12 03:07:51,680 ***** Save model *****
2022-06-12 03:07:57,292 ***** Running evaluation *****
2022-06-12 03:07:57,292   Epoch = 3 iter 559 step
2022-06-12 03:07:57,292   Num examples = 1500
2022-06-12 03:07:57,292   Batch size = 32
2022-06-12 03:07:57,293 ***** Eval results *****
2022-06-12 03:07:57,293   att_loss = 2.648011175068942
2022-06-12 03:07:57,294   global_step = 559
2022-06-12 03:07:57,294   loss = 3.643616708842191
2022-06-12 03:07:57,294   rep_loss = 0.9956055120988325
2022-06-12 03:07:57,294 ***** Save model *****
2022-06-12 03:08:02,903 ***** Running evaluation *****
2022-06-12 03:08:02,904   Epoch = 3 iter 579 step
2022-06-12 03:08:02,904   Num examples = 1500
2022-06-12 03:08:02,904   Batch size = 32
2022-06-12 03:08:02,906 ***** Eval results *****
2022-06-12 03:08:02,906   att_loss = 2.7711135830198015
2022-06-12 03:08:02,906   global_step = 579
2022-06-12 03:08:02,906   loss = 3.7753759509041194
2022-06-12 03:08:02,906   rep_loss = 1.0042623281478882
2022-06-12 03:08:02,907 ***** Save model *****
2022-06-12 03:08:08,567 ***** Running evaluation *****
2022-06-12 03:08:08,567   Epoch = 3 iter 599 step
2022-06-12 03:08:08,567   Num examples = 1500
2022-06-12 03:08:08,567   Batch size = 32
2022-06-12 03:08:08,568 ***** Eval results *****
2022-06-12 03:08:08,568   att_loss = 2.7566062365808794
2022-06-12 03:08:08,569   global_step = 599
2022-06-12 03:08:08,569   loss = 3.7603612061469787
2022-06-12 03:08:08,569   rep_loss = 1.0037549339955854
2022-06-12 03:08:08,569 ***** Save model *****
2022-06-12 03:08:10,322 ***** Running evaluation *****
2022-06-12 03:08:10,323   Epoch = 9 iter 31999 step
2022-06-12 03:08:10,323   Num examples = 5463
2022-06-12 03:08:10,323   Batch size = 32
2022-06-12 03:08:10,324 ***** Eval results *****
2022-06-12 03:08:10,324   att_loss = 3.516895784813996
2022-06-12 03:08:10,324   global_step = 31999
2022-06-12 03:08:10,324   loss = 4.382089485998327
2022-06-12 03:08:10,324   rep_loss = 0.8651937007153719
2022-06-12 03:08:10,324 ***** Save model *****
2022-06-12 03:08:14,222 ***** Running evaluation *****
2022-06-12 03:08:14,222   Epoch = 3 iter 619 step
2022-06-12 03:08:14,222   Num examples = 1500
2022-06-12 03:08:14,222   Batch size = 32
2022-06-12 03:08:14,223 ***** Eval results *****
2022-06-12 03:08:14,223   att_loss = 2.7727634790467053
2022-06-12 03:08:14,223   global_step = 619
2022-06-12 03:08:14,223   loss = 3.7761697885466785
2022-06-12 03:08:14,223   rep_loss = 1.00340628696651
2022-06-12 03:08:14,224 ***** Save model *****
2022-06-12 03:08:19,848 ***** Running evaluation *****
2022-06-12 03:08:19,849   Epoch = 3 iter 639 step
2022-06-12 03:08:19,849   Num examples = 1500
2022-06-12 03:08:19,849   Batch size = 32
2022-06-12 03:08:19,850 ***** Eval results *****
2022-06-12 03:08:19,850   att_loss = 2.784286061922709
2022-06-12 03:08:19,850   global_step = 639
2022-06-12 03:08:19,850   loss = 3.788701281828039
2022-06-12 03:08:19,850   rep_loss = 1.0044152058807074
2022-06-12 03:08:19,850 ***** Save model *****
2022-06-12 03:08:25,438 ***** Running evaluation *****
2022-06-12 03:08:25,438   Epoch = 3 iter 659 step
2022-06-12 03:08:25,438   Num examples = 1500
2022-06-12 03:08:25,438   Batch size = 32
2022-06-12 03:08:25,439 ***** Eval results *****
2022-06-12 03:08:25,439   att_loss = 2.80209376186621
2022-06-12 03:08:25,440   global_step = 659
2022-06-12 03:08:25,440   loss = 3.8056321046391473
2022-06-12 03:08:25,440   rep_loss = 1.0035383354444973
2022-06-12 03:08:25,440 ***** Save model *****
2022-06-12 03:08:31,093 ***** Running evaluation *****
2022-06-12 03:08:31,093   Epoch = 3 iter 679 step
2022-06-12 03:08:31,093   Num examples = 1500
2022-06-12 03:08:31,093   Batch size = 32
2022-06-12 03:08:31,094 ***** Eval results *****
2022-06-12 03:08:31,095   att_loss = 2.769542929152368
2022-06-12 03:08:31,095   global_step = 679
2022-06-12 03:08:31,095   loss = 3.7685559853701522
2022-06-12 03:08:31,095   rep_loss = 0.9990130545387805
2022-06-12 03:08:31,095 ***** Save model *****
2022-06-12 03:08:36,718 ***** Running evaluation *****
2022-06-12 03:08:36,718   Epoch = 3 iter 699 step
2022-06-12 03:08:36,719   Num examples = 1500
2022-06-12 03:08:36,719   Batch size = 32
2022-06-12 03:08:36,720 ***** Eval results *****
2022-06-12 03:08:36,720   att_loss = 2.755408788168872
2022-06-12 03:08:36,720   global_step = 699
2022-06-12 03:08:36,720   loss = 3.7518905136320324
2022-06-12 03:08:36,720   rep_loss = 0.9964817228876515
2022-06-12 03:08:36,720 ***** Save model *****
2022-06-12 03:08:42,355 ***** Running evaluation *****
2022-06-12 03:08:42,355   Epoch = 4 iter 719 step
2022-06-12 03:08:42,355   Num examples = 1500
2022-06-12 03:08:42,355   Batch size = 32
2022-06-12 03:08:42,356 ***** Eval results *****
2022-06-12 03:08:42,356   att_loss = 2.736785888671875
2022-06-12 03:08:42,357   global_step = 719
2022-06-12 03:08:42,357   loss = 3.720285971959432
2022-06-12 03:08:42,357   rep_loss = 0.983500063419342
2022-06-12 03:08:42,357 ***** Save model *****
2022-06-12 03:08:47,943 ***** Running evaluation *****
2022-06-12 03:08:47,943   Epoch = 4 iter 739 step
2022-06-12 03:08:47,943   Num examples = 1500
2022-06-12 03:08:47,943   Batch size = 32
2022-06-12 03:08:47,944 ***** Eval results *****
2022-06-12 03:08:47,944   att_loss = 2.549845902816109
2022-06-12 03:08:47,944   global_step = 739
2022-06-12 03:08:47,944   loss = 3.523441439089568
2022-06-12 03:08:47,944   rep_loss = 0.9735955181329147
2022-06-12 03:08:47,944 ***** Save model *****
2022-06-12 03:08:53,553 ***** Running evaluation *****
2022-06-12 03:08:53,553   Epoch = 4 iter 759 step
2022-06-12 03:08:53,553   Num examples = 1500
2022-06-12 03:08:53,553   Batch size = 32
2022-06-12 03:08:53,554 ***** Eval results *****
2022-06-12 03:08:53,554   att_loss = 2.562500454658686
2022-06-12 03:08:53,554   global_step = 759
2022-06-12 03:08:53,554   loss = 3.5314439618310263
2022-06-12 03:08:53,554   rep_loss = 0.9689434933107953
2022-06-12 03:08:53,554 ***** Save model *****
2022-06-12 03:08:59,154 ***** Running evaluation *****
2022-06-12 03:08:59,154   Epoch = 4 iter 779 step
2022-06-12 03:08:59,154   Num examples = 1500
2022-06-12 03:08:59,154   Batch size = 32
2022-06-12 03:08:59,155 ***** Eval results *****
2022-06-12 03:08:59,155   att_loss = 2.533760157842485
2022-06-12 03:08:59,155   global_step = 779
2022-06-12 03:08:59,155   loss = 3.5006047392648365
2022-06-12 03:08:59,156   rep_loss = 0.9668445738535079
2022-06-12 03:08:59,156 ***** Save model *****
2022-06-12 03:09:04,836 ***** Running evaluation *****
2022-06-12 03:09:04,837   Epoch = 4 iter 799 step
2022-06-12 03:09:04,837   Num examples = 1500
2022-06-12 03:09:04,837   Batch size = 32
2022-06-12 03:09:04,838 ***** Eval results *****
2022-06-12 03:09:04,838   att_loss = 2.56485160000353
2022-06-12 03:09:04,838   global_step = 799
2022-06-12 03:09:04,839   loss = 3.5334174403225083
2022-06-12 03:09:04,839   rep_loss = 0.9685658345739525
2022-06-12 03:09:04,839 ***** Save model *****
2022-06-12 03:09:10,424 ***** Running evaluation *****
2022-06-12 03:09:10,424   Epoch = 4 iter 819 step
2022-06-12 03:09:10,425   Num examples = 1500
2022-06-12 03:09:10,425   Batch size = 32
2022-06-12 03:09:10,426 ***** Eval results *****
2022-06-12 03:09:10,426   att_loss = 2.600703730166537
2022-06-12 03:09:10,426   global_step = 819
2022-06-12 03:09:10,426   loss = 3.5700377306891875
2022-06-12 03:09:10,427   rep_loss = 0.9693339918423625
2022-06-12 03:09:10,427 ***** Save model *****
2022-06-12 03:09:16,055 ***** Running evaluation *****
2022-06-12 03:09:16,055   Epoch = 4 iter 839 step
2022-06-12 03:09:16,055   Num examples = 1500
2022-06-12 03:09:16,055   Batch size = 32
2022-06-12 03:09:16,056 ***** Eval results *****
2022-06-12 03:09:16,056   att_loss = 2.591767337264084
2022-06-12 03:09:16,056   global_step = 839
2022-06-12 03:09:16,056   loss = 3.560263703509075
2022-06-12 03:09:16,056   rep_loss = 0.9684963575223597
2022-06-12 03:09:16,057 ***** Save model *****
2022-06-12 03:09:21,705 ***** Running evaluation *****
2022-06-12 03:09:21,705   Epoch = 4 iter 859 step
2022-06-12 03:09:21,705   Num examples = 1500
2022-06-12 03:09:21,705   Batch size = 32
2022-06-12 03:09:21,707 ***** Eval results *****
2022-06-12 03:09:21,707   att_loss = 2.6204267740249634
2022-06-12 03:09:21,707   global_step = 859
2022-06-12 03:09:21,707   loss = 3.5901957942055653
2022-06-12 03:09:21,707   rep_loss = 0.9697690085097627
2022-06-12 03:09:21,707 ***** Save model *****
2022-06-12 03:09:27,345 ***** Running evaluation *****
2022-06-12 03:09:27,346   Epoch = 4 iter 879 step
2022-06-12 03:09:27,346   Num examples = 1500
2022-06-12 03:09:27,346   Batch size = 32
2022-06-12 03:09:27,348 ***** Eval results *****
2022-06-12 03:09:27,348   att_loss = 2.636543600837146
2022-06-12 03:09:27,348   global_step = 879
2022-06-12 03:09:27,348   loss = 3.6063211988086348
2022-06-12 03:09:27,348   rep_loss = 0.9697775895610178
2022-06-12 03:09:27,348 ***** Save model *****
2022-06-12 03:09:32,996 ***** Running evaluation *****
2022-06-12 03:09:32,996   Epoch = 5 iter 899 step
2022-06-12 03:09:32,996   Num examples = 1500
2022-06-12 03:09:32,996   Batch size = 32
2022-06-12 03:09:32,998 ***** Eval results *****
2022-06-12 03:09:32,999   att_loss = 2.387255311012268
2022-06-12 03:09:32,999   global_step = 899
2022-06-12 03:09:32,999   loss = 3.334076166152954
2022-06-12 03:09:33,000   rep_loss = 0.9468208253383636
2022-06-12 03:09:33,000 ***** Save model *****
2022-06-12 03:09:38,656 ***** Running evaluation *****
2022-06-12 03:09:38,656   Epoch = 5 iter 919 step
2022-06-12 03:09:38,656   Num examples = 1500
2022-06-12 03:09:38,656   Batch size = 32
2022-06-12 03:09:38,657 ***** Eval results *****
2022-06-12 03:09:38,657   att_loss = 2.610825777053833
2022-06-12 03:09:38,657   global_step = 919
2022-06-12 03:09:38,657   loss = 3.5733948151270547
2022-06-12 03:09:38,658   rep_loss = 0.9625690256555876
2022-06-12 03:09:38,658 ***** Save model *****
2022-06-12 03:09:44,261 ***** Running evaluation *****
2022-06-12 03:09:44,262   Epoch = 5 iter 939 step
2022-06-12 03:09:44,262   Num examples = 1500
2022-06-12 03:09:44,262   Batch size = 32
2022-06-12 03:09:44,263 ***** Eval results *****
2022-06-12 03:09:44,263   att_loss = 2.5622407604347575
2022-06-12 03:09:44,263   global_step = 939
2022-06-12 03:09:44,263   loss = 3.5193826014345344
2022-06-12 03:09:44,263   rep_loss = 0.9571418206800114
2022-06-12 03:09:44,263 ***** Save model *****
2022-06-12 03:09:49,866 ***** Running evaluation *****
2022-06-12 03:09:49,866   Epoch = 5 iter 959 step
2022-06-12 03:09:49,866   Num examples = 1500
2022-06-12 03:09:49,866   Batch size = 32
2022-06-12 03:09:49,867 ***** Eval results *****
2022-06-12 03:09:49,867   att_loss = 2.5308523681014776
2022-06-12 03:09:49,868   global_step = 959
2022-06-12 03:09:49,868   loss = 3.4839375764131546
2022-06-12 03:09:49,868   rep_loss = 0.9530851915478706
2022-06-12 03:09:49,868 ***** Save model *****
2022-06-12 03:09:55,460 ***** Running evaluation *****
2022-06-12 03:09:55,461   Epoch = 5 iter 979 step
2022-06-12 03:09:55,461   Num examples = 1500
2022-06-12 03:09:55,461   Batch size = 32
2022-06-12 03:09:55,462 ***** Eval results *****
2022-06-12 03:09:55,462   att_loss = 2.5300000437668393
2022-06-12 03:09:55,462   global_step = 979
2022-06-12 03:09:55,462   loss = 3.4811723175502958
2022-06-12 03:09:55,462   rep_loss = 0.951172262430191
2022-06-12 03:09:55,462 ***** Save model *****
2022-06-12 03:10:01,048 ***** Running evaluation *****
2022-06-12 03:10:01,048   Epoch = 5 iter 999 step
2022-06-12 03:10:01,048   Num examples = 1500
2022-06-12 03:10:01,048   Batch size = 32
2022-06-12 03:10:01,050 ***** Eval results *****
2022-06-12 03:10:01,050   att_loss = 2.5288117287250667
2022-06-12 03:10:01,050   global_step = 999
2022-06-12 03:10:01,050   loss = 3.4773980722977567
2022-06-12 03:10:01,050   rep_loss = 0.948586333829623
2022-06-12 03:10:01,050 ***** Save model *****
2022-06-12 03:10:06,671 ***** Running evaluation *****
2022-06-12 03:10:06,671   Epoch = 5 iter 1019 step
2022-06-12 03:10:06,671   Num examples = 1500
2022-06-12 03:10:06,671   Batch size = 32
2022-06-12 03:10:06,672 ***** Eval results *****
2022-06-12 03:10:06,672   att_loss = 2.554935529347389
2022-06-12 03:10:06,672   global_step = 1019
2022-06-12 03:10:06,672   loss = 3.5046111268381916
2022-06-12 03:10:06,672   rep_loss = 0.9496755912419288
2022-06-12 03:10:06,672 ***** Save model *****
2022-06-12 03:10:12,270 ***** Running evaluation *****
2022-06-12 03:10:12,270   Epoch = 5 iter 1039 step
2022-06-12 03:10:12,270   Num examples = 1500
2022-06-12 03:10:12,270   Batch size = 32
2022-06-12 03:10:12,271 ***** Eval results *****
2022-06-12 03:10:12,272   att_loss = 2.5646742458144822
2022-06-12 03:10:12,272   global_step = 1039
2022-06-12 03:10:12,272   loss = 3.513709697458479
2022-06-12 03:10:12,272   rep_loss = 0.9490354425377316
2022-06-12 03:10:12,272 ***** Save model *****
2022-06-12 03:10:17,883 ***** Running evaluation *****
2022-06-12 03:10:17,883   Epoch = 5 iter 1059 step
2022-06-12 03:10:17,883   Num examples = 1500
2022-06-12 03:10:17,883   Batch size = 32
2022-06-12 03:10:17,885 ***** Eval results *****
2022-06-12 03:10:17,885   att_loss = 2.5571580594632684
2022-06-12 03:10:17,885   global_step = 1059
2022-06-12 03:10:17,885   loss = 3.5057348012924194
2022-06-12 03:10:17,885   rep_loss = 0.9485767327430772
2022-06-12 03:10:17,885 ***** Save model *****
2022-06-12 03:10:17,904 ***** Running evaluation *****
2022-06-12 03:10:17,904   Epoch = 9 iter 32499 step
2022-06-12 03:10:17,905   Num examples = 5463
2022-06-12 03:10:17,905   Batch size = 32
2022-06-12 03:10:17,906 ***** Eval results *****
2022-06-12 03:10:17,906   att_loss = 3.513941328436195
2022-06-12 03:10:17,906   global_step = 32499
2022-06-12 03:10:17,906   loss = 4.378676222547899
2022-06-12 03:10:17,906   rep_loss = 0.8647348941117036
2022-06-12 03:10:17,906 ***** Save model *****
2022-06-12 03:10:23,541 ***** Running evaluation *****
2022-06-12 03:10:23,541   Epoch = 6 iter 1079 step
2022-06-12 03:10:23,541   Num examples = 1500
2022-06-12 03:10:23,541   Batch size = 32
2022-06-12 03:10:23,542 ***** Eval results *****
2022-06-12 03:10:23,542   att_loss = 2.648708534240723
2022-06-12 03:10:23,542   global_step = 1079
2022-06-12 03:10:23,543   loss = 3.575263500213623
2022-06-12 03:10:23,543   rep_loss = 0.9265549421310425
2022-06-12 03:10:23,543 ***** Save model *****
2022-06-12 03:10:29,120 ***** Running evaluation *****
2022-06-12 03:10:29,121   Epoch = 6 iter 1099 step
2022-06-12 03:10:29,121   Num examples = 1500
2022-06-12 03:10:29,121   Batch size = 32
2022-06-12 03:10:29,122 ***** Eval results *****
2022-06-12 03:10:29,122   att_loss = 2.6015267848968504
2022-06-12 03:10:29,122   global_step = 1099
2022-06-12 03:10:29,122   loss = 3.54227837562561
2022-06-12 03:10:29,122   rep_loss = 0.9407515788078308
2022-06-12 03:10:29,122 ***** Save model *****
2022-06-12 03:10:34,694 ***** Running evaluation *****
2022-06-12 03:10:34,695   Epoch = 6 iter 1119 step
2022-06-12 03:10:34,695   Num examples = 1500
2022-06-12 03:10:34,695   Batch size = 32
2022-06-12 03:10:34,696 ***** Eval results *****
2022-06-12 03:10:34,696   att_loss = 2.542477016978794
2022-06-12 03:10:34,696   global_step = 1119
2022-06-12 03:10:34,696   loss = 3.479884306589762
2022-06-12 03:10:34,696   rep_loss = 0.937407292260064
2022-06-12 03:10:34,696 ***** Save model *****
2022-06-12 03:10:40,282 ***** Running evaluation *****
2022-06-12 03:10:40,282   Epoch = 6 iter 1139 step
2022-06-12 03:10:40,282   Num examples = 1500
2022-06-12 03:10:40,282   Batch size = 32
2022-06-12 03:10:40,284 ***** Eval results *****
2022-06-12 03:10:40,284   att_loss = 2.496414639399602
2022-06-12 03:10:40,284   global_step = 1139
2022-06-12 03:10:40,284   loss = 3.431001586180467
2022-06-12 03:10:40,284   rep_loss = 0.9345869541168212
2022-06-12 03:10:40,284 ***** Save model *****
2022-06-12 03:10:45,969 ***** Running evaluation *****
2022-06-12 03:10:45,969   Epoch = 6 iter 1159 step
2022-06-12 03:10:45,969   Num examples = 1500
2022-06-12 03:10:45,969   Batch size = 32
2022-06-12 03:10:45,970 ***** Eval results *****
2022-06-12 03:10:45,970   att_loss = 2.5116172748453476
2022-06-12 03:10:45,970   global_step = 1159
2022-06-12 03:10:45,970   loss = 3.4466226970448215
2022-06-12 03:10:45,970   rep_loss = 0.935005430614247
2022-06-12 03:10:45,971 ***** Save model *****
2022-06-12 03:10:51,550 ***** Running evaluation *****
2022-06-12 03:10:51,551   Epoch = 6 iter 1179 step
2022-06-12 03:10:51,551   Num examples = 1500
2022-06-12 03:10:51,551   Batch size = 32
2022-06-12 03:10:51,552 ***** Eval results *****
2022-06-12 03:10:51,552   att_loss = 2.4933479581560407
2022-06-12 03:10:51,552   global_step = 1179
2022-06-12 03:10:51,552   loss = 3.427096920921689
2022-06-12 03:10:51,552   rep_loss = 0.9337489673069546
2022-06-12 03:10:51,552 ***** Save model *****
2022-06-12 03:10:57,143 ***** Running evaluation *****
2022-06-12 03:10:57,144   Epoch = 6 iter 1199 step
2022-06-12 03:10:57,144   Num examples = 1500
2022-06-12 03:10:57,144   Batch size = 32
2022-06-12 03:10:57,145 ***** Eval results *****
2022-06-12 03:10:57,145   att_loss = 2.48547926902771
2022-06-12 03:10:57,145   global_step = 1199
2022-06-12 03:10:57,145   loss = 3.4175766067504885
2022-06-12 03:10:57,145   rep_loss = 0.9320973362922669
2022-06-12 03:10:57,145 ***** Save model *****
2022-06-12 03:11:02,709 ***** Running evaluation *****
2022-06-12 03:11:02,709   Epoch = 6 iter 1219 step
2022-06-12 03:11:02,709   Num examples = 1500
2022-06-12 03:11:02,709   Batch size = 32
2022-06-12 03:11:02,710 ***** Eval results *****
2022-06-12 03:11:02,710   att_loss = 2.481263884182634
2022-06-12 03:11:02,710   global_step = 1219
2022-06-12 03:11:02,710   loss = 3.413934515262472
2022-06-12 03:11:02,710   rep_loss = 0.9326706319019712
2022-06-12 03:11:02,710 ***** Save model *****
2022-06-12 03:11:08,349 ***** Running evaluation *****
2022-06-12 03:11:08,349   Epoch = 6 iter 1239 step
2022-06-12 03:11:08,349   Num examples = 1500
2022-06-12 03:11:08,349   Batch size = 32
2022-06-12 03:11:08,350 ***** Eval results *****
2022-06-12 03:11:08,350   att_loss = 2.4814411741314513
2022-06-12 03:11:08,350   global_step = 1239
2022-06-12 03:11:08,350   loss = 3.4138852148345022
2022-06-12 03:11:08,351   rep_loss = 0.9324440421480121
2022-06-12 03:11:08,351 ***** Save model *****
2022-06-12 03:11:13,966 ***** Running evaluation *****
2022-06-12 03:11:13,967   Epoch = 7 iter 1259 step
2022-06-12 03:11:13,967   Num examples = 1500
2022-06-12 03:11:13,967   Batch size = 32
2022-06-12 03:11:13,968 ***** Eval results *****
2022-06-12 03:11:13,968   att_loss = 2.723395069440206
2022-06-12 03:11:13,968   global_step = 1259
2022-06-12 03:11:13,968   loss = 3.6635858615239463
2022-06-12 03:11:13,968   rep_loss = 0.9401908417542776
2022-06-12 03:11:13,968 ***** Save model *****
2022-06-12 03:11:17,269 Task finish! 
2022-06-12 03:11:17,270 Task cost 141.10392416666667 minutes, i.e. 2.351732076111111 hours. 
2022-06-12 03:11:19,455 Task start! 
2022-06-12 03:11:19,569 ***** Running evaluation *****
2022-06-12 03:11:19,570   Epoch = 7 iter 1279 step
2022-06-12 03:11:19,570   Num examples = 1500
2022-06-12 03:11:19,570   Batch size = 32
2022-06-12 03:11:19,571 ***** Eval results *****
2022-06-12 03:11:19,571   att_loss = 2.442625797711886
2022-06-12 03:11:19,571   global_step = 1279
2022-06-12 03:11:19,571   loss = 3.366147389778724
2022-06-12 03:11:19,571   rep_loss = 0.9235215943593246
2022-06-12 03:11:19,571 device: cuda n_gpu: 1
2022-06-12 03:11:19,571 ***** Save model *****
2022-06-12 03:11:19,572 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/QNLI', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=500, gpu_id=3, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=10, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/qnli/on_original_data', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/qnli/on_original_data', task_name='qnli', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/qnli/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/qnli/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 03:11:20,242 Writing example 0 of 104743
2022-06-12 03:11:20,243 *** Example ***
2022-06-12 03:11:20,243 guid: train-0
2022-06-12 03:11:20,243 tokens: [CLS] when did the third dig ##imo ##n series begin ? [SEP] unlike the two seasons before it and most of the seasons that followed , dig ##imo ##n tame ##rs takes a darker and more realistic approach to its story featuring dig ##imo ##n who do not rein ##car ##nate after their deaths and more complex character development in the original japanese . [SEP]
2022-06-12 03:11:20,243 input_ids: 101 2043 2106 1996 2353 10667 16339 2078 2186 4088 1029 102 4406 1996 2048 3692 2077 2009 1998 2087 1997 1996 3692 2008 2628 1010 10667 16339 2078 24763 2869 3138 1037 9904 1998 2062 12689 3921 2000 2049 2466 3794 10667 16339 2078 2040 2079 2025 27788 10010 12556 2044 2037 6677 1998 2062 3375 2839 2458 1999 1996 2434 2887 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:11:20,243 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:11:20,243 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:11:20,243 label: not_entailment
2022-06-12 03:11:20,244 label_id: 1
2022-06-12 03:11:25,147 ***** Running evaluation *****
2022-06-12 03:11:25,147   Epoch = 7 iter 1299 step
2022-06-12 03:11:25,147   Num examples = 1500
2022-06-12 03:11:25,148   Batch size = 32
2022-06-12 03:11:25,148 ***** Eval results *****
2022-06-12 03:11:25,149   att_loss = 2.451090561306995
2022-06-12 03:11:25,149   global_step = 1299
2022-06-12 03:11:25,149   loss = 3.370610765788866
2022-06-12 03:11:25,149   rep_loss = 0.9195202044818712
2022-06-12 03:11:25,149 ***** Save model *****
2022-06-12 03:11:27,678 Writing example 10000 of 104743
2022-06-12 03:11:30,712 ***** Running evaluation *****
2022-06-12 03:11:30,712   Epoch = 7 iter 1319 step
2022-06-12 03:11:30,712   Num examples = 1500
2022-06-12 03:11:30,712   Batch size = 32
2022-06-12 03:11:30,713 ***** Eval results *****
2022-06-12 03:11:30,713   att_loss = 2.4752855662143594
2022-06-12 03:11:30,713   global_step = 1319
2022-06-12 03:11:30,713   loss = 3.3977182886817237
2022-06-12 03:11:30,713   rep_loss = 0.9224327152425592
2022-06-12 03:11:30,713 ***** Save model *****
2022-06-12 03:11:35,194 Writing example 20000 of 104743
2022-06-12 03:11:36,336 ***** Running evaluation *****
2022-06-12 03:11:36,337   Epoch = 7 iter 1339 step
2022-06-12 03:11:36,337   Num examples = 1500
2022-06-12 03:11:36,337   Batch size = 32
2022-06-12 03:11:36,338 ***** Eval results *****
2022-06-12 03:11:36,338   att_loss = 2.462355003800503
2022-06-12 03:11:36,338   global_step = 1339
2022-06-12 03:11:36,338   loss = 3.3831038114636445
2022-06-12 03:11:36,338   rep_loss = 0.9207488076631413
2022-06-12 03:11:36,338 ***** Save model *****
2022-06-12 03:11:41,884 ***** Running evaluation *****
2022-06-12 03:11:41,884   Epoch = 7 iter 1359 step
2022-06-12 03:11:41,884   Num examples = 1500
2022-06-12 03:11:41,884   Batch size = 32
2022-06-12 03:11:41,886 ***** Eval results *****
2022-06-12 03:11:41,886   att_loss = 2.434339939423327
2022-06-12 03:11:41,886   global_step = 1359
2022-06-12 03:11:41,886   loss = 3.3521985175474636
2022-06-12 03:11:41,886   rep_loss = 0.9178585753125964
2022-06-12 03:11:41,886 ***** Save model *****
2022-06-12 03:11:42,866 Writing example 30000 of 104743
2022-06-12 03:11:47,467 ***** Running evaluation *****
2022-06-12 03:11:47,467   Epoch = 7 iter 1379 step
2022-06-12 03:11:47,467   Num examples = 1500
2022-06-12 03:11:47,467   Batch size = 32
2022-06-12 03:11:47,468 ***** Eval results *****
2022-06-12 03:11:47,468   att_loss = 2.426798040904696
2022-06-12 03:11:47,468   global_step = 1379
2022-06-12 03:11:47,469   loss = 3.344936085125757
2022-06-12 03:11:47,469   rep_loss = 0.9181380409096914
2022-06-12 03:11:47,469 ***** Save model *****
2022-06-12 03:11:50,334 Writing example 40000 of 104743
2022-06-12 03:11:53,109 ***** Running evaluation *****
2022-06-12 03:11:53,110   Epoch = 7 iter 1399 step
2022-06-12 03:11:53,110   Num examples = 1500
2022-06-12 03:11:53,110   Batch size = 32
2022-06-12 03:11:53,111 ***** Eval results *****
2022-06-12 03:11:53,111   att_loss = 2.4200436170787025
2022-06-12 03:11:53,111   global_step = 1399
2022-06-12 03:11:53,111   loss = 3.337812066078186
2022-06-12 03:11:53,111   rep_loss = 0.9177684461417264
2022-06-12 03:11:53,112 ***** Save model *****
2022-06-12 03:11:57,753 Writing example 50000 of 104743
2022-06-12 03:11:58,686 ***** Running evaluation *****
2022-06-12 03:11:58,686   Epoch = 7 iter 1419 step
2022-06-12 03:11:58,686   Num examples = 1500
2022-06-12 03:11:58,686   Batch size = 32
2022-06-12 03:11:58,687 ***** Eval results *****
2022-06-12 03:11:58,687   att_loss = 2.411622960883451
2022-06-12 03:11:58,687   global_step = 1419
2022-06-12 03:11:58,687   loss = 3.329671500677086
2022-06-12 03:11:58,687   rep_loss = 0.9180485394345709
2022-06-12 03:11:58,688 ***** Save model *****
2022-06-12 03:12:04,268 ***** Running evaluation *****
2022-06-12 03:12:04,268   Epoch = 8 iter 1439 step
2022-06-12 03:12:04,268   Num examples = 1500
2022-06-12 03:12:04,268   Batch size = 32
2022-06-12 03:12:04,269 ***** Eval results *****
2022-06-12 03:12:04,269   att_loss = 2.3444960628237044
2022-06-12 03:12:04,269   global_step = 1439
2022-06-12 03:12:04,269   loss = 3.2691947732652937
2022-06-12 03:12:04,270   rep_loss = 0.9246986934116909
2022-06-12 03:12:04,270 ***** Save model *****
2022-06-12 03:12:05,357 Writing example 60000 of 104743
2022-06-12 03:12:09,844 ***** Running evaluation *****
2022-06-12 03:12:09,844   Epoch = 8 iter 1459 step
2022-06-12 03:12:09,844   Num examples = 1500
2022-06-12 03:12:09,844   Batch size = 32
2022-06-12 03:12:09,845 ***** Eval results *****
2022-06-12 03:12:09,845   att_loss = 2.35601599128158
2022-06-12 03:12:09,845   global_step = 1459
2022-06-12 03:12:09,846   loss = 3.2728968019838685
2022-06-12 03:12:09,846   rep_loss = 0.916880832778083
2022-06-12 03:12:09,846 ***** Save model *****
2022-06-12 03:12:12,675 Writing example 70000 of 104743
2022-06-12 03:12:15,431 ***** Running evaluation *****
2022-06-12 03:12:15,432   Epoch = 8 iter 1479 step
2022-06-12 03:12:15,432   Num examples = 1500
2022-06-12 03:12:15,432   Batch size = 32
2022-06-12 03:12:15,433 ***** Eval results *****
2022-06-12 03:12:15,433   att_loss = 2.2863408251011625
2022-06-12 03:12:15,433   global_step = 1479
2022-06-12 03:12:15,433   loss = 3.1948478475530098
2022-06-12 03:12:15,433   rep_loss = 0.908507024988215
2022-06-12 03:12:15,433 ***** Save model *****
2022-06-12 03:12:20,554 Writing example 80000 of 104743
2022-06-12 03:12:21,150 ***** Running evaluation *****
2022-06-12 03:12:21,150   Epoch = 8 iter 1499 step
2022-06-12 03:12:21,150   Num examples = 1500
2022-06-12 03:12:21,150   Batch size = 32
2022-06-12 03:12:21,152 ***** Eval results *****
2022-06-12 03:12:21,152   att_loss = 2.3200729427052966
2022-06-12 03:12:21,152   global_step = 1499
2022-06-12 03:12:21,152   loss = 3.230164456723341
2022-06-12 03:12:21,152   rep_loss = 0.9100915175765308
2022-06-12 03:12:21,152 ***** Save model *****
2022-06-12 03:12:26,762 ***** Running evaluation *****
2022-06-12 03:12:26,762   Epoch = 8 iter 1519 step
2022-06-12 03:12:26,762   Num examples = 1500
2022-06-12 03:12:26,762   Batch size = 32
2022-06-12 03:12:26,764 ***** Eval results *****
2022-06-12 03:12:26,764   att_loss = 2.328587649882525
2022-06-12 03:12:26,764   global_step = 1519
2022-06-12 03:12:26,764   loss = 3.2380285016421615
2022-06-12 03:12:26,764   rep_loss = 0.9094408586107451
2022-06-12 03:12:26,764 ***** Save model *****
2022-06-12 03:12:28,068 Writing example 90000 of 104743
2022-06-12 03:12:32,360 ***** Running evaluation *****
2022-06-12 03:12:32,360   Epoch = 8 iter 1539 step
2022-06-12 03:12:32,360   Num examples = 1500
2022-06-12 03:12:32,360   Batch size = 32
2022-06-12 03:12:32,361 ***** Eval results *****
2022-06-12 03:12:32,362   att_loss = 2.3331987401035343
2022-06-12 03:12:32,362   global_step = 1539
2022-06-12 03:12:32,362   loss = 3.243400157055008
2022-06-12 03:12:32,362   rep_loss = 0.9102014264213705
2022-06-12 03:12:32,362 ***** Save model *****
2022-06-12 03:12:35,507 Writing example 100000 of 104743
2022-06-12 03:12:37,962 ***** Running evaluation *****
2022-06-12 03:12:37,962   Epoch = 8 iter 1559 step
2022-06-12 03:12:37,962   Num examples = 1500
2022-06-12 03:12:37,962   Batch size = 32
2022-06-12 03:12:37,963 ***** Eval results *****
2022-06-12 03:12:37,964   att_loss = 2.3417390040525303
2022-06-12 03:12:37,964   global_step = 1559
2022-06-12 03:12:37,964   loss = 3.252536309985664
2022-06-12 03:12:37,964   rep_loss = 0.91079731578902
2022-06-12 03:12:37,964 ***** Save model *****
2022-06-12 03:12:40,428 Writing example 0 of 5463
2022-06-12 03:12:40,429 *** Example ***
2022-06-12 03:12:40,429 guid: dev_matched-0
2022-06-12 03:12:40,429 tokens: [CLS] what came into force after the new constitution was herald ? [SEP] as of that day , the new constitution herald ##ing the second republic came into force . [SEP]
2022-06-12 03:12:40,429 input_ids: 101 2054 2234 2046 2486 2044 1996 2047 4552 2001 9536 1029 102 2004 1997 2008 2154 1010 1996 2047 4552 9536 2075 1996 2117 3072 2234 2046 2486 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:12:40,429 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:12:40,429 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:12:40,430 label: entailment
2022-06-12 03:12:40,430 label_id: 0
2022-06-12 03:12:43,577 ***** Running evaluation *****
2022-06-12 03:12:43,578   Epoch = 8 iter 1579 step
2022-06-12 03:12:43,578   Num examples = 1500
2022-06-12 03:12:43,578   Batch size = 32
2022-06-12 03:12:43,579 ***** Eval results *****
2022-06-12 03:12:43,579   att_loss = 2.3335589957075054
2022-06-12 03:12:43,579   global_step = 1579
2022-06-12 03:12:43,579   loss = 3.2427942963684497
2022-06-12 03:12:43,579   rep_loss = 0.9092353091758936
2022-06-12 03:12:43,579 ***** Save model *****
2022-06-12 03:12:44,702 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 03:12:49,154 ***** Running evaluation *****
2022-06-12 03:12:49,154   Epoch = 8 iter 1599 step
2022-06-12 03:12:49,154   Num examples = 1500
2022-06-12 03:12:49,155   Batch size = 32
2022-06-12 03:12:49,155 ***** Eval results *****
2022-06-12 03:12:49,155   att_loss = 2.3301037100260844
2022-06-12 03:12:49,156   global_step = 1599
2022-06-12 03:12:49,156   loss = 3.238031153193491
2022-06-12 03:12:49,156   rep_loss = 0.9079274499487734
2022-06-12 03:12:49,156 ***** Save model *****
2022-06-12 03:12:49,971 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/qnli/on_original_data/pytorch_model.bin
2022-06-12 03:12:50,597 loading model...
2022-06-12 03:12:50,824 done!
2022-06-12 03:12:50,824 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.dense_fit.weight', 'bert.dense_fit.bias']
2022-06-12 03:12:54,411 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 03:12:54,784 ***** Running evaluation *****
2022-06-12 03:12:54,784   Epoch = 9 iter 1619 step
2022-06-12 03:12:54,784   Num examples = 1500
2022-06-12 03:12:54,784   Batch size = 32
2022-06-12 03:12:54,785 ***** Eval results *****
2022-06-12 03:12:54,785   att_loss = 2.287160813808441
2022-06-12 03:12:54,785   global_step = 1619
2022-06-12 03:12:54,785   loss = 3.177633970975876
2022-06-12 03:12:54,785   rep_loss = 0.8904731273651123
2022-06-12 03:12:54,785 ***** Save model *****
2022-06-12 03:12:55,527 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/qnli/on_original_data/pytorch_model.bin
2022-06-12 03:12:55,683 loading model...
2022-06-12 03:12:55,715 done!
2022-06-12 03:12:57,130 ***** Running training *****
2022-06-12 03:12:57,145   Num examples = 104743
2022-06-12 03:12:57,151   Batch size = 32
2022-06-12 03:12:57,161   Num steps = 32730
2022-06-12 03:12:57,177 n: bert.embeddings.word_embeddings.weight
2022-06-12 03:12:57,177 n: bert.embeddings.position_embeddings.weight
2022-06-12 03:12:57,188 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 03:12:57,198 n: bert.embeddings.LayerNorm.weight
2022-06-12 03:12:57,200 n: bert.embeddings.LayerNorm.bias
2022-06-12 03:12:57,200 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 03:12:57,200 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 03:12:57,200 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 03:12:57,201 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 03:12:57,201 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 03:12:57,201 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 03:12:57,201 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 03:12:57,201 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 03:12:57,201 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 03:12:57,201 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 03:12:57,202 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 03:12:57,202 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 03:12:57,202 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 03:12:57,202 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 03:12:57,202 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 03:12:57,202 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 03:12:57,202 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 03:12:57,203 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 03:12:57,204 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 03:12:57,204 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 03:12:57,204 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 03:12:57,204 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 03:12:57,204 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 03:12:57,204 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 03:12:57,204 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 03:12:57,204 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 03:12:57,204 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 03:12:57,204 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 03:12:57,205 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 03:12:57,206 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 03:12:57,206 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 03:12:57,206 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 03:12:57,206 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 03:12:57,206 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 03:12:57,206 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 03:12:57,206 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 03:12:57,206 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 03:12:57,206 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 03:12:57,207 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 03:12:57,207 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 03:12:57,208 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 03:12:57,209 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 03:12:57,209 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 03:12:57,209 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 03:12:57,209 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 03:12:57,209 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 03:12:57,209 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 03:12:57,209 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 03:12:57,209 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 03:12:57,209 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 03:12:57,210 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 03:12:57,211 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 03:12:57,211 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 03:12:57,211 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 03:12:57,211 n: bert.pooler.dense.weight
2022-06-12 03:12:57,211 n: bert.pooler.dense.bias
2022-06-12 03:12:57,211 n: classifier.weight
2022-06-12 03:12:57,211 n: classifier.bias
2022-06-12 03:12:57,211 n: fit_denses.0.weight
2022-06-12 03:12:57,211 n: fit_denses.0.bias
2022-06-12 03:12:57,211 n: fit_denses.1.weight
2022-06-12 03:12:57,211 n: fit_denses.1.bias
2022-06-12 03:12:57,211 n: fit_denses.2.weight
2022-06-12 03:12:57,211 n: fit_denses.2.bias
2022-06-12 03:12:57,211 n: fit_denses.3.weight
2022-06-12 03:12:57,211 n: fit_denses.3.bias
2022-06-12 03:12:57,211 n: fit_denses.4.weight
2022-06-12 03:12:57,211 n: fit_denses.4.bias
2022-06-12 03:12:57,211 n: fit_denses.5.weight
2022-06-12 03:12:57,211 n: fit_denses.5.bias
2022-06-12 03:12:57,211 n: fit_denses.6.weight
2022-06-12 03:12:57,212 n: fit_denses.6.bias
2022-06-12 03:12:57,212 Total parameters: 72468738
2022-06-12 03:13:00,400 ***** Running evaluation *****
2022-06-12 03:13:00,400   Epoch = 9 iter 1639 step
2022-06-12 03:13:00,400   Num examples = 1500
2022-06-12 03:13:00,400   Batch size = 32
2022-06-12 03:13:00,401 ***** Eval results *****
2022-06-12 03:13:00,401   att_loss = 2.3589452547686443
2022-06-12 03:13:00,401   global_step = 1639
2022-06-12 03:13:00,401   loss = 3.258283495903015
2022-06-12 03:13:00,401   rep_loss = 0.899338247520583
2022-06-12 03:13:00,401 ***** Save model *****
2022-06-12 03:13:06,051 ***** Running evaluation *****
2022-06-12 03:13:06,052   Epoch = 9 iter 1659 step
2022-06-12 03:13:06,052   Num examples = 1500
2022-06-12 03:13:06,052   Batch size = 32
2022-06-12 03:13:06,053 ***** Eval results *****
2022-06-12 03:13:06,053   att_loss = 2.308503878613313
2022-06-12 03:13:06,053   global_step = 1659
2022-06-12 03:13:06,053   loss = 3.2090335935354233
2022-06-12 03:13:06,053   rep_loss = 0.9005297174056371
2022-06-12 03:13:06,054 ***** Save model *****
2022-06-12 03:13:11,626 ***** Running evaluation *****
2022-06-12 03:13:11,626   Epoch = 9 iter 1679 step
2022-06-12 03:13:11,626   Num examples = 1500
2022-06-12 03:13:11,626   Batch size = 32
2022-06-12 03:13:11,627 ***** Eval results *****
2022-06-12 03:13:11,627   att_loss = 2.2836516587173237
2022-06-12 03:13:11,627   global_step = 1679
2022-06-12 03:13:11,627   loss = 3.1822718522127937
2022-06-12 03:13:11,627   rep_loss = 0.8986202005077811
2022-06-12 03:13:11,627 ***** Save model *****
2022-06-12 03:13:17,244 ***** Running evaluation *****
2022-06-12 03:13:17,244   Epoch = 9 iter 1699 step
2022-06-12 03:13:17,244   Num examples = 1500
2022-06-12 03:13:17,244   Batch size = 32
2022-06-12 03:13:17,245 ***** Eval results *****
2022-06-12 03:13:17,245   att_loss = 2.2943441569805145
2022-06-12 03:13:17,246   global_step = 1699
2022-06-12 03:13:17,246   loss = 3.1931064318526876
2022-06-12 03:13:17,246   rep_loss = 0.8987622830000791
2022-06-12 03:13:17,246 ***** Save model *****
2022-06-12 03:13:22,847 ***** Running evaluation *****
2022-06-12 03:13:22,847   Epoch = 9 iter 1719 step
2022-06-12 03:13:22,847   Num examples = 1500
2022-06-12 03:13:22,847   Batch size = 32
2022-06-12 03:13:22,848 ***** Eval results *****
2022-06-12 03:13:22,848   att_loss = 2.300921296631848
2022-06-12 03:13:22,848   global_step = 1719
2022-06-12 03:13:22,848   loss = 3.2007666958702936
2022-06-12 03:13:22,849   rep_loss = 0.8998454003422348
2022-06-12 03:13:22,849 ***** Save model *****
2022-06-12 03:13:28,474 ***** Running evaluation *****
2022-06-12 03:13:28,475   Epoch = 9 iter 1739 step
2022-06-12 03:13:28,475   Num examples = 1500
2022-06-12 03:13:28,475   Batch size = 32
2022-06-12 03:13:28,476 ***** Eval results *****
2022-06-12 03:13:28,476   att_loss = 2.3013145299628377
2022-06-12 03:13:28,476   global_step = 1739
2022-06-12 03:13:28,476   loss = 3.201728329062462
2022-06-12 03:13:28,476   rep_loss = 0.9004138042218983
2022-06-12 03:13:28,476 ***** Save model *****
2022-06-12 03:13:34,106 ***** Running evaluation *****
2022-06-12 03:13:34,107   Epoch = 9 iter 1759 step
2022-06-12 03:13:34,107   Num examples = 1500
2022-06-12 03:13:34,107   Batch size = 32
2022-06-12 03:13:34,108 ***** Eval results *****
2022-06-12 03:13:34,108   att_loss = 2.2794402233652167
2022-06-12 03:13:34,108   global_step = 1759
2022-06-12 03:13:34,108   loss = 3.17741463152138
2022-06-12 03:13:34,108   rep_loss = 0.8979744109753016
2022-06-12 03:13:34,108 ***** Save model *****
2022-06-12 03:13:39,714 ***** Running evaluation *****
2022-06-12 03:13:39,714   Epoch = 9 iter 1779 step
2022-06-12 03:13:39,714   Num examples = 1500
2022-06-12 03:13:39,714   Batch size = 32
2022-06-12 03:13:39,715 ***** Eval results *****
2022-06-12 03:13:39,716   att_loss = 2.2730152025109245
2022-06-12 03:13:39,716   global_step = 1779
2022-06-12 03:13:39,716   loss = 3.169080527055831
2022-06-12 03:13:39,716   rep_loss = 0.8960653280928021
2022-06-12 03:13:39,716 ***** Save model *****
2022-06-12 03:13:45,347 ***** Running evaluation *****
2022-06-12 03:13:45,347   Epoch = 10 iter 1799 step
2022-06-12 03:13:45,347   Num examples = 1500
2022-06-12 03:13:45,347   Batch size = 32
2022-06-12 03:13:45,348 ***** Eval results *****
2022-06-12 03:13:45,348   att_loss = 2.1968583133485584
2022-06-12 03:13:45,348   global_step = 1799
2022-06-12 03:13:45,348   loss = 3.078208049138387
2022-06-12 03:13:45,348   rep_loss = 0.8813497689035203
2022-06-12 03:13:45,348 ***** Save model *****
2022-06-12 03:13:50,967 ***** Running evaluation *****
2022-06-12 03:13:50,968   Epoch = 10 iter 1819 step
2022-06-12 03:13:50,968   Num examples = 1500
2022-06-12 03:13:50,968   Batch size = 32
2022-06-12 03:13:50,969 ***** Eval results *****
2022-06-12 03:13:50,969   att_loss = 2.286803829258886
2022-06-12 03:13:50,969   global_step = 1819
2022-06-12 03:13:50,969   loss = 3.1781742654997727
2022-06-12 03:13:50,969   rep_loss = 0.8913704732368732
2022-06-12 03:13:50,969 ***** Save model *****
2022-06-12 03:13:56,647 ***** Running evaluation *****
2022-06-12 03:13:56,648   Epoch = 10 iter 1839 step
2022-06-12 03:13:56,648   Num examples = 1500
2022-06-12 03:13:56,648   Batch size = 32
2022-06-12 03:13:56,649 ***** Eval results *****
2022-06-12 03:13:56,649   att_loss = 2.2619924958871334
2022-06-12 03:13:56,649   global_step = 1839
2022-06-12 03:13:56,649   loss = 3.1551471729667817
2022-06-12 03:13:56,649   rep_loss = 0.8931547026244961
2022-06-12 03:13:56,649 ***** Save model *****
2022-06-12 03:14:02,227 ***** Running evaluation *****
2022-06-12 03:14:02,227   Epoch = 10 iter 1859 step
2022-06-12 03:14:02,227   Num examples = 1500
2022-06-12 03:14:02,228   Batch size = 32
2022-06-12 03:14:02,228 ***** Eval results *****
2022-06-12 03:14:02,228   att_loss = 2.272243962771651
2022-06-12 03:14:02,229   global_step = 1859
2022-06-12 03:14:02,229   loss = 3.165038606394892
2022-06-12 03:14:02,229   rep_loss = 0.8927946591722793
2022-06-12 03:14:02,229 ***** Save model *****
2022-06-12 03:14:07,814 ***** Running evaluation *****
2022-06-12 03:14:07,814   Epoch = 10 iter 1879 step
2022-06-12 03:14:07,814   Num examples = 1500
2022-06-12 03:14:07,814   Batch size = 32
2022-06-12 03:14:07,815 ***** Eval results *****
2022-06-12 03:14:07,816   att_loss = 2.261546607767598
2022-06-12 03:14:07,816   global_step = 1879
2022-06-12 03:14:07,816   loss = 3.15403706304143
2022-06-12 03:14:07,816   rep_loss = 0.8924904733561398
2022-06-12 03:14:07,816 ***** Save model *****
2022-06-12 03:14:13,430 ***** Running evaluation *****
2022-06-12 03:14:13,431   Epoch = 10 iter 1899 step
2022-06-12 03:14:13,431   Num examples = 1500
2022-06-12 03:14:13,431   Batch size = 32
2022-06-12 03:14:13,432 ***** Eval results *****
2022-06-12 03:14:13,432   att_loss = 2.2728115418635375
2022-06-12 03:14:13,432   global_step = 1899
2022-06-12 03:14:13,432   loss = 3.1648860130835015
2022-06-12 03:14:13,432   rep_loss = 0.8920744843439224
2022-06-12 03:14:13,433 ***** Save model *****
2022-06-12 03:14:19,022 ***** Running evaluation *****
2022-06-12 03:14:19,023   Epoch = 10 iter 1919 step
2022-06-12 03:14:19,023   Num examples = 1500
2022-06-12 03:14:19,023   Batch size = 32
2022-06-12 03:14:19,024 ***** Eval results *****
2022-06-12 03:14:19,024   att_loss = 2.23255319003911
2022-06-12 03:14:19,024   global_step = 1919
2022-06-12 03:14:19,024   loss = 3.1199660338172617
2022-06-12 03:14:19,024   rep_loss = 0.8874128576396971
2022-06-12 03:14:19,024 ***** Save model *****
2022-06-12 03:14:24,629 ***** Running evaluation *****
2022-06-12 03:14:24,629   Epoch = 10 iter 1939 step
2022-06-12 03:14:24,629   Num examples = 1500
2022-06-12 03:14:24,629   Batch size = 32
2022-06-12 03:14:24,630 ***** Eval results *****
2022-06-12 03:14:24,630   att_loss = 2.2498082026539232
2022-06-12 03:14:24,630   global_step = 1939
2022-06-12 03:14:24,630   loss = 3.1374114119766543
2022-06-12 03:14:24,630   rep_loss = 0.8876032229237909
2022-06-12 03:14:24,630 ***** Save model *****
2022-06-12 03:14:30,254 ***** Running evaluation *****
2022-06-12 03:14:30,254   Epoch = 10 iter 1959 step
2022-06-12 03:14:30,254   Num examples = 1500
2022-06-12 03:14:30,254   Batch size = 32
2022-06-12 03:14:30,255 ***** Eval results *****
2022-06-12 03:14:30,255   att_loss = 2.2510070892480702
2022-06-12 03:14:30,256   global_step = 1959
2022-06-12 03:14:30,256   loss = 3.1384145014384797
2022-06-12 03:14:30,256   rep_loss = 0.8874074224184251
2022-06-12 03:14:30,256 ***** Save model *****
2022-06-12 03:14:35,889 ***** Running evaluation *****
2022-06-12 03:14:35,890   Epoch = 11 iter 1979 step
2022-06-12 03:14:35,890   Num examples = 1500
2022-06-12 03:14:35,890   Batch size = 32
2022-06-12 03:14:35,891 ***** Eval results *****
2022-06-12 03:14:35,891   att_loss = 2.2039075255393983
2022-06-12 03:14:35,891   global_step = 1979
2022-06-12 03:14:35,891   loss = 3.0805856466293333
2022-06-12 03:14:35,891   rep_loss = 0.8766781151294708
2022-06-12 03:14:35,891 ***** Save model *****
2022-06-12 03:14:41,534 ***** Running evaluation *****
2022-06-12 03:14:41,535   Epoch = 11 iter 1999 step
2022-06-12 03:14:41,535   Num examples = 1500
2022-06-12 03:14:41,535   Batch size = 32
2022-06-12 03:14:41,536 ***** Eval results *****
2022-06-12 03:14:41,536   att_loss = 2.258590658505758
2022-06-12 03:14:41,536   global_step = 1999
2022-06-12 03:14:41,536   loss = 3.144341476758321
2022-06-12 03:14:41,536   rep_loss = 0.8857508222262065
2022-06-12 03:14:41,536 ***** Save model *****
2022-06-12 03:14:47,178 ***** Running evaluation *****
2022-06-12 03:14:47,178   Epoch = 11 iter 2019 step
2022-06-12 03:14:47,179   Num examples = 1500
2022-06-12 03:14:47,179   Batch size = 32
2022-06-12 03:14:47,180 ***** Eval results *****
2022-06-12 03:14:47,180   att_loss = 2.1998411417007446
2022-06-12 03:14:47,180   global_step = 2019
2022-06-12 03:14:47,180   loss = 3.080725564956665
2022-06-12 03:14:47,180   rep_loss = 0.8808844256401062
2022-06-12 03:14:47,180 ***** Save model *****
2022-06-12 03:14:52,787 ***** Running evaluation *****
2022-06-12 03:14:52,787   Epoch = 11 iter 2039 step
2022-06-12 03:14:52,787   Num examples = 1500
2022-06-12 03:14:52,787   Batch size = 32
2022-06-12 03:14:52,788 ***** Eval results *****
2022-06-12 03:14:52,788   att_loss = 2.178836989402771
2022-06-12 03:14:52,788   global_step = 2039
2022-06-12 03:14:52,788   loss = 3.056682617323739
2022-06-12 03:14:52,788   rep_loss = 0.8778456364359174
2022-06-12 03:14:52,788 ***** Save model *****
2022-06-12 03:14:55,328 ***** Running evaluation *****
2022-06-12 03:14:55,328   Epoch = 0 iter 499 step
2022-06-12 03:14:55,328   Num examples = 5463
2022-06-12 03:14:55,328   Batch size = 32
2022-06-12 03:14:58,407 ***** Running evaluation *****
2022-06-12 03:14:58,407   Epoch = 11 iter 2059 step
2022-06-12 03:14:58,407   Num examples = 1500
2022-06-12 03:14:58,407   Batch size = 32
2022-06-12 03:14:58,408 ***** Eval results *****
2022-06-12 03:14:58,408   att_loss = 2.182102025879754
2022-06-12 03:14:58,408   global_step = 2059
2022-06-12 03:14:58,409   loss = 3.060104560852051
2022-06-12 03:14:58,409   rep_loss = 0.878002546230952
2022-06-12 03:14:58,409 ***** Save model *****
2022-06-12 03:14:59,843 ***** Eval results *****
2022-06-12 03:14:59,843   acc = 0.8725974739154311
2022-06-12 03:14:59,843   cls_loss = 0.13378125114482128
2022-06-12 03:14:59,843   eval_loss = 0.38641475620325544
2022-06-12 03:14:59,843   global_step = 499
2022-06-12 03:14:59,843   loss = 0.13378125114482128
2022-06-12 03:14:59,844 ***** Save model *****
2022-06-12 03:15:04,019 ***** Running evaluation *****
2022-06-12 03:15:04,020   Epoch = 11 iter 2079 step
2022-06-12 03:15:04,020   Num examples = 1500
2022-06-12 03:15:04,020   Batch size = 32
2022-06-12 03:15:04,021 ***** Eval results *****
2022-06-12 03:15:04,021   att_loss = 2.1924941832369025
2022-06-12 03:15:04,021   global_step = 2079
2022-06-12 03:15:04,021   loss = 3.072012255408547
2022-06-12 03:15:04,021   rep_loss = 0.8795180840925737
2022-06-12 03:15:04,022 ***** Save model *****
2022-06-12 03:15:09,655 ***** Running evaluation *****
2022-06-12 03:15:09,655   Epoch = 11 iter 2099 step
2022-06-12 03:15:09,655   Num examples = 1500
2022-06-12 03:15:09,655   Batch size = 32
2022-06-12 03:15:09,656 ***** Eval results *****
2022-06-12 03:15:09,656   att_loss = 2.1891840512935934
2022-06-12 03:15:09,656   global_step = 2099
2022-06-12 03:15:09,656   loss = 3.068296586550199
2022-06-12 03:15:09,656   rep_loss = 0.8791125490115239
2022-06-12 03:15:09,656 ***** Save model *****
2022-06-12 03:15:15,294 ***** Running evaluation *****
2022-06-12 03:15:15,294   Epoch = 11 iter 2119 step
2022-06-12 03:15:15,294   Num examples = 1500
2022-06-12 03:15:15,294   Batch size = 32
2022-06-12 03:15:15,295 ***** Eval results *****
2022-06-12 03:15:15,295   att_loss = 2.19666303396225
2022-06-12 03:15:15,295   global_step = 2119
2022-06-12 03:15:15,296   loss = 3.0756451018651325
2022-06-12 03:15:15,296   rep_loss = 0.8789820794264476
2022-06-12 03:15:15,296 ***** Save model *****
2022-06-12 03:15:20,883 ***** Running evaluation *****
2022-06-12 03:15:20,884   Epoch = 11 iter 2139 step
2022-06-12 03:15:20,884   Num examples = 1500
2022-06-12 03:15:20,884   Batch size = 32
2022-06-12 03:15:20,885 ***** Eval results *****
2022-06-12 03:15:20,885   att_loss = 2.199049082924338
2022-06-12 03:15:20,885   global_step = 2139
2022-06-12 03:15:20,885   loss = 3.0781002423342536
2022-06-12 03:15:20,885   rep_loss = 0.8790511667728425
2022-06-12 03:15:20,885 ***** Save model *****
2022-06-12 03:15:26,473 ***** Running evaluation *****
2022-06-12 03:15:26,473   Epoch = 12 iter 2159 step
2022-06-12 03:15:26,473   Num examples = 1500
2022-06-12 03:15:26,473   Batch size = 32
2022-06-12 03:15:26,474 ***** Eval results *****
2022-06-12 03:15:26,474   att_loss = 2.1376226165077905
2022-06-12 03:15:26,474   global_step = 2159
2022-06-12 03:15:26,474   loss = 3.013399904424494
2022-06-12 03:15:26,474   rep_loss = 0.8757772499864752
2022-06-12 03:15:26,474 ***** Save model *****
2022-06-12 03:15:32,049 ***** Running evaluation *****
2022-06-12 03:15:32,050   Epoch = 12 iter 2179 step
2022-06-12 03:15:32,050   Num examples = 1500
2022-06-12 03:15:32,050   Batch size = 32
2022-06-12 03:15:32,051 ***** Eval results *****
2022-06-12 03:15:32,051   att_loss = 2.1009804394937333
2022-06-12 03:15:32,051   global_step = 2179
2022-06-12 03:15:32,051   loss = 2.9714870837426957
2022-06-12 03:15:32,052   rep_loss = 0.8705066269443881
2022-06-12 03:15:32,052 ***** Save model *****
2022-06-12 03:15:37,656 ***** Running evaluation *****
2022-06-12 03:15:37,657   Epoch = 12 iter 2199 step
2022-06-12 03:15:37,657   Num examples = 1500
2022-06-12 03:15:37,657   Batch size = 32
2022-06-12 03:15:37,658 ***** Eval results *****
2022-06-12 03:15:37,658   att_loss = 2.144249161084493
2022-06-12 03:15:37,658   global_step = 2199
2022-06-12 03:15:37,658   loss = 3.0165866403018726
2022-06-12 03:15:37,658   rep_loss = 0.8723374581804463
2022-06-12 03:15:37,658 ***** Save model *****
2022-06-12 03:15:43,244 ***** Running evaluation *****
2022-06-12 03:15:43,244   Epoch = 12 iter 2219 step
2022-06-12 03:15:43,244   Num examples = 1500
2022-06-12 03:15:43,244   Batch size = 32
2022-06-12 03:15:43,245 ***** Eval results *****
2022-06-12 03:15:43,245   att_loss = 2.104537354388707
2022-06-12 03:15:43,245   global_step = 2219
2022-06-12 03:15:43,246   loss = 2.9728541609267114
2022-06-12 03:15:43,246   rep_loss = 0.8683167830319471
2022-06-12 03:15:43,246 ***** Save model *****
2022-06-12 03:15:48,824 ***** Running evaluation *****
2022-06-12 03:15:48,825   Epoch = 12 iter 2239 step
2022-06-12 03:15:48,825   Num examples = 1500
2022-06-12 03:15:48,825   Batch size = 32
2022-06-12 03:15:48,826 ***** Eval results *****
2022-06-12 03:15:48,826   att_loss = 2.151696586346888
2022-06-12 03:15:48,826   global_step = 2239
2022-06-12 03:15:48,826   loss = 3.0249591712113264
2022-06-12 03:15:48,826   rep_loss = 0.8732625750394968
2022-06-12 03:15:48,826 ***** Save model *****
2022-06-12 03:15:54,407 ***** Running evaluation *****
2022-06-12 03:15:54,408   Epoch = 12 iter 2259 step
2022-06-12 03:15:54,408   Num examples = 1500
2022-06-12 03:15:54,408   Batch size = 32
2022-06-12 03:15:54,409 ***** Eval results *****
2022-06-12 03:15:54,409   att_loss = 2.1391852685997077
2022-06-12 03:15:54,409   global_step = 2259
2022-06-12 03:15:54,409   loss = 3.01073491465938
2022-06-12 03:15:54,409   rep_loss = 0.8715496439117569
2022-06-12 03:15:54,409 ***** Save model *****
2022-06-12 03:15:59,998 ***** Running evaluation *****
2022-06-12 03:15:59,999   Epoch = 12 iter 2279 step
2022-06-12 03:15:59,999   Num examples = 1500
2022-06-12 03:15:59,999   Batch size = 32
2022-06-12 03:16:00,000 ***** Eval results *****
2022-06-12 03:16:00,000   att_loss = 2.1471470166708677
2022-06-12 03:16:00,000   global_step = 2279
2022-06-12 03:16:00,000   loss = 3.0182796434591745
2022-06-12 03:16:00,000   rep_loss = 0.8711326267883068
2022-06-12 03:16:00,000 ***** Save model *****
2022-06-12 03:16:05,554 ***** Running evaluation *****
2022-06-12 03:16:05,554   Epoch = 12 iter 2299 step
2022-06-12 03:16:05,554   Num examples = 1500
2022-06-12 03:16:05,554   Batch size = 32
2022-06-12 03:16:05,555 ***** Eval results *****
2022-06-12 03:16:05,555   att_loss = 2.147830308667871
2022-06-12 03:16:05,555   global_step = 2299
2022-06-12 03:16:05,555   loss = 3.0176528100146363
2022-06-12 03:16:05,555   rep_loss = 0.8698225033204288
2022-06-12 03:16:05,555 ***** Save model *****
2022-06-12 03:16:11,107 ***** Running evaluation *****
2022-06-12 03:16:11,107   Epoch = 12 iter 2319 step
2022-06-12 03:16:11,107   Num examples = 1500
2022-06-12 03:16:11,107   Batch size = 32
2022-06-12 03:16:11,108 ***** Eval results *****
2022-06-12 03:16:11,108   att_loss = 2.1544903084548594
2022-06-12 03:16:11,108   global_step = 2319
2022-06-12 03:16:11,108   loss = 3.024341040884542
2022-06-12 03:16:11,108   rep_loss = 0.869850730686857
2022-06-12 03:16:11,108 ***** Save model *****
2022-06-12 03:16:16,740 ***** Running evaluation *****
2022-06-12 03:16:16,741   Epoch = 13 iter 2339 step
2022-06-12 03:16:16,741   Num examples = 1500
2022-06-12 03:16:16,741   Batch size = 32
2022-06-12 03:16:16,742 ***** Eval results *****
2022-06-12 03:16:16,742   att_loss = 2.132267822821935
2022-06-12 03:16:16,742   global_step = 2339
2022-06-12 03:16:16,742   loss = 2.987742821375529
2022-06-12 03:16:16,742   rep_loss = 0.8554749588171641
2022-06-12 03:16:16,742 ***** Save model *****
2022-06-12 03:16:22,335 ***** Running evaluation *****
2022-06-12 03:16:22,335   Epoch = 13 iter 2359 step
2022-06-12 03:16:22,335   Num examples = 1500
2022-06-12 03:16:22,335   Batch size = 32
2022-06-12 03:16:22,336 ***** Eval results *****
2022-06-12 03:16:22,336   att_loss = 2.1344302892684937
2022-06-12 03:16:22,336   global_step = 2359
2022-06-12 03:16:22,336   loss = 2.9993222802877426
2022-06-12 03:16:22,336   rep_loss = 0.8648919798433781
2022-06-12 03:16:22,336 ***** Save model *****
2022-06-12 03:16:27,967 ***** Running evaluation *****
2022-06-12 03:16:27,968   Epoch = 13 iter 2379 step
2022-06-12 03:16:27,968   Num examples = 1500
2022-06-12 03:16:27,968   Batch size = 32
2022-06-12 03:16:27,969 ***** Eval results *****
2022-06-12 03:16:27,969   att_loss = 2.132874140372643
2022-06-12 03:16:27,969   global_step = 2379
2022-06-12 03:16:27,969   loss = 3.000051961495326
2022-06-12 03:16:27,969   rep_loss = 0.8671778108064945
2022-06-12 03:16:27,969 ***** Save model *****
2022-06-12 03:16:33,510 ***** Running evaluation *****
2022-06-12 03:16:33,511   Epoch = 13 iter 2399 step
2022-06-12 03:16:33,511   Num examples = 1500
2022-06-12 03:16:33,511   Batch size = 32
2022-06-12 03:16:33,512 ***** Eval results *****
2022-06-12 03:16:33,512   att_loss = 2.106736794114113
2022-06-12 03:16:33,512   global_step = 2399
2022-06-12 03:16:33,512   loss = 2.969041976663801
2022-06-12 03:16:33,512   rep_loss = 0.8623051717877388
2022-06-12 03:16:33,513 ***** Save model *****
2022-06-12 03:16:39,142 ***** Running evaluation *****
2022-06-12 03:16:39,142   Epoch = 13 iter 2419 step
2022-06-12 03:16:39,142   Num examples = 1500
2022-06-12 03:16:39,143   Batch size = 32
2022-06-12 03:16:39,144 ***** Eval results *****
2022-06-12 03:16:39,144   att_loss = 2.131703361220982
2022-06-12 03:16:39,144   global_step = 2419
2022-06-12 03:16:39,144   loss = 2.9962527492771978
2022-06-12 03:16:39,144   rep_loss = 0.8645493763944377
2022-06-12 03:16:39,144 ***** Save model *****
2022-06-12 03:16:44,746 ***** Running evaluation *****
2022-06-12 03:16:44,746   Epoch = 13 iter 2439 step
2022-06-12 03:16:44,746   Num examples = 1500
2022-06-12 03:16:44,746   Batch size = 32
2022-06-12 03:16:44,747 ***** Eval results *****
2022-06-12 03:16:44,747   att_loss = 2.127681879060609
2022-06-12 03:16:44,748   global_step = 2439
2022-06-12 03:16:44,748   loss = 2.991830355354718
2022-06-12 03:16:44,748   rep_loss = 0.8641484736331871
2022-06-12 03:16:44,748 ***** Save model *****
2022-06-12 03:16:50,314 ***** Running evaluation *****
2022-06-12 03:16:50,315   Epoch = 13 iter 2459 step
2022-06-12 03:16:50,315   Num examples = 1500
2022-06-12 03:16:50,315   Batch size = 32
2022-06-12 03:16:50,315 ***** Eval results *****
2022-06-12 03:16:50,316   att_loss = 2.1310529194094916
2022-06-12 03:16:50,316   global_step = 2459
2022-06-12 03:16:50,316   loss = 2.9941976720636543
2022-06-12 03:16:50,316   rep_loss = 0.8631447512995113
2022-06-12 03:16:50,316 ***** Save model *****
2022-06-12 03:16:55,916 ***** Running evaluation *****
2022-06-12 03:16:55,916   Epoch = 13 iter 2479 step
2022-06-12 03:16:55,916   Num examples = 1500
2022-06-12 03:16:55,916   Batch size = 32
2022-06-12 03:16:55,917 ***** Eval results *****
2022-06-12 03:16:55,917   att_loss = 2.1259215387858843
2022-06-12 03:16:55,917   global_step = 2479
2022-06-12 03:16:55,917   loss = 2.988016664981842
2022-06-12 03:16:55,917   rep_loss = 0.8620951254116861
2022-06-12 03:16:55,917 ***** Save model *****
2022-06-12 03:16:58,797 ***** Running evaluation *****
2022-06-12 03:16:58,797   Epoch = 0 iter 999 step
2022-06-12 03:16:58,797   Num examples = 5463
2022-06-12 03:16:58,798   Batch size = 32
2022-06-12 03:17:01,473 ***** Running evaluation *****
2022-06-12 03:17:01,474   Epoch = 13 iter 2499 step
2022-06-12 03:17:01,474   Num examples = 1500
2022-06-12 03:17:01,474   Batch size = 32
2022-06-12 03:17:01,475 ***** Eval results *****
2022-06-12 03:17:01,475   att_loss = 2.1299062981161963
2022-06-12 03:17:01,475   global_step = 2499
2022-06-12 03:17:01,475   loss = 2.993155988149865
2022-06-12 03:17:01,475   rep_loss = 0.863249690726746
2022-06-12 03:17:01,475 ***** Save model *****
2022-06-12 03:17:03,316 ***** Eval results *****
2022-06-12 03:17:03,316   acc = 0.8757093172249679
2022-06-12 03:17:03,316   cls_loss = 0.09654089206786784
2022-06-12 03:17:03,316   eval_loss = 0.35680034083493967
2022-06-12 03:17:03,316   global_step = 999
2022-06-12 03:17:03,317   loss = 0.09654089206786784
2022-06-12 03:17:03,317 ***** Save model *****
2022-06-12 03:17:07,059 ***** Running evaluation *****
2022-06-12 03:17:07,060   Epoch = 14 iter 2519 step
2022-06-12 03:17:07,060   Num examples = 1500
2022-06-12 03:17:07,060   Batch size = 32
2022-06-12 03:17:07,061 ***** Eval results *****
2022-06-12 03:17:07,061   att_loss = 2.0483195323210497
2022-06-12 03:17:07,061   global_step = 2519
2022-06-12 03:17:07,061   loss = 2.90943457530095
2022-06-12 03:17:07,061   rep_loss = 0.8611150200550373
2022-06-12 03:17:07,061 ***** Save model *****
2022-06-12 03:17:12,647 ***** Running evaluation *****
2022-06-12 03:17:12,647   Epoch = 14 iter 2539 step
2022-06-12 03:17:12,647   Num examples = 1500
2022-06-12 03:17:12,647   Batch size = 32
2022-06-12 03:17:12,648 ***** Eval results *****
2022-06-12 03:17:12,648   att_loss = 2.121036525928613
2022-06-12 03:17:12,648   global_step = 2539
2022-06-12 03:17:12,648   loss = 2.9805863192587188
2022-06-12 03:17:12,648   rep_loss = 0.8595497806866964
2022-06-12 03:17:12,648 ***** Save model *****
2022-06-12 03:17:18,216 ***** Running evaluation *****
2022-06-12 03:17:18,217   Epoch = 14 iter 2559 step
2022-06-12 03:17:18,217   Num examples = 1500
2022-06-12 03:17:18,217   Batch size = 32
2022-06-12 03:17:18,218 ***** Eval results *****
2022-06-12 03:17:18,218   att_loss = 2.1093206990439937
2022-06-12 03:17:18,218   global_step = 2559
2022-06-12 03:17:18,218   loss = 2.9667855928528986
2022-06-12 03:17:18,218   rep_loss = 0.8574648859365931
2022-06-12 03:17:18,218 ***** Save model *****
2022-06-12 03:17:23,818 ***** Running evaluation *****
2022-06-12 03:17:23,819   Epoch = 14 iter 2579 step
2022-06-12 03:17:23,819   Num examples = 1500
2022-06-12 03:17:23,819   Batch size = 32
2022-06-12 03:17:23,820 ***** Eval results *****
2022-06-12 03:17:23,820   att_loss = 2.103070232966175
2022-06-12 03:17:23,820   global_step = 2579
2022-06-12 03:17:23,820   loss = 2.9608263969421387
2022-06-12 03:17:23,820   rep_loss = 0.8577561525449361
2022-06-12 03:17:23,820 ***** Save model *****
2022-06-12 03:17:29,466 ***** Running evaluation *****
2022-06-12 03:17:29,466   Epoch = 14 iter 2599 step
2022-06-12 03:17:29,466   Num examples = 1500
2022-06-12 03:17:29,466   Batch size = 32
2022-06-12 03:17:29,467 ***** Eval results *****
2022-06-12 03:17:29,467   att_loss = 2.0998353714584024
2022-06-12 03:17:29,467   global_step = 2599
2022-06-12 03:17:29,467   loss = 2.954931507828415
2022-06-12 03:17:29,467   rep_loss = 0.8550961241927199
2022-06-12 03:17:29,467 ***** Save model *****
2022-06-12 03:17:35,064 ***** Running evaluation *****
2022-06-12 03:17:35,065   Epoch = 14 iter 2619 step
2022-06-12 03:17:35,065   Num examples = 1500
2022-06-12 03:17:35,065   Batch size = 32
2022-06-12 03:17:35,066 ***** Eval results *****
2022-06-12 03:17:35,066   att_loss = 2.0961866705818513
2022-06-12 03:17:35,067   global_step = 2619
2022-06-12 03:17:35,067   loss = 2.9508093606054255
2022-06-12 03:17:35,067   rep_loss = 0.8546226821114532
2022-06-12 03:17:35,067 ***** Save model *****
2022-06-12 03:17:40,690 ***** Running evaluation *****
2022-06-12 03:17:40,691   Epoch = 14 iter 2639 step
2022-06-12 03:17:40,691   Num examples = 1500
2022-06-12 03:17:40,691   Batch size = 32
2022-06-12 03:17:40,692 ***** Eval results *****
2022-06-12 03:17:40,692   att_loss = 2.1107523799838876
2022-06-12 03:17:40,692   global_step = 2639
2022-06-12 03:17:40,693   loss = 2.9674063607266077
2022-06-12 03:17:40,693   rep_loss = 0.8566539708833049
2022-06-12 03:17:40,694 ***** Save model *****
2022-06-12 03:17:46,285 ***** Running evaluation *****
2022-06-12 03:17:46,285   Epoch = 14 iter 2659 step
2022-06-12 03:17:46,285   Num examples = 1500
2022-06-12 03:17:46,285   Batch size = 32
2022-06-12 03:17:46,286 ***** Eval results *****
2022-06-12 03:17:46,286   att_loss = 2.1070061236425164
2022-06-12 03:17:46,286   global_step = 2659
2022-06-12 03:17:46,286   loss = 2.963291803995768
2022-06-12 03:17:46,286   rep_loss = 0.8562856713930765
2022-06-12 03:17:46,286 ***** Save model *****
2022-06-12 03:17:51,876 ***** Running evaluation *****
2022-06-12 03:17:51,876   Epoch = 14 iter 2679 step
2022-06-12 03:17:51,876   Num examples = 1500
2022-06-12 03:17:51,876   Batch size = 32
2022-06-12 03:17:51,877 ***** Eval results *****
2022-06-12 03:17:51,877   att_loss = 2.1033657616962587
2022-06-12 03:17:51,878   global_step = 2679
2022-06-12 03:17:51,878   loss = 2.959089789087373
2022-06-12 03:17:51,878   rep_loss = 0.8557240201558681
2022-06-12 03:17:51,878 ***** Save model *****
2022-06-12 03:17:57,468 ***** Running evaluation *****
2022-06-12 03:17:57,469   Epoch = 15 iter 2699 step
2022-06-12 03:17:57,469   Num examples = 1500
2022-06-12 03:17:57,469   Batch size = 32
2022-06-12 03:17:57,470 ***** Eval results *****
2022-06-12 03:17:57,470   att_loss = 2.0374420625822887
2022-06-12 03:17:57,470   global_step = 2699
2022-06-12 03:17:57,470   loss = 2.893120322908674
2022-06-12 03:17:57,470   rep_loss = 0.8556782858712333
2022-06-12 03:17:57,470 ***** Save model *****
2022-06-12 03:18:03,056 ***** Running evaluation *****
2022-06-12 03:18:03,056   Epoch = 15 iter 2719 step
2022-06-12 03:18:03,056   Num examples = 1500
2022-06-12 03:18:03,056   Batch size = 32
2022-06-12 03:18:03,057 ***** Eval results *****
2022-06-12 03:18:03,057   att_loss = 2.113503428066478
2022-06-12 03:18:03,057   global_step = 2719
2022-06-12 03:18:03,057   loss = 2.9716589661205517
2022-06-12 03:18:03,057   rep_loss = 0.8581555363009957
2022-06-12 03:18:03,058 ***** Save model *****
2022-06-12 03:18:08,635 ***** Running evaluation *****
2022-06-12 03:18:08,635   Epoch = 15 iter 2739 step
2022-06-12 03:18:08,635   Num examples = 1500
2022-06-12 03:18:08,635   Batch size = 32
2022-06-12 03:18:08,636 ***** Eval results *****
2022-06-12 03:18:08,636   att_loss = 2.1130745499222368
2022-06-12 03:18:08,636   global_step = 2739
2022-06-12 03:18:08,636   loss = 2.9712571965323553
2022-06-12 03:18:08,636   rep_loss = 0.8581826444025393
2022-06-12 03:18:08,636 ***** Save model *****
2022-06-12 03:18:14,234 ***** Running evaluation *****
2022-06-12 03:18:14,235   Epoch = 15 iter 2759 step
2022-06-12 03:18:14,235   Num examples = 1500
2022-06-12 03:18:14,235   Batch size = 32
2022-06-12 03:18:14,236 ***** Eval results *****
2022-06-12 03:18:14,236   att_loss = 2.111532235467756
2022-06-12 03:18:14,236   global_step = 2759
2022-06-12 03:18:14,236   loss = 2.968191030863169
2022-06-12 03:18:14,236   rep_loss = 0.8566587897571357
2022-06-12 03:18:14,236 ***** Save model *****
2022-06-12 03:18:19,823 ***** Running evaluation *****
2022-06-12 03:18:19,824   Epoch = 15 iter 2779 step
2022-06-12 03:18:19,824   Num examples = 1500
2022-06-12 03:18:19,824   Batch size = 32
2022-06-12 03:18:19,825 ***** Eval results *****
2022-06-12 03:18:19,825   att_loss = 2.1042363504146007
2022-06-12 03:18:19,825   global_step = 2779
2022-06-12 03:18:19,825   loss = 2.9589029931007547
2022-06-12 03:18:19,825   rep_loss = 0.8546666407838781
2022-06-12 03:18:19,825 ***** Save model *****
2022-06-12 03:18:25,422 ***** Running evaluation *****
2022-06-12 03:18:25,423   Epoch = 15 iter 2799 step
2022-06-12 03:18:25,423   Num examples = 1500
2022-06-12 03:18:25,423   Batch size = 32
2022-06-12 03:18:25,424 ***** Eval results *****
2022-06-12 03:18:25,424   att_loss = 2.0934962274735436
2022-06-12 03:18:25,424   global_step = 2799
2022-06-12 03:18:25,424   loss = 2.9460802768406116
2022-06-12 03:18:25,424   rep_loss = 0.852584047275677
2022-06-12 03:18:25,424 ***** Save model *****
2022-06-12 03:18:31,052 ***** Running evaluation *****
2022-06-12 03:18:31,053   Epoch = 15 iter 2819 step
2022-06-12 03:18:31,053   Num examples = 1500
2022-06-12 03:18:31,053   Batch size = 32
2022-06-12 03:18:31,054 ***** Eval results *****
2022-06-12 03:18:31,054   att_loss = 2.0798790668373677
2022-06-12 03:18:31,054   global_step = 2819
2022-06-12 03:18:31,054   loss = 2.930933664094156
2022-06-12 03:18:31,054   rep_loss = 0.8510545945879239
2022-06-12 03:18:31,054 ***** Save model *****
2022-06-12 03:18:36,660 ***** Running evaluation *****
2022-06-12 03:18:36,661   Epoch = 15 iter 2839 step
2022-06-12 03:18:36,661   Num examples = 1500
2022-06-12 03:18:36,661   Batch size = 32
2022-06-12 03:18:36,662 ***** Eval results *****
2022-06-12 03:18:36,662   att_loss = 2.086509153440401
2022-06-12 03:18:36,662   global_step = 2839
2022-06-12 03:18:36,662   loss = 2.9371355384975284
2022-06-12 03:18:36,662   rep_loss = 0.8506263804126095
2022-06-12 03:18:36,662 ***** Save model *****
2022-06-12 03:18:42,274 ***** Running evaluation *****
2022-06-12 03:18:42,274   Epoch = 15 iter 2859 step
2022-06-12 03:18:42,274   Num examples = 1500
2022-06-12 03:18:42,274   Batch size = 32
2022-06-12 03:18:42,275 ***** Eval results *****
2022-06-12 03:18:42,276   att_loss = 2.0705529232134765
2022-06-12 03:18:42,276   global_step = 2859
2022-06-12 03:18:42,276   loss = 2.9198021998350647
2022-06-12 03:18:42,276   rep_loss = 0.849249270798146
2022-06-12 03:18:42,276 ***** Save model *****
2022-06-12 03:18:47,889 ***** Running evaluation *****
2022-06-12 03:18:47,889   Epoch = 16 iter 2879 step
2022-06-12 03:18:47,889   Num examples = 1500
2022-06-12 03:18:47,889   Batch size = 32
2022-06-12 03:18:47,891 ***** Eval results *****
2022-06-12 03:18:47,891   att_loss = 2.046697648366292
2022-06-12 03:18:47,891   global_step = 2879
2022-06-12 03:18:47,891   loss = 2.893727461496989
2022-06-12 03:18:47,891   rep_loss = 0.8470298012097677
2022-06-12 03:18:47,891 ***** Save model *****
2022-06-12 03:18:53,466 ***** Running evaluation *****
2022-06-12 03:18:53,467   Epoch = 16 iter 2899 step
2022-06-12 03:18:53,467   Num examples = 1500
2022-06-12 03:18:53,467   Batch size = 32
2022-06-12 03:18:53,468 ***** Eval results *****
2022-06-12 03:18:53,468   att_loss = 2.0526729004723685
2022-06-12 03:18:53,468   global_step = 2899
2022-06-12 03:18:53,468   loss = 2.899381119864328
2022-06-12 03:18:53,468   rep_loss = 0.8467082125799996
2022-06-12 03:18:53,468 ***** Save model *****
2022-06-12 03:18:59,039 ***** Running evaluation *****
2022-06-12 03:18:59,039   Epoch = 16 iter 2919 step
2022-06-12 03:18:59,039   Num examples = 1500
2022-06-12 03:18:59,039   Batch size = 32
2022-06-12 03:18:59,040 ***** Eval results *****
2022-06-12 03:18:59,040   att_loss = 2.0607236992229114
2022-06-12 03:18:59,040   global_step = 2919
2022-06-12 03:18:59,041   loss = 2.9098459287123246
2022-06-12 03:18:59,041   rep_loss = 0.8491222284056924
2022-06-12 03:18:59,041 ***** Save model *****
2022-06-12 03:19:02,505 ***** Running evaluation *****
2022-06-12 03:19:02,505   Epoch = 0 iter 1499 step
2022-06-12 03:19:02,505   Num examples = 5463
2022-06-12 03:19:02,505   Batch size = 32
2022-06-12 03:19:04,611 ***** Running evaluation *****
2022-06-12 03:19:04,611   Epoch = 16 iter 2939 step
2022-06-12 03:19:04,611   Num examples = 1500
2022-06-12 03:19:04,611   Batch size = 32
2022-06-12 03:19:04,612 ***** Eval results *****
2022-06-12 03:19:04,612   att_loss = 2.0625687646865845
2022-06-12 03:19:04,612   global_step = 2939
2022-06-12 03:19:04,612   loss = 2.909957526524862
2022-06-12 03:19:04,612   rep_loss = 0.8473887618382772
2022-06-12 03:19:04,612 ***** Save model *****
2022-06-12 03:19:07,025 ***** Eval results *****
2022-06-12 03:19:07,025   acc = 0.8751601684056379
2022-06-12 03:19:07,025   cls_loss = 0.0847589566836443
2022-06-12 03:19:07,025   eval_loss = 0.37322130256838965
2022-06-12 03:19:07,026   global_step = 1499
2022-06-12 03:19:07,026   loss = 0.0847589566836443
2022-06-12 03:19:10,185 ***** Running evaluation *****
2022-06-12 03:19:10,185   Epoch = 16 iter 2959 step
2022-06-12 03:19:10,186   Num examples = 1500
2022-06-12 03:19:10,186   Batch size = 32
2022-06-12 03:19:10,187 ***** Eval results *****
2022-06-12 03:19:10,187   att_loss = 2.079331808341177
2022-06-12 03:19:10,187   global_step = 2959
2022-06-12 03:19:10,187   loss = 2.926026321712293
2022-06-12 03:19:10,187   rep_loss = 0.8466945133711162
2022-06-12 03:19:10,187 ***** Save model *****
2022-06-12 03:19:15,797 ***** Running evaluation *****
2022-06-12 03:19:15,798   Epoch = 16 iter 2979 step
2022-06-12 03:19:15,798   Num examples = 1500
2022-06-12 03:19:15,798   Batch size = 32
2022-06-12 03:19:15,799 ***** Eval results *****
2022-06-12 03:19:15,799   att_loss = 2.0606065439141314
2022-06-12 03:19:15,799   global_step = 2979
2022-06-12 03:19:15,799   loss = 2.905894136428833
2022-06-12 03:19:15,799   rep_loss = 0.8452875919964002
2022-06-12 03:19:15,799 ***** Save model *****
2022-06-12 03:19:21,433 ***** Running evaluation *****
2022-06-12 03:19:21,433   Epoch = 16 iter 2999 step
2022-06-12 03:19:21,433   Num examples = 1500
2022-06-12 03:19:21,433   Batch size = 32
2022-06-12 03:19:21,434 ***** Eval results *****
2022-06-12 03:19:21,434   att_loss = 2.063314836113541
2022-06-12 03:19:21,434   global_step = 2999
2022-06-12 03:19:21,434   loss = 2.908329419736509
2022-06-12 03:19:21,435   rep_loss = 0.8450145809738724
2022-06-12 03:19:21,435 ***** Save model *****
2022-06-12 03:19:27,071 ***** Running evaluation *****
2022-06-12 03:19:27,072   Epoch = 16 iter 3019 step
2022-06-12 03:19:27,072   Num examples = 1500
2022-06-12 03:19:27,072   Batch size = 32
2022-06-12 03:19:27,073 ***** Eval results *****
2022-06-12 03:19:27,073   att_loss = 2.045716940203021
2022-06-12 03:19:27,073   global_step = 3019
2022-06-12 03:19:27,073   loss = 2.8898679087238928
2022-06-12 03:19:27,073   rep_loss = 0.8441509665981416
2022-06-12 03:19:27,073 ***** Save model *****
2022-06-12 03:19:32,709 ***** Running evaluation *****
2022-06-12 03:19:32,709   Epoch = 16 iter 3039 step
2022-06-12 03:19:32,709   Num examples = 1500
2022-06-12 03:19:32,709   Batch size = 32
2022-06-12 03:19:32,710 ***** Eval results *****
2022-06-12 03:19:32,710   att_loss = 2.0444888394219536
2022-06-12 03:19:32,710   global_step = 3039
2022-06-12 03:19:32,710   loss = 2.888676515306745
2022-06-12 03:19:32,711   rep_loss = 0.844187673500606
2022-06-12 03:19:32,711 ***** Save model *****
2022-06-12 03:19:38,259 ***** Running evaluation *****
2022-06-12 03:19:38,259   Epoch = 17 iter 3059 step
2022-06-12 03:19:38,259   Num examples = 1500
2022-06-12 03:19:38,259   Batch size = 32
2022-06-12 03:19:38,260 ***** Eval results *****
2022-06-12 03:19:38,260   att_loss = 2.1664563938975334
2022-06-12 03:19:38,260   global_step = 3059
2022-06-12 03:19:38,260   loss = 3.01659794151783
2022-06-12 03:19:38,261   rep_loss = 0.8501414954662323
2022-06-12 03:19:38,261 ***** Save model *****
2022-06-12 03:19:43,839 ***** Running evaluation *****
2022-06-12 03:19:43,839   Epoch = 17 iter 3079 step
2022-06-12 03:19:43,839   Num examples = 1500
2022-06-12 03:19:43,839   Batch size = 32
2022-06-12 03:19:43,840 ***** Eval results *****
2022-06-12 03:19:43,840   att_loss = 2.0522581107086606
2022-06-12 03:19:43,841   global_step = 3079
2022-06-12 03:19:43,841   loss = 2.8915330833858914
2022-06-12 03:19:43,841   rep_loss = 0.8392749461862776
2022-06-12 03:19:43,841 ***** Save model *****
2022-06-12 03:19:49,457 ***** Running evaluation *****
2022-06-12 03:19:49,457   Epoch = 17 iter 3099 step
2022-06-12 03:19:49,457   Num examples = 1500
2022-06-12 03:19:49,457   Batch size = 32
2022-06-12 03:19:49,458 ***** Eval results *****
2022-06-12 03:19:49,458   att_loss = 2.0190433923687254
2022-06-12 03:19:49,458   global_step = 3099
2022-06-12 03:19:49,458   loss = 2.8561616284506663
2022-06-12 03:19:49,458   rep_loss = 0.8371182201164109
2022-06-12 03:19:49,459 ***** Save model *****
2022-06-12 03:19:55,000 ***** Running evaluation *****
2022-06-12 03:19:55,000   Epoch = 17 iter 3119 step
2022-06-12 03:19:55,000   Num examples = 1500
2022-06-12 03:19:55,000   Batch size = 32
2022-06-12 03:19:55,001 ***** Eval results *****
2022-06-12 03:19:55,001   att_loss = 2.022566721627587
2022-06-12 03:19:55,001   global_step = 3119
2022-06-12 03:19:55,001   loss = 2.861287807163439
2022-06-12 03:19:55,001   rep_loss = 0.8387210698504197
2022-06-12 03:19:55,001 ***** Save model *****
2022-06-12 03:20:00,593 ***** Running evaluation *****
2022-06-12 03:20:00,593   Epoch = 17 iter 3139 step
2022-06-12 03:20:00,593   Num examples = 1500
2022-06-12 03:20:00,593   Batch size = 32
2022-06-12 03:20:00,594 ***** Eval results *****
2022-06-12 03:20:00,594   att_loss = 2.043501451611519
2022-06-12 03:20:00,594   global_step = 3139
2022-06-12 03:20:00,594   loss = 2.883482741812865
2022-06-12 03:20:00,594   rep_loss = 0.8399812796463569
2022-06-12 03:20:00,594 ***** Save model *****
2022-06-12 03:20:06,205 ***** Running evaluation *****
2022-06-12 03:20:06,205   Epoch = 17 iter 3159 step
2022-06-12 03:20:06,206   Num examples = 1500
2022-06-12 03:20:06,206   Batch size = 32
2022-06-12 03:20:06,207 ***** Eval results *****
2022-06-12 03:20:06,207   att_loss = 2.04974840735567
2022-06-12 03:20:06,207   global_step = 3159
2022-06-12 03:20:06,207   loss = 2.889618725612246
2022-06-12 03:20:06,207   rep_loss = 0.8398703064384132
2022-06-12 03:20:06,207 ***** Save model *****
2022-06-12 03:20:11,794 ***** Running evaluation *****
2022-06-12 03:20:11,794   Epoch = 17 iter 3179 step
2022-06-12 03:20:11,794   Num examples = 1500
2022-06-12 03:20:11,794   Batch size = 32
2022-06-12 03:20:11,795 ***** Eval results *****
2022-06-12 03:20:11,795   att_loss = 2.059513584655874
2022-06-12 03:20:11,795   global_step = 3179
2022-06-12 03:20:11,795   loss = 2.8998059297309204
2022-06-12 03:20:11,795   rep_loss = 0.8402923354331184
2022-06-12 03:20:11,795 ***** Save model *****
2022-06-12 03:20:17,396 ***** Running evaluation *****
2022-06-12 03:20:17,397   Epoch = 17 iter 3199 step
2022-06-12 03:20:17,397   Num examples = 1500
2022-06-12 03:20:17,397   Batch size = 32
2022-06-12 03:20:17,398 ***** Eval results *****
2022-06-12 03:20:17,398   att_loss = 2.0574146845401864
2022-06-12 03:20:17,398   global_step = 3199
2022-06-12 03:20:17,398   loss = 2.898534594438015
2022-06-12 03:20:17,398   rep_loss = 0.8411199011099644
2022-06-12 03:20:17,398 ***** Save model *****
2022-06-12 03:20:22,991 ***** Running evaluation *****
2022-06-12 03:20:22,992   Epoch = 17 iter 3219 step
2022-06-12 03:20:22,992   Num examples = 1500
2022-06-12 03:20:22,992   Batch size = 32
2022-06-12 03:20:22,993 ***** Eval results *****
2022-06-12 03:20:22,993   att_loss = 2.028096068989147
2022-06-12 03:20:22,993   global_step = 3219
2022-06-12 03:20:22,993   loss = 2.8665468245744705
2022-06-12 03:20:22,993   rep_loss = 0.8384507511827078
2022-06-12 03:20:22,993 ***** Save model *****
2022-06-12 03:20:28,612 ***** Running evaluation *****
2022-06-12 03:20:28,613   Epoch = 18 iter 3239 step
2022-06-12 03:20:28,613   Num examples = 1500
2022-06-12 03:20:28,613   Batch size = 32
2022-06-12 03:20:28,614 ***** Eval results *****
2022-06-12 03:20:28,614   att_loss = 2.0374513934640324
2022-06-12 03:20:28,614   global_step = 3239
2022-06-12 03:20:28,614   loss = 2.8709709644317627
2022-06-12 03:20:28,614   rep_loss = 0.8335195464246413
2022-06-12 03:20:28,614 ***** Save model *****
2022-06-12 03:20:34,144 ***** Running evaluation *****
2022-06-12 03:20:34,145   Epoch = 18 iter 3259 step
2022-06-12 03:20:34,145   Num examples = 1500
2022-06-12 03:20:34,145   Batch size = 32
2022-06-12 03:20:34,146 ***** Eval results *****
2022-06-12 03:20:34,146   att_loss = 2.01652848076176
2022-06-12 03:20:34,146   global_step = 3259
2022-06-12 03:20:34,146   loss = 2.8501614686605095
2022-06-12 03:20:34,146   rep_loss = 0.8336329798440676
2022-06-12 03:20:34,146 ***** Save model *****
2022-06-12 03:20:39,754 ***** Running evaluation *****
2022-06-12 03:20:39,755   Epoch = 18 iter 3279 step
2022-06-12 03:20:39,755   Num examples = 1500
2022-06-12 03:20:39,755   Batch size = 32
2022-06-12 03:20:39,756 ***** Eval results *****
2022-06-12 03:20:39,756   att_loss = 2.0409607050711647
2022-06-12 03:20:39,756   global_step = 3279
2022-06-12 03:20:39,756   loss = 2.876303145759984
2022-06-12 03:20:39,756   rep_loss = 0.8353424427802103
2022-06-12 03:20:39,756 ***** Save model *****
2022-06-12 03:20:45,372 ***** Running evaluation *****
2022-06-12 03:20:45,372   Epoch = 18 iter 3299 step
2022-06-12 03:20:45,372   Num examples = 1500
2022-06-12 03:20:45,372   Batch size = 32
2022-06-12 03:20:45,374 ***** Eval results *****
2022-06-12 03:20:45,374   att_loss = 2.035865834781102
2022-06-12 03:20:45,374   global_step = 3299
2022-06-12 03:20:45,374   loss = 2.8715624066142293
2022-06-12 03:20:45,374   rep_loss = 0.8356965733813001
2022-06-12 03:20:45,374 ***** Save model *****
2022-06-12 03:20:50,963 ***** Running evaluation *****
2022-06-12 03:20:50,963   Epoch = 18 iter 3319 step
2022-06-12 03:20:50,963   Num examples = 1500
2022-06-12 03:20:50,964   Batch size = 32
2022-06-12 03:20:50,965 ***** Eval results *****
2022-06-12 03:20:50,965   att_loss = 2.035402623648496
2022-06-12 03:20:50,965   global_step = 3319
2022-06-12 03:20:50,965   loss = 2.8717450407362475
2022-06-12 03:20:50,965   rep_loss = 0.8363424164732707
2022-06-12 03:20:50,965 ***** Save model *****
2022-06-12 03:20:56,574 ***** Running evaluation *****
2022-06-12 03:20:56,574   Epoch = 18 iter 3339 step
2022-06-12 03:20:56,574   Num examples = 1500
2022-06-12 03:20:56,574   Batch size = 32
2022-06-12 03:20:56,575 ***** Eval results *****
2022-06-12 03:20:56,575   att_loss = 2.0269269209641676
2022-06-12 03:20:56,575   global_step = 3339
2022-06-12 03:20:56,575   loss = 2.8613591051509237
2022-06-12 03:20:56,575   rep_loss = 0.8344321801112249
2022-06-12 03:20:56,575 ***** Save model *****
2022-06-12 03:21:02,209 ***** Running evaluation *****
2022-06-12 03:21:02,210   Epoch = 18 iter 3359 step
2022-06-12 03:21:02,210   Num examples = 1500
2022-06-12 03:21:02,210   Batch size = 32
2022-06-12 03:21:02,211 ***** Eval results *****
2022-06-12 03:21:02,211   att_loss = 2.0153559846599607
2022-06-12 03:21:02,211   global_step = 3359
2022-06-12 03:21:02,211   loss = 2.8486717356382494
2022-06-12 03:21:02,211   rep_loss = 0.8333157474977256
2022-06-12 03:21:02,211 ***** Save model *****
2022-06-12 03:21:06,038 ***** Running evaluation *****
2022-06-12 03:21:06,039   Epoch = 0 iter 1999 step
2022-06-12 03:21:06,039   Num examples = 5463
2022-06-12 03:21:06,039   Batch size = 32
2022-06-12 03:21:07,866 ***** Running evaluation *****
2022-06-12 03:21:07,867   Epoch = 18 iter 3379 step
2022-06-12 03:21:07,867   Num examples = 1500
2022-06-12 03:21:07,867   Batch size = 32
2022-06-12 03:21:07,868 ***** Eval results *****
2022-06-12 03:21:07,868   att_loss = 2.0114530757733973
2022-06-12 03:21:07,868   global_step = 3379
2022-06-12 03:21:07,868   loss = 2.843628274407356
2022-06-12 03:21:07,868   rep_loss = 0.8321751948374851
2022-06-12 03:21:07,868 ***** Save model *****
2022-06-12 03:21:10,559 ***** Eval results *****
2022-06-12 03:21:10,559   acc = 0.8639941424125938
2022-06-12 03:21:10,559   cls_loss = 0.0792685703781171
2022-06-12 03:21:10,559   eval_loss = 0.35340186953544617
2022-06-12 03:21:10,559   global_step = 1999
2022-06-12 03:21:10,559   loss = 0.0792685703781171
2022-06-12 03:21:13,533 ***** Running evaluation *****
2022-06-12 03:21:13,534   Epoch = 18 iter 3399 step
2022-06-12 03:21:13,534   Num examples = 1500
2022-06-12 03:21:13,534   Batch size = 32
2022-06-12 03:21:13,535 ***** Eval results *****
2022-06-12 03:21:13,535   att_loss = 2.002651188333156
2022-06-12 03:21:13,535   global_step = 3399
2022-06-12 03:21:13,535   loss = 2.8342440316906083
2022-06-12 03:21:13,536   rep_loss = 0.8315928406634573
2022-06-12 03:21:13,536 ***** Save model *****
2022-06-12 03:21:19,141 ***** Running evaluation *****
2022-06-12 03:21:19,141   Epoch = 19 iter 3419 step
2022-06-12 03:21:19,141   Num examples = 1500
2022-06-12 03:21:19,141   Batch size = 32
2022-06-12 03:21:19,143 ***** Eval results *****
2022-06-12 03:21:19,143   att_loss = 2.0318134427070618
2022-06-12 03:21:19,143   global_step = 3419
2022-06-12 03:21:19,143   loss = 2.8640565474828086
2022-06-12 03:21:19,143   rep_loss = 0.8322431047757467
2022-06-12 03:21:19,143 ***** Save model *****
2022-06-12 03:21:24,754 ***** Running evaluation *****
2022-06-12 03:21:24,754   Epoch = 19 iter 3439 step
2022-06-12 03:21:24,754   Num examples = 1500
2022-06-12 03:21:24,754   Batch size = 32
2022-06-12 03:21:24,756 ***** Eval results *****
2022-06-12 03:21:24,756   att_loss = 1.9991334896338613
2022-06-12 03:21:24,756   global_step = 3439
2022-06-12 03:21:24,756   loss = 2.8305985864840055
2022-06-12 03:21:24,756   rep_loss = 0.8314651031243173
2022-06-12 03:21:24,756 ***** Save model *****
2022-06-12 03:21:30,385 ***** Running evaluation *****
2022-06-12 03:21:30,386   Epoch = 19 iter 3459 step
2022-06-12 03:21:30,386   Num examples = 1500
2022-06-12 03:21:30,386   Batch size = 32
2022-06-12 03:21:30,387 ***** Eval results *****
2022-06-12 03:21:30,387   att_loss = 2.0196215325388414
2022-06-12 03:21:30,387   global_step = 3459
2022-06-12 03:21:30,387   loss = 2.8523468436865973
2022-06-12 03:21:30,387   rep_loss = 0.8327253193690859
2022-06-12 03:21:30,387 ***** Save model *****
2022-06-12 03:21:35,962 ***** Running evaluation *****
2022-06-12 03:21:35,962   Epoch = 19 iter 3479 step
2022-06-12 03:21:35,962   Num examples = 1500
2022-06-12 03:21:35,962   Batch size = 32
2022-06-12 03:21:35,963 ***** Eval results *****
2022-06-12 03:21:35,963   att_loss = 1.9976648168686109
2022-06-12 03:21:35,963   global_step = 3479
2022-06-12 03:21:35,963   loss = 2.8284303897466416
2022-06-12 03:21:35,963   rep_loss = 0.830765575170517
2022-06-12 03:21:35,963 ***** Save model *****
2022-06-12 03:21:41,563 ***** Running evaluation *****
2022-06-12 03:21:41,563   Epoch = 19 iter 3499 step
2022-06-12 03:21:41,563   Num examples = 1500
2022-06-12 03:21:41,563   Batch size = 32
2022-06-12 03:21:41,565 ***** Eval results *****
2022-06-12 03:21:41,565   att_loss = 1.9949891311781747
2022-06-12 03:21:41,565   global_step = 3499
2022-06-12 03:21:41,565   loss = 2.8240621138592155
2022-06-12 03:21:41,565   rep_loss = 0.8290729826810409
2022-06-12 03:21:41,565 ***** Save model *****
2022-06-12 03:21:47,175 ***** Running evaluation *****
2022-06-12 03:21:47,176   Epoch = 19 iter 3519 step
2022-06-12 03:21:47,176   Num examples = 1500
2022-06-12 03:21:47,176   Batch size = 32
2022-06-12 03:21:47,177 ***** Eval results *****
2022-06-12 03:21:47,177   att_loss = 1.9874087804454867
2022-06-12 03:21:47,177   global_step = 3519
2022-06-12 03:21:47,177   loss = 2.8156962495739175
2022-06-12 03:21:47,177   rep_loss = 0.8282874716540515
2022-06-12 03:21:47,177 ***** Save model *****
2022-06-12 03:21:52,772 ***** Running evaluation *****
2022-06-12 03:21:52,773   Epoch = 19 iter 3539 step
2022-06-12 03:21:52,773   Num examples = 1500
2022-06-12 03:21:52,773   Batch size = 32
2022-06-12 03:21:52,774 ***** Eval results *****
2022-06-12 03:21:52,774   att_loss = 1.996088097060936
2022-06-12 03:21:52,774   global_step = 3539
2022-06-12 03:21:52,774   loss = 2.8259857080984805
2022-06-12 03:21:52,774   rep_loss = 0.8298976157886394
2022-06-12 03:21:52,774 ***** Save model *****
2022-06-12 03:21:58,407 ***** Running evaluation *****
2022-06-12 03:21:58,408   Epoch = 19 iter 3559 step
2022-06-12 03:21:58,408   Num examples = 1500
2022-06-12 03:21:58,408   Batch size = 32
2022-06-12 03:21:58,409 ***** Eval results *****
2022-06-12 03:21:58,409   att_loss = 1.9914317613915553
2022-06-12 03:21:58,409   global_step = 3559
2022-06-12 03:21:58,409   loss = 2.820654787594759
2022-06-12 03:21:58,409   rep_loss = 0.8292230295984051
2022-06-12 03:21:58,409 ***** Save model *****
2022-06-12 03:22:04,010 ***** Running evaluation *****
2022-06-12 03:22:04,010   Epoch = 19 iter 3579 step
2022-06-12 03:22:04,010   Num examples = 1500
2022-06-12 03:22:04,011   Batch size = 32
2022-06-12 03:22:04,011 ***** Eval results *****
2022-06-12 03:22:04,012   att_loss = 1.984650511420175
2022-06-12 03:22:04,012   global_step = 3579
2022-06-12 03:22:04,012   loss = 2.8135751083995517
2022-06-12 03:22:04,012   rep_loss = 0.8289245979839497
2022-06-12 03:22:04,012 ***** Save model *****
2022-06-12 03:22:04,637 Task finish! 
2022-06-12 03:22:04,638 Task cost 17.04726425 minutes, i.e. 0.2841210736111111 hours. 
2022-06-12 03:22:06,672 Task start! 
2022-06-12 03:22:06,694 device: cuda n_gpu: 1
2022-06-12 03:22:06,694 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/STS-B', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=20, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=15, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/sts-b/on_original_data', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/sts-b/on_original_data', task_name='sts-b', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/sts-b/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/sts-b/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 03:22:06,742 Writing example 0 of 5749
2022-06-12 03:22:06,742 *** Example ***
2022-06-12 03:22:06,742 guid: train-0
2022-06-12 03:22:06,742 tokens: [CLS] a plane is taking off . [SEP] an air plane is taking off . [SEP]
2022-06-12 03:22:06,743 input_ids: 101 1037 4946 2003 2635 2125 1012 102 2019 2250 4946 2003 2635 2125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:22:06,743 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:22:06,743 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:22:06,743 label: 5.000
2022-06-12 03:22:06,743 label_id: 5.0
2022-06-12 03:22:09,269 Writing example 0 of 1500
2022-06-12 03:22:09,269 *** Example ***
2022-06-12 03:22:09,269 guid: dev-0
2022-06-12 03:22:09,270 tokens: [CLS] a man with a hard hat is dancing . [SEP] a man wearing a hard hat is dancing . [SEP]
2022-06-12 03:22:09,270 input_ids: 101 1037 2158 2007 1037 2524 6045 2003 5613 1012 102 1037 2158 4147 1037 2524 6045 2003 5613 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:22:09,270 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:22:09,270 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:22:09,270 label: 5.000
2022-06-12 03:22:09,270 label_id: 5.0
2022-06-12 03:22:09,969 Model config {
  "_num_labels": 1,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "sts-b",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 03:22:15,457 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/sts-b/on_original_data/pytorch_model.bin
2022-06-12 03:22:16,378 loading model...
2022-06-12 03:22:16,960 done!
2022-06-12 03:22:20,827 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 03:22:21,972 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/sts-b/on_original_data/pytorch_model.bin
2022-06-12 03:22:22,176 loading model...
2022-06-12 03:22:22,210 done!
2022-06-12 03:22:23,618 ***** Running training *****
2022-06-12 03:22:23,629   Num examples = 5749
2022-06-12 03:22:23,634   Batch size = 32
2022-06-12 03:22:23,634   Num steps = 2685
2022-06-12 03:22:23,634 n: bert.embeddings.word_embeddings.weight
2022-06-12 03:22:23,635 n: bert.embeddings.position_embeddings.weight
2022-06-12 03:22:23,645 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 03:22:23,650 n: bert.embeddings.LayerNorm.weight
2022-06-12 03:22:23,650 n: bert.embeddings.LayerNorm.bias
2022-06-12 03:22:23,650 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 03:22:23,650 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 03:22:23,650 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 03:22:23,650 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 03:22:23,650 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 03:22:23,650 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 03:22:23,651 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 03:22:23,661 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 03:22:23,667 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 03:22:23,667 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 03:22:23,667 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 03:22:23,667 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 03:22:23,667 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 03:22:23,667 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 03:22:23,667 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 03:22:23,667 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 03:22:23,677 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 03:22:23,683 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 03:22:23,683 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 03:22:23,683 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 03:22:23,683 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 03:22:23,683 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 03:22:23,693 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 03:22:23,697 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 03:22:23,697 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 03:22:23,697 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 03:22:23,697 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 03:22:23,697 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 03:22:23,697 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 03:22:23,698 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 03:22:23,699 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 03:22:23,700 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 03:22:23,700 n: bert.pooler.dense.weight
2022-06-12 03:22:23,700 n: bert.pooler.dense.bias
2022-06-12 03:22:23,700 n: classifier.weight
2022-06-12 03:22:23,701 n: classifier.bias
2022-06-12 03:22:23,701 n: fit_denses.0.weight
2022-06-12 03:22:23,701 n: fit_denses.0.bias
2022-06-12 03:22:23,701 n: fit_denses.1.weight
2022-06-12 03:22:23,701 n: fit_denses.1.bias
2022-06-12 03:22:23,701 n: fit_denses.2.weight
2022-06-12 03:22:23,701 n: fit_denses.2.bias
2022-06-12 03:22:23,701 n: fit_denses.3.weight
2022-06-12 03:22:23,701 n: fit_denses.3.bias
2022-06-12 03:22:23,701 n: fit_denses.4.weight
2022-06-12 03:22:23,701 n: fit_denses.4.bias
2022-06-12 03:22:23,701 n: fit_denses.5.weight
2022-06-12 03:22:23,701 n: fit_denses.5.bias
2022-06-12 03:22:23,701 n: fit_denses.6.weight
2022-06-12 03:22:23,701 n: fit_denses.6.bias
2022-06-12 03:22:23,701 Total parameters: 72467969
2022-06-12 03:22:28,376 ***** Running evaluation *****
2022-06-12 03:22:28,376   Epoch = 0 iter 19 step
2022-06-12 03:22:28,376   Num examples = 1500
2022-06-12 03:22:28,376   Batch size = 32
2022-06-12 03:22:29,680 ***** Eval results *****
2022-06-12 03:22:29,680   cls_loss = 2.508403778076172
2022-06-12 03:22:29,680   corr = 0.5624002789455643
2022-06-12 03:22:29,680   eval_loss = 6.211194200718657
2022-06-12 03:22:29,680   global_step = 19
2022-06-12 03:22:29,680   loss = 2.508403778076172
2022-06-12 03:22:29,680   pearson = 0.5886645619765507
2022-06-12 03:22:29,680   spearman = 0.536135995914578
2022-06-12 03:22:29,681 ***** Save model *****
2022-06-12 03:22:35,029 ***** Running evaluation *****
2022-06-12 03:22:35,030   Epoch = 0 iter 39 step
2022-06-12 03:22:35,030   Num examples = 1500
2022-06-12 03:22:35,030   Batch size = 32
2022-06-12 03:22:36,339 ***** Eval results *****
2022-06-12 03:22:36,339   cls_loss = 1.6817692843003151
2022-06-12 03:22:36,339   corr = 0.8364006872189416
2022-06-12 03:22:36,340   eval_loss = 2.551281393842494
2022-06-12 03:22:36,340   global_step = 39
2022-06-12 03:22:36,340   loss = 1.6817692843003151
2022-06-12 03:22:36,340   pearson = 0.818836215712395
2022-06-12 03:22:36,340   spearman = 0.8539651587254882
2022-06-12 03:22:36,340 ***** Save model *****
2022-06-12 03:22:41,773 ***** Running evaluation *****
2022-06-12 03:22:41,774   Epoch = 0 iter 59 step
2022-06-12 03:22:41,774   Num examples = 1500
2022-06-12 03:22:41,774   Batch size = 32
2022-06-12 03:22:43,076 ***** Eval results *****
2022-06-12 03:22:43,076   cls_loss = 1.1381049778123022
2022-06-12 03:22:43,076   corr = 0.8604350254776747
2022-06-12 03:22:43,076   eval_loss = 1.5427754749643041
2022-06-12 03:22:43,076   global_step = 59
2022-06-12 03:22:43,076   loss = 1.1381049778123022
2022-06-12 03:22:43,076   pearson = 0.8512492861133473
2022-06-12 03:22:43,076   spearman = 0.8696207648420021
2022-06-12 03:22:43,076 ***** Save model *****
2022-06-12 03:22:48,461 ***** Running evaluation *****
2022-06-12 03:22:48,461   Epoch = 0 iter 79 step
2022-06-12 03:22:48,461   Num examples = 1500
2022-06-12 03:22:48,461   Batch size = 32
2022-06-12 03:22:49,763 ***** Eval results *****
2022-06-12 03:22:49,763   cls_loss = 0.8640997992191888
2022-06-12 03:22:49,763   corr = 0.8749445649000993
2022-06-12 03:22:49,763   eval_loss = 1.9524041642533971
2022-06-12 03:22:49,763   global_step = 79
2022-06-12 03:22:49,763   loss = 0.8640997992191888
2022-06-12 03:22:49,763   pearson = 0.8747350266249261
2022-06-12 03:22:49,763   spearman = 0.8751541031752724
2022-06-12 03:22:49,764 ***** Save model *****
2022-06-12 03:22:55,144 ***** Running evaluation *****
2022-06-12 03:22:55,145   Epoch = 0 iter 99 step
2022-06-12 03:22:55,145   Num examples = 1500
2022-06-12 03:22:55,145   Batch size = 32
2022-06-12 03:22:56,446 ***** Eval results *****
2022-06-12 03:22:56,446   cls_loss = 0.6971489679670394
2022-06-12 03:22:56,447   corr = 0.8495050129883077
2022-06-12 03:22:56,447   eval_loss = 1.7453319886897474
2022-06-12 03:22:56,447   global_step = 99
2022-06-12 03:22:56,447   loss = 0.6971489679670394
2022-06-12 03:22:56,447   pearson = 0.8429156263291651
2022-06-12 03:22:56,447   spearman = 0.8560943996474505
2022-06-12 03:23:01,403 ***** Running evaluation *****
2022-06-12 03:23:01,403   Epoch = 0 iter 119 step
2022-06-12 03:23:01,403   Num examples = 1500
2022-06-12 03:23:01,403   Batch size = 32
2022-06-12 03:23:02,702 ***** Eval results *****
2022-06-12 03:23:02,702   cls_loss = 0.5865828528092438
2022-06-12 03:23:02,702   corr = 0.8685238059222852
2022-06-12 03:23:02,702   eval_loss = 1.8675220405801813
2022-06-12 03:23:02,702   global_step = 119
2022-06-12 03:23:02,702   loss = 0.5865828528092438
2022-06-12 03:23:02,702   pearson = 0.8634764741731236
2022-06-12 03:23:02,703   spearman = 0.8735711376714468
2022-06-12 03:23:07,651 ***** Running evaluation *****
2022-06-12 03:23:07,652   Epoch = 0 iter 139 step
2022-06-12 03:23:07,652   Num examples = 1500
2022-06-12 03:23:07,652   Batch size = 32
2022-06-12 03:23:08,947 ***** Eval results *****
2022-06-12 03:23:08,947   cls_loss = 0.5083161675699538
2022-06-12 03:23:08,947   corr = 0.8764947225332045
2022-06-12 03:23:08,947   eval_loss = 1.8880051551981176
2022-06-12 03:23:08,948   global_step = 139
2022-06-12 03:23:08,948   loss = 0.5083161675699538
2022-06-12 03:23:08,948   pearson = 0.8749382234555265
2022-06-12 03:23:08,948   spearman = 0.8780512216108824
2022-06-12 03:23:08,948 ***** Save model *****
2022-06-12 03:23:09,524 ***** Running evaluation *****
2022-06-12 03:23:09,525   Epoch = 0 iter 2499 step
2022-06-12 03:23:09,525   Num examples = 5463
2022-06-12 03:23:09,525   Batch size = 32
2022-06-12 03:23:14,052 ***** Eval results *****
2022-06-12 03:23:14,053   acc = 0.8685703825736775
2022-06-12 03:23:14,053   cls_loss = 0.07684665426564198
2022-06-12 03:23:14,053   eval_loss = 0.3639357813269074
2022-06-12 03:23:14,053   global_step = 2499
2022-06-12 03:23:14,053   loss = 0.07684665426564198
2022-06-12 03:23:14,368 ***** Running evaluation *****
2022-06-12 03:23:14,369   Epoch = 0 iter 159 step
2022-06-12 03:23:14,369   Num examples = 1500
2022-06-12 03:23:14,369   Batch size = 32
2022-06-12 03:23:15,664 ***** Eval results *****
2022-06-12 03:23:15,664   cls_loss = 0.4491864500813327
2022-06-12 03:23:15,664   corr = 0.8804711644127822
2022-06-12 03:23:15,664   eval_loss = 1.9093884863752
2022-06-12 03:23:15,664   global_step = 159
2022-06-12 03:23:15,664   loss = 0.4491864500813327
2022-06-12 03:23:15,664   pearson = 0.8805652149330415
2022-06-12 03:23:15,664   spearman = 0.8803771138925228
2022-06-12 03:23:15,665 ***** Save model *****
2022-06-12 03:23:21,083 ***** Running evaluation *****
2022-06-12 03:23:21,084   Epoch = 0 iter 179 step
2022-06-12 03:23:21,084   Num examples = 1500
2022-06-12 03:23:21,084   Batch size = 32
2022-06-12 03:23:22,380 ***** Eval results *****
2022-06-12 03:23:22,380   cls_loss = 0.40271665636637355
2022-06-12 03:23:22,380   corr = 0.8622002877165618
2022-06-12 03:23:22,380   eval_loss = 1.7168955143461837
2022-06-12 03:23:22,380   global_step = 179
2022-06-12 03:23:22,381   loss = 0.40271665636637355
2022-06-12 03:23:22,381   pearson = 0.8555167649530397
2022-06-12 03:23:22,381   spearman = 0.8688838104800837
2022-06-12 03:23:27,372 ***** Running evaluation *****
2022-06-12 03:23:27,373   Epoch = 1 iter 199 step
2022-06-12 03:23:27,373   Num examples = 1500
2022-06-12 03:23:27,373   Batch size = 32
2022-06-12 03:23:28,668 ***** Eval results *****
2022-06-12 03:23:28,669   cls_loss = 0.034221382718533276
2022-06-12 03:23:28,669   corr = 0.8695557815916783
2022-06-12 03:23:28,669   eval_loss = 1.6993652886532722
2022-06-12 03:23:28,669   global_step = 199
2022-06-12 03:23:28,669   loss = 0.034221382718533276
2022-06-12 03:23:28,669   pearson = 0.8624423522022733
2022-06-12 03:23:28,669   spearman = 0.8766692109810834
2022-06-12 03:23:33,624 ***** Running evaluation *****
2022-06-12 03:23:33,625   Epoch = 1 iter 219 step
2022-06-12 03:23:33,625   Num examples = 1500
2022-06-12 03:23:33,625   Batch size = 32
2022-06-12 03:23:34,919 ***** Eval results *****
2022-06-12 03:23:34,920   cls_loss = 0.032408522954210636
2022-06-12 03:23:34,920   corr = 0.8751471130486534
2022-06-12 03:23:34,920   eval_loss = 1.8428943372787314
2022-06-12 03:23:34,920   global_step = 219
2022-06-12 03:23:34,920   loss = 0.032408522954210636
2022-06-12 03:23:34,920   pearson = 0.873337539829121
2022-06-12 03:23:34,920   spearman = 0.8769566862681858
2022-06-12 03:23:39,855 ***** Running evaluation *****
2022-06-12 03:23:39,856   Epoch = 1 iter 239 step
2022-06-12 03:23:39,856   Num examples = 1500
2022-06-12 03:23:39,856   Batch size = 32
2022-06-12 03:23:41,152 ***** Eval results *****
2022-06-12 03:23:41,152   cls_loss = 0.03230010494589806
2022-06-12 03:23:41,152   corr = 0.867722838831706
2022-06-12 03:23:41,152   eval_loss = 1.6025822682583586
2022-06-12 03:23:41,152   global_step = 239
2022-06-12 03:23:41,152   loss = 0.03230010494589806
2022-06-12 03:23:41,152   pearson = 0.8614381179829887
2022-06-12 03:23:41,152   spearman = 0.8740075596804233
2022-06-12 03:23:46,106 ***** Running evaluation *****
2022-06-12 03:23:46,107   Epoch = 1 iter 259 step
2022-06-12 03:23:46,107   Num examples = 1500
2022-06-12 03:23:46,107   Batch size = 32
2022-06-12 03:23:47,403 ***** Eval results *****
2022-06-12 03:23:47,403   cls_loss = 0.03184577077627182
2022-06-12 03:23:47,403   corr = 0.8657821390517141
2022-06-12 03:23:47,403   eval_loss = 1.8210326395136245
2022-06-12 03:23:47,403   global_step = 259
2022-06-12 03:23:47,403   loss = 0.03184577077627182
2022-06-12 03:23:47,403   pearson = 0.8624818926360474
2022-06-12 03:23:47,403   spearman = 0.8690823854673808
2022-06-12 03:23:52,355 ***** Running evaluation *****
2022-06-12 03:23:52,355   Epoch = 1 iter 279 step
2022-06-12 03:23:52,355   Num examples = 1500
2022-06-12 03:23:52,355   Batch size = 32
2022-06-12 03:23:53,652 ***** Eval results *****
2022-06-12 03:23:53,652   cls_loss = 0.03258716069161892
2022-06-12 03:23:53,652   corr = 0.8683417600466774
2022-06-12 03:23:53,652   eval_loss = 1.8907898829338399
2022-06-12 03:23:53,652   global_step = 279
2022-06-12 03:23:53,652   loss = 0.03258716069161892
2022-06-12 03:23:53,652   pearson = 0.8686583595539248
2022-06-12 03:23:53,652   spearman = 0.86802516053943
2022-06-12 03:23:58,609 ***** Running evaluation *****
2022-06-12 03:23:58,609   Epoch = 1 iter 299 step
2022-06-12 03:23:58,609   Num examples = 1500
2022-06-12 03:23:58,609   Batch size = 32
2022-06-12 03:23:59,906 ***** Eval results *****
2022-06-12 03:23:59,906   cls_loss = 0.03345002693434556
2022-06-12 03:23:59,906   corr = 0.8606020095213491
2022-06-12 03:23:59,906   eval_loss = 1.5392645026775116
2022-06-12 03:23:59,906   global_step = 299
2022-06-12 03:23:59,907   loss = 0.03345002693434556
2022-06-12 03:23:59,907   pearson = 0.8418052102028991
2022-06-12 03:23:59,907   spearman = 0.8793988088397992
2022-06-12 03:24:04,847 ***** Running evaluation *****
2022-06-12 03:24:04,848   Epoch = 1 iter 319 step
2022-06-12 03:24:04,848   Num examples = 1500
2022-06-12 03:24:04,848   Batch size = 32
2022-06-12 03:24:06,145 ***** Eval results *****
2022-06-12 03:24:06,145   cls_loss = 0.03532697195187211
2022-06-12 03:24:06,145   corr = 0.8621804817533693
2022-06-12 03:24:06,146   eval_loss = 1.9673852971259584
2022-06-12 03:24:06,146   global_step = 319
2022-06-12 03:24:06,146   loss = 0.03532697195187211
2022-06-12 03:24:06,146   pearson = 0.8565490362103589
2022-06-12 03:24:06,146   spearman = 0.8678119272963796
2022-06-12 03:24:11,092 ***** Running evaluation *****
2022-06-12 03:24:11,093   Epoch = 1 iter 339 step
2022-06-12 03:24:11,093   Num examples = 1500
2022-06-12 03:24:11,093   Batch size = 32
2022-06-12 03:24:12,389 ***** Eval results *****
2022-06-12 03:24:12,390   cls_loss = 0.03584918190026656
2022-06-12 03:24:12,390   corr = 0.8770897014125394
2022-06-12 03:24:12,390   eval_loss = 1.7270906960710566
2022-06-12 03:24:12,390   global_step = 339
2022-06-12 03:24:12,390   loss = 0.03584918190026656
2022-06-12 03:24:12,390   pearson = 0.8735399773638648
2022-06-12 03:24:12,390   spearman = 0.880639425461214
2022-06-12 03:24:12,390 ***** Save model *****
2022-06-12 03:24:17,774 ***** Running evaluation *****
2022-06-12 03:24:17,774   Epoch = 2 iter 359 step
2022-06-12 03:24:17,774   Num examples = 1500
2022-06-12 03:24:17,774   Batch size = 32
2022-06-12 03:24:19,069 ***** Eval results *****
2022-06-12 03:24:19,070   cls_loss = 0.03129730373620987
2022-06-12 03:24:19,070   corr = 0.8686338541198562
2022-06-12 03:24:19,070   eval_loss = 1.990054356290939
2022-06-12 03:24:19,070   global_step = 359
2022-06-12 03:24:19,070   loss = 0.03129730373620987
2022-06-12 03:24:19,070   pearson = 0.8663996436056596
2022-06-12 03:24:19,070   spearman = 0.8708680646340526
2022-06-12 03:24:24,017 ***** Running evaluation *****
2022-06-12 03:24:24,018   Epoch = 2 iter 379 step
2022-06-12 03:24:24,018   Num examples = 1500
2022-06-12 03:24:24,018   Batch size = 32
2022-06-12 03:24:25,314 ***** Eval results *****
2022-06-12 03:24:25,315   cls_loss = 0.037190272827588376
2022-06-12 03:24:25,315   corr = 0.8763600304885935
2022-06-12 03:24:25,315   eval_loss = 1.8358903565305345
2022-06-12 03:24:25,315   global_step = 379
2022-06-12 03:24:25,315   loss = 0.037190272827588376
2022-06-12 03:24:25,315   pearson = 0.8724918190733909
2022-06-12 03:24:25,315   spearman = 0.8802282419037961
2022-06-12 03:24:30,274 ***** Running evaluation *****
2022-06-12 03:24:30,275   Epoch = 2 iter 399 step
2022-06-12 03:24:30,275   Num examples = 1500
2022-06-12 03:24:30,275   Batch size = 32
2022-06-12 03:24:31,572 ***** Eval results *****
2022-06-12 03:24:31,572   cls_loss = 0.0352372891246909
2022-06-12 03:24:31,572   corr = 0.8724285350336438
2022-06-12 03:24:31,572   eval_loss = 1.830919916325427
2022-06-12 03:24:31,572   global_step = 399
2022-06-12 03:24:31,572   loss = 0.0352372891246909
2022-06-12 03:24:31,572   pearson = 0.869903484823811
2022-06-12 03:24:31,572   spearman = 0.8749535852434768
2022-06-12 03:24:36,533 ***** Running evaluation *****
2022-06-12 03:24:36,534   Epoch = 2 iter 419 step
2022-06-12 03:24:36,534   Num examples = 1500
2022-06-12 03:24:36,534   Batch size = 32
2022-06-12 03:24:37,839 ***** Eval results *****
2022-06-12 03:24:37,839   cls_loss = 0.03264707346736896
2022-06-12 03:24:37,839   corr = 0.876825281028107
2022-06-12 03:24:37,839   eval_loss = 1.824710509878524
2022-06-12 03:24:37,840   global_step = 419
2022-06-12 03:24:37,840   loss = 0.03264707346736896
2022-06-12 03:24:37,840   pearson = 0.8756067807327288
2022-06-12 03:24:37,840   spearman = 0.8780437813234853
2022-06-12 03:24:42,800 ***** Running evaluation *****
2022-06-12 03:24:42,801   Epoch = 2 iter 439 step
2022-06-12 03:24:42,801   Num examples = 1500
2022-06-12 03:24:42,801   Batch size = 32
2022-06-12 03:24:44,098 ***** Eval results *****
2022-06-12 03:24:44,098   cls_loss = 0.031077022618257705
2022-06-12 03:24:44,098   corr = 0.872105360437865
2022-06-12 03:24:44,098   eval_loss = 1.8209844066741618
2022-06-12 03:24:44,098   global_step = 439
2022-06-12 03:24:44,099   loss = 0.031077022618257705
2022-06-12 03:24:44,099   pearson = 0.8702153053317907
2022-06-12 03:24:44,099   spearman = 0.8739954155439394
2022-06-12 03:24:49,096 ***** Running evaluation *****
2022-06-12 03:24:49,097   Epoch = 2 iter 459 step
2022-06-12 03:24:49,097   Num examples = 1500
2022-06-12 03:24:49,097   Batch size = 32
2022-06-12 03:24:50,394 ***** Eval results *****
2022-06-12 03:24:50,394   cls_loss = 0.029311335138460196
2022-06-12 03:24:50,394   corr = 0.879337224282475
2022-06-12 03:24:50,394   eval_loss = 1.8039767260247088
2022-06-12 03:24:50,394   global_step = 459
2022-06-12 03:24:50,394   loss = 0.029311335138460196
2022-06-12 03:24:50,394   pearson = 0.8774632949516812
2022-06-12 03:24:50,394   spearman = 0.8812111536132687
2022-06-12 03:24:50,395 ***** Save model *****
2022-06-12 03:24:55,827 ***** Running evaluation *****
2022-06-12 03:24:55,828   Epoch = 2 iter 479 step
2022-06-12 03:24:55,828   Num examples = 1500
2022-06-12 03:24:55,828   Batch size = 32
2022-06-12 03:24:57,123 ***** Eval results *****
2022-06-12 03:24:57,123   cls_loss = 0.02808152650259743
2022-06-12 03:24:57,123   corr = 0.8797769776026096
2022-06-12 03:24:57,124   eval_loss = 1.7310260027012927
2022-06-12 03:24:57,124   global_step = 479
2022-06-12 03:24:57,124   loss = 0.02808152650259743
2022-06-12 03:24:57,124   pearson = 0.8778748760025421
2022-06-12 03:24:57,124   spearman = 0.8816790792026771
2022-06-12 03:24:57,124 ***** Save model *****
2022-06-12 03:25:02,508 ***** Running evaluation *****
2022-06-12 03:25:02,509   Epoch = 2 iter 499 step
2022-06-12 03:25:02,509   Num examples = 1500
2022-06-12 03:25:02,509   Batch size = 32
2022-06-12 03:25:03,805 ***** Eval results *****
2022-06-12 03:25:03,805   cls_loss = 0.027603156633463733
2022-06-12 03:25:03,805   corr = 0.8727337849556867
2022-06-12 03:25:03,805   eval_loss = 1.7384148874181382
2022-06-12 03:25:03,805   global_step = 499
2022-06-12 03:25:03,805   loss = 0.027603156633463733
2022-06-12 03:25:03,805   pearson = 0.8712489104413272
2022-06-12 03:25:03,805   spearman = 0.8742186594700461
2022-06-12 03:25:08,741 ***** Running evaluation *****
2022-06-12 03:25:08,741   Epoch = 2 iter 519 step
2022-06-12 03:25:08,742   Num examples = 1500
2022-06-12 03:25:08,742   Batch size = 32
2022-06-12 03:25:10,037 ***** Eval results *****
2022-06-12 03:25:10,037   cls_loss = 0.027198394829904812
2022-06-12 03:25:10,037   corr = 0.8761392692635495
2022-06-12 03:25:10,037   eval_loss = 1.7828365298027689
2022-06-12 03:25:10,038   global_step = 519
2022-06-12 03:25:10,038   loss = 0.027198394829904812
2022-06-12 03:25:10,038   pearson = 0.8743133729161603
2022-06-12 03:25:10,038   spearman = 0.8779651656109386
2022-06-12 03:25:12,963 ***** Running evaluation *****
2022-06-12 03:25:12,963   Epoch = 0 iter 2999 step
2022-06-12 03:25:12,964   Num examples = 5463
2022-06-12 03:25:12,964   Batch size = 32
2022-06-12 03:25:14,973 ***** Running evaluation *****
2022-06-12 03:25:14,973   Epoch = 3 iter 539 step
2022-06-12 03:25:14,973   Num examples = 1500
2022-06-12 03:25:14,973   Batch size = 32
2022-06-12 03:25:16,283 ***** Eval results *****
2022-06-12 03:25:16,283   cls_loss = 0.026377911679446697
2022-06-12 03:25:16,284   corr = 0.8818084020777477
2022-06-12 03:25:16,284   eval_loss = 1.8549743573716346
2022-06-12 03:25:16,284   global_step = 539
2022-06-12 03:25:16,284   loss = 0.026377911679446697
2022-06-12 03:25:16,284   pearson = 0.8802798163368418
2022-06-12 03:25:16,284   spearman = 0.8833369878186534
2022-06-12 03:25:16,284 ***** Save model *****
2022-06-12 03:25:17,479 ***** Eval results *****
2022-06-12 03:25:17,479   acc = 0.8742449203734212
2022-06-12 03:25:17,479   cls_loss = 0.07583472943762594
2022-06-12 03:25:17,479   eval_loss = 0.36217559175838154
2022-06-12 03:25:17,479   global_step = 2999
2022-06-12 03:25:17,479   loss = 0.07583472943762594
2022-06-12 03:25:21,648 ***** Running evaluation *****
2022-06-12 03:25:21,649   Epoch = 3 iter 559 step
2022-06-12 03:25:21,649   Num examples = 1500
2022-06-12 03:25:21,649   Batch size = 32
2022-06-12 03:25:22,948 ***** Eval results *****
2022-06-12 03:25:22,948   cls_loss = 0.02166346511380239
2022-06-12 03:25:22,948   corr = 0.8806061559591682
2022-06-12 03:25:22,948   eval_loss = 1.9463400764668242
2022-06-12 03:25:22,948   global_step = 559
2022-06-12 03:25:22,948   loss = 0.02166346511380239
2022-06-12 03:25:22,948   pearson = 0.8771034961655872
2022-06-12 03:25:22,949   spearman = 0.8841088157527494
2022-06-12 03:25:22,949 ***** Save model *****
2022-06-12 03:25:28,320 ***** Running evaluation *****
2022-06-12 03:25:28,320   Epoch = 3 iter 579 step
2022-06-12 03:25:28,321   Num examples = 1500
2022-06-12 03:25:28,321   Batch size = 32
2022-06-12 03:25:29,616 ***** Eval results *****
2022-06-12 03:25:29,616   cls_loss = 0.021482941883039616
2022-06-12 03:25:29,617   corr = 0.8760145653412519
2022-06-12 03:25:29,617   eval_loss = 1.8384479893014787
2022-06-12 03:25:29,617   global_step = 579
2022-06-12 03:25:29,617   loss = 0.021482941883039616
2022-06-12 03:25:29,617   pearson = 0.8747386950739704
2022-06-12 03:25:29,617   spearman = 0.8772904356085333
2022-06-12 03:25:34,560 ***** Running evaluation *****
2022-06-12 03:25:34,561   Epoch = 3 iter 599 step
2022-06-12 03:25:34,561   Num examples = 1500
2022-06-12 03:25:34,561   Batch size = 32
2022-06-12 03:25:35,857 ***** Eval results *****
2022-06-12 03:25:35,857   cls_loss = 0.020908674127572486
2022-06-12 03:25:35,857   corr = 0.8713586463493074
2022-06-12 03:25:35,857   eval_loss = 1.9195673579865313
2022-06-12 03:25:35,858   global_step = 599
2022-06-12 03:25:35,858   loss = 0.020908674127572486
2022-06-12 03:25:35,858   pearson = 0.8663377153691103
2022-06-12 03:25:35,858   spearman = 0.8763795773295044
2022-06-12 03:25:40,859 ***** Running evaluation *****
2022-06-12 03:25:40,859   Epoch = 3 iter 619 step
2022-06-12 03:25:40,859   Num examples = 1500
2022-06-12 03:25:40,859   Batch size = 32
2022-06-12 03:25:42,159 ***** Eval results *****
2022-06-12 03:25:42,159   cls_loss = 0.02129749732841624
2022-06-12 03:25:42,159   corr = 0.8748611219724367
2022-06-12 03:25:42,159   eval_loss = 1.7687127552133926
2022-06-12 03:25:42,160   global_step = 619
2022-06-12 03:25:42,160   loss = 0.02129749732841624
2022-06-12 03:25:42,160   pearson = 0.8704990970398605
2022-06-12 03:25:42,160   spearman = 0.8792231469050129
2022-06-12 03:25:47,141 ***** Running evaluation *****
2022-06-12 03:25:47,141   Epoch = 3 iter 639 step
2022-06-12 03:25:47,142   Num examples = 1500
2022-06-12 03:25:47,142   Batch size = 32
2022-06-12 03:25:48,438 ***** Eval results *****
2022-06-12 03:25:48,438   cls_loss = 0.020896052955375874
2022-06-12 03:25:48,439   corr = 0.8787584324662141
2022-06-12 03:25:48,439   eval_loss = 1.784899039471403
2022-06-12 03:25:48,439   global_step = 639
2022-06-12 03:25:48,439   loss = 0.020896052955375874
2022-06-12 03:25:48,439   pearson = 0.8734659050914405
2022-06-12 03:25:48,439   spearman = 0.8840509598409877
2022-06-12 03:25:53,387 ***** Running evaluation *****
2022-06-12 03:25:53,388   Epoch = 3 iter 659 step
2022-06-12 03:25:53,388   Num examples = 1500
2022-06-12 03:25:53,388   Batch size = 32
2022-06-12 03:25:54,684 ***** Eval results *****
2022-06-12 03:25:54,684   cls_loss = 0.020445984059211907
2022-06-12 03:25:54,684   corr = 0.8751756457558877
2022-06-12 03:25:54,684   eval_loss = 1.7677351413889135
2022-06-12 03:25:54,684   global_step = 659
2022-06-12 03:25:54,684   loss = 0.020445984059211907
2022-06-12 03:25:54,685   pearson = 0.8696184549541518
2022-06-12 03:25:54,685   spearman = 0.8807328365576236
2022-06-12 03:25:59,639 ***** Running evaluation *****
2022-06-12 03:25:59,639   Epoch = 3 iter 679 step
2022-06-12 03:25:59,639   Num examples = 1500
2022-06-12 03:25:59,639   Batch size = 32
2022-06-12 03:26:00,935 ***** Eval results *****
2022-06-12 03:26:00,936   cls_loss = 0.020162790892383372
2022-06-12 03:26:00,936   corr = 0.8793149853033304
2022-06-12 03:26:00,936   eval_loss = 1.721246153750318
2022-06-12 03:26:00,936   global_step = 679
2022-06-12 03:26:00,936   loss = 0.020162790892383372
2022-06-12 03:26:00,936   pearson = 0.8754295451597092
2022-06-12 03:26:00,936   spearman = 0.8832004254469517
2022-06-12 03:26:05,889 ***** Running evaluation *****
2022-06-12 03:26:05,889   Epoch = 3 iter 699 step
2022-06-12 03:26:05,890   Num examples = 1500
2022-06-12 03:26:05,890   Batch size = 32
2022-06-12 03:26:07,187 ***** Eval results *****
2022-06-12 03:26:07,188   cls_loss = 0.02011057970481982
2022-06-12 03:26:07,188   corr = 0.8691316748845872
2022-06-12 03:26:07,188   eval_loss = 1.728306956747745
2022-06-12 03:26:07,188   global_step = 699
2022-06-12 03:26:07,188   loss = 0.02011057970481982
2022-06-12 03:26:07,189   pearson = 0.8652767623226765
2022-06-12 03:26:07,189   spearman = 0.8729865874464979
2022-06-12 03:26:12,135 ***** Running evaluation *****
2022-06-12 03:26:12,135   Epoch = 4 iter 719 step
2022-06-12 03:26:12,135   Num examples = 1500
2022-06-12 03:26:12,135   Batch size = 32
2022-06-12 03:26:13,433 ***** Eval results *****
2022-06-12 03:26:13,433   cls_loss = 0.015222158593436083
2022-06-12 03:26:13,433   corr = 0.8776371915284681
2022-06-12 03:26:13,433   eval_loss = 1.811828591722123
2022-06-12 03:26:13,433   global_step = 719
2022-06-12 03:26:13,433   loss = 0.015222158593436083
2022-06-12 03:26:13,433   pearson = 0.8732012217321152
2022-06-12 03:26:13,433   spearman = 0.8820731613248209
2022-06-12 03:26:18,365 ***** Running evaluation *****
2022-06-12 03:26:18,365   Epoch = 4 iter 739 step
2022-06-12 03:26:18,365   Num examples = 1500
2022-06-12 03:26:18,365   Batch size = 32
2022-06-12 03:26:19,661 ***** Eval results *****
2022-06-12 03:26:19,661   cls_loss = 0.016599624782152798
2022-06-12 03:26:19,661   corr = 0.8805879787253084
2022-06-12 03:26:19,661   eval_loss = 1.7610059000076133
2022-06-12 03:26:19,661   global_step = 739
2022-06-12 03:26:19,661   loss = 0.016599624782152798
2022-06-12 03:26:19,662   pearson = 0.8764039597461262
2022-06-12 03:26:19,662   spearman = 0.8847719977044907
2022-06-12 03:26:19,662 ***** Save model *****
2022-06-12 03:26:25,044 ***** Running evaluation *****
2022-06-12 03:26:25,045   Epoch = 4 iter 759 step
2022-06-12 03:26:25,045   Num examples = 1500
2022-06-12 03:26:25,045   Batch size = 32
2022-06-12 03:26:26,342 ***** Eval results *****
2022-06-12 03:26:26,343   cls_loss = 0.016952706717474515
2022-06-12 03:26:26,343   corr = 0.8755412104626716
2022-06-12 03:26:26,343   eval_loss = 1.694791907959796
2022-06-12 03:26:26,343   global_step = 759
2022-06-12 03:26:26,343   loss = 0.016952706717474515
2022-06-12 03:26:26,343   pearson = 0.8705591076438003
2022-06-12 03:26:26,343   spearman = 0.880523313281543
2022-06-12 03:26:31,263 ***** Running evaluation *****
2022-06-12 03:26:31,263   Epoch = 4 iter 779 step
2022-06-12 03:26:31,263   Num examples = 1500
2022-06-12 03:26:31,263   Batch size = 32
2022-06-12 03:26:32,558 ***** Eval results *****
2022-06-12 03:26:32,558   cls_loss = 0.01689737183707101
2022-06-12 03:26:32,558   corr = 0.881546345351707
2022-06-12 03:26:32,558   eval_loss = 1.8535926063010033
2022-06-12 03:26:32,558   global_step = 779
2022-06-12 03:26:32,558   loss = 0.01689737183707101
2022-06-12 03:26:32,559   pearson = 0.8805876180488819
2022-06-12 03:26:32,559   spearman = 0.882505072654532
2022-06-12 03:26:37,470 ***** Running evaluation *****
2022-06-12 03:26:37,471   Epoch = 4 iter 799 step
2022-06-12 03:26:37,471   Num examples = 1500
2022-06-12 03:26:37,471   Batch size = 32
2022-06-12 03:26:38,767 ***** Eval results *****
2022-06-12 03:26:38,767   cls_loss = 0.01745094915470445
2022-06-12 03:26:38,767   corr = 0.8747856613871321
2022-06-12 03:26:38,767   eval_loss = 1.8383583185520578
2022-06-12 03:26:38,767   global_step = 799
2022-06-12 03:26:38,767   loss = 0.01745094915470445
2022-06-12 03:26:38,767   pearson = 0.8693725737733575
2022-06-12 03:26:38,767   spearman = 0.8801987490009066
2022-06-12 03:26:43,700 ***** Running evaluation *****
2022-06-12 03:26:43,701   Epoch = 4 iter 819 step
2022-06-12 03:26:43,701   Num examples = 1500
2022-06-12 03:26:43,701   Batch size = 32
2022-06-12 03:26:45,001 ***** Eval results *****
2022-06-12 03:26:45,001   cls_loss = 0.017348425309128553
2022-06-12 03:26:45,001   corr = 0.8781605958364596
2022-06-12 03:26:45,001   eval_loss = 1.749992937483686
2022-06-12 03:26:45,001   global_step = 819
2022-06-12 03:26:45,001   loss = 0.017348425309128553
2022-06-12 03:26:45,001   pearson = 0.8752667712637598
2022-06-12 03:26:45,001   spearman = 0.8810544204091593
2022-06-12 03:26:49,924 ***** Running evaluation *****
2022-06-12 03:26:49,924   Epoch = 4 iter 839 step
2022-06-12 03:26:49,924   Num examples = 1500
2022-06-12 03:26:49,924   Batch size = 32
2022-06-12 03:26:51,220 ***** Eval results *****
2022-06-12 03:26:51,220   cls_loss = 0.01699608532196986
2022-06-12 03:26:51,220   corr = 0.8818435409555565
2022-06-12 03:26:51,220   eval_loss = 1.707256943621534
2022-06-12 03:26:51,220   global_step = 839
2022-06-12 03:26:51,220   loss = 0.01699608532196986
2022-06-12 03:26:51,220   pearson = 0.8783669414800759
2022-06-12 03:26:51,220   spearman = 0.885320140431037
2022-06-12 03:26:51,221 ***** Save model *****
2022-06-12 03:26:56,602 ***** Running evaluation *****
2022-06-12 03:26:56,602   Epoch = 4 iter 859 step
2022-06-12 03:26:56,602   Num examples = 1500
2022-06-12 03:26:56,602   Batch size = 32
2022-06-12 03:26:57,900 ***** Eval results *****
2022-06-12 03:26:57,900   cls_loss = 0.01685717854635803
2022-06-12 03:26:57,900   corr = 0.8796809046055694
2022-06-12 03:26:57,900   eval_loss = 1.8178847292636304
2022-06-12 03:26:57,900   global_step = 859
2022-06-12 03:26:57,900   loss = 0.01685717854635803
2022-06-12 03:26:57,901   pearson = 0.876898539076521
2022-06-12 03:26:57,901   spearman = 0.8824632701346178
2022-06-12 03:27:02,832 ***** Running evaluation *****
2022-06-12 03:27:02,832   Epoch = 4 iter 879 step
2022-06-12 03:27:02,832   Num examples = 1500
2022-06-12 03:27:02,832   Batch size = 32
2022-06-12 03:27:04,128 ***** Eval results *****
2022-06-12 03:27:04,129   cls_loss = 0.016889343069626327
2022-06-12 03:27:04,129   corr = 0.8800435726984553
2022-06-12 03:27:04,129   eval_loss = 1.8104278521334871
2022-06-12 03:27:04,129   global_step = 879
2022-06-12 03:27:04,129   loss = 0.016889343069626327
2022-06-12 03:27:04,129   pearson = 0.8765786200934818
2022-06-12 03:27:04,129   spearman = 0.8835085253034289
2022-06-12 03:27:09,039 ***** Running evaluation *****
2022-06-12 03:27:09,039   Epoch = 5 iter 899 step
2022-06-12 03:27:09,039   Num examples = 1500
2022-06-12 03:27:09,039   Batch size = 32
2022-06-12 03:27:10,336 ***** Eval results *****
2022-06-12 03:27:10,336   cls_loss = 0.012829180806875229
2022-06-12 03:27:10,336   corr = 0.8778351280479348
2022-06-12 03:27:10,336   eval_loss = 1.848175823688507
2022-06-12 03:27:10,336   global_step = 899
2022-06-12 03:27:10,337   loss = 0.012829180806875229
2022-06-12 03:27:10,337   pearson = 0.8738310130817366
2022-06-12 03:27:10,337   spearman = 0.8818392430141332
2022-06-12 03:27:15,266 ***** Running evaluation *****
2022-06-12 03:27:15,266   Epoch = 5 iter 919 step
2022-06-12 03:27:15,266   Num examples = 1500
2022-06-12 03:27:15,266   Batch size = 32
2022-06-12 03:27:16,298 ***** Running evaluation *****
2022-06-12 03:27:16,298   Epoch = 1 iter 3499 step
2022-06-12 03:27:16,298   Num examples = 5463
2022-06-12 03:27:16,299   Batch size = 32
2022-06-12 03:27:16,564 ***** Eval results *****
2022-06-12 03:27:16,564   cls_loss = 0.01496066595427692
2022-06-12 03:27:16,564   corr = 0.8713730394840236
2022-06-12 03:27:16,564   eval_loss = 1.7398441941180127
2022-06-12 03:27:16,564   global_step = 919
2022-06-12 03:27:16,564   loss = 0.01496066595427692
2022-06-12 03:27:16,565   pearson = 0.8664890444276283
2022-06-12 03:27:16,565   spearman = 0.8762570345404189
2022-06-12 03:27:20,836 ***** Eval results *****
2022-06-12 03:27:20,836   acc = 0.8389163463298553
2022-06-12 03:27:20,836   cls_loss = 0.06479837917681552
2022-06-12 03:27:20,836   eval_loss = 0.5586208161075561
2022-06-12 03:27:20,836   global_step = 3499
2022-06-12 03:27:20,836   loss = 0.06479837917681552
2022-06-12 03:27:21,509 ***** Running evaluation *****
2022-06-12 03:27:21,510   Epoch = 5 iter 939 step
2022-06-12 03:27:21,510   Num examples = 1500
2022-06-12 03:27:21,510   Batch size = 32
2022-06-12 03:27:22,805 ***** Eval results *****
2022-06-12 03:27:22,805   cls_loss = 0.01486099551601166
2022-06-12 03:27:22,805   corr = 0.8772026988988607
2022-06-12 03:27:22,805   eval_loss = 1.7787389514294076
2022-06-12 03:27:22,806   global_step = 939
2022-06-12 03:27:22,806   loss = 0.01486099551601166
2022-06-12 03:27:22,806   pearson = 0.872232232845257
2022-06-12 03:27:22,806   spearman = 0.8821731649524645
2022-06-12 03:27:27,725 ***** Running evaluation *****
2022-06-12 03:27:27,726   Epoch = 5 iter 959 step
2022-06-12 03:27:27,726   Num examples = 1500
2022-06-12 03:27:27,726   Batch size = 32
2022-06-12 03:27:29,021 ***** Eval results *****
2022-06-12 03:27:29,021   cls_loss = 0.014599450951209292
2022-06-12 03:27:29,021   corr = 0.8836813184990671
2022-06-12 03:27:29,021   eval_loss = 1.8329344531323046
2022-06-12 03:27:29,021   global_step = 959
2022-06-12 03:27:29,021   loss = 0.014599450951209292
2022-06-12 03:27:29,022   pearson = 0.883125573826033
2022-06-12 03:27:29,022   spearman = 0.8842370631721012
2022-06-12 03:27:33,960 ***** Running evaluation *****
2022-06-12 03:27:33,960   Epoch = 5 iter 979 step
2022-06-12 03:27:33,960   Num examples = 1500
2022-06-12 03:27:33,960   Batch size = 32
2022-06-12 03:27:35,257 ***** Eval results *****
2022-06-12 03:27:35,258   cls_loss = 0.014824346927482458
2022-06-12 03:27:35,258   corr = 0.8825443683417942
2022-06-12 03:27:35,258   eval_loss = 1.8277647000678041
2022-06-12 03:27:35,258   global_step = 979
2022-06-12 03:27:35,258   loss = 0.014824346927482458
2022-06-12 03:27:35,258   pearson = 0.8800628829322484
2022-06-12 03:27:35,258   spearman = 0.88502585375134
2022-06-12 03:27:40,182 ***** Running evaluation *****
2022-06-12 03:27:40,183   Epoch = 5 iter 999 step
2022-06-12 03:27:40,183   Num examples = 1500
2022-06-12 03:27:40,183   Batch size = 32
2022-06-12 03:27:41,481 ***** Eval results *****
2022-06-12 03:27:41,481   cls_loss = 0.015069943548251804
2022-06-12 03:27:41,481   corr = 0.8778768816711868
2022-06-12 03:27:41,481   eval_loss = 1.7991275191307068
2022-06-12 03:27:41,482   global_step = 999
2022-06-12 03:27:41,482   loss = 0.015069943548251804
2022-06-12 03:27:41,482   pearson = 0.8742648266195012
2022-06-12 03:27:41,482   spearman = 0.8814889367228724
2022-06-12 03:27:46,398 ***** Running evaluation *****
2022-06-12 03:27:46,398   Epoch = 5 iter 1019 step
2022-06-12 03:27:46,398   Num examples = 1500
2022-06-12 03:27:46,398   Batch size = 32
2022-06-12 03:27:47,711 ***** Eval results *****
2022-06-12 03:27:47,712   cls_loss = 0.014650701732945538
2022-06-12 03:27:47,712   corr = 0.8805758757177795
2022-06-12 03:27:47,712   eval_loss = 1.7884211603631364
2022-06-12 03:27:47,712   global_step = 1019
2022-06-12 03:27:47,712   loss = 0.014650701732945538
2022-06-12 03:27:47,712   pearson = 0.8777328311760944
2022-06-12 03:27:47,712   spearman = 0.8834189202594647
2022-06-12 03:27:52,639 ***** Running evaluation *****
2022-06-12 03:27:52,639   Epoch = 5 iter 1039 step
2022-06-12 03:27:52,640   Num examples = 1500
2022-06-12 03:27:52,640   Batch size = 32
2022-06-12 03:27:53,935 ***** Eval results *****
2022-06-12 03:27:53,935   cls_loss = 0.014391762091286687
2022-06-12 03:27:53,935   corr = 0.8808442949897934
2022-06-12 03:27:53,935   eval_loss = 1.8698890576971339
2022-06-12 03:27:53,935   global_step = 1039
2022-06-12 03:27:53,935   loss = 0.014391762091286687
2022-06-12 03:27:53,935   pearson = 0.878740538372057
2022-06-12 03:27:53,936   spearman = 0.8829480516075298
2022-06-12 03:27:58,829 ***** Running evaluation *****
2022-06-12 03:27:58,829   Epoch = 5 iter 1059 step
2022-06-12 03:27:58,829   Num examples = 1500
2022-06-12 03:27:58,829   Batch size = 32
2022-06-12 03:28:00,124 ***** Eval results *****
2022-06-12 03:28:00,125   cls_loss = 0.01420065172005263
2022-06-12 03:28:00,125   corr = 0.8824973102601366
2022-06-12 03:28:00,125   eval_loss = 1.7523467908514307
2022-06-12 03:28:00,125   global_step = 1059
2022-06-12 03:28:00,125   loss = 0.01420065172005263
2022-06-12 03:28:00,125   pearson = 0.8797037604612817
2022-06-12 03:28:00,125   spearman = 0.8852908600589915
2022-06-12 03:28:05,016 ***** Running evaluation *****
2022-06-12 03:28:05,016   Epoch = 6 iter 1079 step
2022-06-12 03:28:05,016   Num examples = 1500
2022-06-12 03:28:05,016   Batch size = 32
2022-06-12 03:28:06,311 ***** Eval results *****
2022-06-12 03:28:06,312   cls_loss = 0.016120522283017635
2022-06-12 03:28:06,312   corr = 0.8827467741792104
2022-06-12 03:28:06,312   eval_loss = 1.823075825863696
2022-06-12 03:28:06,312   global_step = 1079
2022-06-12 03:28:06,312   loss = 0.016120522283017635
2022-06-12 03:28:06,312   pearson = 0.8810680651276309
2022-06-12 03:28:06,312   spearman = 0.88442548323079
2022-06-12 03:28:11,216 ***** Running evaluation *****
2022-06-12 03:28:11,216   Epoch = 6 iter 1099 step
2022-06-12 03:28:11,216   Num examples = 1500
2022-06-12 03:28:11,216   Batch size = 32
2022-06-12 03:28:12,511 ***** Eval results *****
2022-06-12 03:28:12,511   cls_loss = 0.014820086546242237
2022-06-12 03:28:12,511   corr = 0.8825256179451824
2022-06-12 03:28:12,511   eval_loss = 1.8793988620981257
2022-06-12 03:28:12,511   global_step = 1099
2022-06-12 03:28:12,511   loss = 0.014820086546242237
2022-06-12 03:28:12,511   pearson = 0.8801883448847942
2022-06-12 03:28:12,511   spearman = 0.8848628910055705
2022-06-12 03:28:17,474 ***** Running evaluation *****
2022-06-12 03:28:17,475   Epoch = 6 iter 1119 step
2022-06-12 03:28:17,475   Num examples = 1500
2022-06-12 03:28:17,475   Batch size = 32
2022-06-12 03:28:18,772 ***** Eval results *****
2022-06-12 03:28:18,772   cls_loss = 0.013923371624615457
2022-06-12 03:28:18,772   corr = 0.8802808276439225
2022-06-12 03:28:18,772   eval_loss = 1.8138372403510072
2022-06-12 03:28:18,772   global_step = 1119
2022-06-12 03:28:18,772   loss = 0.013923371624615457
2022-06-12 03:28:18,772   pearson = 0.8781556894631852
2022-06-12 03:28:18,772   spearman = 0.8824059658246597
2022-06-12 03:28:23,757 ***** Running evaluation *****
2022-06-12 03:28:23,758   Epoch = 6 iter 1139 step
2022-06-12 03:28:23,758   Num examples = 1500
2022-06-12 03:28:23,758   Batch size = 32
2022-06-12 03:28:25,056 ***** Eval results *****
2022-06-12 03:28:25,056   cls_loss = 0.013702507223933935
2022-06-12 03:28:25,056   corr = 0.8811414867837
2022-06-12 03:28:25,057   eval_loss = 1.7588986082279936
2022-06-12 03:28:25,057   global_step = 1139
2022-06-12 03:28:25,057   loss = 0.013702507223933935
2022-06-12 03:28:25,057   pearson = 0.8783961455121332
2022-06-12 03:28:25,057   spearman = 0.8838868280552666
2022-06-12 03:28:29,987 ***** Running evaluation *****
2022-06-12 03:28:29,987   Epoch = 6 iter 1159 step
2022-06-12 03:28:29,987   Num examples = 1500
2022-06-12 03:28:29,987   Batch size = 32
2022-06-12 03:28:31,287 ***** Eval results *****
2022-06-12 03:28:31,288   cls_loss = 0.013452957077499697
2022-06-12 03:28:31,288   corr = 0.8804468793627614
2022-06-12 03:28:31,288   eval_loss = 1.704523983153891
2022-06-12 03:28:31,288   global_step = 1159
2022-06-12 03:28:31,288   loss = 0.013452957077499697
2022-06-12 03:28:31,288   pearson = 0.8766780852478793
2022-06-12 03:28:31,288   spearman = 0.8842156734776436
2022-06-12 03:28:36,233 ***** Running evaluation *****
2022-06-12 03:28:36,234   Epoch = 6 iter 1179 step
2022-06-12 03:28:36,234   Num examples = 1500
2022-06-12 03:28:36,234   Batch size = 32
2022-06-12 03:28:37,531 ***** Eval results *****
2022-06-12 03:28:37,531   cls_loss = 0.013363117769005752
2022-06-12 03:28:37,531   corr = 0.8774800846577023
2022-06-12 03:28:37,531   eval_loss = 1.8251058015417545
2022-06-12 03:28:37,532   global_step = 1179
2022-06-12 03:28:37,532   loss = 0.013363117769005752
2022-06-12 03:28:37,532   pearson = 0.8715133630201177
2022-06-12 03:28:37,532   spearman = 0.8834468062952869
2022-06-12 03:28:42,470 ***** Running evaluation *****
2022-06-12 03:28:42,471   Epoch = 6 iter 1199 step
2022-06-12 03:28:42,471   Num examples = 1500
2022-06-12 03:28:42,471   Batch size = 32
2022-06-12 03:28:43,768 ***** Eval results *****
2022-06-12 03:28:43,768   cls_loss = 0.013314375195652246
2022-06-12 03:28:43,768   corr = 0.8771582471215456
2022-06-12 03:28:43,768   eval_loss = 1.810353620255247
2022-06-12 03:28:43,768   global_step = 1199
2022-06-12 03:28:43,768   loss = 0.013314375195652246
2022-06-12 03:28:43,768   pearson = 0.8701288937385327
2022-06-12 03:28:43,768   spearman = 0.8841876005045585
2022-06-12 03:28:48,694 ***** Running evaluation *****
2022-06-12 03:28:48,694   Epoch = 6 iter 1219 step
2022-06-12 03:28:48,694   Num examples = 1500
2022-06-12 03:28:48,694   Batch size = 32
2022-06-12 03:28:49,991 ***** Eval results *****
2022-06-12 03:28:49,991   cls_loss = 0.013138870938263577
2022-06-12 03:28:49,991   corr = 0.8752428380119456
2022-06-12 03:28:49,991   eval_loss = 1.883660208671651
2022-06-12 03:28:49,991   global_step = 1219
2022-06-12 03:28:49,991   loss = 0.013138870938263577
2022-06-12 03:28:49,991   pearson = 0.8699539285728342
2022-06-12 03:28:49,991   spearman = 0.880531747451057
2022-06-12 03:28:54,959 ***** Running evaluation *****
2022-06-12 03:28:54,960   Epoch = 6 iter 1239 step
2022-06-12 03:28:54,960   Num examples = 1500
2022-06-12 03:28:54,960   Batch size = 32
2022-06-12 03:28:56,255 ***** Eval results *****
2022-06-12 03:28:56,255   cls_loss = 0.013126478932628577
2022-06-12 03:28:56,255   corr = 0.8783076524778326
2022-06-12 03:28:56,255   eval_loss = 1.773505371935824
2022-06-12 03:28:56,256   global_step = 1239
2022-06-12 03:28:56,256   loss = 0.013126478932628577
2022-06-12 03:28:56,256   pearson = 0.8747783484199763
2022-06-12 03:28:56,256   spearman = 0.881836956535689
2022-06-12 03:29:01,247 ***** Running evaluation *****
2022-06-12 03:29:01,247   Epoch = 7 iter 1259 step
2022-06-12 03:29:01,248   Num examples = 1500
2022-06-12 03:29:01,248   Batch size = 32
2022-06-12 03:29:02,544 ***** Eval results *****
2022-06-12 03:29:02,544   cls_loss = 0.017591354437172413
2022-06-12 03:29:02,544   corr = 0.8795473301504372
2022-06-12 03:29:02,544   eval_loss = 1.8962571722395876
2022-06-12 03:29:02,544   global_step = 1259
2022-06-12 03:29:02,544   loss = 0.017591354437172413
2022-06-12 03:29:02,544   pearson = 0.8769015095695106
2022-06-12 03:29:02,545   spearman = 0.8821931507313637
2022-06-12 03:29:07,556 ***** Running evaluation *****
2022-06-12 03:29:07,556   Epoch = 7 iter 1279 step
2022-06-12 03:29:07,556   Num examples = 1500
2022-06-12 03:29:07,557   Batch size = 32
2022-06-12 03:29:08,855 ***** Eval results *****
2022-06-12 03:29:08,855   cls_loss = 0.01476455949103603
2022-06-12 03:29:08,855   corr = 0.8847585851954765
2022-06-12 03:29:08,855   eval_loss = 1.8835160009404446
2022-06-12 03:29:08,855   global_step = 1279
2022-06-12 03:29:08,856   loss = 0.01476455949103603
2022-06-12 03:29:08,856   pearson = 0.8844015336595832
2022-06-12 03:29:08,856   spearman = 0.8851156367313698
2022-06-12 03:29:13,840 ***** Running evaluation *****
2022-06-12 03:29:13,841   Epoch = 7 iter 1299 step
2022-06-12 03:29:13,841   Num examples = 1500
2022-06-12 03:29:13,841   Batch size = 32
2022-06-12 03:29:15,141 ***** Eval results *****
2022-06-12 03:29:15,141   cls_loss = 0.014711192211784099
2022-06-12 03:29:15,141   corr = 0.8705759754773664
2022-06-12 03:29:15,141   eval_loss = 1.8117317950471918
2022-06-12 03:29:15,141   global_step = 1299
2022-06-12 03:29:15,141   loss = 0.014711192211784099
2022-06-12 03:29:15,141   pearson = 0.8639763072402633
2022-06-12 03:29:15,141   spearman = 0.8771756437144695
2022-06-12 03:29:19,897 ***** Running evaluation *****
2022-06-12 03:29:19,898   Epoch = 1 iter 3999 step
2022-06-12 03:29:19,898   Num examples = 5463
2022-06-12 03:29:19,898   Batch size = 32
2022-06-12 03:29:20,100 ***** Running evaluation *****
2022-06-12 03:29:20,101   Epoch = 7 iter 1319 step
2022-06-12 03:29:20,101   Num examples = 1500
2022-06-12 03:29:20,101   Batch size = 32
2022-06-12 03:29:21,399 ***** Eval results *****
2022-06-12 03:29:21,399   cls_loss = 0.01337419891955726
2022-06-12 03:29:21,399   corr = 0.8752041852458456
2022-06-12 03:29:21,400   eval_loss = 1.7738517446720854
2022-06-12 03:29:21,400   global_step = 1319
2022-06-12 03:29:21,400   loss = 0.01337419891955726
2022-06-12 03:29:21,400   pearson = 0.871123024255553
2022-06-12 03:29:21,400   spearman = 0.8792853462361383
2022-06-12 03:29:24,418 ***** Eval results *****
2022-06-12 03:29:24,418   acc = 0.8650924400512539
2022-06-12 03:29:24,418   cls_loss = 0.06597724070223485
2022-06-12 03:29:24,419   eval_loss = 0.44615129004532134
2022-06-12 03:29:24,419   global_step = 3999
2022-06-12 03:29:24,419   loss = 0.06597724070223485
2022-06-12 03:29:26,387 ***** Running evaluation *****
2022-06-12 03:29:26,388   Epoch = 7 iter 1339 step
2022-06-12 03:29:26,388   Num examples = 1500
2022-06-12 03:29:26,388   Batch size = 32
2022-06-12 03:29:27,684 ***** Eval results *****
2022-06-12 03:29:27,684   cls_loss = 0.013029378351517195
2022-06-12 03:29:27,684   corr = 0.8780506372464931
2022-06-12 03:29:27,684   eval_loss = 1.7664466969510342
2022-06-12 03:29:27,684   global_step = 1339
2022-06-12 03:29:27,684   loss = 0.013029378351517195
2022-06-12 03:29:27,684   pearson = 0.873419838727577
2022-06-12 03:29:27,685   spearman = 0.8826814357654091
2022-06-12 03:29:32,632 ***** Running evaluation *****
2022-06-12 03:29:32,633   Epoch = 7 iter 1359 step
2022-06-12 03:29:32,633   Num examples = 1500
2022-06-12 03:29:32,633   Batch size = 32
2022-06-12 03:29:33,929 ***** Eval results *****
2022-06-12 03:29:33,930   cls_loss = 0.012935220439619612
2022-06-12 03:29:33,930   corr = 0.8814463973152822
2022-06-12 03:29:33,930   eval_loss = 1.781425695470039
2022-06-12 03:29:33,930   global_step = 1359
2022-06-12 03:29:33,930   loss = 0.012935220439619612
2022-06-12 03:29:33,930   pearson = 0.8795681263199219
2022-06-12 03:29:33,930   spearman = 0.8833246683106424
2022-06-12 03:29:38,866 ***** Running evaluation *****
2022-06-12 03:29:38,866   Epoch = 7 iter 1379 step
2022-06-12 03:29:38,867   Num examples = 1500
2022-06-12 03:29:38,867   Batch size = 32
2022-06-12 03:29:40,163 ***** Eval results *****
2022-06-12 03:29:40,164   cls_loss = 0.012614326867541032
2022-06-12 03:29:40,164   corr = 0.8826381181567831
2022-06-12 03:29:40,164   eval_loss = 1.8423818920521027
2022-06-12 03:29:40,164   global_step = 1379
2022-06-12 03:29:40,164   loss = 0.012614326867541032
2022-06-12 03:29:40,164   pearson = 0.880632596348788
2022-06-12 03:29:40,164   spearman = 0.8846436399647781
2022-06-12 03:29:45,067 ***** Running evaluation *****
2022-06-12 03:29:45,067   Epoch = 7 iter 1399 step
2022-06-12 03:29:45,067   Num examples = 1500
2022-06-12 03:29:45,067   Batch size = 32
2022-06-12 03:29:46,364 ***** Eval results *****
2022-06-12 03:29:46,364   cls_loss = 0.012677007582842384
2022-06-12 03:29:46,364   corr = 0.8792342379935142
2022-06-12 03:29:46,364   eval_loss = 1.7254993788739468
2022-06-12 03:29:46,364   global_step = 1399
2022-06-12 03:29:46,364   loss = 0.012677007582842384
2022-06-12 03:29:46,364   pearson = 0.8765020389350523
2022-06-12 03:29:46,364   spearman = 0.8819664370519761
2022-06-12 03:29:51,300 ***** Running evaluation *****
2022-06-12 03:29:51,301   Epoch = 7 iter 1419 step
2022-06-12 03:29:51,301   Num examples = 1500
2022-06-12 03:29:51,301   Batch size = 32
2022-06-12 03:29:52,600 ***** Eval results *****
2022-06-12 03:29:52,600   cls_loss = 0.012513804778243494
2022-06-12 03:29:52,600   corr = 0.8796688249416882
2022-06-12 03:29:52,600   eval_loss = 1.7459162397587553
2022-06-12 03:29:52,600   global_step = 1419
2022-06-12 03:29:52,600   loss = 0.012513804778243494
2022-06-12 03:29:52,600   pearson = 0.8770544546059662
2022-06-12 03:29:52,600   spearman = 0.8822831952774101
2022-06-12 03:29:57,566 ***** Running evaluation *****
2022-06-12 03:29:57,567   Epoch = 8 iter 1439 step
2022-06-12 03:29:57,567   Num examples = 1500
2022-06-12 03:29:57,567   Batch size = 32
2022-06-12 03:29:58,868 ***** Eval results *****
2022-06-12 03:29:58,868   cls_loss = 0.01005232400660004
2022-06-12 03:29:58,868   corr = 0.8798355503612558
2022-06-12 03:29:58,868   eval_loss = 1.7507454664149182
2022-06-12 03:29:58,868   global_step = 1439
2022-06-12 03:29:58,868   loss = 0.01005232400660004
2022-06-12 03:29:58,869   pearson = 0.877618361119951
2022-06-12 03:29:58,869   spearman = 0.8820527396025607
2022-06-12 03:30:03,845 ***** Running evaluation *****
2022-06-12 03:30:03,845   Epoch = 8 iter 1459 step
2022-06-12 03:30:03,845   Num examples = 1500
2022-06-12 03:30:03,845   Batch size = 32
2022-06-12 03:30:05,144 ***** Eval results *****
2022-06-12 03:30:05,144   cls_loss = 0.012218000556997679
2022-06-12 03:30:05,144   corr = 0.8804033614581745
2022-06-12 03:30:05,144   eval_loss = 1.7489750676966729
2022-06-12 03:30:05,144   global_step = 1459
2022-06-12 03:30:05,144   loss = 0.012218000556997679
2022-06-12 03:30:05,144   pearson = 0.8789987397102379
2022-06-12 03:30:05,144   spearman = 0.8818079832061113
2022-06-12 03:30:10,083 ***** Running evaluation *****
2022-06-12 03:30:10,083   Epoch = 8 iter 1479 step
2022-06-12 03:30:10,084   Num examples = 1500
2022-06-12 03:30:10,084   Batch size = 32
2022-06-12 03:30:11,387 ***** Eval results *****
2022-06-12 03:30:11,387   cls_loss = 0.011659475994553971
2022-06-12 03:30:11,387   corr = 0.8810115986923457
2022-06-12 03:30:11,387   eval_loss = 1.764786957426274
2022-06-12 03:30:11,387   global_step = 1479
2022-06-12 03:30:11,387   loss = 0.011659475994553971
2022-06-12 03:30:11,387   pearson = 0.8786101080900994
2022-06-12 03:30:11,387   spearman = 0.8834130892945918
2022-06-12 03:30:16,322 ***** Running evaluation *****
2022-06-12 03:30:16,323   Epoch = 8 iter 1499 step
2022-06-12 03:30:16,323   Num examples = 1500
2022-06-12 03:30:16,323   Batch size = 32
2022-06-12 03:30:17,619 ***** Eval results *****
2022-06-12 03:30:17,620   cls_loss = 0.011572452060489067
2022-06-12 03:30:17,620   corr = 0.8808098371505068
2022-06-12 03:30:17,620   eval_loss = 1.8454274522497298
2022-06-12 03:30:17,620   global_step = 1499
2022-06-12 03:30:17,620   loss = 0.011572452060489067
2022-06-12 03:30:17,620   pearson = 0.8777255704400205
2022-06-12 03:30:17,620   spearman = 0.883894103860993
2022-06-12 03:30:22,529 ***** Running evaluation *****
2022-06-12 03:30:22,530   Epoch = 8 iter 1519 step
2022-06-12 03:30:22,530   Num examples = 1500
2022-06-12 03:30:22,530   Batch size = 32
2022-06-12 03:30:23,826 ***** Eval results *****
2022-06-12 03:30:23,827   cls_loss = 0.011470319585467893
2022-06-12 03:30:23,827   corr = 0.879260379132097
2022-06-12 03:30:23,827   eval_loss = 1.770226777868068
2022-06-12 03:30:23,827   global_step = 1519
2022-06-12 03:30:23,827   loss = 0.011470319585467893
2022-06-12 03:30:23,827   pearson = 0.8764334545808712
2022-06-12 03:30:23,827   spearman = 0.8820873036833227
2022-06-12 03:30:28,750 ***** Running evaluation *****
2022-06-12 03:30:28,751   Epoch = 8 iter 1539 step
2022-06-12 03:30:28,751   Num examples = 1500
2022-06-12 03:30:28,751   Batch size = 32
2022-06-12 03:30:30,047 ***** Eval results *****
2022-06-12 03:30:30,048   cls_loss = 0.011268161263351685
2022-06-12 03:30:30,048   corr = 0.8786780494823223
2022-06-12 03:30:30,048   eval_loss = 1.7998423310036356
2022-06-12 03:30:30,048   global_step = 1539
2022-06-12 03:30:30,048   loss = 0.011268161263351685
2022-06-12 03:30:30,048   pearson = 0.8746554977055991
2022-06-12 03:30:30,048   spearman = 0.8827006012590456
2022-06-12 03:30:34,976 ***** Running evaluation *****
2022-06-12 03:30:34,977   Epoch = 8 iter 1559 step
2022-06-12 03:30:34,977   Num examples = 1500
2022-06-12 03:30:34,977   Batch size = 32
2022-06-12 03:30:36,271 ***** Eval results *****
2022-06-12 03:30:36,272   cls_loss = 0.01149020164354345
2022-06-12 03:30:36,272   corr = 0.8823714110867439
2022-06-12 03:30:36,272   eval_loss = 1.8375952155032056
2022-06-12 03:30:36,272   global_step = 1559
2022-06-12 03:30:36,272   loss = 0.01149020164354345
2022-06-12 03:30:36,272   pearson = 0.8797767958655174
2022-06-12 03:30:36,272   spearman = 0.8849660263079704
2022-06-12 03:30:41,184 ***** Running evaluation *****
2022-06-12 03:30:41,184   Epoch = 8 iter 1579 step
2022-06-12 03:30:41,184   Num examples = 1500
2022-06-12 03:30:41,184   Batch size = 32
2022-06-12 03:30:42,479 ***** Eval results *****
2022-06-12 03:30:42,480   cls_loss = 0.011590813692076271
2022-06-12 03:30:42,480   corr = 0.8821721284722778
2022-06-12 03:30:42,480   eval_loss = 1.8484263940060393
2022-06-12 03:30:42,480   global_step = 1579
2022-06-12 03:30:42,480   loss = 0.011590813692076271
2022-06-12 03:30:42,480   pearson = 0.8812796323029445
2022-06-12 03:30:42,480   spearman = 0.8830646246416111
2022-06-12 03:30:47,413 ***** Running evaluation *****
2022-06-12 03:30:47,413   Epoch = 8 iter 1599 step
2022-06-12 03:30:47,413   Num examples = 1500
2022-06-12 03:30:47,413   Batch size = 32
2022-06-12 03:30:48,712 ***** Eval results *****
2022-06-12 03:30:48,712   cls_loss = 0.011591285706949448
2022-06-12 03:30:48,712   corr = 0.877473987250537
2022-06-12 03:30:48,712   eval_loss = 1.785086545538395
2022-06-12 03:30:48,712   global_step = 1599
2022-06-12 03:30:48,713   loss = 0.011591285706949448
2022-06-12 03:30:48,713   pearson = 0.8742184884795676
2022-06-12 03:30:48,713   spearman = 0.8807294860215065
2022-06-12 03:30:53,627 ***** Running evaluation *****
2022-06-12 03:30:53,627   Epoch = 9 iter 1619 step
2022-06-12 03:30:53,627   Num examples = 1500
2022-06-12 03:30:53,627   Batch size = 32
2022-06-12 03:30:54,922 ***** Eval results *****
2022-06-12 03:30:54,922   cls_loss = 0.010945659189019352
2022-06-12 03:30:54,922   corr = 0.87931007313401
2022-06-12 03:30:54,922   eval_loss = 1.7990773840153471
2022-06-12 03:30:54,923   global_step = 1619
2022-06-12 03:30:54,923   loss = 0.010945659189019352
2022-06-12 03:30:54,923   pearson = 0.8746415957296074
2022-06-12 03:30:54,923   spearman = 0.8839785505384126
2022-06-12 03:30:59,833 ***** Running evaluation *****
2022-06-12 03:30:59,833   Epoch = 9 iter 1639 step
2022-06-12 03:30:59,833   Num examples = 1500
2022-06-12 03:30:59,833   Batch size = 32
2022-06-12 03:31:01,130 ***** Eval results *****
2022-06-12 03:31:01,130   cls_loss = 0.011360795070816363
2022-06-12 03:31:01,130   corr = 0.8805909806401733
2022-06-12 03:31:01,130   eval_loss = 1.8850375274394422
2022-06-12 03:31:01,130   global_step = 1639
2022-06-12 03:31:01,131   loss = 0.011360795070816363
2022-06-12 03:31:01,131   pearson = 0.8777152936570125
2022-06-12 03:31:01,131   spearman = 0.8834666676233341
2022-06-12 03:31:06,022 ***** Running evaluation *****
2022-06-12 03:31:06,022   Epoch = 9 iter 1659 step
2022-06-12 03:31:06,023   Num examples = 1500
2022-06-12 03:31:06,023   Batch size = 32
2022-06-12 03:31:07,318 ***** Eval results *****
2022-06-12 03:31:07,319   cls_loss = 0.011218875045112023
2022-06-12 03:31:07,319   corr = 0.8836880411982198
2022-06-12 03:31:07,319   eval_loss = 1.7935680987987113
2022-06-12 03:31:07,319   global_step = 1659
2022-06-12 03:31:07,319   loss = 0.011218875045112023
2022-06-12 03:31:07,319   pearson = 0.882347445725032
2022-06-12 03:31:07,319   spearman = 0.8850286366714076
2022-06-12 03:31:12,240 ***** Running evaluation *****
2022-06-12 03:31:12,241   Epoch = 9 iter 1679 step
2022-06-12 03:31:12,241   Num examples = 1500
2022-06-12 03:31:12,241   Batch size = 32
2022-06-12 03:31:13,537 ***** Eval results *****
2022-06-12 03:31:13,537   cls_loss = 0.010906974702854367
2022-06-12 03:31:13,538   corr = 0.8822751148026313
2022-06-12 03:31:13,538   eval_loss = 1.8029570249800986
2022-06-12 03:31:13,538   global_step = 1679
2022-06-12 03:31:13,538   loss = 0.010906974702854367
2022-06-12 03:31:13,538   pearson = 0.8800193441665958
2022-06-12 03:31:13,538   spearman = 0.8845308854386668
2022-06-12 03:31:18,485 ***** Running evaluation *****
2022-06-12 03:31:18,486   Epoch = 9 iter 1699 step
2022-06-12 03:31:18,486   Num examples = 1500
2022-06-12 03:31:18,486   Batch size = 32
2022-06-12 03:31:19,784 ***** Eval results *****
2022-06-12 03:31:19,784   cls_loss = 0.010703353126617994
2022-06-12 03:31:19,784   corr = 0.8801773314734402
2022-06-12 03:31:19,784   eval_loss = 1.8489872445451452
2022-06-12 03:31:19,784   global_step = 1699
2022-06-12 03:31:19,785   loss = 0.010703353126617994
2022-06-12 03:31:19,785   pearson = 0.8769412676022554
2022-06-12 03:31:19,785   spearman = 0.8834133953446249
2022-06-12 03:31:23,341 ***** Running evaluation *****
2022-06-12 03:31:23,342   Epoch = 1 iter 4499 step
2022-06-12 03:31:23,342   Num examples = 5463
2022-06-12 03:31:23,342   Batch size = 32
2022-06-12 03:31:24,747 ***** Running evaluation *****
2022-06-12 03:31:24,747   Epoch = 9 iter 1719 step
2022-06-12 03:31:24,747   Num examples = 1500
2022-06-12 03:31:24,747   Batch size = 32
2022-06-12 03:31:26,045 ***** Eval results *****
2022-06-12 03:31:26,046   cls_loss = 0.010732823366264778
2022-06-12 03:31:26,046   corr = 0.8827092435659438
2022-06-12 03:31:26,046   eval_loss = 1.7804028670838539
2022-06-12 03:31:26,046   global_step = 1719
2022-06-12 03:31:26,046   loss = 0.010732823366264778
2022-06-12 03:31:26,046   pearson = 0.8804527597565075
2022-06-12 03:31:26,046   spearman = 0.88496572737538
2022-06-12 03:31:27,859 ***** Eval results *****
2022-06-12 03:31:27,859   acc = 0.8647263408383672
2022-06-12 03:31:27,859   cls_loss = 0.06536386826122711
2022-06-12 03:31:27,859   eval_loss = 0.39952335881385187
2022-06-12 03:31:27,859   global_step = 4499
2022-06-12 03:31:27,859   loss = 0.06536386826122711
2022-06-12 03:31:31,012 ***** Running evaluation *****
2022-06-12 03:31:31,013   Epoch = 9 iter 1739 step
2022-06-12 03:31:31,013   Num examples = 1500
2022-06-12 03:31:31,013   Batch size = 32
2022-06-12 03:31:32,311 ***** Eval results *****
2022-06-12 03:31:32,311   cls_loss = 0.010878238419536501
2022-06-12 03:31:32,311   corr = 0.8811259257323418
2022-06-12 03:31:32,311   eval_loss = 1.8038094424186868
2022-06-12 03:31:32,311   global_step = 1739
2022-06-12 03:31:32,311   loss = 0.010878238419536501
2022-06-12 03:31:32,311   pearson = 0.8786071143236023
2022-06-12 03:31:32,311   spearman = 0.8836447371410814
2022-06-12 03:31:37,253 ***** Running evaluation *****
2022-06-12 03:31:37,253   Epoch = 9 iter 1759 step
2022-06-12 03:31:37,253   Num examples = 1500
2022-06-12 03:31:37,253   Batch size = 32
2022-06-12 03:31:38,548 ***** Eval results *****
2022-06-12 03:31:38,548   cls_loss = 0.01082504157769821
2022-06-12 03:31:38,548   corr = 0.8800843390341769
2022-06-12 03:31:38,548   eval_loss = 1.8254074809399057
2022-06-12 03:31:38,548   global_step = 1759
2022-06-12 03:31:38,548   loss = 0.01082504157769821
2022-06-12 03:31:38,548   pearson = 0.878104476819744
2022-06-12 03:31:38,549   spearman = 0.8820642012486098
2022-06-12 03:31:43,504 ***** Running evaluation *****
2022-06-12 03:31:43,505   Epoch = 9 iter 1779 step
2022-06-12 03:31:43,505   Num examples = 1500
2022-06-12 03:31:43,505   Batch size = 32
2022-06-12 03:31:44,801 ***** Eval results *****
2022-06-12 03:31:44,801   cls_loss = 0.010917290005766387
2022-06-12 03:31:44,801   corr = 0.879849318332686
2022-06-12 03:31:44,801   eval_loss = 1.8051553698296243
2022-06-12 03:31:44,801   global_step = 1779
2022-06-12 03:31:44,801   loss = 0.010917290005766387
2022-06-12 03:31:44,801   pearson = 0.8753232469890462
2022-06-12 03:31:44,801   spearman = 0.8843753896763258
2022-06-12 03:31:49,698 ***** Running evaluation *****
2022-06-12 03:31:49,699   Epoch = 10 iter 1799 step
2022-06-12 03:31:49,699   Num examples = 1500
2022-06-12 03:31:49,699   Batch size = 32
2022-06-12 03:31:50,995 ***** Eval results *****
2022-06-12 03:31:50,995   cls_loss = 0.010276967556112342
2022-06-12 03:31:50,995   corr = 0.8821454155922253
2022-06-12 03:31:50,995   eval_loss = 1.769486887657896
2022-06-12 03:31:50,996   global_step = 1799
2022-06-12 03:31:50,996   loss = 0.010276967556112342
2022-06-12 03:31:50,996   pearson = 0.879721999100413
2022-06-12 03:31:50,996   spearman = 0.8845688320840376
2022-06-12 03:31:55,891 ***** Running evaluation *****
2022-06-12 03:31:55,892   Epoch = 10 iter 1819 step
2022-06-12 03:31:55,892   Num examples = 1500
2022-06-12 03:31:55,892   Batch size = 32
2022-06-12 03:31:57,188 ***** Eval results *****
2022-06-12 03:31:57,189   cls_loss = 0.010508037300716186
2022-06-12 03:31:57,189   corr = 0.8806924762248978
2022-06-12 03:31:57,189   eval_loss = 1.8010621907863211
2022-06-12 03:31:57,189   global_step = 1819
2022-06-12 03:31:57,189   loss = 0.010508037300716186
2022-06-12 03:31:57,189   pearson = 0.8777580783103496
2022-06-12 03:31:57,189   spearman = 0.883626874139446
2022-06-12 03:32:02,113 ***** Running evaluation *****
2022-06-12 03:32:02,113   Epoch = 10 iter 1839 step
2022-06-12 03:32:02,114   Num examples = 1500
2022-06-12 03:32:02,114   Batch size = 32
2022-06-12 03:32:03,411 ***** Eval results *****
2022-06-12 03:32:03,411   cls_loss = 0.010048295960438495
2022-06-12 03:32:03,411   corr = 0.8801466536452542
2022-06-12 03:32:03,411   eval_loss = 1.8227815082732668
2022-06-12 03:32:03,411   global_step = 1839
2022-06-12 03:32:03,411   loss = 0.010048295960438495
2022-06-12 03:32:03,411   pearson = 0.876938184228039
2022-06-12 03:32:03,411   spearman = 0.8833551230624694
2022-06-12 03:32:08,381 ***** Running evaluation *****
2022-06-12 03:32:08,382   Epoch = 10 iter 1859 step
2022-06-12 03:32:08,382   Num examples = 1500
2022-06-12 03:32:08,382   Batch size = 32
2022-06-12 03:32:09,679 ***** Eval results *****
2022-06-12 03:32:09,679   cls_loss = 0.009908311001524546
2022-06-12 03:32:09,679   corr = 0.8808506374623211
2022-06-12 03:32:09,679   eval_loss = 1.7052852967952161
2022-06-12 03:32:09,679   global_step = 1859
2022-06-12 03:32:09,679   loss = 0.009908311001524546
2022-06-12 03:32:09,679   pearson = 0.8785384117811594
2022-06-12 03:32:09,679   spearman = 0.8831628631434828
2022-06-12 03:32:14,634 ***** Running evaluation *****
2022-06-12 03:32:14,635   Epoch = 10 iter 1879 step
2022-06-12 03:32:14,635   Num examples = 1500
2022-06-12 03:32:14,635   Batch size = 32
2022-06-12 03:32:15,932 ***** Eval results *****
2022-06-12 03:32:15,932   cls_loss = 0.010210151281835658
2022-06-12 03:32:15,932   corr = 0.8780224606692129
2022-06-12 03:32:15,932   eval_loss = 1.8134467690548999
2022-06-12 03:32:15,932   global_step = 1879
2022-06-12 03:32:15,933   loss = 0.010210151281835658
2022-06-12 03:32:15,933   pearson = 0.8746294561964407
2022-06-12 03:32:15,933   spearman = 0.881415465141985
2022-06-12 03:32:20,883 ***** Running evaluation *****
2022-06-12 03:32:20,883   Epoch = 10 iter 1899 step
2022-06-12 03:32:20,883   Num examples = 1500
2022-06-12 03:32:20,883   Batch size = 32
2022-06-12 03:32:22,180 ***** Eval results *****
2022-06-12 03:32:22,180   cls_loss = 0.010020689017763105
2022-06-12 03:32:22,180   corr = 0.877702282358894
2022-06-12 03:32:22,180   eval_loss = 1.7945291780410928
2022-06-12 03:32:22,180   global_step = 1899
2022-06-12 03:32:22,180   loss = 0.010020689017763105
2022-06-12 03:32:22,180   pearson = 0.8739623540057682
2022-06-12 03:32:22,180   spearman = 0.88144221071202
2022-06-12 03:32:27,122 ***** Running evaluation *****
2022-06-12 03:32:27,122   Epoch = 10 iter 1919 step
2022-06-12 03:32:27,122   Num examples = 1500
2022-06-12 03:32:27,123   Batch size = 32
2022-06-12 03:32:28,421 ***** Eval results *****
2022-06-12 03:32:28,421   cls_loss = 0.009993468147753051
2022-06-12 03:32:28,421   corr = 0.879998527729942
2022-06-12 03:32:28,421   eval_loss = 1.7613572199293908
2022-06-12 03:32:28,421   global_step = 1919
2022-06-12 03:32:28,421   loss = 0.009993468147753051
2022-06-12 03:32:28,422   pearson = 0.8771325377177568
2022-06-12 03:32:28,422   spearman = 0.8828645177421272
2022-06-12 03:32:33,379 ***** Running evaluation *****
2022-06-12 03:32:33,379   Epoch = 10 iter 1939 step
2022-06-12 03:32:33,379   Num examples = 1500
2022-06-12 03:32:33,379   Batch size = 32
2022-06-12 03:32:34,677 ***** Eval results *****
2022-06-12 03:32:34,677   cls_loss = 0.010006265730390812
2022-06-12 03:32:34,677   corr = 0.882856045886558
2022-06-12 03:32:34,677   eval_loss = 1.8524760768768636
2022-06-12 03:32:34,677   global_step = 1939
2022-06-12 03:32:34,677   loss = 0.010006265730390812
2022-06-12 03:32:34,678   pearson = 0.8816663899428696
2022-06-12 03:32:34,678   spearman = 0.8840457018302463
2022-06-12 03:32:39,625 ***** Running evaluation *****
2022-06-12 03:32:39,625   Epoch = 10 iter 1959 step
2022-06-12 03:32:39,625   Num examples = 1500
2022-06-12 03:32:39,626   Batch size = 32
2022-06-12 03:32:40,922 ***** Eval results *****
2022-06-12 03:32:40,922   cls_loss = 0.009984731109338929
2022-06-12 03:32:40,922   corr = 0.8830801681333407
2022-06-12 03:32:40,922   eval_loss = 1.8342406166360734
2022-06-12 03:32:40,922   global_step = 1959
2022-06-12 03:32:40,922   loss = 0.009984731109338929
2022-06-12 03:32:40,922   pearson = 0.8807577117930294
2022-06-12 03:32:40,922   spearman = 0.885402624473652
2022-06-12 03:32:40,923 ***** Save model *****
2022-06-12 03:32:46,324 ***** Running evaluation *****
2022-06-12 03:32:46,325   Epoch = 11 iter 1979 step
2022-06-12 03:32:46,325   Num examples = 1500
2022-06-12 03:32:46,325   Batch size = 32
2022-06-12 03:32:47,622 ***** Eval results *****
2022-06-12 03:32:47,622   cls_loss = 0.009099109098315239
2022-06-12 03:32:47,622   corr = 0.8836319075373559
2022-06-12 03:32:47,622   eval_loss = 1.8681198916536697
2022-06-12 03:32:47,622   global_step = 1979
2022-06-12 03:32:47,622   loss = 0.009099109098315239
2022-06-12 03:32:47,622   pearson = 0.8816664075486615
2022-06-12 03:32:47,622   spearman = 0.8855974075260502
2022-06-12 03:32:47,622 ***** Save model *****
2022-06-12 03:32:53,062 ***** Running evaluation *****
2022-06-12 03:32:53,063   Epoch = 11 iter 1999 step
2022-06-12 03:32:53,063   Num examples = 1500
2022-06-12 03:32:53,063   Batch size = 32
2022-06-12 03:32:54,357 ***** Eval results *****
2022-06-12 03:32:54,357   cls_loss = 0.009846401851003368
2022-06-12 03:32:54,357   corr = 0.8820192761997706
2022-06-12 03:32:54,357   eval_loss = 1.7706561151971207
2022-06-12 03:32:54,357   global_step = 1999
2022-06-12 03:32:54,357   loss = 0.009846401851003368
2022-06-12 03:32:54,357   pearson = 0.8790217538145846
2022-06-12 03:32:54,358   spearman = 0.8850167985849566
2022-06-12 03:32:59,295 ***** Running evaluation *****
2022-06-12 03:32:59,296   Epoch = 11 iter 2019 step
2022-06-12 03:32:59,296   Num examples = 1500
2022-06-12 03:32:59,296   Batch size = 32
2022-06-12 03:33:00,592 ***** Eval results *****
2022-06-12 03:33:00,592   cls_loss = 0.010380911575630307
2022-06-12 03:33:00,592   corr = 0.8797739783801941
2022-06-12 03:33:00,592   eval_loss = 1.7708247912690995
2022-06-12 03:33:00,592   global_step = 2019
2022-06-12 03:33:00,592   loss = 0.010380911575630307
2022-06-12 03:33:00,592   pearson = 0.8751683454473351
2022-06-12 03:33:00,593   spearman = 0.884379611313053
2022-06-12 03:33:05,518 ***** Running evaluation *****
2022-06-12 03:33:05,518   Epoch = 11 iter 2039 step
2022-06-12 03:33:05,518   Num examples = 1500
2022-06-12 03:33:05,518   Batch size = 32
2022-06-12 03:33:06,814 ***** Eval results *****
2022-06-12 03:33:06,814   cls_loss = 0.01004556064227862
2022-06-12 03:33:06,814   corr = 0.879463625317257
2022-06-12 03:33:06,814   eval_loss = 1.7742953300476074
2022-06-12 03:33:06,814   global_step = 2039
2022-06-12 03:33:06,814   loss = 0.01004556064227862
2022-06-12 03:33:06,814   pearson = 0.8745661535690967
2022-06-12 03:33:06,814   spearman = 0.8843610970654172
2022-06-12 03:33:11,758 ***** Running evaluation *****
2022-06-12 03:33:11,758   Epoch = 11 iter 2059 step
2022-06-12 03:33:11,758   Num examples = 1500
2022-06-12 03:33:11,758   Batch size = 32
2022-06-12 03:33:13,053 ***** Eval results *****
2022-06-12 03:33:13,053   cls_loss = 0.010378711918989817
2022-06-12 03:33:13,053   corr = 0.8824358503911254
2022-06-12 03:33:13,053   eval_loss = 1.7918198349628043
2022-06-12 03:33:13,053   global_step = 2059
2022-06-12 03:33:13,054   loss = 0.010378711918989817
2022-06-12 03:33:13,054   pearson = 0.8799835926774495
2022-06-12 03:33:13,054   spearman = 0.8848881081048011
2022-06-12 03:33:17,978 ***** Running evaluation *****
2022-06-12 03:33:17,978   Epoch = 11 iter 2079 step
2022-06-12 03:33:17,978   Num examples = 1500
2022-06-12 03:33:17,978   Batch size = 32
2022-06-12 03:33:19,286 ***** Eval results *****
2022-06-12 03:33:19,286   cls_loss = 0.010204222662882371
2022-06-12 03:33:19,286   corr = 0.8829508853833399
2022-06-12 03:33:19,286   eval_loss = 1.8122131266492478
2022-06-12 03:33:19,286   global_step = 2079
2022-06-12 03:33:19,286   loss = 0.010204222662882371
2022-06-12 03:33:19,287   pearson = 0.880110724317134
2022-06-12 03:33:19,287   spearman = 0.8857910464495459
2022-06-12 03:33:19,287 ***** Save model *****
2022-06-12 03:33:24,926 ***** Running evaluation *****
2022-06-12 03:33:24,926   Epoch = 11 iter 2099 step
2022-06-12 03:33:24,926   Num examples = 1500
2022-06-12 03:33:24,926   Batch size = 32
2022-06-12 03:33:26,230 ***** Eval results *****
2022-06-12 03:33:26,230   cls_loss = 0.01004326705319377
2022-06-12 03:33:26,230   corr = 0.8837906809706072
2022-06-12 03:33:26,230   eval_loss = 1.8313130279804797
2022-06-12 03:33:26,230   global_step = 2099
2022-06-12 03:33:26,230   loss = 0.01004326705319377
2022-06-12 03:33:26,230   pearson = 0.8822593691957334
2022-06-12 03:33:26,231   spearman = 0.885321992745481
2022-06-12 03:33:26,542 ***** Running evaluation *****
2022-06-12 03:33:26,542   Epoch = 1 iter 4999 step
2022-06-12 03:33:26,542   Num examples = 5463
2022-06-12 03:33:26,542   Batch size = 32
2022-06-12 03:33:31,063 ***** Eval results *****
2022-06-12 03:33:31,063   acc = 0.8590518030386235
2022-06-12 03:33:31,063   cls_loss = 0.066119302132168
2022-06-12 03:33:31,064   eval_loss = 0.3753906256840592
2022-06-12 03:33:31,064   global_step = 4999
2022-06-12 03:33:31,064   loss = 0.066119302132168
2022-06-12 03:33:31,200 ***** Running evaluation *****
2022-06-12 03:33:31,201   Epoch = 11 iter 2119 step
2022-06-12 03:33:31,201   Num examples = 1500
2022-06-12 03:33:31,201   Batch size = 32
2022-06-12 03:33:32,501 ***** Eval results *****
2022-06-12 03:33:32,501   cls_loss = 0.00999536393210292
2022-06-12 03:33:32,501   corr = 0.8807774059305453
2022-06-12 03:33:32,501   eval_loss = 1.8123932138402412
2022-06-12 03:33:32,501   global_step = 2119
2022-06-12 03:33:32,502   loss = 0.00999536393210292
2022-06-12 03:33:32,502   pearson = 0.8770761115117718
2022-06-12 03:33:32,502   spearman = 0.8844787003493189
2022-06-12 03:33:37,434 ***** Running evaluation *****
2022-06-12 03:33:37,434   Epoch = 11 iter 2139 step
2022-06-12 03:33:37,435   Num examples = 1500
2022-06-12 03:33:37,435   Batch size = 32
2022-06-12 03:33:38,731 ***** Eval results *****
2022-06-12 03:33:38,731   cls_loss = 0.009978659497573972
2022-06-12 03:33:38,731   corr = 0.8827541528904155
2022-06-12 03:33:38,731   eval_loss = 1.7665335637457826
2022-06-12 03:33:38,731   global_step = 2139
2022-06-12 03:33:38,731   loss = 0.009978659497573972
2022-06-12 03:33:38,731   pearson = 0.8800624001385586
2022-06-12 03:33:38,731   spearman = 0.8854459056422723
2022-06-12 03:33:43,658 ***** Running evaluation *****
2022-06-12 03:33:43,659   Epoch = 12 iter 2159 step
2022-06-12 03:33:43,659   Num examples = 1500
2022-06-12 03:33:43,659   Batch size = 32
2022-06-12 03:33:44,954 ***** Eval results *****
2022-06-12 03:33:44,955   cls_loss = 0.008770342556421052
2022-06-12 03:33:44,955   corr = 0.880647107061185
2022-06-12 03:33:44,955   eval_loss = 1.7617544848868187
2022-06-12 03:33:44,955   global_step = 2159
2022-06-12 03:33:44,955   loss = 0.008770342556421052
2022-06-12 03:33:44,955   pearson = 0.8767897771005088
2022-06-12 03:33:44,955   spearman = 0.8845044370218613
2022-06-12 03:33:49,897 ***** Running evaluation *****
2022-06-12 03:33:49,898   Epoch = 12 iter 2179 step
2022-06-12 03:33:49,898   Num examples = 1500
2022-06-12 03:33:49,898   Batch size = 32
2022-06-12 03:33:51,192 ***** Eval results *****
2022-06-12 03:33:51,192   cls_loss = 0.00961967221941919
2022-06-12 03:33:51,193   corr = 0.8814083577325365
2022-06-12 03:33:51,193   eval_loss = 1.8266636450239953
2022-06-12 03:33:51,193   global_step = 2179
2022-06-12 03:33:51,193   loss = 0.00961967221941919
2022-06-12 03:33:51,193   pearson = 0.8789509532606244
2022-06-12 03:33:51,193   spearman = 0.8838657622044486
2022-06-12 03:33:56,089 ***** Running evaluation *****
2022-06-12 03:33:56,089   Epoch = 12 iter 2199 step
2022-06-12 03:33:56,089   Num examples = 1500
2022-06-12 03:33:56,090   Batch size = 32
2022-06-12 03:33:57,385 ***** Eval results *****
2022-06-12 03:33:57,385   cls_loss = 0.009430240962983054
2022-06-12 03:33:57,385   corr = 0.8822007888800447
2022-06-12 03:33:57,385   eval_loss = 1.8382118775489482
2022-06-12 03:33:57,386   global_step = 2199
2022-06-12 03:33:57,386   loss = 0.009430240962983054
2022-06-12 03:33:57,386   pearson = 0.8794850390307112
2022-06-12 03:33:57,386   spearman = 0.8849165387293784
2022-06-12 03:34:02,283 ***** Running evaluation *****
2022-06-12 03:34:02,284   Epoch = 12 iter 2219 step
2022-06-12 03:34:02,284   Num examples = 1500
2022-06-12 03:34:02,284   Batch size = 32
2022-06-12 03:34:03,579 ***** Eval results *****
2022-06-12 03:34:03,579   cls_loss = 0.00949061723281695
2022-06-12 03:34:03,579   corr = 0.8834979009498413
2022-06-12 03:34:03,579   eval_loss = 1.8098456846906783
2022-06-12 03:34:03,579   global_step = 2219
2022-06-12 03:34:03,580   loss = 0.00949061723281695
2022-06-12 03:34:03,580   pearson = 0.8817963697368936
2022-06-12 03:34:03,580   spearman = 0.885199432162789
2022-06-12 03:34:08,587 ***** Running evaluation *****
2022-06-12 03:34:08,587   Epoch = 12 iter 2239 step
2022-06-12 03:34:08,587   Num examples = 1500
2022-06-12 03:34:08,587   Batch size = 32
2022-06-12 03:34:09,887 ***** Eval results *****
2022-06-12 03:34:09,887   cls_loss = 0.009396513399494055
2022-06-12 03:34:09,887   corr = 0.8812856237059871
2022-06-12 03:34:09,887   eval_loss = 1.8088158394428009
2022-06-12 03:34:09,887   global_step = 2239
2022-06-12 03:34:09,887   loss = 0.009396513399494055
2022-06-12 03:34:09,887   pearson = 0.8768393985121251
2022-06-12 03:34:09,887   spearman = 0.885731848899849
2022-06-12 03:34:14,855 ***** Running evaluation *****
2022-06-12 03:34:14,855   Epoch = 12 iter 2259 step
2022-06-12 03:34:14,855   Num examples = 1500
2022-06-12 03:34:14,855   Batch size = 32
2022-06-12 03:34:16,153 ***** Eval results *****
2022-06-12 03:34:16,153   cls_loss = 0.009284556992268106
2022-06-12 03:34:16,153   corr = 0.882984243549771
2022-06-12 03:34:16,153   eval_loss = 1.7950448457230912
2022-06-12 03:34:16,153   global_step = 2259
2022-06-12 03:34:16,153   loss = 0.009284556992268106
2022-06-12 03:34:16,153   pearson = 0.8799341024039903
2022-06-12 03:34:16,153   spearman = 0.8860343846955518
2022-06-12 03:34:16,154 ***** Save model *****
2022-06-12 03:34:21,699 ***** Running evaluation *****
2022-06-12 03:34:21,699   Epoch = 12 iter 2279 step
2022-06-12 03:34:21,699   Num examples = 1500
2022-06-12 03:34:21,699   Batch size = 32
2022-06-12 03:34:22,999 ***** Eval results *****
2022-06-12 03:34:22,999   cls_loss = 0.009300002744705736
2022-06-12 03:34:22,999   corr = 0.8828082047700476
2022-06-12 03:34:22,999   eval_loss = 1.784682886397585
2022-06-12 03:34:22,999   global_step = 2279
2022-06-12 03:34:22,999   loss = 0.009300002744705736
2022-06-12 03:34:22,999   pearson = 0.880370858128438
2022-06-12 03:34:22,999   spearman = 0.8852455514116572
2022-06-12 03:34:27,929 ***** Running evaluation *****
2022-06-12 03:34:27,929   Epoch = 12 iter 2299 step
2022-06-12 03:34:27,929   Num examples = 1500
2022-06-12 03:34:27,930   Batch size = 32
2022-06-12 03:34:29,225 ***** Eval results *****
2022-06-12 03:34:29,225   cls_loss = 0.009245788140395126
2022-06-12 03:34:29,225   corr = 0.8829114897113787
2022-06-12 03:34:29,225   eval_loss = 1.8177078442370638
2022-06-12 03:34:29,226   global_step = 2299
2022-06-12 03:34:29,226   loss = 0.009245788140395126
2022-06-12 03:34:29,226   pearson = 0.8808594993798872
2022-06-12 03:34:29,226   spearman = 0.8849634800428702
2022-06-12 03:34:34,182 ***** Running evaluation *****
2022-06-12 03:34:34,182   Epoch = 12 iter 2319 step
2022-06-12 03:34:34,182   Num examples = 1500
2022-06-12 03:34:34,182   Batch size = 32
2022-06-12 03:34:35,479 ***** Eval results *****
2022-06-12 03:34:35,479   cls_loss = 0.009135787085254202
2022-06-12 03:34:35,479   corr = 0.8830580487602191
2022-06-12 03:34:35,479   eval_loss = 1.8220703994974177
2022-06-12 03:34:35,480   global_step = 2319
2022-06-12 03:34:35,480   loss = 0.009135787085254202
2022-06-12 03:34:35,480   pearson = 0.880828601240014
2022-06-12 03:34:35,480   spearman = 0.8852874962804241
2022-06-12 03:34:40,423 ***** Running evaluation *****
2022-06-12 03:34:40,423   Epoch = 13 iter 2339 step
2022-06-12 03:34:40,424   Num examples = 1500
2022-06-12 03:34:40,424   Batch size = 32
2022-06-12 03:34:41,719 ***** Eval results *****
2022-06-12 03:34:41,720   cls_loss = 0.008599981782026589
2022-06-12 03:34:41,720   corr = 0.881984968021515
2022-06-12 03:34:41,720   eval_loss = 1.8196090548596484
2022-06-12 03:34:41,720   global_step = 2339
2022-06-12 03:34:41,720   loss = 0.008599981782026589
2022-06-12 03:34:41,720   pearson = 0.87836746445655
2022-06-12 03:34:41,720   spearman = 0.8856024715864801
2022-06-12 03:34:46,656 ***** Running evaluation *****
2022-06-12 03:34:46,656   Epoch = 13 iter 2359 step
2022-06-12 03:34:46,656   Num examples = 1500
2022-06-12 03:34:46,656   Batch size = 32
2022-06-12 03:34:47,955 ***** Eval results *****
2022-06-12 03:34:47,955   cls_loss = 0.008470561166177504
2022-06-12 03:34:47,955   corr = 0.8832088249704556
2022-06-12 03:34:47,955   eval_loss = 1.8034300347591967
2022-06-12 03:34:47,955   global_step = 2359
2022-06-12 03:34:47,955   loss = 0.008470561166177504
2022-06-12 03:34:47,956   pearson = 0.8811103031129979
2022-06-12 03:34:47,956   spearman = 0.8853073468279133
2022-06-12 03:34:52,902 ***** Running evaluation *****
2022-06-12 03:34:52,902   Epoch = 13 iter 2379 step
2022-06-12 03:34:52,903   Num examples = 1500
2022-06-12 03:34:52,903   Batch size = 32
2022-06-12 03:34:54,199 ***** Eval results *****
2022-06-12 03:34:54,199   cls_loss = 0.009058193867811216
2022-06-12 03:34:54,199   corr = 0.8822202800128817
2022-06-12 03:34:54,199   eval_loss = 1.7731508194132055
2022-06-12 03:34:54,199   global_step = 2379
2022-06-12 03:34:54,199   loss = 0.009058193867811216
2022-06-12 03:34:54,199   pearson = 0.8798126353743075
2022-06-12 03:34:54,199   spearman = 0.8846279246514558
2022-06-12 03:34:59,137 ***** Running evaluation *****
2022-06-12 03:34:59,138   Epoch = 13 iter 2399 step
2022-06-12 03:34:59,138   Num examples = 1500
2022-06-12 03:34:59,138   Batch size = 32
2022-06-12 03:35:00,433 ***** Eval results *****
2022-06-12 03:35:00,433   cls_loss = 0.009210078588997325
2022-06-12 03:35:00,433   corr = 0.8817438337624379
2022-06-12 03:35:00,433   eval_loss = 1.7723269399176254
2022-06-12 03:35:00,433   global_step = 2399
2022-06-12 03:35:00,433   loss = 0.009210078588997325
2022-06-12 03:35:00,433   pearson = 0.8785299365962882
2022-06-12 03:35:00,434   spearman = 0.8849577309285875
2022-06-12 03:35:05,373 ***** Running evaluation *****
2022-06-12 03:35:05,374   Epoch = 13 iter 2419 step
2022-06-12 03:35:05,374   Num examples = 1500
2022-06-12 03:35:05,374   Batch size = 32
2022-06-12 03:35:06,670 ***** Eval results *****
2022-06-12 03:35:06,670   cls_loss = 0.009255911638636304
2022-06-12 03:35:06,670   corr = 0.8824744757459455
2022-06-12 03:35:06,670   eval_loss = 1.8101674153449687
2022-06-12 03:35:06,671   global_step = 2419
2022-06-12 03:35:06,671   loss = 0.009255911638636304
2022-06-12 03:35:06,671   pearson = 0.8796609587698436
2022-06-12 03:35:06,671   spearman = 0.8852879927220474
2022-06-12 03:35:11,642 ***** Running evaluation *****
2022-06-12 03:35:11,642   Epoch = 13 iter 2439 step
2022-06-12 03:35:11,642   Num examples = 1500
2022-06-12 03:35:11,642   Batch size = 32
2022-06-12 03:35:12,940 ***** Eval results *****
2022-06-12 03:35:12,941   cls_loss = 0.009226394671713933
2022-06-12 03:35:12,941   corr = 0.8824613239236823
2022-06-12 03:35:12,941   eval_loss = 1.790205530663754
2022-06-12 03:35:12,941   global_step = 2439
2022-06-12 03:35:12,941   loss = 0.009226394671713933
2022-06-12 03:35:12,941   pearson = 0.8800541998984623
2022-06-12 03:35:12,942   spearman = 0.8848684479489023
2022-06-12 03:35:17,882 ***** Running evaluation *****
2022-06-12 03:35:17,883   Epoch = 13 iter 2459 step
2022-06-12 03:35:17,883   Num examples = 1500
2022-06-12 03:35:17,883   Batch size = 32
2022-06-12 03:35:19,178 ***** Eval results *****
2022-06-12 03:35:19,178   cls_loss = 0.009092832347519245
2022-06-12 03:35:19,178   corr = 0.8824904284635127
2022-06-12 03:35:19,178   eval_loss = 1.8154207480714677
2022-06-12 03:35:19,178   global_step = 2459
2022-06-12 03:35:19,178   loss = 0.009092832347519245
2022-06-12 03:35:19,178   pearson = 0.8799511953438794
2022-06-12 03:35:19,178   spearman = 0.8850296615831459
2022-06-12 03:35:24,107 ***** Running evaluation *****
2022-06-12 03:35:24,108   Epoch = 13 iter 2479 step
2022-06-12 03:35:24,108   Num examples = 1500
2022-06-12 03:35:24,108   Batch size = 32
2022-06-12 03:35:25,404 ***** Eval results *****
2022-06-12 03:35:25,404   cls_loss = 0.009161985433341837
2022-06-12 03:35:25,404   corr = 0.8823893100676915
2022-06-12 03:35:25,404   eval_loss = 1.7875216933006937
2022-06-12 03:35:25,404   global_step = 2479
2022-06-12 03:35:25,404   loss = 0.009161985433341837
2022-06-12 03:35:25,404   pearson = 0.8797298517775025
2022-06-12 03:35:25,404   spearman = 0.8850487683578805
2022-06-12 03:35:29,756 ***** Running evaluation *****
2022-06-12 03:35:29,756   Epoch = 1 iter 5499 step
2022-06-12 03:35:29,756   Num examples = 5463
2022-06-12 03:35:29,756   Batch size = 32
2022-06-12 03:35:30,340 ***** Running evaluation *****
2022-06-12 03:35:30,341   Epoch = 13 iter 2499 step
2022-06-12 03:35:30,341   Num examples = 1500
2022-06-12 03:35:30,341   Batch size = 32
2022-06-12 03:35:31,638 ***** Eval results *****
2022-06-12 03:35:31,638   cls_loss = 0.009093577112612683
2022-06-12 03:35:31,638   corr = 0.8809818151195897
2022-06-12 03:35:31,639   eval_loss = 1.7845025290834142
2022-06-12 03:35:31,639   global_step = 2499
2022-06-12 03:35:31,639   loss = 0.009093577112612683
2022-06-12 03:35:31,639   pearson = 0.8773495383374711
2022-06-12 03:35:31,639   spearman = 0.8846140919017084
2022-06-12 03:35:34,277 ***** Eval results *****
2022-06-12 03:35:34,277   acc = 0.861980596741717
2022-06-12 03:35:34,277   cls_loss = 0.06624107666042535
2022-06-12 03:35:34,277   eval_loss = 0.4497404089687686
2022-06-12 03:35:34,277   global_step = 5499
2022-06-12 03:35:34,277   loss = 0.06624107666042535
2022-06-12 03:35:36,584 ***** Running evaluation *****
2022-06-12 03:35:36,585   Epoch = 14 iter 2519 step
2022-06-12 03:35:36,585   Num examples = 1500
2022-06-12 03:35:36,585   Batch size = 32
2022-06-12 03:35:37,880 ***** Eval results *****
2022-06-12 03:35:37,880   cls_loss = 0.008896843207856784
2022-06-12 03:35:37,880   corr = 0.8813794513160174
2022-06-12 03:35:37,880   eval_loss = 1.7750117017867717
2022-06-12 03:35:37,880   global_step = 2519
2022-06-12 03:35:37,880   loss = 0.008896843207856784
2022-06-12 03:35:37,880   pearson = 0.8780572946557734
2022-06-12 03:35:37,880   spearman = 0.8847016079762613
2022-06-12 03:35:42,810 ***** Running evaluation *****
2022-06-12 03:35:42,811   Epoch = 14 iter 2539 step
2022-06-12 03:35:42,811   Num examples = 1500
2022-06-12 03:35:42,811   Batch size = 32
2022-06-12 03:35:44,107 ***** Eval results *****
2022-06-12 03:35:44,107   cls_loss = 0.008721038431042072
2022-06-12 03:35:44,107   corr = 0.8812770878525438
2022-06-12 03:35:44,107   eval_loss = 1.8100388062761186
2022-06-12 03:35:44,107   global_step = 2539
2022-06-12 03:35:44,107   loss = 0.008721038431042072
2022-06-12 03:35:44,107   pearson = 0.8777393256578947
2022-06-12 03:35:44,107   spearman = 0.8848148500471927
2022-06-12 03:35:49,082 ***** Running evaluation *****
2022-06-12 03:35:49,082   Epoch = 14 iter 2559 step
2022-06-12 03:35:49,082   Num examples = 1500
2022-06-12 03:35:49,083   Batch size = 32
2022-06-12 03:35:50,382 ***** Eval results *****
2022-06-12 03:35:50,382   cls_loss = 0.009061555922874864
2022-06-12 03:35:50,382   corr = 0.8813895513154832
2022-06-12 03:35:50,382   eval_loss = 1.7900419856639618
2022-06-12 03:35:50,383   global_step = 2559
2022-06-12 03:35:50,383   loss = 0.009061555922874864
2022-06-12 03:35:50,383   pearson = 0.8780688088089288
2022-06-12 03:35:50,383   spearman = 0.8847102938220376
2022-06-12 03:35:55,399 ***** Running evaluation *****
2022-06-12 03:35:55,400   Epoch = 14 iter 2579 step
2022-06-12 03:35:55,400   Num examples = 1500
2022-06-12 03:35:55,400   Batch size = 32
2022-06-12 03:35:56,696 ***** Eval results *****
2022-06-12 03:35:56,696   cls_loss = 0.00927872462708452
2022-06-12 03:35:56,696   corr = 0.882237684522392
2022-06-12 03:35:56,697   eval_loss = 1.7936497723802607
2022-06-12 03:35:56,697   global_step = 2579
2022-06-12 03:35:56,697   loss = 0.00927872462708452
2022-06-12 03:35:56,697   pearson = 0.8797498197708833
2022-06-12 03:35:56,697   spearman = 0.8847255492739009
2022-06-12 03:36:01,636 ***** Running evaluation *****
2022-06-12 03:36:01,636   Epoch = 14 iter 2599 step
2022-06-12 03:36:01,636   Num examples = 1500
2022-06-12 03:36:01,636   Batch size = 32
2022-06-12 03:36:02,934 ***** Eval results *****
2022-06-12 03:36:02,934   cls_loss = 0.009211357152189618
2022-06-12 03:36:02,934   corr = 0.8819077724313806
2022-06-12 03:36:02,934   eval_loss = 1.7887447336886793
2022-06-12 03:36:02,934   global_step = 2599
2022-06-12 03:36:02,934   loss = 0.009211357152189618
2022-06-12 03:36:02,934   pearson = 0.8794935848349942
2022-06-12 03:36:02,934   spearman = 0.884321960027767
2022-06-12 03:36:07,885 ***** Running evaluation *****
2022-06-12 03:36:07,885   Epoch = 14 iter 2619 step
2022-06-12 03:36:07,885   Num examples = 1500
2022-06-12 03:36:07,885   Batch size = 32
2022-06-12 03:36:09,180 ***** Eval results *****
2022-06-12 03:36:09,181   cls_loss = 0.009147870890070907
2022-06-12 03:36:09,181   corr = 0.8818536667960485
2022-06-12 03:36:09,181   eval_loss = 1.7922780742036535
2022-06-12 03:36:09,181   global_step = 2619
2022-06-12 03:36:09,181   loss = 0.009147870890070907
2022-06-12 03:36:09,181   pearson = 0.8793085745790141
2022-06-12 03:36:09,181   spearman = 0.8843987590130828
2022-06-12 03:36:14,129 ***** Running evaluation *****
2022-06-12 03:36:14,129   Epoch = 14 iter 2639 step
2022-06-12 03:36:14,130   Num examples = 1500
2022-06-12 03:36:14,130   Batch size = 32
2022-06-12 03:36:15,429 ***** Eval results *****
2022-06-12 03:36:15,429   cls_loss = 0.009024240185079494
2022-06-12 03:36:15,429   corr = 0.8815691020811275
2022-06-12 03:36:15,429   eval_loss = 1.8034231852977833
2022-06-12 03:36:15,429   global_step = 2639
2022-06-12 03:36:15,429   loss = 0.009024240185079494
2022-06-12 03:36:15,429   pearson = 0.8787402778899596
2022-06-12 03:36:15,430   spearman = 0.8843979262722954
2022-06-12 03:36:20,394 ***** Running evaluation *****
2022-06-12 03:36:20,394   Epoch = 14 iter 2659 step
2022-06-12 03:36:20,394   Num examples = 1500
2022-06-12 03:36:20,394   Batch size = 32
2022-06-12 03:36:21,690 ***** Eval results *****
2022-06-12 03:36:21,690   cls_loss = 0.008921382948756218
2022-06-12 03:36:21,690   corr = 0.8814148804748293
2022-06-12 03:36:21,690   eval_loss = 1.7928516636503504
2022-06-12 03:36:21,690   global_step = 2659
2022-06-12 03:36:21,690   loss = 0.008921382948756218
2022-06-12 03:36:21,690   pearson = 0.8784199556656174
2022-06-12 03:36:21,691   spearman = 0.8844098052840412
2022-06-12 03:36:26,648 ***** Running evaluation *****
2022-06-12 03:36:26,649   Epoch = 14 iter 2679 step
2022-06-12 03:36:26,649   Num examples = 1500
2022-06-12 03:36:26,649   Batch size = 32
2022-06-12 03:36:27,944 ***** Eval results *****
2022-06-12 03:36:27,945   cls_loss = 0.008973205812017009
2022-06-12 03:36:27,945   corr = 0.8815429575242701
2022-06-12 03:36:27,945   eval_loss = 1.7931620656175817
2022-06-12 03:36:27,945   global_step = 2679
2022-06-12 03:36:27,945   loss = 0.008973205812017009
2022-06-12 03:36:27,945   pearson = 0.8786682022857395
2022-06-12 03:36:27,945   spearman = 0.8844177127628009
2022-06-12 03:36:29,427 **************S*************
task_name = sts-b
best_metirc = 0.8860343846955518
**************E*************

2022-06-12 03:36:29,464 Task finish! 
2022-06-12 03:36:29,465 Task cost 14.379880116666667 minutes, i.e. 0.23966467111111112 hours. 
2022-06-12 03:36:31,721 Task start! 
2022-06-12 03:36:31,744 device: cuda n_gpu: 1
2022-06-12 03:36:31,745 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/QQP', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=500, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=6, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/qqp/on_original_data', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='qqp', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/qqp/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/qqp/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 03:36:33,866 Writing example 0 of 363846
2022-06-12 03:36:33,867 *** Example ***
2022-06-12 03:36:33,867 guid: train-133273
2022-06-12 03:36:33,867 tokens: [CLS] how is the life of a math student ? could you describe your own experiences ? [SEP] which level of prep ##ration is enough for the exam j ##lp ##t ##5 ? [SEP]
2022-06-12 03:36:33,867 input_ids: 101 2129 2003 1996 2166 1997 1037 8785 3076 1029 2071 2017 6235 2115 2219 6322 1029 102 2029 2504 1997 17463 8156 2003 2438 2005 1996 11360 1046 14277 2102 2629 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:36:33,867 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:36:33,867 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:36:33,867 label: 0
2022-06-12 03:36:33,867 label_id: 0
2022-06-12 03:36:38,128 Writing example 10000 of 363846
2022-06-12 03:36:42,262 Writing example 20000 of 363846
2022-06-12 03:36:46,456 Writing example 30000 of 363846
2022-06-12 03:36:50,780 Writing example 40000 of 363846
2022-06-12 03:36:54,900 Writing example 50000 of 363846
2022-06-12 03:36:59,059 Writing example 60000 of 363846
2022-06-12 03:37:03,553 Writing example 70000 of 363846
2022-06-12 03:37:07,733 Writing example 80000 of 363846
2022-06-12 03:37:11,843 Writing example 90000 of 363846
2022-06-12 03:37:15,984 Writing example 100000 of 363846
2022-06-12 03:37:20,574 Writing example 110000 of 363846
2022-06-12 03:37:24,682 Writing example 120000 of 363846
2022-06-12 03:37:28,794 Writing example 130000 of 363846
2022-06-12 03:37:32,945 Writing example 140000 of 363846
2022-06-12 03:37:33,252 ***** Running evaluation *****
2022-06-12 03:37:33,253   Epoch = 1 iter 5999 step
2022-06-12 03:37:33,253   Num examples = 5463
2022-06-12 03:37:33,253   Batch size = 32
2022-06-12 03:37:37,051 Writing example 150000 of 363846
2022-06-12 03:37:37,767 ***** Eval results *****
2022-06-12 03:37:37,768   acc = 0.8605161998901703
2022-06-12 03:37:37,768   cls_loss = 0.06637179053860688
2022-06-12 03:37:37,768   eval_loss = 0.39908135438348813
2022-06-12 03:37:37,768   global_step = 5999
2022-06-12 03:37:37,768   loss = 0.06637179053860688
2022-06-12 03:37:41,180 Writing example 160000 of 363846
2022-06-12 03:37:45,955 Writing example 170000 of 363846
2022-06-12 03:37:50,048 Writing example 180000 of 363846
2022-06-12 03:37:54,163 Writing example 190000 of 363846
2022-06-12 03:37:58,301 Writing example 200000 of 363846
2022-06-12 03:38:02,441 Writing example 210000 of 363846
2022-06-12 03:38:06,594 Writing example 220000 of 363846
2022-06-12 03:38:11,496 Writing example 230000 of 363846
2022-06-12 03:38:15,609 Writing example 240000 of 363846
2022-06-12 03:38:19,725 Writing example 250000 of 363846
2022-06-12 03:38:23,861 Writing example 260000 of 363846
2022-06-12 03:38:27,985 Writing example 270000 of 363846
2022-06-12 03:38:32,139 Writing example 280000 of 363846
2022-06-12 03:38:36,295 Writing example 290000 of 363846
2022-06-12 03:38:40,422 Writing example 300000 of 363846
2022-06-12 03:38:44,536 Writing example 310000 of 363846
2022-06-12 03:38:49,887 Writing example 320000 of 363846
2022-06-12 03:38:54,020 Writing example 330000 of 363846
2022-06-12 03:38:58,190 Writing example 340000 of 363846
2022-06-12 03:39:02,338 Writing example 350000 of 363846
2022-06-12 03:39:06,475 Writing example 360000 of 363846
2022-06-12 03:39:10,881 Writing example 0 of 40430
2022-06-12 03:39:10,881 *** Example ***
2022-06-12 03:39:10,881 guid: dev-201359
2022-06-12 03:39:10,881 tokens: [CLS] why are african - americans so beautiful ? [SEP] why are hispanic ##s so beautiful ? [SEP]
2022-06-12 03:39:10,881 input_ids: 101 2339 2024 3060 1011 4841 2061 3376 1029 102 2339 2024 6696 2015 2061 3376 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:39:10,882 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:39:10,882 segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 03:39:10,882 label: 0
2022-06-12 03:39:10,882 label_id: 0
2022-06-12 03:39:15,022 Writing example 10000 of 40430
2022-06-12 03:39:19,194 Writing example 20000 of 40430
2022-06-12 03:39:23,344 Writing example 30000 of 40430
2022-06-12 03:39:28,887 Writing example 40000 of 40430
2022-06-12 03:39:29,362 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "qqp",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 03:39:34,574 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/qqp/on_original_data/pytorch_model.bin
2022-06-12 03:39:36,007 loading model...
2022-06-12 03:39:36,278 done!
2022-06-12 03:39:36,462 ***** Running evaluation *****
2022-06-12 03:39:36,463   Epoch = 1 iter 6499 step
2022-06-12 03:39:36,463   Num examples = 5463
2022-06-12 03:39:36,463   Batch size = 32
2022-06-12 03:39:39,551 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 03:39:40,655 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-12 03:39:40,845 loading model...
2022-06-12 03:39:40,885 done!
2022-06-12 03:39:40,885 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-12 03:39:40,885 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-12 03:39:40,977 ***** Eval results *****
2022-06-12 03:39:40,977   acc = 0.8627127951674904
2022-06-12 03:39:40,977   cls_loss = 0.06662914202815894
2022-06-12 03:39:40,977   eval_loss = 0.3538054169444313
2022-06-12 03:39:40,977   global_step = 6499
2022-06-12 03:39:40,977   loss = 0.06662914202815894
2022-06-12 03:39:42,281 ***** Running training *****
2022-06-12 03:39:42,297   Num examples = 363846
2022-06-12 03:39:42,307   Batch size = 32
2022-06-12 03:39:42,307   Num steps = 68220
2022-06-12 03:39:42,308 n: bert.embeddings.word_embeddings.weight
2022-06-12 03:39:42,319 n: bert.embeddings.position_embeddings.weight
2022-06-12 03:39:42,334 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 03:39:42,350 n: bert.embeddings.LayerNorm.weight
2022-06-12 03:39:42,353 n: bert.embeddings.LayerNorm.bias
2022-06-12 03:39:42,353 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 03:39:42,353 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 03:39:42,354 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 03:39:42,354 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 03:39:42,354 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 03:39:42,354 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 03:39:42,355 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 03:39:42,355 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 03:39:42,355 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 03:39:42,355 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 03:39:42,355 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 03:39:42,356 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 03:39:42,356 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 03:39:42,356 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 03:39:42,356 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 03:39:42,356 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 03:39:42,357 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 03:39:42,357 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 03:39:42,357 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 03:39:42,357 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 03:39:42,358 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 03:39:42,358 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 03:39:42,358 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 03:39:42,358 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 03:39:42,358 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 03:39:42,359 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 03:39:42,359 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 03:39:42,359 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 03:39:42,359 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 03:39:42,359 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 03:39:42,360 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 03:39:42,360 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 03:39:42,360 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 03:39:42,360 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 03:39:42,360 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 03:39:42,361 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 03:39:42,361 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 03:39:42,361 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 03:39:42,361 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 03:39:42,362 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 03:39:42,362 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 03:39:42,363 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 03:39:42,364 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 03:39:42,364 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 03:39:42,364 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 03:39:42,364 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 03:39:42,364 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 03:39:42,364 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 03:39:42,364 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 03:39:42,364 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 03:39:42,364 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 03:39:42,364 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 03:39:42,364 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 03:39:42,365 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 03:39:42,365 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 03:39:42,366 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 03:39:42,367 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 03:39:42,367 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 03:39:42,367 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 03:39:42,367 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 03:39:42,367 n: bert.pooler.dense.weight
2022-06-12 03:39:42,367 n: bert.pooler.dense.bias
2022-06-12 03:39:42,367 n: classifier.weight
2022-06-12 03:39:42,367 n: classifier.bias
2022-06-12 03:39:42,367 n: fit_denses.0.weight
2022-06-12 03:39:42,367 n: fit_denses.0.bias
2022-06-12 03:39:42,367 n: fit_denses.1.weight
2022-06-12 03:39:42,368 n: fit_denses.1.bias
2022-06-12 03:39:42,368 n: fit_denses.2.weight
2022-06-12 03:39:42,368 n: fit_denses.2.bias
2022-06-12 03:39:42,368 n: fit_denses.3.weight
2022-06-12 03:39:42,368 n: fit_denses.3.bias
2022-06-12 03:39:42,368 n: fit_denses.4.weight
2022-06-12 03:39:42,368 n: fit_denses.4.bias
2022-06-12 03:39:42,368 n: fit_denses.5.weight
2022-06-12 03:39:42,368 n: fit_denses.5.bias
2022-06-12 03:39:42,368 n: fit_denses.6.weight
2022-06-12 03:39:42,368 n: fit_denses.6.bias
2022-06-12 03:39:42,368 Total parameters: 72468738
2022-06-12 03:41:39,730 ***** Running evaluation *****
2022-06-12 03:41:39,731   Epoch = 2 iter 6999 step
2022-06-12 03:41:39,731   Num examples = 5463
2022-06-12 03:41:39,731   Batch size = 32
2022-06-12 03:41:44,253 ***** Eval results *****
2022-06-12 03:41:44,253   acc = 0.8641771920190372
2022-06-12 03:41:44,253   cls_loss = 0.04910996085481007
2022-06-12 03:41:44,253   eval_loss = 0.43818522264298637
2022-06-12 03:41:44,254   global_step = 6999
2022-06-12 03:41:44,254   loss = 0.04910996085481007
2022-06-12 03:41:53,910 ***** Running evaluation *****
2022-06-12 03:41:53,911   Epoch = 0 iter 499 step
2022-06-12 03:41:53,911   Num examples = 40430
2022-06-12 03:41:53,911   Batch size = 32
2022-06-12 03:41:53,912 ***** Eval results *****
2022-06-12 03:41:53,913   att_loss = 8.72072053385641
2022-06-12 03:41:53,913   global_step = 499
2022-06-12 03:41:53,913   loss = 10.351524585234618
2022-06-12 03:41:53,913   rep_loss = 1.6308040623674411
2022-06-12 03:41:53,913 ***** Save model *****
2022-06-12 03:43:42,941 ***** Running evaluation *****
2022-06-12 03:43:42,941   Epoch = 2 iter 7499 step
2022-06-12 03:43:42,941   Num examples = 5463
2022-06-12 03:43:42,941   Batch size = 32
2022-06-12 03:43:47,460 ***** Eval results *****
2022-06-12 03:43:47,461   acc = 0.8647263408383672
2022-06-12 03:43:47,461   cls_loss = 0.05298968545646883
2022-06-12 03:43:47,461   eval_loss = 0.43172701108351086
2022-06-12 03:43:47,461   global_step = 7499
2022-06-12 03:43:47,461   loss = 0.05298968545646883
2022-06-12 03:44:05,832 ***** Running evaluation *****
2022-06-12 03:44:05,833   Epoch = 0 iter 999 step
2022-06-12 03:44:05,833   Num examples = 40430
2022-06-12 03:44:05,833   Batch size = 32
2022-06-12 03:44:05,834 ***** Eval results *****
2022-06-12 03:44:05,834   att_loss = 7.926216154127149
2022-06-12 03:44:05,834   global_step = 999
2022-06-12 03:44:05,834   loss = 9.460423786002952
2022-06-12 03:44:05,834   rep_loss = 1.534207641780078
2022-06-12 03:44:05,834 ***** Save model *****
2022-06-12 03:45:46,494 ***** Running evaluation *****
2022-06-12 03:45:46,495   Epoch = 2 iter 7999 step
2022-06-12 03:45:46,495   Num examples = 5463
2022-06-12 03:45:46,495   Batch size = 32
2022-06-12 03:45:51,013 ***** Eval results *****
2022-06-12 03:45:51,013   acc = 0.861980596741717
2022-06-12 03:45:51,014   cls_loss = 0.053091616612709724
2022-06-12 03:45:51,014   eval_loss = 0.47002435486480504
2022-06-12 03:45:51,014   global_step = 7999
2022-06-12 03:45:51,014   loss = 0.053091616612709724
2022-06-12 03:46:17,839 ***** Running evaluation *****
2022-06-12 03:46:17,839   Epoch = 0 iter 1499 step
2022-06-12 03:46:17,839   Num examples = 40430
2022-06-12 03:46:17,839   Batch size = 32
2022-06-12 03:46:17,841 ***** Eval results *****
2022-06-12 03:46:17,841   att_loss = 7.53704346458939
2022-06-12 03:46:17,841   global_step = 1499
2022-06-12 03:46:17,841   loss = 9.01956288443954
2022-06-12 03:46:17,841   rep_loss = 1.482519426132696
2022-06-12 03:46:17,841 ***** Save model *****
2022-06-12 03:47:49,863 ***** Running evaluation *****
2022-06-12 03:47:49,864   Epoch = 2 iter 8499 step
2022-06-12 03:47:49,864   Num examples = 5463
2022-06-12 03:47:49,864   Batch size = 32
2022-06-12 03:47:54,386 ***** Eval results *****
2022-06-12 03:47:54,386   acc = 0.8716822258832143
2022-06-12 03:47:54,386   cls_loss = 0.054037820419064866
2022-06-12 03:47:54,386   eval_loss = 0.4036506345789683
2022-06-12 03:47:54,386   global_step = 8499
2022-06-12 03:47:54,386   loss = 0.054037820419064866
2022-06-12 03:48:29,661 ***** Running evaluation *****
2022-06-12 03:48:29,661   Epoch = 0 iter 1999 step
2022-06-12 03:48:29,661   Num examples = 40430
2022-06-12 03:48:29,661   Batch size = 32
2022-06-12 03:48:29,663 ***** Eval results *****
2022-06-12 03:48:29,663   att_loss = 7.274681227990781
2022-06-12 03:48:29,663   global_step = 1999
2022-06-12 03:48:29,663   loss = 8.721804761004007
2022-06-12 03:48:29,663   rep_loss = 1.4471235369491005
2022-06-12 03:48:29,663 ***** Save model *****
2022-06-12 03:49:53,199 ***** Running evaluation *****
2022-06-12 03:49:53,200   Epoch = 2 iter 8999 step
2022-06-12 03:49:53,200   Num examples = 5463
2022-06-12 03:49:53,200   Batch size = 32
2022-06-12 03:49:57,719 ***** Eval results *****
2022-06-12 03:49:57,719   acc = 0.8647263408383672
2022-06-12 03:49:57,719   cls_loss = 0.054377425129419434
2022-06-12 03:49:57,719   eval_loss = 0.389108442695953
2022-06-12 03:49:57,719   global_step = 8999
2022-06-12 03:49:57,720   loss = 0.054377425129419434
2022-06-12 03:50:41,616 ***** Running evaluation *****
2022-06-12 03:50:41,617   Epoch = 0 iter 2499 step
2022-06-12 03:50:41,617   Num examples = 40430
2022-06-12 03:50:41,617   Batch size = 32
2022-06-12 03:50:41,618 ***** Eval results *****
2022-06-12 03:50:41,618   att_loss = 7.077377531136356
2022-06-12 03:50:41,618   global_step = 2499
2022-06-12 03:50:41,618   loss = 8.497494461155739
2022-06-12 03:50:41,618   rep_loss = 1.4201169370793971
2022-06-12 03:50:41,619 ***** Save model *****
2022-06-12 03:51:56,447 ***** Running evaluation *****
2022-06-12 03:51:56,448   Epoch = 2 iter 9499 step
2022-06-12 03:51:56,448   Num examples = 5463
2022-06-12 03:51:56,448   Batch size = 32
2022-06-12 03:52:00,969 ***** Eval results *****
2022-06-12 03:52:00,969   acc = 0.862529745561047
2022-06-12 03:52:00,969   cls_loss = 0.05478819256507761
2022-06-12 03:52:00,969   eval_loss = 0.41877219902231677
2022-06-12 03:52:00,969   global_step = 9499
2022-06-12 03:52:00,969   loss = 0.05478819256507761
2022-06-12 03:52:53,359 ***** Running evaluation *****
2022-06-12 03:52:53,359   Epoch = 0 iter 2999 step
2022-06-12 03:52:53,359   Num examples = 40430
2022-06-12 03:52:53,359   Batch size = 32
2022-06-12 03:52:53,361 ***** Eval results *****
2022-06-12 03:52:53,361   att_loss = 6.927748590598467
2022-06-12 03:52:53,361   global_step = 2999
2022-06-12 03:52:53,361   loss = 8.326216854783606
2022-06-12 03:52:53,361   rep_loss = 1.3984682706245863
2022-06-12 03:52:53,361 ***** Save model *****
2022-06-12 03:53:59,854 ***** Running evaluation *****
2022-06-12 03:53:59,854   Epoch = 3 iter 9999 step
2022-06-12 03:53:59,854   Num examples = 5463
2022-06-12 03:53:59,854   Batch size = 32
2022-06-12 03:54:04,377 ***** Eval results *****
2022-06-12 03:54:04,377   acc = 0.867289035328574
2022-06-12 03:54:04,377   cls_loss = 0.04128099716165
2022-06-12 03:54:04,377   eval_loss = 0.45788219002516645
2022-06-12 03:54:04,377   global_step = 9999
2022-06-12 03:54:04,377   loss = 0.04128099716165
2022-06-12 03:55:05,378 ***** Running evaluation *****
2022-06-12 03:55:05,379   Epoch = 0 iter 3499 step
2022-06-12 03:55:05,379   Num examples = 40430
2022-06-12 03:55:05,379   Batch size = 32
2022-06-12 03:55:05,380 ***** Eval results *****
2022-06-12 03:55:05,380   att_loss = 6.803504294891226
2022-06-12 03:55:05,380   global_step = 3499
2022-06-12 03:55:05,380   loss = 8.183642162803787
2022-06-12 03:55:05,380   rep_loss = 1.3801378733296161
2022-06-12 03:55:05,380 ***** Save model *****
2022-06-12 03:56:03,367 ***** Running evaluation *****
2022-06-12 03:56:03,367   Epoch = 3 iter 10499 step
2022-06-12 03:56:03,368   Num examples = 5463
2022-06-12 03:56:03,368   Batch size = 32
2022-06-12 03:56:07,889 ***** Eval results *****
2022-06-12 03:56:07,889   acc = 0.861431447922387
2022-06-12 03:56:07,889   cls_loss = 0.044337864526930976
2022-06-12 03:56:07,889   eval_loss = 0.4550434735805145
2022-06-12 03:56:07,889   global_step = 10499
2022-06-12 03:56:07,889   loss = 0.044337864526930976
2022-06-12 03:57:17,780 ***** Running evaluation *****
2022-06-12 03:57:17,781   Epoch = 0 iter 3999 step
2022-06-12 03:57:17,781   Num examples = 40430
2022-06-12 03:57:17,781   Batch size = 32
2022-06-12 03:57:17,782 ***** Eval results *****
2022-06-12 03:57:17,782   att_loss = 6.696941014080234
2022-06-12 03:57:17,782   global_step = 3999
2022-06-12 03:57:17,782   loss = 8.061539291769124
2022-06-12 03:57:17,782   rep_loss = 1.3645982819814986
2022-06-12 03:57:17,783 ***** Save model *****
2022-06-12 03:58:06,744 ***** Running evaluation *****
2022-06-12 03:58:06,744   Epoch = 3 iter 10999 step
2022-06-12 03:58:06,744   Num examples = 5463
2022-06-12 03:58:06,744   Batch size = 32
2022-06-12 03:58:11,261 ***** Eval results *****
2022-06-12 03:58:11,261   acc = 0.8647263408383672
2022-06-12 03:58:11,261   cls_loss = 0.044625104534423955
2022-06-12 03:58:11,262   eval_loss = 0.43617297081570877
2022-06-12 03:58:11,262   global_step = 10999
2022-06-12 03:58:11,262   loss = 0.044625104534423955
2022-06-12 03:59:29,516 ***** Running evaluation *****
2022-06-12 03:59:29,517   Epoch = 0 iter 4499 step
2022-06-12 03:59:29,517   Num examples = 40430
2022-06-12 03:59:29,517   Batch size = 32
2022-06-12 03:59:29,518 ***** Eval results *****
2022-06-12 03:59:29,518   att_loss = 6.59731138608381
2022-06-12 03:59:29,518   global_step = 4499
2022-06-12 03:59:29,518   loss = 7.947615042663357
2022-06-12 03:59:29,518   rep_loss = 1.3503036595206972
2022-06-12 03:59:29,519 ***** Save model *****
2022-06-12 04:00:09,923 ***** Running evaluation *****
2022-06-12 04:00:09,924   Epoch = 3 iter 11499 step
2022-06-12 04:00:09,924   Num examples = 5463
2022-06-12 04:00:09,924   Batch size = 32
2022-06-12 04:00:14,439 ***** Eval results *****
2022-06-12 04:00:14,439   acc = 0.8601501006772836
2022-06-12 04:00:14,439   cls_loss = 0.04516890256976088
2022-06-12 04:00:14,440   eval_loss = 0.45575757186358784
2022-06-12 04:00:14,440   global_step = 11499
2022-06-12 04:00:14,440   loss = 0.04516890256976088
2022-06-12 04:01:41,143 ***** Running evaluation *****
2022-06-12 04:01:41,143   Epoch = 0 iter 4999 step
2022-06-12 04:01:41,143   Num examples = 40430
2022-06-12 04:01:41,143   Batch size = 32
2022-06-12 04:01:41,145 ***** Eval results *****
2022-06-12 04:01:41,145   att_loss = 6.522223245479937
2022-06-12 04:01:41,145   global_step = 4999
2022-06-12 04:01:41,145   loss = 7.860437966270622
2022-06-12 04:01:41,145   rep_loss = 1.338214722531489
2022-06-12 04:01:41,145 ***** Save model *****
2022-06-12 04:02:13,222 ***** Running evaluation *****
2022-06-12 04:02:13,222   Epoch = 3 iter 11999 step
2022-06-12 04:02:13,222   Num examples = 5463
2022-06-12 04:02:13,222   Batch size = 32
2022-06-12 04:02:17,743 ***** Eval results *****
2022-06-12 04:02:17,744   acc = 0.8716822258832143
2022-06-12 04:02:17,744   cls_loss = 0.04626040976973028
2022-06-12 04:02:17,744   eval_loss = 0.40711233250753226
2022-06-12 04:02:17,744   global_step = 11999
2022-06-12 04:02:17,744   loss = 0.04626040976973028
2022-06-12 04:03:53,508 ***** Running evaluation *****
2022-06-12 04:03:53,509   Epoch = 0 iter 5499 step
2022-06-12 04:03:53,509   Num examples = 40430
2022-06-12 04:03:53,509   Batch size = 32
2022-06-12 04:03:53,510 ***** Eval results *****
2022-06-12 04:03:53,510   att_loss = 6.438435376698504
2022-06-12 04:03:53,510   global_step = 5499
2022-06-12 04:03:53,510   loss = 7.765290423336192
2022-06-12 04:03:53,510   rep_loss = 1.3268550493258047
2022-06-12 04:03:53,511 ***** Save model *****
2022-06-12 04:04:16,893 ***** Running evaluation *****
2022-06-12 04:04:16,893   Epoch = 3 iter 12499 step
2022-06-12 04:04:16,893   Num examples = 5463
2022-06-12 04:04:16,894   Batch size = 32
2022-06-12 04:04:21,424 ***** Eval results *****
2022-06-12 04:04:21,424   acc = 0.8663737872963573
2022-06-12 04:04:21,424   cls_loss = 0.04655875166419393
2022-06-12 04:04:21,424   eval_loss = 0.4535891892747921
2022-06-12 04:04:21,424   global_step = 12499
2022-06-12 04:04:21,425   loss = 0.04655875166419393
2022-06-12 04:06:05,152 ***** Running evaluation *****
2022-06-12 04:06:05,152   Epoch = 0 iter 5999 step
2022-06-12 04:06:05,152   Num examples = 40430
2022-06-12 04:06:05,152   Batch size = 32
2022-06-12 04:06:05,153 ***** Eval results *****
2022-06-12 04:06:05,154   att_loss = 6.372053351396719
2022-06-12 04:06:05,154   global_step = 5999
2022-06-12 04:06:05,154   loss = 7.688878773490396
2022-06-12 04:06:05,154   rep_loss = 1.3168254248558193
2022-06-12 04:06:05,154 ***** Save model *****
2022-06-12 04:06:20,151 ***** Running evaluation *****
2022-06-12 04:06:20,152   Epoch = 3 iter 12999 step
2022-06-12 04:06:20,152   Num examples = 5463
2022-06-12 04:06:20,152   Batch size = 32
2022-06-12 04:06:24,679 ***** Eval results *****
2022-06-12 04:06:24,679   acc = 0.8738788211605345
2022-06-12 04:06:24,679   cls_loss = 0.04697312593647519
2022-06-12 04:06:24,679   eval_loss = 0.44147627796345984
2022-06-12 04:06:24,680   global_step = 12999
2022-06-12 04:06:24,680   loss = 0.04697312593647519
2022-06-12 04:08:16,995 ***** Running evaluation *****
2022-06-12 04:08:16,995   Epoch = 0 iter 6499 step
2022-06-12 04:08:16,995   Num examples = 40430
2022-06-12 04:08:16,995   Batch size = 32
2022-06-12 04:08:16,997 ***** Eval results *****
2022-06-12 04:08:16,997   att_loss = 6.312115980232838
2022-06-12 04:08:16,997   global_step = 6499
2022-06-12 04:08:16,997   loss = 7.619959967487462
2022-06-12 04:08:16,997   rep_loss = 1.3078439887954119
2022-06-12 04:08:16,997 ***** Save model *****
2022-06-12 04:08:23,671 ***** Running evaluation *****
2022-06-12 04:08:23,672   Epoch = 4 iter 13499 step
2022-06-12 04:08:23,672   Num examples = 5463
2022-06-12 04:08:23,672   Batch size = 32
2022-06-12 04:08:28,185 ***** Eval results *****
2022-06-12 04:08:28,186   acc = 0.861980596741717
2022-06-12 04:08:28,186   cls_loss = 0.041138094010092406
2022-06-12 04:08:28,186   eval_loss = 0.468841277672882
2022-06-12 04:08:28,186   global_step = 13499
2022-06-12 04:08:28,186   loss = 0.041138094010092406
2022-06-12 04:10:27,024 ***** Running evaluation *****
2022-06-12 04:10:27,025   Epoch = 4 iter 13999 step
2022-06-12 04:10:27,025   Num examples = 5463
2022-06-12 04:10:27,025   Batch size = 32
2022-06-12 04:10:28,647 ***** Running evaluation *****
2022-06-12 04:10:28,648   Epoch = 0 iter 6999 step
2022-06-12 04:10:28,648   Num examples = 40430
2022-06-12 04:10:28,648   Batch size = 32
2022-06-12 04:10:28,649 ***** Eval results *****
2022-06-12 04:10:28,649   att_loss = 6.250340585896655
2022-06-12 04:10:28,649   global_step = 6999
2022-06-12 04:10:28,649   loss = 7.54974412386682
2022-06-12 04:10:28,650   rep_loss = 1.2994035398607544
2022-06-12 04:10:28,650 ***** Save model *****
2022-06-12 04:10:31,542 ***** Eval results *****
2022-06-12 04:10:31,542   acc = 0.8590518030386235
2022-06-12 04:10:31,542   cls_loss = 0.041005263758090126
2022-06-12 04:10:31,542   eval_loss = 0.4928151559716428
2022-06-12 04:10:31,542   global_step = 13999
2022-06-12 04:10:31,542   loss = 0.041005263758090126
2022-06-12 04:12:30,472 ***** Running evaluation *****
2022-06-12 04:12:30,472   Epoch = 4 iter 14499 step
2022-06-12 04:12:30,472   Num examples = 5463
2022-06-12 04:12:30,472   Batch size = 32
2022-06-12 04:12:34,990 ***** Eval results *****
2022-06-12 04:12:34,991   acc = 0.8696686802123376
2022-06-12 04:12:34,991   cls_loss = 0.04171219525439102
2022-06-12 04:12:34,991   eval_loss = 0.4495408644365986
2022-06-12 04:12:34,991   global_step = 14499
2022-06-12 04:12:34,991   loss = 0.04171219525439102
2022-06-12 04:12:40,281 ***** Running evaluation *****
2022-06-12 04:12:40,281   Epoch = 0 iter 7499 step
2022-06-12 04:12:40,281   Num examples = 40430
2022-06-12 04:12:40,281   Batch size = 32
2022-06-12 04:12:40,282 ***** Eval results *****
2022-06-12 04:12:40,282   att_loss = 6.19429932659285
2022-06-12 04:12:40,282   global_step = 7499
2022-06-12 04:12:40,282   loss = 7.485884981324537
2022-06-12 04:12:40,283   rep_loss = 1.2915856573069504
2022-06-12 04:12:40,283 ***** Save model *****
2022-06-12 04:14:33,709 ***** Running evaluation *****
2022-06-12 04:14:33,710   Epoch = 4 iter 14999 step
2022-06-12 04:14:33,710   Num examples = 5463
2022-06-12 04:14:33,710   Batch size = 32
2022-06-12 04:14:38,225 ***** Eval results *****
2022-06-12 04:14:38,225   acc = 0.8583196046128501
2022-06-12 04:14:38,226   cls_loss = 0.041440796588587084
2022-06-12 04:14:38,226   eval_loss = 0.46483044191236383
2022-06-12 04:14:38,226   global_step = 14999
2022-06-12 04:14:38,226   loss = 0.041440796588587084
2022-06-12 04:14:52,253 ***** Running evaluation *****
2022-06-12 04:14:52,253   Epoch = 0 iter 7999 step
2022-06-12 04:14:52,253   Num examples = 40430
2022-06-12 04:14:52,253   Batch size = 32
2022-06-12 04:14:52,254 ***** Eval results *****
2022-06-12 04:14:52,255   att_loss = 6.140148497981002
2022-06-12 04:14:52,255   global_step = 7999
2022-06-12 04:14:52,255   loss = 7.424515064812732
2022-06-12 04:14:52,255   rep_loss = 1.2843665687244137
2022-06-12 04:14:52,255 ***** Save model *****
2022-06-12 04:16:36,897 ***** Running evaluation *****
2022-06-12 04:16:36,897   Epoch = 4 iter 15499 step
2022-06-12 04:16:36,897   Num examples = 5463
2022-06-12 04:16:36,897   Batch size = 32
2022-06-12 04:16:41,433 ***** Eval results *****
2022-06-12 04:16:41,434   acc = 0.8606992494966136
2022-06-12 04:16:41,434   cls_loss = 0.04180243163666365
2022-06-12 04:16:41,434   eval_loss = 0.4494186275970866
2022-06-12 04:16:41,434   global_step = 15499
2022-06-12 04:16:41,434   loss = 0.04180243163666365
2022-06-12 04:17:04,529 ***** Running evaluation *****
2022-06-12 04:17:04,529   Epoch = 0 iter 8499 step
2022-06-12 04:17:04,529   Num examples = 40430
2022-06-12 04:17:04,530   Batch size = 32
2022-06-12 04:17:04,531 ***** Eval results *****
2022-06-12 04:17:04,531   att_loss = 6.089562204981259
2022-06-12 04:17:04,531   global_step = 8499
2022-06-12 04:17:04,531   loss = 7.367119716523549
2022-06-12 04:17:04,531   rep_loss = 1.2775575133516783
2022-06-12 04:17:04,531 ***** Save model *****
2022-06-12 04:18:40,465 ***** Running evaluation *****
2022-06-12 04:18:40,465   Epoch = 4 iter 15999 step
2022-06-12 04:18:40,465   Num examples = 5463
2022-06-12 04:18:40,466   Batch size = 32
2022-06-12 04:18:44,981 ***** Eval results *****
2022-06-12 04:18:44,982   acc = 0.865641588870584
2022-06-12 04:18:44,982   cls_loss = 0.041448240925432644
2022-06-12 04:18:44,982   eval_loss = 0.4614638984257802
2022-06-12 04:18:44,982   global_step = 15999
2022-06-12 04:18:44,982   loss = 0.041448240925432644
2022-06-12 04:19:16,302 ***** Running evaluation *****
2022-06-12 04:19:16,303   Epoch = 0 iter 8999 step
2022-06-12 04:19:16,303   Num examples = 40430
2022-06-12 04:19:16,303   Batch size = 32
2022-06-12 04:19:16,304 ***** Eval results *****
2022-06-12 04:19:16,304   att_loss = 6.041544481096883
2022-06-12 04:19:16,304   global_step = 8999
2022-06-12 04:19:16,305   loss = 7.31274787764216
2022-06-12 04:19:16,305   rep_loss = 1.271203398651541
2022-06-12 04:19:16,305 ***** Save model *****
2022-06-12 04:20:43,867 ***** Running evaluation *****
2022-06-12 04:20:43,867   Epoch = 5 iter 16499 step
2022-06-12 04:20:43,867   Num examples = 5463
2022-06-12 04:20:43,867   Batch size = 32
2022-06-12 04:20:48,389 ***** Eval results *****
2022-06-12 04:20:48,389   acc = 0.8638110928061504
2022-06-12 04:20:48,389   cls_loss = 0.03793038715566717
2022-06-12 04:20:48,389   eval_loss = 0.45289947158433713
2022-06-12 04:20:48,389   global_step = 16499
2022-06-12 04:20:48,389   loss = 0.03793038715566717
2022-06-12 04:21:28,505 ***** Running evaluation *****
2022-06-12 04:21:28,506   Epoch = 0 iter 9499 step
2022-06-12 04:21:28,506   Num examples = 40430
2022-06-12 04:21:28,506   Batch size = 32
2022-06-12 04:21:28,507 ***** Eval results *****
2022-06-12 04:21:28,507   att_loss = 5.999952135192481
2022-06-12 04:21:28,507   global_step = 9499
2022-06-12 04:21:28,507   loss = 7.2653299078262155
2022-06-12 04:21:28,507   rep_loss = 1.265377775269165
2022-06-12 04:21:28,507 ***** Save model *****
2022-06-12 04:22:47,705 ***** Running evaluation *****
2022-06-12 04:22:47,706   Epoch = 5 iter 16999 step
2022-06-12 04:22:47,706   Num examples = 5463
2022-06-12 04:22:47,706   Batch size = 32
2022-06-12 04:22:52,224 ***** Eval results *****
2022-06-12 04:22:52,225   acc = 0.867289035328574
2022-06-12 04:22:52,225   cls_loss = 0.038308239098321
2022-06-12 04:22:52,225   eval_loss = 0.46388881110003466
2022-06-12 04:22:52,225   global_step = 16999
2022-06-12 04:22:52,225   loss = 0.038308239098321
2022-06-12 04:23:40,360 ***** Running evaluation *****
2022-06-12 04:23:40,361   Epoch = 0 iter 9999 step
2022-06-12 04:23:40,361   Num examples = 40430
2022-06-12 04:23:40,361   Batch size = 32
2022-06-12 04:23:40,362 ***** Eval results *****
2022-06-12 04:23:40,362   att_loss = 5.9579970169239065
2022-06-12 04:23:40,362   global_step = 9999
2022-06-12 04:23:40,363   loss = 7.217767380919382
2022-06-12 04:23:40,363   rep_loss = 1.2597703669879279
2022-06-12 04:23:40,363 ***** Save model *****
2022-06-12 04:24:50,834 ***** Running evaluation *****
2022-06-12 04:24:50,835   Epoch = 5 iter 17499 step
2022-06-12 04:24:50,835   Num examples = 5463
2022-06-12 04:24:50,835   Batch size = 32
2022-06-12 04:24:55,359 ***** Eval results *****
2022-06-12 04:24:55,359   acc = 0.8531942156324364
2022-06-12 04:24:55,359   cls_loss = 0.03809007156370914
2022-06-12 04:24:55,360   eval_loss = 0.46960604861269745
2022-06-12 04:24:55,360   global_step = 17499
2022-06-12 04:24:55,360   loss = 0.03809007156370914
2022-06-12 04:25:52,494 ***** Running evaluation *****
2022-06-12 04:25:52,495   Epoch = 0 iter 10499 step
2022-06-12 04:25:52,495   Num examples = 40430
2022-06-12 04:25:52,495   Batch size = 32
2022-06-12 04:25:52,496 ***** Eval results *****
2022-06-12 04:25:52,496   att_loss = 5.9180238277120285
2022-06-12 04:25:52,496   global_step = 10499
2022-06-12 04:25:52,496   loss = 7.172543815912911
2022-06-12 04:25:52,496   rep_loss = 1.2545199906193583
2022-06-12 04:25:52,496 ***** Save model *****
2022-06-12 04:26:54,293 ***** Running evaluation *****
2022-06-12 04:26:54,294   Epoch = 5 iter 17999 step
2022-06-12 04:26:54,294   Num examples = 5463
2022-06-12 04:26:54,294   Batch size = 32
2022-06-12 04:26:58,814 ***** Eval results *****
2022-06-12 04:26:58,815   acc = 0.8638110928061504
2022-06-12 04:26:58,815   cls_loss = 0.03844871431602806
2022-06-12 04:26:58,815   eval_loss = 0.4626916184672835
2022-06-12 04:26:58,815   global_step = 17999
2022-06-12 04:26:58,815   loss = 0.03844871431602806
2022-06-12 04:28:04,779 ***** Running evaluation *****
2022-06-12 04:28:04,780   Epoch = 0 iter 10999 step
2022-06-12 04:28:04,780   Num examples = 40430
2022-06-12 04:28:04,780   Batch size = 32
2022-06-12 04:28:04,781 ***** Eval results *****
2022-06-12 04:28:04,781   att_loss = 5.884451491365174
2022-06-12 04:28:04,781   global_step = 10999
2022-06-12 04:28:04,781   loss = 7.134090855637727
2022-06-12 04:28:04,782   rep_loss = 1.2496393666786327
2022-06-12 04:28:04,782 ***** Save model *****
2022-06-12 04:28:57,453 ***** Running evaluation *****
2022-06-12 04:28:57,454   Epoch = 5 iter 18499 step
2022-06-12 04:28:57,454   Num examples = 5463
2022-06-12 04:28:57,454   Batch size = 32
2022-06-12 04:29:01,971 ***** Eval results *****
2022-06-12 04:29:01,972   acc = 0.861980596741717
2022-06-12 04:29:01,972   cls_loss = 0.03872270312862019
2022-06-12 04:29:01,972   eval_loss = 0.4951130709779716
2022-06-12 04:29:01,972   global_step = 18499
2022-06-12 04:29:01,972   loss = 0.03872270312862019
2022-06-12 04:30:16,782 ***** Running evaluation *****
2022-06-12 04:30:16,782   Epoch = 1 iter 11499 step
2022-06-12 04:30:16,782   Num examples = 40430
2022-06-12 04:30:16,782   Batch size = 32
2022-06-12 04:30:16,784 ***** Eval results *****
2022-06-12 04:30:16,784   att_loss = 4.793019834414933
2022-06-12 04:30:16,784   global_step = 11499
2022-06-12 04:30:16,784   loss = 5.924628912016403
2022-06-12 04:30:16,784   rep_loss = 1.1316090692845426
2022-06-12 04:30:16,785 ***** Save model *****
2022-06-12 04:31:01,081 ***** Running evaluation *****
2022-06-12 04:31:01,082   Epoch = 5 iter 18999 step
2022-06-12 04:31:01,082   Num examples = 5463
2022-06-12 04:31:01,082   Batch size = 32
2022-06-12 04:31:05,603 ***** Eval results *****
2022-06-12 04:31:05,604   acc = 0.8689364817865641
2022-06-12 04:31:05,604   cls_loss = 0.03855399756594055
2022-06-12 04:31:05,604   eval_loss = 0.4626739057467172
2022-06-12 04:31:05,604   global_step = 18999
2022-06-12 04:31:05,604   loss = 0.03855399756594055
2022-06-12 04:32:29,323 ***** Running evaluation *****
2022-06-12 04:32:29,323   Epoch = 1 iter 11999 step
2022-06-12 04:32:29,323   Num examples = 40430
2022-06-12 04:32:29,323   Batch size = 32
2022-06-12 04:32:29,325 ***** Eval results *****
2022-06-12 04:32:29,325   att_loss = 4.78245126386136
2022-06-12 04:32:29,325   global_step = 11999
2022-06-12 04:32:29,325   loss = 5.912167125362281
2022-06-12 04:32:29,325   rep_loss = 1.1297158560047846
2022-06-12 04:32:29,325 ***** Save model *****
2022-06-12 04:33:04,632 ***** Running evaluation *****
2022-06-12 04:33:04,633   Epoch = 5 iter 19499 step
2022-06-12 04:33:04,633   Num examples = 5463
2022-06-12 04:33:04,633   Batch size = 32
2022-06-12 04:33:09,151 ***** Eval results *****
2022-06-12 04:33:09,151   acc = 0.8647263408383672
2022-06-12 04:33:09,152   cls_loss = 0.03847087732021148
2022-06-12 04:33:09,152   eval_loss = 0.4607382268966203
2022-06-12 04:33:09,152   global_step = 19499
2022-06-12 04:33:09,152   loss = 0.03847087732021148
2022-06-12 04:34:40,981 ***** Running evaluation *****
2022-06-12 04:34:40,982   Epoch = 1 iter 12499 step
2022-06-12 04:34:40,982   Num examples = 40430
2022-06-12 04:34:40,982   Batch size = 32
2022-06-12 04:34:40,983 ***** Eval results *****
2022-06-12 04:34:40,983   att_loss = 4.77242789255823
2022-06-12 04:34:40,983   global_step = 12499
2022-06-12 04:34:40,983   loss = 5.900515842691156
2022-06-12 04:34:40,983   rep_loss = 1.1280879470708625
2022-06-12 04:34:40,983 ***** Save model *****
2022-06-12 04:35:07,760 ***** Running evaluation *****
2022-06-12 04:35:07,760   Epoch = 6 iter 19999 step
2022-06-12 04:35:07,760   Num examples = 5463
2022-06-12 04:35:07,760   Batch size = 32
2022-06-12 04:35:12,279 ***** Eval results *****
2022-06-12 04:35:12,280   acc = 0.8674720849350174
2022-06-12 04:35:12,280   cls_loss = 0.03628109925403324
2022-06-12 04:35:12,280   eval_loss = 0.4672134128230357
2022-06-12 04:35:12,280   global_step = 19999
2022-06-12 04:35:12,280   loss = 0.03628109925403324
2022-06-12 04:36:52,973 ***** Running evaluation *****
2022-06-12 04:36:52,974   Epoch = 1 iter 12999 step
2022-06-12 04:36:52,974   Num examples = 40430
2022-06-12 04:36:52,974   Batch size = 32
2022-06-12 04:36:52,975 ***** Eval results *****
2022-06-12 04:36:52,975   att_loss = 4.768291768155411
2022-06-12 04:36:52,975   global_step = 12999
2022-06-12 04:36:52,975   loss = 5.894941089927528
2022-06-12 04:36:52,975   rep_loss = 1.1266493143809946
2022-06-12 04:36:52,976 ***** Save model *****
2022-06-12 04:37:10,965 ***** Running evaluation *****
2022-06-12 04:37:10,966   Epoch = 6 iter 20499 step
2022-06-12 04:37:10,966   Num examples = 5463
2022-06-12 04:37:10,966   Batch size = 32
2022-06-12 04:37:15,482 ***** Eval results *****
2022-06-12 04:37:15,482   acc = 0.8489840746842394
2022-06-12 04:37:15,482   cls_loss = 0.036326520034608525
2022-06-12 04:37:15,482   eval_loss = 0.5223285806370758
2022-06-12 04:37:15,482   global_step = 20499
2022-06-12 04:37:15,482   loss = 0.036326520034608525
2022-06-12 04:39:05,454 ***** Running evaluation *****
2022-06-12 04:39:05,454   Epoch = 1 iter 13499 step
2022-06-12 04:39:05,454   Num examples = 40430
2022-06-12 04:39:05,455   Batch size = 32
2022-06-12 04:39:05,456 ***** Eval results *****
2022-06-12 04:39:05,456   att_loss = 4.744090589350408
2022-06-12 04:39:05,456   global_step = 13499
2022-06-12 04:39:05,456   loss = 5.868389941762455
2022-06-12 04:39:05,456   rep_loss = 1.1242993472046894
2022-06-12 04:39:05,456 ***** Save model *****
2022-06-12 04:39:14,494 ***** Running evaluation *****
2022-06-12 04:39:14,494   Epoch = 6 iter 20999 step
2022-06-12 04:39:14,494   Num examples = 5463
2022-06-12 04:39:14,494   Batch size = 32
2022-06-12 04:39:19,016 ***** Eval results *****
2022-06-12 04:39:19,016   acc = 0.8641771920190372
2022-06-12 04:39:19,016   cls_loss = 0.03593247988474098
2022-06-12 04:39:19,016   eval_loss = 0.506643887140859
2022-06-12 04:39:19,016   global_step = 20999
2022-06-12 04:39:19,016   loss = 0.03593247988474098
2022-06-12 04:41:17,194 ***** Running evaluation *****
2022-06-12 04:41:17,195   Epoch = 1 iter 13999 step
2022-06-12 04:41:17,195   Num examples = 40430
2022-06-12 04:41:17,195   Batch size = 32
2022-06-12 04:41:17,196 ***** Eval results *****
2022-06-12 04:41:17,196   att_loss = 4.738849736112476
2022-06-12 04:41:17,196   global_step = 13999
2022-06-12 04:41:17,196   loss = 5.862060430250317
2022-06-12 04:41:17,196   rep_loss = 1.1232106886965643
2022-06-12 04:41:17,197 ***** Save model *****
2022-06-12 04:41:18,069 ***** Running evaluation *****
2022-06-12 04:41:18,069   Epoch = 6 iter 21499 step
2022-06-12 04:41:18,069   Num examples = 5463
2022-06-12 04:41:18,069   Batch size = 32
2022-06-12 04:41:22,590 ***** Eval results *****
2022-06-12 04:41:22,590   acc = 0.8654585392641406
2022-06-12 04:41:22,590   cls_loss = 0.03566716811361972
2022-06-12 04:41:22,590   eval_loss = 0.49526191410766535
2022-06-12 04:41:22,590   global_step = 21499
2022-06-12 04:41:22,591   loss = 0.03566716811361972
2022-06-12 04:43:21,730 ***** Running evaluation *****
2022-06-12 04:43:21,730   Epoch = 6 iter 21999 step
2022-06-12 04:43:21,730   Num examples = 5463
2022-06-12 04:43:21,730   Batch size = 32
2022-06-12 04:43:26,247 ***** Eval results *****
2022-06-12 04:43:26,247   acc = 0.8581365550064067
2022-06-12 04:43:26,247   cls_loss = 0.03560639491516975
2022-06-12 04:43:26,247   eval_loss = 0.5279914154589438
2022-06-12 04:43:26,247   global_step = 21999
2022-06-12 04:43:26,247   loss = 0.03560639491516975
2022-06-12 04:43:29,217 ***** Running evaluation *****
2022-06-12 04:43:29,217   Epoch = 1 iter 14499 step
2022-06-12 04:43:29,217   Num examples = 40430
2022-06-12 04:43:29,217   Batch size = 32
2022-06-12 04:43:29,219 ***** Eval results *****
2022-06-12 04:43:29,219   att_loss = 4.719437205170473
2022-06-12 04:43:29,219   global_step = 14499
2022-06-12 04:43:29,219   loss = 5.840971591641019
2022-06-12 04:43:29,219   rep_loss = 1.121534382013057
2022-06-12 04:43:29,220 ***** Save model *****
2022-06-12 04:45:25,011 ***** Running evaluation *****
2022-06-12 04:45:25,011   Epoch = 6 iter 22499 step
2022-06-12 04:45:25,011   Num examples = 5463
2022-06-12 04:45:25,011   Batch size = 32
2022-06-12 04:45:29,528 ***** Eval results *****
2022-06-12 04:45:29,528   acc = 0.8654585392641406
2022-06-12 04:45:29,528   cls_loss = 0.035879554098087234
2022-06-12 04:45:29,528   eval_loss = 0.4480036832065436
2022-06-12 04:45:29,528   global_step = 22499
2022-06-12 04:45:29,528   loss = 0.035879554098087234
2022-06-12 04:45:41,151 ***** Running evaluation *****
2022-06-12 04:45:41,152   Epoch = 1 iter 14999 step
2022-06-12 04:45:41,152   Num examples = 40430
2022-06-12 04:45:41,152   Batch size = 32
2022-06-12 04:45:41,153 ***** Eval results *****
2022-06-12 04:45:41,153   att_loss = 4.70516837211508
2022-06-12 04:45:41,153   global_step = 14999
2022-06-12 04:45:41,153   loss = 5.8254258228616305
2022-06-12 04:45:41,153   rep_loss = 1.1202574490383985
2022-06-12 04:45:41,154 ***** Save model *****
2022-06-12 04:47:28,303 ***** Running evaluation *****
2022-06-12 04:47:28,303   Epoch = 7 iter 22999 step
2022-06-12 04:47:28,303   Num examples = 5463
2022-06-12 04:47:28,303   Batch size = 32
2022-06-12 04:47:32,822 ***** Eval results *****
2022-06-12 04:47:32,822   acc = 0.8663737872963573
2022-06-12 04:47:32,822   cls_loss = 0.03676994106816975
2022-06-12 04:47:32,822   eval_loss = 0.45257726484876976
2022-06-12 04:47:32,822   global_step = 22999
2022-06-12 04:47:32,822   loss = 0.03676994106816975
2022-06-12 04:47:53,190 ***** Running evaluation *****
2022-06-12 04:47:53,190   Epoch = 1 iter 15499 step
2022-06-12 04:47:53,190   Num examples = 40430
2022-06-12 04:47:53,190   Batch size = 32
2022-06-12 04:47:53,191 ***** Eval results *****
2022-06-12 04:47:53,191   att_loss = 4.700476064610406
2022-06-12 04:47:53,191   global_step = 15499
2022-06-12 04:47:53,191   loss = 5.819524765592073
2022-06-12 04:47:53,191   rep_loss = 1.119048699336007
2022-06-12 04:47:53,191 ***** Save model *****
2022-06-12 04:49:31,647 ***** Running evaluation *****
2022-06-12 04:49:31,647   Epoch = 7 iter 23499 step
2022-06-12 04:49:31,647   Num examples = 5463
2022-06-12 04:49:31,648   Batch size = 32
2022-06-12 04:49:36,168 ***** Eval results *****
2022-06-12 04:49:36,168   acc = 0.8707669778509977
2022-06-12 04:49:36,169   cls_loss = 0.03378652101129192
2022-06-12 04:49:36,169   eval_loss = 0.4571599763387825
2022-06-12 04:49:36,169   global_step = 23499
2022-06-12 04:49:36,169   loss = 0.03378652101129192
2022-06-12 04:50:04,881 ***** Running evaluation *****
2022-06-12 04:50:04,881   Epoch = 1 iter 15999 step
2022-06-12 04:50:04,882   Num examples = 40430
2022-06-12 04:50:04,882   Batch size = 32
2022-06-12 04:50:04,883 ***** Eval results *****
2022-06-12 04:50:04,883   att_loss = 4.690830590014685
2022-06-12 04:50:04,883   global_step = 15999
2022-06-12 04:50:04,883   loss = 5.808387914881332
2022-06-12 04:50:04,883   rep_loss = 1.117557322548903
2022-06-12 04:50:04,883 ***** Save model *****
2022-06-12 04:51:35,467 ***** Running evaluation *****
2022-06-12 04:51:35,468   Epoch = 7 iter 23999 step
2022-06-12 04:51:35,468   Num examples = 5463
2022-06-12 04:51:35,468   Batch size = 32
2022-06-12 04:51:40,000 ***** Eval results *****
2022-06-12 04:51:40,000   acc = 0.8608822991030569
2022-06-12 04:51:40,000   cls_loss = 0.03366315445643576
2022-06-12 04:51:40,000   eval_loss = 0.5132132023790775
2022-06-12 04:51:40,000   global_step = 23999
2022-06-12 04:51:40,000   loss = 0.03366315445643576
2022-06-12 04:52:16,540 ***** Running evaluation *****
2022-06-12 04:52:16,541   Epoch = 1 iter 16499 step
2022-06-12 04:52:16,541   Num examples = 40430
2022-06-12 04:52:16,541   Batch size = 32
2022-06-12 04:52:16,542 ***** Eval results *****
2022-06-12 04:52:16,542   att_loss = 4.682436205300004
2022-06-12 04:52:16,542   global_step = 16499
2022-06-12 04:52:16,542   loss = 5.798792781316646
2022-06-12 04:52:16,542   rep_loss = 1.116356573971328
2022-06-12 04:52:16,542 ***** Save model *****
2022-06-12 04:53:38,851 ***** Running evaluation *****
2022-06-12 04:53:38,852   Epoch = 7 iter 24499 step
2022-06-12 04:53:38,852   Num examples = 5463
2022-06-12 04:53:38,852   Batch size = 32
2022-06-12 04:53:43,367 ***** Eval results *****
2022-06-12 04:53:43,367   acc = 0.8764415156507414
2022-06-12 04:53:43,367   cls_loss = 0.033893998541050505
2022-06-12 04:53:43,367   eval_loss = 0.4191820134479574
2022-06-12 04:53:43,367   global_step = 24499
2022-06-12 04:53:43,367   loss = 0.033893998541050505
2022-06-12 04:53:43,368 ***** Save model *****
2022-06-12 04:54:29,097 ***** Running evaluation *****
2022-06-12 04:54:29,097   Epoch = 1 iter 16999 step
2022-06-12 04:54:29,098   Num examples = 40430
2022-06-12 04:54:29,098   Batch size = 32
2022-06-12 04:54:29,099 ***** Eval results *****
2022-06-12 04:54:29,099   att_loss = 4.6697952051869205
2022-06-12 04:54:29,099   global_step = 16999
2022-06-12 04:54:29,099   loss = 5.784712931554553
2022-06-12 04:54:29,099   rep_loss = 1.1149177238051307
2022-06-12 04:54:29,099 ***** Save model *****
2022-06-12 04:55:42,713 ***** Running evaluation *****
2022-06-12 04:55:42,713   Epoch = 7 iter 24999 step
2022-06-12 04:55:42,713   Num examples = 5463
2022-06-12 04:55:42,713   Batch size = 32
2022-06-12 04:55:47,244 ***** Eval results *****
2022-06-12 04:55:47,244   acc = 0.8711330770638843
2022-06-12 04:55:47,244   cls_loss = 0.033749635976300325
2022-06-12 04:55:47,244   eval_loss = 0.47106979644655833
2022-06-12 04:55:47,244   global_step = 24999
2022-06-12 04:55:47,244   loss = 0.033749635976300325
2022-06-12 04:56:41,210 ***** Running evaluation *****
2022-06-12 04:56:41,211   Epoch = 1 iter 17499 step
2022-06-12 04:56:41,211   Num examples = 40430
2022-06-12 04:56:41,211   Batch size = 32
2022-06-12 04:56:41,212 ***** Eval results *****
2022-06-12 04:56:41,212   att_loss = 4.658383710949587
2022-06-12 04:56:41,212   global_step = 17499
2022-06-12 04:56:41,213   loss = 5.771869727071064
2022-06-12 04:56:41,213   rep_loss = 1.1134860134762716
2022-06-12 04:56:41,213 ***** Save model *****
2022-06-12 04:57:45,849 ***** Running evaluation *****
2022-06-12 04:57:45,849   Epoch = 7 iter 25499 step
2022-06-12 04:57:45,849   Num examples = 5463
2022-06-12 04:57:45,849   Batch size = 32
2022-06-12 04:57:50,367 ***** Eval results *****
2022-06-12 04:57:50,367   acc = 0.865641588870584
2022-06-12 04:57:50,367   cls_loss = 0.033791924813126074
2022-06-12 04:57:50,368   eval_loss = 0.4907019155437661
2022-06-12 04:57:50,368   global_step = 25499
2022-06-12 04:57:50,368   loss = 0.033791924813126074
2022-06-12 04:58:52,985 ***** Running evaluation *****
2022-06-12 04:58:52,985   Epoch = 1 iter 17999 step
2022-06-12 04:58:52,985   Num examples = 40430
2022-06-12 04:58:52,985   Batch size = 32
2022-06-12 04:58:52,987 ***** Eval results *****
2022-06-12 04:58:52,987   att_loss = 4.6465895285630125
2022-06-12 04:58:52,987   global_step = 17999
2022-06-12 04:58:52,987   loss = 5.758627871363604
2022-06-12 04:58:52,987   rep_loss = 1.1120383407864964
2022-06-12 04:58:52,987 ***** Save model *****
2022-06-12 04:59:49,228 ***** Running evaluation *****
2022-06-12 04:59:49,229   Epoch = 7 iter 25999 step
2022-06-12 04:59:49,229   Num examples = 5463
2022-06-12 04:59:49,229   Batch size = 32
2022-06-12 04:59:53,753 ***** Eval results *****
2022-06-12 04:59:53,753   acc = 0.8665568369028006
2022-06-12 04:59:53,754   cls_loss = 0.03385815938266351
2022-06-12 04:59:53,754   eval_loss = 0.4647044894661297
2022-06-12 04:59:53,754   global_step = 25999
2022-06-12 04:59:53,754   loss = 0.03385815938266351
2022-06-12 05:01:05,242 ***** Running evaluation *****
2022-06-12 05:01:05,242   Epoch = 1 iter 18499 step
2022-06-12 05:01:05,242   Num examples = 40430
2022-06-12 05:01:05,242   Batch size = 32
2022-06-12 05:01:05,244 ***** Eval results *****
2022-06-12 05:01:05,244   att_loss = 4.639702801551236
2022-06-12 05:01:05,244   global_step = 18499
2022-06-12 05:01:05,244   loss = 5.750660278834272
2022-06-12 05:01:05,244   rep_loss = 1.1109574753265927
2022-06-12 05:01:05,245 ***** Save model *****
2022-06-12 05:01:52,105 ***** Running evaluation *****
2022-06-12 05:01:52,106   Epoch = 8 iter 26499 step
2022-06-12 05:01:52,106   Num examples = 5463
2022-06-12 05:01:52,106   Batch size = 32
2022-06-12 05:01:56,620 ***** Eval results *****
2022-06-12 05:01:56,620   acc = 0.8771737140765148
2022-06-12 05:01:56,620   cls_loss = 0.03304157461084071
2022-06-12 05:01:56,620   eval_loss = 0.4369967408064339
2022-06-12 05:01:56,620   global_step = 26499
2022-06-12 05:01:56,621   loss = 0.03304157461084071
2022-06-12 05:01:56,621 ***** Save model *****
2022-06-12 05:03:17,163 ***** Running evaluation *****
2022-06-12 05:03:17,164   Epoch = 1 iter 18999 step
2022-06-12 05:03:17,164   Num examples = 40430
2022-06-12 05:03:17,164   Batch size = 32
2022-06-12 05:03:17,165 ***** Eval results *****
2022-06-12 05:03:17,165   att_loss = 4.6297876875809
2022-06-12 05:03:17,165   global_step = 18999
2022-06-12 05:03:17,166   loss = 5.739403466375803
2022-06-12 05:03:17,166   rep_loss = 1.1096157766229158
2022-06-12 05:03:17,166 ***** Save model *****
2022-06-12 05:03:55,403 ***** Running evaluation *****
2022-06-12 05:03:55,403   Epoch = 8 iter 26999 step
2022-06-12 05:03:55,403   Num examples = 5463
2022-06-12 05:03:55,403   Batch size = 32
2022-06-12 05:03:59,915 ***** Eval results *****
2022-06-12 05:03:59,915   acc = 0.8654585392641406
2022-06-12 05:03:59,915   cls_loss = 0.0330117157060493
2022-06-12 05:03:59,915   eval_loss = 0.4884657544732007
2022-06-12 05:03:59,915   global_step = 26999
2022-06-12 05:03:59,916   loss = 0.0330117157060493
2022-06-12 05:05:29,168 ***** Running evaluation *****
2022-06-12 05:05:29,169   Epoch = 1 iter 19499 step
2022-06-12 05:05:29,169   Num examples = 40430
2022-06-12 05:05:29,169   Batch size = 32
2022-06-12 05:05:29,170 ***** Eval results *****
2022-06-12 05:05:29,171   att_loss = 4.618794136344176
2022-06-12 05:05:29,171   global_step = 19499
2022-06-12 05:05:29,171   loss = 5.727038686808397
2022-06-12 05:05:29,171   rep_loss = 1.1082445482278547
2022-06-12 05:05:29,171 ***** Save model *****
2022-06-12 05:05:58,321 ***** Running evaluation *****
2022-06-12 05:05:58,321   Epoch = 8 iter 27499 step
2022-06-12 05:05:58,321   Num examples = 5463
2022-06-12 05:05:58,321   Batch size = 32
2022-06-12 05:06:02,835 ***** Eval results *****
2022-06-12 05:06:02,836   acc = 0.8707669778509977
2022-06-12 05:06:02,836   cls_loss = 0.03306659154678932
2022-06-12 05:06:02,836   eval_loss = 0.48220554843806385
2022-06-12 05:06:02,836   global_step = 27499
2022-06-12 05:06:02,836   loss = 0.03306659154678932
2022-06-12 05:07:42,432 ***** Running evaluation *****
2022-06-12 05:07:42,433   Epoch = 1 iter 19999 step
2022-06-12 05:07:42,433   Num examples = 40430
2022-06-12 05:07:42,433   Batch size = 32
2022-06-12 05:07:42,434 ***** Eval results *****
2022-06-12 05:07:42,434   att_loss = 4.609073598769742
2022-06-12 05:07:42,435   global_step = 19999
2022-06-12 05:07:42,435   loss = 5.716181184646221
2022-06-12 05:07:42,435   rep_loss = 1.1071075836246405
2022-06-12 05:07:42,435 ***** Save model *****
2022-06-12 05:08:01,251 ***** Running evaluation *****
2022-06-12 05:08:01,251   Epoch = 8 iter 27999 step
2022-06-12 05:08:01,251   Num examples = 5463
2022-06-12 05:08:01,251   Batch size = 32
2022-06-12 05:08:05,763 ***** Eval results *****
2022-06-12 05:08:05,764   acc = 0.8707669778509977
2022-06-12 05:08:05,764   cls_loss = 0.03276743224357606
2022-06-12 05:08:05,764   eval_loss = 0.4944800352732166
2022-06-12 05:08:05,764   global_step = 27999
2022-06-12 05:08:05,764   loss = 0.03276743224357606
2022-06-12 05:09:53,803 ***** Running evaluation *****
2022-06-12 05:09:53,804   Epoch = 1 iter 20499 step
2022-06-12 05:09:53,804   Num examples = 40430
2022-06-12 05:09:53,804   Batch size = 32
2022-06-12 05:09:53,806 ***** Eval results *****
2022-06-12 05:09:53,806   att_loss = 4.603356721955148
2022-06-12 05:09:53,806   global_step = 20499
2022-06-12 05:09:53,806   loss = 5.709419667335575
2022-06-12 05:09:53,807   rep_loss = 1.1060629430821645
2022-06-12 05:09:53,807 ***** Save model *****
2022-06-12 05:10:04,414 ***** Running evaluation *****
2022-06-12 05:10:04,414   Epoch = 8 iter 28499 step
2022-06-12 05:10:04,414   Num examples = 5463
2022-06-12 05:10:04,414   Batch size = 32
2022-06-12 05:10:08,930 ***** Eval results *****
2022-06-12 05:10:08,930   acc = 0.8716822258832143
2022-06-12 05:10:08,931   cls_loss = 0.032554225407975285
2022-06-12 05:10:08,931   eval_loss = 0.4617101144895219
2022-06-12 05:10:08,931   global_step = 28499
2022-06-12 05:10:08,931   loss = 0.032554225407975285
2022-06-12 05:12:06,155 ***** Running evaluation *****
2022-06-12 05:12:06,156   Epoch = 1 iter 20999 step
2022-06-12 05:12:06,156   Num examples = 40430
2022-06-12 05:12:06,156   Batch size = 32
2022-06-12 05:12:06,157 ***** Eval results *****
2022-06-12 05:12:06,158   att_loss = 4.596490074979366
2022-06-12 05:12:06,158   global_step = 20999
2022-06-12 05:12:06,158   loss = 5.701552882522843
2022-06-12 05:12:06,158   rep_loss = 1.105062805686441
2022-06-12 05:12:06,158 ***** Save model *****
2022-06-12 05:12:07,473 ***** Running evaluation *****
2022-06-12 05:12:07,474   Epoch = 8 iter 28999 step
2022-06-12 05:12:07,474   Num examples = 5463
2022-06-12 05:12:07,474   Batch size = 32
2022-06-12 05:12:11,983 ***** Eval results *****
2022-06-12 05:12:11,984   acc = 0.8713161266703277
2022-06-12 05:12:11,984   cls_loss = 0.032571697564626884
2022-06-12 05:12:11,984   eval_loss = 0.4674496777867626
2022-06-12 05:12:11,984   global_step = 28999
2022-06-12 05:12:11,984   loss = 0.032571697564626884
2022-06-12 05:14:10,535 ***** Running evaluation *****
2022-06-12 05:14:10,535   Epoch = 9 iter 29499 step
2022-06-12 05:14:10,536   Num examples = 5463
2022-06-12 05:14:10,536   Batch size = 32
2022-06-12 05:14:15,046 ***** Eval results *****
2022-06-12 05:14:15,046   acc = 0.8742449203734212
2022-06-12 05:14:15,046   cls_loss = 0.029584509497951893
2022-06-12 05:14:15,046   eval_loss = 0.47201137051901276
2022-06-12 05:14:15,046   global_step = 29499
2022-06-12 05:14:15,046   loss = 0.029584509497951893
2022-06-12 05:14:18,996 ***** Running evaluation *****
2022-06-12 05:14:18,996   Epoch = 1 iter 21499 step
2022-06-12 05:14:18,996   Num examples = 40430
2022-06-12 05:14:18,996   Batch size = 32
2022-06-12 05:14:18,998 ***** Eval results *****
2022-06-12 05:14:18,998   att_loss = 4.5881579665373176
2022-06-12 05:14:18,998   global_step = 21499
2022-06-12 05:14:18,998   loss = 5.692163814807741
2022-06-12 05:14:18,998   rep_loss = 1.1040058466227485
2022-06-12 05:14:18,998 ***** Save model *****
2022-06-12 05:16:13,577 ***** Running evaluation *****
2022-06-12 05:16:13,578   Epoch = 9 iter 29999 step
2022-06-12 05:16:13,578   Num examples = 5463
2022-06-12 05:16:13,578   Batch size = 32
2022-06-12 05:16:18,092 ***** Eval results *****
2022-06-12 05:16:18,092   acc = 0.8725974739154311
2022-06-12 05:16:18,092   cls_loss = 0.03154870076620491
2022-06-12 05:16:18,093   eval_loss = 0.4699349245110973
2022-06-12 05:16:18,093   global_step = 29999
2022-06-12 05:16:18,093   loss = 0.03154870076620491
2022-06-12 05:16:32,035 ***** Running evaluation *****
2022-06-12 05:16:32,035   Epoch = 1 iter 21999 step
2022-06-12 05:16:32,035   Num examples = 40430
2022-06-12 05:16:32,035   Batch size = 32
2022-06-12 05:16:32,036 ***** Eval results *****
2022-06-12 05:16:32,036   att_loss = 4.581613151360258
2022-06-12 05:16:32,036   global_step = 21999
2022-06-12 05:16:32,037   loss = 5.6845564237261375
2022-06-12 05:16:32,037   rep_loss = 1.102943270335878
2022-06-12 05:16:32,037 ***** Save model *****
2022-06-12 05:18:16,608 ***** Running evaluation *****
2022-06-12 05:18:16,608   Epoch = 9 iter 30499 step
2022-06-12 05:18:16,608   Num examples = 5463
2022-06-12 05:18:16,608   Batch size = 32
2022-06-12 05:18:21,124 ***** Eval results *****
2022-06-12 05:18:21,125   acc = 0.8722313747025444
2022-06-12 05:18:21,125   cls_loss = 0.0315448996877287
2022-06-12 05:18:21,125   eval_loss = 0.4687655426518262
2022-06-12 05:18:21,125   global_step = 30499
2022-06-12 05:18:21,125   loss = 0.0315448996877287
2022-06-12 05:18:43,475 ***** Running evaluation *****
2022-06-12 05:18:43,475   Epoch = 1 iter 22499 step
2022-06-12 05:18:43,475   Num examples = 40430
2022-06-12 05:18:43,475   Batch size = 32
2022-06-12 05:18:43,476 ***** Eval results *****
2022-06-12 05:18:43,477   att_loss = 4.574352440095803
2022-06-12 05:18:43,477   global_step = 22499
2022-06-12 05:18:43,477   loss = 5.676309502042307
2022-06-12 05:18:43,477   rep_loss = 1.101957059788119
2022-06-12 05:18:43,477 ***** Save model *****
2022-06-12 05:20:19,765 ***** Running evaluation *****
2022-06-12 05:20:19,766   Epoch = 9 iter 30999 step
2022-06-12 05:20:19,766   Num examples = 5463
2022-06-12 05:20:19,766   Batch size = 32
2022-06-12 05:20:24,281 ***** Eval results *****
2022-06-12 05:20:24,281   acc = 0.8733296723412045
2022-06-12 05:20:24,281   cls_loss = 0.0315970216047903
2022-06-12 05:20:24,281   eval_loss = 0.47227474623931603
2022-06-12 05:20:24,281   global_step = 30999
2022-06-12 05:20:24,281   loss = 0.0315970216047903
2022-06-12 05:20:54,971 ***** Running evaluation *****
2022-06-12 05:20:54,971   Epoch = 2 iter 22999 step
2022-06-12 05:20:54,971   Num examples = 40430
2022-06-12 05:20:54,971   Batch size = 32
2022-06-12 05:20:54,972 ***** Eval results *****
2022-06-12 05:20:54,973   att_loss = 4.117676728480571
2022-06-12 05:20:54,973   global_step = 22999
2022-06-12 05:20:54,973   loss = 5.187379929089638
2022-06-12 05:20:54,973   rep_loss = 1.0697032091240164
2022-06-12 05:20:54,973 ***** Save model *****
2022-06-12 05:22:22,809 ***** Running evaluation *****
2022-06-12 05:22:22,809   Epoch = 9 iter 31499 step
2022-06-12 05:22:22,809   Num examples = 5463
2022-06-12 05:22:22,809   Batch size = 32
2022-06-12 05:22:27,325 ***** Eval results *****
2022-06-12 05:22:27,325   acc = 0.8711330770638843
2022-06-12 05:22:27,325   cls_loss = 0.03152516892637754
2022-06-12 05:22:27,325   eval_loss = 0.480059012280483
2022-06-12 05:22:27,326   global_step = 31499
2022-06-12 05:22:27,326   loss = 0.03152516892637754
2022-06-12 05:23:06,476 ***** Running evaluation *****
2022-06-12 05:23:06,476   Epoch = 2 iter 23499 step
2022-06-12 05:23:06,476   Num examples = 40430
2022-06-12 05:23:06,476   Batch size = 32
2022-06-12 05:23:06,477 ***** Eval results *****
2022-06-12 05:23:06,478   att_loss = 4.116618427206248
2022-06-12 05:23:06,478   global_step = 23499
2022-06-12 05:23:06,478   loss = 5.185165805150712
2022-06-12 05:23:06,478   rep_loss = 1.068547380693031
2022-06-12 05:23:06,478 ***** Save model *****
2022-06-12 05:24:26,455 ***** Running evaluation *****
2022-06-12 05:24:26,455   Epoch = 9 iter 31999 step
2022-06-12 05:24:26,455   Num examples = 5463
2022-06-12 05:24:26,455   Batch size = 32
2022-06-12 05:24:30,970 ***** Eval results *****
2022-06-12 05:24:30,971   acc = 0.8727805235218744
2022-06-12 05:24:30,971   cls_loss = 0.03167783739638788
2022-06-12 05:24:30,971   eval_loss = 0.47312563995977763
2022-06-12 05:24:30,971   global_step = 31999
2022-06-12 05:24:30,971   loss = 0.03167783739638788
2022-06-12 05:25:19,392 ***** Running evaluation *****
2022-06-12 05:25:19,392   Epoch = 2 iter 23999 step
2022-06-12 05:25:19,392   Num examples = 40430
2022-06-12 05:25:19,392   Batch size = 32
2022-06-12 05:25:19,394 ***** Eval results *****
2022-06-12 05:25:19,394   att_loss = 4.107280126348955
2022-06-12 05:25:19,394   global_step = 23999
2022-06-12 05:25:19,394   loss = 5.173997328146949
2022-06-12 05:25:19,394   rep_loss = 1.0667171998569376
2022-06-12 05:25:19,394 ***** Save model *****
2022-06-12 05:26:29,778 ***** Running evaluation *****
2022-06-12 05:26:29,779   Epoch = 9 iter 32499 step
2022-06-12 05:26:29,779   Num examples = 5463
2022-06-12 05:26:29,779   Batch size = 32
2022-06-12 05:26:34,290 ***** Eval results *****
2022-06-12 05:26:34,290   acc = 0.8704008786381109
2022-06-12 05:26:34,290   cls_loss = 0.03162921538403812
2022-06-12 05:26:34,290   eval_loss = 0.4763995135606032
2022-06-12 05:26:34,290   global_step = 32499
2022-06-12 05:26:34,290   loss = 0.03162921538403812
2022-06-12 05:27:29,354 **************S*************
task_name = qnli
best_metirc = 0.8771737140765148
**************E*************

2022-06-12 05:27:29,668 Task finish! 
2022-06-12 05:27:29,669 Task cost 136.17022836666666 minutes, i.e. 2.2695038133333334 hours. 
2022-06-12 05:27:31,810 Task start! 
2022-06-12 05:27:31,832 device: cuda n_gpu: 1
2022-06-12 05:27:31,833 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=500, gpu_id=3, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=6, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/mnli/on_original_data', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='mnli', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mnli/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/mnli/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 05:27:32,463 ***** Running evaluation *****
2022-06-12 05:27:32,463   Epoch = 2 iter 24499 step
2022-06-12 05:27:32,463   Num examples = 40430
2022-06-12 05:27:32,463   Batch size = 32
2022-06-12 05:27:32,465 ***** Eval results *****
2022-06-12 05:27:32,465   att_loss = 4.116835334614097
2022-06-12 05:27:32,465   global_step = 24499
2022-06-12 05:27:32,465   loss = 5.183548293512202
2022-06-12 05:27:32,465   rep_loss = 1.0667129578137682
2022-06-12 05:27:32,465 ***** Save model *****
2022-06-12 05:27:37,737 Writing example 0 of 392702
2022-06-12 05:27:37,738 *** Example ***
2022-06-12 05:27:37,738 guid: train-0
2022-06-12 05:27:37,739 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2022-06-12 05:27:37,739 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 05:27:37,739 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 05:27:37,739 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 05:27:37,739 label: neutral
2022-06-12 05:27:37,739 label_id: 2
2022-06-12 05:27:43,468 Writing example 10000 of 392702
2022-06-12 05:27:49,103 Writing example 20000 of 392702
2022-06-12 05:27:54,779 Writing example 30000 of 392702
2022-06-12 05:28:00,704 Writing example 40000 of 392702
2022-06-12 05:28:06,453 Writing example 50000 of 392702
2022-06-12 05:28:12,139 Writing example 60000 of 392702
2022-06-12 05:28:17,858 Writing example 70000 of 392702
2022-06-12 05:28:23,967 Writing example 80000 of 392702
2022-06-12 05:28:29,679 Writing example 90000 of 392702
2022-06-12 05:28:35,366 Writing example 100000 of 392702
2022-06-12 05:28:41,062 Writing example 110000 of 392702
2022-06-12 05:28:46,785 Writing example 120000 of 392702
2022-06-12 05:28:53,046 Writing example 130000 of 392702
2022-06-12 05:28:58,742 Writing example 140000 of 392702
2022-06-12 05:29:04,447 Writing example 150000 of 392702
2022-06-12 05:29:10,206 Writing example 160000 of 392702
2022-06-12 05:29:15,929 Writing example 170000 of 392702
2022-06-12 05:29:21,654 Writing example 180000 of 392702
2022-06-12 05:29:27,999 Writing example 190000 of 392702
2022-06-12 05:29:33,734 Writing example 200000 of 392702
2022-06-12 05:29:39,471 Writing example 210000 of 392702
2022-06-12 05:29:44,946 ***** Running evaluation *****
2022-06-12 05:29:44,947   Epoch = 2 iter 24999 step
2022-06-12 05:29:44,947   Num examples = 40430
2022-06-12 05:29:44,947   Batch size = 32
2022-06-12 05:29:44,948 ***** Eval results *****
2022-06-12 05:29:44,948   att_loss = 4.121702807305291
2022-06-12 05:29:44,948   global_step = 24999
2022-06-12 05:29:44,949   loss = 5.187776594470169
2022-06-12 05:29:44,949   rep_loss = 1.0660737884577633
2022-06-12 05:29:44,949 ***** Save model *****
2022-06-12 05:29:45,200 Writing example 220000 of 392702
2022-06-12 05:29:50,952 Writing example 230000 of 392702
2022-06-12 05:29:56,635 Writing example 240000 of 392702
2022-06-12 05:30:02,370 Writing example 250000 of 392702
2022-06-12 05:30:09,100 Writing example 260000 of 392702
2022-06-12 05:30:14,868 Writing example 270000 of 392702
2022-06-12 05:30:20,513 Writing example 280000 of 392702
2022-06-12 05:30:26,208 Writing example 290000 of 392702
2022-06-12 05:30:31,910 Writing example 300000 of 392702
2022-06-12 05:30:37,680 Writing example 310000 of 392702
2022-06-12 05:30:43,402 Writing example 320000 of 392702
2022-06-12 05:30:49,121 Writing example 330000 of 392702
2022-06-12 05:30:54,909 Writing example 340000 of 392702
2022-06-12 05:31:01,930 Writing example 350000 of 392702
2022-06-12 05:31:07,677 Writing example 360000 of 392702
2022-06-12 05:31:13,377 Writing example 370000 of 392702
2022-06-12 05:31:19,106 Writing example 380000 of 392702
2022-06-12 05:31:24,826 Writing example 390000 of 392702
2022-06-12 05:31:29,439 Writing example 0 of 9815
2022-06-12 05:31:29,439 *** Example ***
2022-06-12 05:31:29,439 guid: dev_matched-0
2022-06-12 05:31:29,440 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2022-06-12 05:31:29,440 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 05:31:29,440 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 05:31:29,440 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 05:31:29,440 label: neutral
2022-06-12 05:31:29,440 label_id: 2
2022-06-12 05:31:35,027 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 05:31:40,139 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mnli/on_original_data/pytorch_model.bin
2022-06-12 05:31:41,559 loading model...
2022-06-12 05:31:41,943 done!
2022-06-12 05:31:41,944 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.dense_fit.weight', 'bert.dense_fit.bias']
2022-06-12 05:31:44,759 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 05:31:45,846 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-12 05:31:45,973 loading model...
2022-06-12 05:31:46,039 done!
2022-06-12 05:31:46,039 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-12 05:31:46,039 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-12 05:31:47,397 ***** Running training *****
2022-06-12 05:31:47,408   Num examples = 392702
2022-06-12 05:31:47,418   Batch size = 32
2022-06-12 05:31:47,434   Num steps = 73626
2022-06-12 05:31:47,450 n: bert.embeddings.word_embeddings.weight
2022-06-12 05:31:47,461 n: bert.embeddings.position_embeddings.weight
2022-06-12 05:31:47,462 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 05:31:47,462 n: bert.embeddings.LayerNorm.weight
2022-06-12 05:31:47,462 n: bert.embeddings.LayerNorm.bias
2022-06-12 05:31:47,462 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 05:31:47,462 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 05:31:47,463 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 05:31:47,463 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 05:31:47,463 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 05:31:47,463 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 05:31:47,463 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 05:31:47,463 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 05:31:47,463 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 05:31:47,463 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 05:31:47,464 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 05:31:47,464 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 05:31:47,464 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 05:31:47,464 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 05:31:47,464 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 05:31:47,464 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 05:31:47,464 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 05:31:47,464 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 05:31:47,464 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 05:31:47,464 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 05:31:47,465 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 05:31:47,465 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 05:31:47,465 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 05:31:47,465 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 05:31:47,465 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 05:31:47,465 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 05:31:47,465 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 05:31:47,465 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 05:31:47,466 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 05:31:47,466 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 05:31:47,466 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 05:31:47,466 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 05:31:47,466 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 05:31:47,466 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 05:31:47,466 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 05:31:47,466 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 05:31:47,466 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 05:31:47,466 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 05:31:47,466 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 05:31:47,466 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 05:31:47,467 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 05:31:47,467 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 05:31:47,467 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 05:31:47,467 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 05:31:47,467 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 05:31:47,467 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 05:31:47,467 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 05:31:47,467 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 05:31:47,467 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 05:31:47,467 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 05:31:47,467 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 05:31:47,467 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 05:31:47,467 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 05:31:47,468 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 05:31:47,468 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 05:31:47,469 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 05:31:47,470 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 05:31:47,470 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 05:31:47,470 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 05:31:47,470 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 05:31:47,471 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 05:31:47,471 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 05:31:47,471 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 05:31:47,471 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 05:31:47,471 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 05:31:47,471 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 05:31:47,471 n: bert.pooler.dense.weight
2022-06-12 05:31:47,471 n: bert.pooler.dense.bias
2022-06-12 05:31:47,471 n: classifier.weight
2022-06-12 05:31:47,471 n: classifier.bias
2022-06-12 05:31:47,471 n: fit_denses.0.weight
2022-06-12 05:31:47,471 n: fit_denses.0.bias
2022-06-12 05:31:47,471 n: fit_denses.1.weight
2022-06-12 05:31:47,472 n: fit_denses.1.bias
2022-06-12 05:31:47,472 n: fit_denses.2.weight
2022-06-12 05:31:47,472 n: fit_denses.2.bias
2022-06-12 05:31:47,472 n: fit_denses.3.weight
2022-06-12 05:31:47,472 n: fit_denses.3.bias
2022-06-12 05:31:47,472 n: fit_denses.4.weight
2022-06-12 05:31:47,472 n: fit_denses.4.bias
2022-06-12 05:31:47,472 n: fit_denses.5.weight
2022-06-12 05:31:47,472 n: fit_denses.5.bias
2022-06-12 05:31:47,472 n: fit_denses.6.weight
2022-06-12 05:31:47,472 n: fit_denses.6.bias
2022-06-12 05:31:47,472 Total parameters: 72469507
2022-06-12 05:31:56,962 ***** Running evaluation *****
2022-06-12 05:31:56,962   Epoch = 2 iter 25499 step
2022-06-12 05:31:56,963   Num examples = 40430
2022-06-12 05:31:56,963   Batch size = 32
2022-06-12 05:31:56,964 ***** Eval results *****
2022-06-12 05:31:56,964   att_loss = 4.119044049661546
2022-06-12 05:31:56,964   global_step = 25499
2022-06-12 05:31:56,964   loss = 5.1843254223125665
2022-06-12 05:31:56,964   rep_loss = 1.0652813744441285
2022-06-12 05:31:56,964 ***** Save model *****
2022-06-12 05:33:53,661 ***** Running evaluation *****
2022-06-12 05:33:53,662   Epoch = 0 iter 499 step
2022-06-12 05:33:53,662   Num examples = 9815
2022-06-12 05:33:53,662   Batch size = 32
2022-06-12 05:33:53,663 ***** Eval results *****
2022-06-12 05:33:53,663   att_loss = 20.339023704758148
2022-06-12 05:33:53,663   global_step = 499
2022-06-12 05:33:53,663   loss = 21.815174274788593
2022-06-12 05:33:53,663   rep_loss = 1.4761505298958513
2022-06-12 05:33:53,663 ***** Save model *****
2022-06-12 05:34:07,825 ***** Running evaluation *****
2022-06-12 05:34:07,826   Epoch = 2 iter 25999 step
2022-06-12 05:34:07,826   Num examples = 40430
2022-06-12 05:34:07,826   Batch size = 32
2022-06-12 05:34:07,827 ***** Eval results *****
2022-06-12 05:34:07,827   att_loss = 4.124734140020384
2022-06-12 05:34:07,827   global_step = 25999
2022-06-12 05:34:07,827   loss = 5.189629355834722
2022-06-12 05:34:07,827   rep_loss = 1.0648952185943033
2022-06-12 05:34:07,828 ***** Save model *****
2022-06-12 05:36:00,560 ***** Running evaluation *****
2022-06-12 05:36:00,561   Epoch = 0 iter 999 step
2022-06-12 05:36:00,561   Num examples = 9815
2022-06-12 05:36:00,561   Batch size = 32
2022-06-12 05:36:00,562 ***** Eval results *****
2022-06-12 05:36:00,562   att_loss = 19.409184481646562
2022-06-12 05:36:00,562   global_step = 999
2022-06-12 05:36:00,563   loss = 20.77665069415882
2022-06-12 05:36:00,563   rep_loss = 1.3674661925843767
2022-06-12 05:36:00,563 ***** Save model *****
2022-06-12 05:36:19,020 ***** Running evaluation *****
2022-06-12 05:36:19,020   Epoch = 2 iter 26499 step
2022-06-12 05:36:19,020   Num examples = 40430
2022-06-12 05:36:19,020   Batch size = 32
2022-06-12 05:36:19,021 ***** Eval results *****
2022-06-12 05:36:19,021   att_loss = 4.126595613835308
2022-06-12 05:36:19,021   global_step = 26499
2022-06-12 05:36:19,022   loss = 5.191113939688668
2022-06-12 05:36:19,022   rep_loss = 1.0645183285806812
2022-06-12 05:36:19,022 ***** Save model *****
2022-06-12 05:38:07,772 ***** Running evaluation *****
2022-06-12 05:38:07,773   Epoch = 0 iter 1499 step
2022-06-12 05:38:07,773   Num examples = 9815
2022-06-12 05:38:07,773   Batch size = 32
2022-06-12 05:38:07,774 ***** Eval results *****
2022-06-12 05:38:07,774   att_loss = 18.825915156562303
2022-06-12 05:38:07,774   global_step = 1499
2022-06-12 05:38:07,774   loss = 20.135970081306443
2022-06-12 05:38:07,774   rep_loss = 1.310054894365256
2022-06-12 05:38:07,774 ***** Save model *****
2022-06-12 05:38:30,751 ***** Running evaluation *****
2022-06-12 05:38:30,752   Epoch = 2 iter 26999 step
2022-06-12 05:38:30,752   Num examples = 40430
2022-06-12 05:38:30,752   Batch size = 32
2022-06-12 05:38:30,754 ***** Eval results *****
2022-06-12 05:38:30,754   att_loss = 4.120432771576153
2022-06-12 05:38:30,754   global_step = 26999
2022-06-12 05:38:30,754   loss = 5.184236647070056
2022-06-12 05:38:30,754   rep_loss = 1.0638038765715165
2022-06-12 05:38:30,754 ***** Save model *****
2022-06-12 05:40:14,767 ***** Running evaluation *****
2022-06-12 05:40:14,768   Epoch = 0 iter 1999 step
2022-06-12 05:40:14,768   Num examples = 9815
2022-06-12 05:40:14,768   Batch size = 32
2022-06-12 05:40:14,769 ***** Eval results *****
2022-06-12 05:40:14,770   att_loss = 18.40313874560037
2022-06-12 05:40:14,770   global_step = 1999
2022-06-12 05:40:14,770   loss = 19.67438819230706
2022-06-12 05:40:14,770   rep_loss = 1.271249420407893
2022-06-12 05:40:14,770 ***** Save model *****
2022-06-12 05:40:42,562 ***** Running evaluation *****
2022-06-12 05:40:42,563   Epoch = 2 iter 27499 step
2022-06-12 05:40:42,563   Num examples = 40430
2022-06-12 05:40:42,563   Batch size = 32
2022-06-12 05:40:42,564 ***** Eval results *****
2022-06-12 05:40:42,564   att_loss = 4.113721655506776
2022-06-12 05:40:42,565   global_step = 27499
2022-06-12 05:40:42,565   loss = 5.176922123280361
2022-06-12 05:40:42,565   rep_loss = 1.0632004688006034
2022-06-12 05:40:42,565 ***** Save model *****
2022-06-12 05:42:22,781 ***** Running evaluation *****
2022-06-12 05:42:22,782   Epoch = 0 iter 2499 step
2022-06-12 05:42:22,782   Num examples = 9815
2022-06-12 05:42:22,782   Batch size = 32
2022-06-12 05:42:22,784 ***** Eval results *****
2022-06-12 05:42:22,784   att_loss = 18.093963136478347
2022-06-12 05:42:22,784   global_step = 2499
2022-06-12 05:42:22,784   loss = 19.336584895264867
2022-06-12 05:42:22,784   rep_loss = 1.242621740373243
2022-06-12 05:42:22,784 ***** Save model *****
2022-06-12 05:42:54,990 ***** Running evaluation *****
2022-06-12 05:42:54,990   Epoch = 2 iter 27999 step
2022-06-12 05:42:54,990   Num examples = 40430
2022-06-12 05:42:54,991   Batch size = 32
2022-06-12 05:42:54,992 ***** Eval results *****
2022-06-12 05:42:54,992   att_loss = 4.1134492697247
2022-06-12 05:42:54,992   global_step = 27999
2022-06-12 05:42:54,992   loss = 5.17608887552195
2022-06-12 05:42:54,992   rep_loss = 1.0626396069646358
2022-06-12 05:42:54,992 ***** Save model *****
2022-06-12 05:44:32,738 ***** Running evaluation *****
2022-06-12 05:44:32,739   Epoch = 0 iter 2999 step
2022-06-12 05:44:32,739   Num examples = 9815
2022-06-12 05:44:32,739   Batch size = 32
2022-06-12 05:44:32,742 ***** Eval results *****
2022-06-12 05:44:32,742   att_loss = 17.81969226348714
2022-06-12 05:44:32,742   global_step = 2999
2022-06-12 05:44:32,742   loss = 19.03938803079725
2022-06-12 05:44:32,742   rep_loss = 1.2196957525232308
2022-06-12 05:44:32,743 ***** Save model *****
2022-06-12 05:45:07,693 ***** Running evaluation *****
2022-06-12 05:45:07,693   Epoch = 2 iter 28499 step
2022-06-12 05:45:07,693   Num examples = 40430
2022-06-12 05:45:07,693   Batch size = 32
2022-06-12 05:45:07,694 ***** Eval results *****
2022-06-12 05:45:07,695   att_loss = 4.113313323570884
2022-06-12 05:45:07,695   global_step = 28499
2022-06-12 05:45:07,695   loss = 5.175407482654938
2022-06-12 05:45:07,695   rep_loss = 1.0620941609987722
2022-06-12 05:45:07,695 ***** Save model *****
2022-06-12 05:46:43,776 ***** Running evaluation *****
2022-06-12 05:46:43,776   Epoch = 0 iter 3499 step
2022-06-12 05:46:43,776   Num examples = 9815
2022-06-12 05:46:43,776   Batch size = 32
2022-06-12 05:46:43,778 ***** Eval results *****
2022-06-12 05:46:43,778   att_loss = 17.592091793944476
2022-06-12 05:46:43,778   global_step = 3499
2022-06-12 05:46:43,778   loss = 18.792715829247985
2022-06-12 05:46:43,778   rep_loss = 1.20062402140314
2022-06-12 05:46:43,778 ***** Save model *****
2022-06-12 05:47:20,180 ***** Running evaluation *****
2022-06-12 05:47:20,180   Epoch = 2 iter 28999 step
2022-06-12 05:47:20,180   Num examples = 40430
2022-06-12 05:47:20,180   Batch size = 32
2022-06-12 05:47:20,181 ***** Eval results *****
2022-06-12 05:47:20,181   att_loss = 4.113206311334225
2022-06-12 05:47:20,182   global_step = 28999
2022-06-12 05:47:20,182   loss = 5.1748831346404875
2022-06-12 05:47:20,182   rep_loss = 1.0616768259917573
2022-06-12 05:47:20,182 ***** Save model *****
2022-06-12 05:48:54,195 ***** Running evaluation *****
2022-06-12 05:48:54,195   Epoch = 0 iter 3999 step
2022-06-12 05:48:54,195   Num examples = 9815
2022-06-12 05:48:54,196   Batch size = 32
2022-06-12 05:48:54,197 ***** Eval results *****
2022-06-12 05:48:54,197   att_loss = 17.394579516556536
2022-06-12 05:48:54,197   global_step = 3999
2022-06-12 05:48:54,197   loss = 18.57900606533622
2022-06-12 05:48:54,198   rep_loss = 1.184426533502172
2022-06-12 05:48:54,198 ***** Save model *****
2022-06-12 05:49:32,129 ***** Running evaluation *****
2022-06-12 05:49:32,129   Epoch = 2 iter 29499 step
2022-06-12 05:49:32,129   Num examples = 40430
2022-06-12 05:49:32,129   Batch size = 32
2022-06-12 05:49:32,130 ***** Eval results *****
2022-06-12 05:49:32,131   att_loss = 4.1078646931886
2022-06-12 05:49:32,131   global_step = 29499
2022-06-12 05:49:32,131   loss = 5.168949297536564
2022-06-12 05:49:32,131   rep_loss = 1.061084606005853
2022-06-12 05:49:32,131 ***** Save model *****
2022-06-12 05:51:05,663 ***** Running evaluation *****
2022-06-12 05:51:05,664   Epoch = 0 iter 4499 step
2022-06-12 05:51:05,664   Num examples = 9815
2022-06-12 05:51:05,664   Batch size = 32
2022-06-12 05:51:05,665 ***** Eval results *****
2022-06-12 05:51:05,665   att_loss = 17.218315633887
2022-06-12 05:51:05,665   global_step = 4499
2022-06-12 05:51:05,665   loss = 18.38874292956588
2022-06-12 05:51:05,665   rep_loss = 1.1704272809598837
2022-06-12 05:51:05,666 ***** Save model *****
2022-06-12 05:51:42,872 ***** Running evaluation *****
2022-06-12 05:51:42,872   Epoch = 2 iter 29999 step
2022-06-12 05:51:42,872   Num examples = 40430
2022-06-12 05:51:42,872   Batch size = 32
2022-06-12 05:51:42,874 ***** Eval results *****
2022-06-12 05:51:42,874   att_loss = 4.110490746234695
2022-06-12 05:51:42,874   global_step = 29999
2022-06-12 05:51:42,874   loss = 5.1713419955104225
2022-06-12 05:51:42,874   rep_loss = 1.0608512511314447
2022-06-12 05:51:42,874 ***** Save model *****
2022-06-12 05:53:16,388 ***** Running evaluation *****
2022-06-12 05:53:16,389   Epoch = 0 iter 4999 step
2022-06-12 05:53:16,389   Num examples = 9815
2022-06-12 05:53:16,389   Batch size = 32
2022-06-12 05:53:16,390 ***** Eval results *****
2022-06-12 05:53:16,390   att_loss = 17.074350972680193
2022-06-12 05:53:16,391   global_step = 4999
2022-06-12 05:53:16,391   loss = 18.232736983759974
2022-06-12 05:53:16,391   rep_loss = 1.158386000229564
2022-06-12 05:53:16,391 ***** Save model *****
2022-06-12 05:53:55,208 ***** Running evaluation *****
2022-06-12 05:53:55,209   Epoch = 2 iter 30499 step
2022-06-12 05:53:55,209   Num examples = 40430
2022-06-12 05:53:55,209   Batch size = 32
2022-06-12 05:53:55,210 ***** Eval results *****
2022-06-12 05:53:55,211   att_loss = 4.109692710071883
2022-06-12 05:53:55,211   global_step = 30499
2022-06-12 05:53:55,211   loss = 5.170048119435222
2022-06-12 05:53:55,211   rep_loss = 1.0603554112223839
2022-06-12 05:53:55,211 ***** Save model *****
2022-06-12 05:55:26,800 ***** Running evaluation *****
2022-06-12 05:55:26,801   Epoch = 0 iter 5499 step
2022-06-12 05:55:26,801   Num examples = 9815
2022-06-12 05:55:26,801   Batch size = 32
2022-06-12 05:55:26,803 ***** Eval results *****
2022-06-12 05:55:26,803   att_loss = 16.935367400396043
2022-06-12 05:55:26,803   global_step = 5499
2022-06-12 05:55:26,803   loss = 18.082692446676596
2022-06-12 05:55:26,804   rep_loss = 1.1473250349861293
2022-06-12 05:55:26,804 ***** Save model *****
2022-06-12 05:56:07,891 ***** Running evaluation *****
2022-06-12 05:56:07,891   Epoch = 2 iter 30999 step
2022-06-12 05:56:07,891   Num examples = 40430
2022-06-12 05:56:07,891   Batch size = 32
2022-06-12 05:56:07,893 ***** Eval results *****
2022-06-12 05:56:07,893   att_loss = 4.1065011841720525
2022-06-12 05:56:07,893   global_step = 30999
2022-06-12 05:56:07,893   loss = 5.16625121298274
2022-06-12 05:56:07,893   rep_loss = 1.059750030311809
2022-06-12 05:56:07,893 ***** Save model *****
2022-06-12 05:57:36,270 ***** Running evaluation *****
2022-06-12 05:57:36,271   Epoch = 0 iter 5999 step
2022-06-12 05:57:36,271   Num examples = 9815
2022-06-12 05:57:36,271   Batch size = 32
2022-06-12 05:57:36,272 ***** Eval results *****
2022-06-12 05:57:36,272   att_loss = 16.80880211217937
2022-06-12 05:57:36,272   global_step = 5999
2022-06-12 05:57:36,272   loss = 17.94637524086071
2022-06-12 05:57:36,273   rep_loss = 1.1375731189045475
2022-06-12 05:57:36,273 ***** Save model *****
2022-06-12 05:58:18,954 ***** Running evaluation *****
2022-06-12 05:58:18,954   Epoch = 2 iter 31499 step
2022-06-12 05:58:18,954   Num examples = 40430
2022-06-12 05:58:18,954   Batch size = 32
2022-06-12 05:58:18,956 ***** Eval results *****
2022-06-12 05:58:18,956   att_loss = 4.103589097146153
2022-06-12 05:58:18,956   global_step = 31499
2022-06-12 05:58:18,956   loss = 5.162866886408719
2022-06-12 05:58:18,956   rep_loss = 1.0592777907188278
2022-06-12 05:58:18,956 ***** Save model *****
2022-06-12 05:59:46,301 ***** Running evaluation *****
2022-06-12 05:59:46,302   Epoch = 0 iter 6499 step
2022-06-12 05:59:46,302   Num examples = 9815
2022-06-12 05:59:46,302   Batch size = 32
2022-06-12 05:59:46,303 ***** Eval results *****
2022-06-12 05:59:46,304   att_loss = 16.68340213263213
2022-06-12 05:59:46,304   global_step = 6499
2022-06-12 05:59:46,304   loss = 17.812020004226678
2022-06-12 05:59:46,304   rep_loss = 1.1286178634503856
2022-06-12 05:59:46,304 ***** Save model *****
2022-06-12 06:00:30,851 ***** Running evaluation *****
2022-06-12 06:00:30,851   Epoch = 2 iter 31999 step
2022-06-12 06:00:30,851   Num examples = 40430
2022-06-12 06:00:30,851   Batch size = 32
2022-06-12 06:00:30,853 ***** Eval results *****
2022-06-12 06:00:30,853   att_loss = 4.101143614186724
2022-06-12 06:00:30,853   global_step = 31999
2022-06-12 06:00:30,853   loss = 5.15988405256295
2022-06-12 06:00:30,853   rep_loss = 1.0587404392645985
2022-06-12 06:00:30,853 ***** Save model *****
2022-06-12 06:01:56,937 ***** Running evaluation *****
2022-06-12 06:01:56,937   Epoch = 0 iter 6999 step
2022-06-12 06:01:56,937   Num examples = 9815
2022-06-12 06:01:56,938   Batch size = 32
2022-06-12 06:01:56,939 ***** Eval results *****
2022-06-12 06:01:56,939   att_loss = 16.5798726776086
2022-06-12 06:01:56,939   global_step = 6999
2022-06-12 06:01:56,939   loss = 17.700430888178552
2022-06-12 06:01:56,939   rep_loss = 1.1205582023263063
2022-06-12 06:01:56,939 ***** Save model *****
2022-06-12 06:02:42,254 ***** Running evaluation *****
2022-06-12 06:02:42,255   Epoch = 2 iter 32499 step
2022-06-12 06:02:42,255   Num examples = 40430
2022-06-12 06:02:42,255   Batch size = 32
2022-06-12 06:02:42,256 ***** Eval results *****
2022-06-12 06:02:42,256   att_loss = 4.100620653238657
2022-06-12 06:02:42,256   global_step = 32499
2022-06-12 06:02:42,256   loss = 5.158915986266216
2022-06-12 06:02:42,256   rep_loss = 1.0582953338337697
2022-06-12 06:02:42,256 ***** Save model *****
2022-06-12 06:04:07,428 ***** Running evaluation *****
2022-06-12 06:04:07,428   Epoch = 0 iter 7499 step
2022-06-12 06:04:07,428   Num examples = 9815
2022-06-12 06:04:07,428   Batch size = 32
2022-06-12 06:04:07,430 ***** Eval results *****
2022-06-12 06:04:07,430   att_loss = 16.47828048915128
2022-06-12 06:04:07,430   global_step = 7499
2022-06-12 06:04:07,430   loss = 17.591499798901
2022-06-12 06:04:07,430   rep_loss = 1.1132193027233852
2022-06-12 06:04:07,430 ***** Save model *****
2022-06-12 06:04:54,265 ***** Running evaluation *****
2022-06-12 06:04:54,265   Epoch = 2 iter 32999 step
2022-06-12 06:04:54,265   Num examples = 40430
2022-06-12 06:04:54,265   Batch size = 32
2022-06-12 06:04:54,266 ***** Eval results *****
2022-06-12 06:04:54,267   att_loss = 4.098062362200522
2022-06-12 06:04:54,267   global_step = 32999
2022-06-12 06:04:54,267   loss = 5.155943731174493
2022-06-12 06:04:54,267   rep_loss = 1.0578813703683678
2022-06-12 06:04:54,267 ***** Save model *****
2022-06-12 06:06:17,627 ***** Running evaluation *****
2022-06-12 06:06:17,627   Epoch = 0 iter 7999 step
2022-06-12 06:06:17,627   Num examples = 9815
2022-06-12 06:06:17,627   Batch size = 32
2022-06-12 06:06:17,629 ***** Eval results *****
2022-06-12 06:06:17,629   att_loss = 16.379456034838103
2022-06-12 06:06:17,629   global_step = 7999
2022-06-12 06:06:17,629   loss = 17.485868968193437
2022-06-12 06:06:17,629   rep_loss = 1.1064129277443435
2022-06-12 06:06:17,630 ***** Save model *****
2022-06-12 06:07:05,505 ***** Running evaluation *****
2022-06-12 06:07:05,505   Epoch = 2 iter 33499 step
2022-06-12 06:07:05,505   Num examples = 40430
2022-06-12 06:07:05,505   Batch size = 32
2022-06-12 06:07:05,507 ***** Eval results *****
2022-06-12 06:07:05,507   att_loss = 4.09630149037258
2022-06-12 06:07:05,507   global_step = 33499
2022-06-12 06:07:05,507   loss = 5.15365581065264
2022-06-12 06:07:05,507   rep_loss = 1.0573543214046752
2022-06-12 06:07:05,507 ***** Save model *****
2022-06-12 06:08:27,920 ***** Running evaluation *****
2022-06-12 06:08:27,921   Epoch = 0 iter 8499 step
2022-06-12 06:08:27,921   Num examples = 9815
2022-06-12 06:08:27,921   Batch size = 32
2022-06-12 06:08:27,922 ***** Eval results *****
2022-06-12 06:08:27,923   att_loss = 16.27994329450495
2022-06-12 06:08:27,923   global_step = 8499
2022-06-12 06:08:27,923   loss = 17.379999483034236
2022-06-12 06:08:27,923   rep_loss = 1.1000561821753874
2022-06-12 06:08:27,923 ***** Save model *****
2022-06-12 06:09:16,918 ***** Running evaluation *****
2022-06-12 06:09:16,918   Epoch = 2 iter 33999 step
2022-06-12 06:09:16,918   Num examples = 40430
2022-06-12 06:09:16,919   Batch size = 32
2022-06-12 06:09:16,920 ***** Eval results *****
2022-06-12 06:09:16,920   att_loss = 4.094848467810496
2022-06-12 06:09:16,920   global_step = 33999
2022-06-12 06:09:16,920   loss = 5.151698954226715
2022-06-12 06:09:16,920   rep_loss = 1.0568504872950157
2022-06-12 06:09:16,920 ***** Save model *****
2022-06-12 06:10:37,393 ***** Running evaluation *****
2022-06-12 06:10:37,393   Epoch = 0 iter 8999 step
2022-06-12 06:10:37,393   Num examples = 9815
2022-06-12 06:10:37,393   Batch size = 32
2022-06-12 06:10:37,394 ***** Eval results *****
2022-06-12 06:10:37,395   att_loss = 16.19106876226197
2022-06-12 06:10:37,395   global_step = 8999
2022-06-12 06:10:37,395   loss = 17.285270146733325
2022-06-12 06:10:37,395   rep_loss = 1.0942013776955413
2022-06-12 06:10:37,395 ***** Save model *****
2022-06-12 06:11:28,539 ***** Running evaluation *****
2022-06-12 06:11:28,539   Epoch = 3 iter 34499 step
2022-06-12 06:11:28,539   Num examples = 40430
2022-06-12 06:11:28,539   Batch size = 32
2022-06-12 06:11:28,540 ***** Eval results *****
2022-06-12 06:11:28,541   att_loss = 3.8428550771698546
2022-06-12 06:11:28,541   global_step = 34499
2022-06-12 06:11:28,541   loss = 4.881181992111598
2022-06-12 06:11:28,541   rep_loss = 1.0383269085062807
2022-06-12 06:11:28,541 ***** Save model *****
2022-06-12 06:12:47,935 ***** Running evaluation *****
2022-06-12 06:12:47,935   Epoch = 0 iter 9499 step
2022-06-12 06:12:47,935   Num examples = 9815
2022-06-12 06:12:47,935   Batch size = 32
2022-06-12 06:12:47,936 ***** Eval results *****
2022-06-12 06:12:47,936   att_loss = 16.106409787705026
2022-06-12 06:12:47,937   global_step = 9499
2022-06-12 06:12:47,937   loss = 17.195143213823275
2022-06-12 06:12:47,937   rep_loss = 1.088733420194808
2022-06-12 06:12:47,937 ***** Save model *****
2022-06-12 06:13:41,493 ***** Running evaluation *****
2022-06-12 06:13:41,493   Epoch = 3 iter 34999 step
2022-06-12 06:13:41,494   Num examples = 40430
2022-06-12 06:13:41,494   Batch size = 32
2022-06-12 06:13:41,495 ***** Eval results *****
2022-06-12 06:13:41,495   att_loss = 3.8148304447950743
2022-06-12 06:13:41,495   global_step = 34999
2022-06-12 06:13:41,495   loss = 4.85098499763669
2022-06-12 06:13:41,495   rep_loss = 1.0361545514336319
2022-06-12 06:13:41,495 ***** Save model *****
2022-06-12 06:14:58,020 ***** Running evaluation *****
2022-06-12 06:14:58,021   Epoch = 0 iter 9999 step
2022-06-12 06:14:58,021   Num examples = 9815
2022-06-12 06:14:58,021   Batch size = 32
2022-06-12 06:14:58,022 ***** Eval results *****
2022-06-12 06:14:58,022   att_loss = 16.03255905977713
2022-06-12 06:14:58,022   global_step = 9999
2022-06-12 06:14:58,022   loss = 17.116189790899867
2022-06-12 06:14:58,022   rep_loss = 1.083630725131868
2022-06-12 06:14:58,023 ***** Save model *****
2022-06-12 06:15:53,153 ***** Running evaluation *****
2022-06-12 06:15:53,154   Epoch = 3 iter 35499 step
2022-06-12 06:15:53,154   Num examples = 40430
2022-06-12 06:15:53,154   Batch size = 32
2022-06-12 06:15:53,155 ***** Eval results *****
2022-06-12 06:15:53,155   att_loss = 3.806779717609469
2022-06-12 06:15:53,155   global_step = 35499
2022-06-12 06:15:53,155   loss = 4.8424636231127876
2022-06-12 06:15:53,155   rep_loss = 1.0356839052458475
2022-06-12 06:15:53,155 ***** Save model *****
2022-06-12 06:17:08,190 ***** Running evaluation *****
2022-06-12 06:17:08,190   Epoch = 0 iter 10499 step
2022-06-12 06:17:08,190   Num examples = 9815
2022-06-12 06:17:08,190   Batch size = 32
2022-06-12 06:17:08,192 ***** Eval results *****
2022-06-12 06:17:08,192   att_loss = 15.956013001695293
2022-06-12 06:17:08,192   global_step = 10499
2022-06-12 06:17:08,192   loss = 17.034786860844513
2022-06-12 06:17:08,192   rep_loss = 1.0787738530746425
2022-06-12 06:17:08,192 ***** Save model *****
2022-06-12 06:18:04,050 ***** Running evaluation *****
2022-06-12 06:18:04,050   Epoch = 3 iter 35999 step
2022-06-12 06:18:04,050   Num examples = 40430
2022-06-12 06:18:04,050   Batch size = 32
2022-06-12 06:18:04,052 ***** Eval results *****
2022-06-12 06:18:04,052   att_loss = 3.80485848613111
2022-06-12 06:18:04,052   global_step = 35999
2022-06-12 06:18:04,052   loss = 4.840490403662541
2022-06-12 06:18:04,052   rep_loss = 1.0356319157959861
2022-06-12 06:18:04,052 ***** Save model *****
2022-06-12 06:19:18,326 ***** Running evaluation *****
2022-06-12 06:19:18,327   Epoch = 0 iter 10999 step
2022-06-12 06:19:18,327   Num examples = 9815
2022-06-12 06:19:18,327   Batch size = 32
2022-06-12 06:19:18,329 ***** Eval results *****
2022-06-12 06:19:18,329   att_loss = 15.884153004093207
2022-06-12 06:19:18,329   global_step = 10999
2022-06-12 06:19:18,329   loss = 16.958361384629963
2022-06-12 06:19:18,329   rep_loss = 1.0742083763694728
2022-06-12 06:19:18,329 ***** Save model *****
2022-06-12 06:20:15,190 ***** Running evaluation *****
2022-06-12 06:20:15,191   Epoch = 3 iter 36499 step
2022-06-12 06:20:15,191   Num examples = 40430
2022-06-12 06:20:15,191   Batch size = 32
2022-06-12 06:20:15,192 ***** Eval results *****
2022-06-12 06:20:15,192   att_loss = 3.8058274641551946
2022-06-12 06:20:15,192   global_step = 36499
2022-06-12 06:20:15,192   loss = 4.841194211214145
2022-06-12 06:20:15,192   rep_loss = 1.0353667482814812
2022-06-12 06:20:15,192 ***** Save model *****
2022-06-12 06:21:29,436 ***** Running evaluation *****
2022-06-12 06:21:29,437   Epoch = 0 iter 11499 step
2022-06-12 06:21:29,437   Num examples = 9815
2022-06-12 06:21:29,437   Batch size = 32
2022-06-12 06:21:29,438 ***** Eval results *****
2022-06-12 06:21:29,438   att_loss = 15.814952652374966
2022-06-12 06:21:29,438   global_step = 11499
2022-06-12 06:21:29,438   loss = 16.884852109717144
2022-06-12 06:21:29,438   rep_loss = 1.0698994531902253
2022-06-12 06:21:29,439 ***** Save model *****
2022-06-12 06:22:26,514 ***** Running evaluation *****
2022-06-12 06:22:26,514   Epoch = 3 iter 36999 step
2022-06-12 06:22:26,514   Num examples = 40430
2022-06-12 06:22:26,514   Batch size = 32
2022-06-12 06:22:26,516 ***** Eval results *****
2022-06-12 06:22:26,516   att_loss = 3.8131876385471903
2022-06-12 06:22:26,516   global_step = 36999
2022-06-12 06:22:26,516   loss = 4.848483496365889
2022-06-12 06:22:26,516   rep_loss = 1.035295860872173
2022-06-12 06:22:26,516 ***** Save model *****
2022-06-12 06:23:40,080 ***** Running evaluation *****
2022-06-12 06:23:40,081   Epoch = 0 iter 11999 step
2022-06-12 06:23:40,081   Num examples = 9815
2022-06-12 06:23:40,081   Batch size = 32
2022-06-12 06:23:40,083 ***** Eval results *****
2022-06-12 06:23:40,083   att_loss = 15.746760209070285
2022-06-12 06:23:40,083   global_step = 11999
2022-06-12 06:23:40,083   loss = 16.812520645272585
2022-06-12 06:23:40,083   rep_loss = 1.0657604315775873
2022-06-12 06:23:40,083 ***** Save model *****
2022-06-12 06:24:37,982 ***** Running evaluation *****
2022-06-12 06:24:37,982   Epoch = 3 iter 37499 step
2022-06-12 06:24:37,982   Num examples = 40430
2022-06-12 06:24:37,982   Batch size = 32
2022-06-12 06:24:37,983 ***** Eval results *****
2022-06-12 06:24:37,984   att_loss = 3.816955027168006
2022-06-12 06:24:37,984   global_step = 37499
2022-06-12 06:24:37,984   loss = 4.85203992282792
2022-06-12 06:24:37,984   rep_loss = 1.0350848984739427
2022-06-12 06:24:37,984 ***** Save model *****
2022-06-12 06:25:49,280 ***** Running evaluation *****
2022-06-12 06:25:49,280   Epoch = 1 iter 12499 step
2022-06-12 06:25:49,280   Num examples = 9815
2022-06-12 06:25:49,280   Batch size = 32
2022-06-12 06:25:49,282 ***** Eval results *****
2022-06-12 06:25:49,282   att_loss = 13.899899871725784
2022-06-12 06:25:49,282   global_step = 12499
2022-06-12 06:25:49,282   loss = 14.869013681746365
2022-06-12 06:25:49,282   rep_loss = 0.9691138460970762
2022-06-12 06:25:49,282 ***** Save model *****
2022-06-12 06:26:49,171 ***** Running evaluation *****
2022-06-12 06:26:49,171   Epoch = 3 iter 37999 step
2022-06-12 06:26:49,171   Num examples = 40430
2022-06-12 06:26:49,171   Batch size = 32
2022-06-12 06:26:49,172 ***** Eval results *****
2022-06-12 06:26:49,172   att_loss = 3.8182169023961574
2022-06-12 06:26:49,173   global_step = 37999
2022-06-12 06:26:49,173   loss = 4.8530662102085
2022-06-12 06:26:49,173   rep_loss = 1.0348493103258836
2022-06-12 06:26:49,173 ***** Save model *****
2022-06-12 06:27:58,469 ***** Running evaluation *****
2022-06-12 06:27:58,469   Epoch = 1 iter 12999 step
2022-06-12 06:27:58,469   Num examples = 9815
2022-06-12 06:27:58,469   Batch size = 32
2022-06-12 06:27:58,471 ***** Eval results *****
2022-06-12 06:27:58,471   att_loss = 13.797712492418814
2022-06-12 06:27:58,471   global_step = 12999
2022-06-12 06:27:58,471   loss = 14.7630685541656
2022-06-12 06:27:58,471   rep_loss = 0.9653560698523627
2022-06-12 06:27:58,471 ***** Save model *****
2022-06-12 06:29:01,629 ***** Running evaluation *****
2022-06-12 06:29:01,630   Epoch = 3 iter 38499 step
2022-06-12 06:29:01,630   Num examples = 40430
2022-06-12 06:29:01,630   Batch size = 32
2022-06-12 06:29:01,631 ***** Eval results *****
2022-06-12 06:29:01,631   att_loss = 3.8174063397257822
2022-06-12 06:29:01,632   global_step = 38499
2022-06-12 06:29:01,632   loss = 4.851781843082865
2022-06-12 06:29:01,632   rep_loss = 1.0343755060460145
2022-06-12 06:29:01,632 ***** Save model *****
2022-06-12 06:30:09,350 ***** Running evaluation *****
2022-06-12 06:30:09,350   Epoch = 1 iter 13499 step
2022-06-12 06:30:09,350   Num examples = 9815
2022-06-12 06:30:09,351   Batch size = 32
2022-06-12 06:30:09,352 ***** Eval results *****
2022-06-12 06:30:09,352   att_loss = 13.776845572437448
2022-06-12 06:30:09,352   global_step = 13499
2022-06-12 06:30:09,352   loss = 14.741072793736908
2022-06-12 06:30:09,352   rep_loss = 0.9642272205228526
2022-06-12 06:30:09,353 ***** Save model *****
2022-06-12 06:31:13,123 ***** Running evaluation *****
2022-06-12 06:31:13,123   Epoch = 3 iter 38999 step
2022-06-12 06:31:13,124   Num examples = 40430
2022-06-12 06:31:13,124   Batch size = 32
2022-06-12 06:31:13,125 ***** Eval results *****
2022-06-12 06:31:13,125   att_loss = 3.8193907877691298
2022-06-12 06:31:13,125   global_step = 38999
2022-06-12 06:31:13,125   loss = 4.853557783978486
2022-06-12 06:31:13,125   rep_loss = 1.0341669982209676
2022-06-12 06:31:13,125 ***** Save model *****
2022-06-12 06:32:19,688 ***** Running evaluation *****
2022-06-12 06:32:19,688   Epoch = 1 iter 13999 step
2022-06-12 06:32:19,688   Num examples = 9815
2022-06-12 06:32:19,688   Batch size = 32
2022-06-12 06:32:19,690 ***** Eval results *****
2022-06-12 06:32:19,690   att_loss = 13.776128104439488
2022-06-12 06:32:19,690   global_step = 13999
2022-06-12 06:32:19,690   loss = 14.73888079987632
2022-06-12 06:32:19,690   rep_loss = 0.9627526941950675
2022-06-12 06:32:19,691 ***** Save model *****
2022-06-12 06:33:24,538 ***** Running evaluation *****
2022-06-12 06:33:24,539   Epoch = 3 iter 39499 step
2022-06-12 06:33:24,539   Num examples = 40430
2022-06-12 06:33:24,539   Batch size = 32
2022-06-12 06:33:24,540 ***** Eval results *****
2022-06-12 06:33:24,540   att_loss = 3.8165939801432556
2022-06-12 06:33:24,540   global_step = 39499
2022-06-12 06:33:24,540   loss = 4.850365998904677
2022-06-12 06:33:24,541   rep_loss = 1.0337720214491055
2022-06-12 06:33:24,541 ***** Save model *****
2022-06-12 06:34:30,288 ***** Running evaluation *****
2022-06-12 06:34:30,288   Epoch = 1 iter 14499 step
2022-06-12 06:34:30,288   Num examples = 9815
2022-06-12 06:34:30,288   Batch size = 32
2022-06-12 06:34:30,289 ***** Eval results *****
2022-06-12 06:34:30,290   att_loss = 13.73811108248452
2022-06-12 06:34:30,290   global_step = 14499
2022-06-12 06:34:30,290   loss = 14.69955675178199
2022-06-12 06:34:30,290   rep_loss = 0.9614456732300924
2022-06-12 06:34:30,290 ***** Save model *****
2022-06-12 06:35:36,471 ***** Running evaluation *****
2022-06-12 06:35:36,472   Epoch = 3 iter 39999 step
2022-06-12 06:35:36,472   Num examples = 40430
2022-06-12 06:35:36,472   Batch size = 32
2022-06-12 06:35:36,474 ***** Eval results *****
2022-06-12 06:35:36,474   att_loss = 3.8143575327123926
2022-06-12 06:35:36,474   global_step = 39999
2022-06-12 06:35:36,475   loss = 4.847678643654311
2022-06-12 06:35:36,475   rep_loss = 1.03332111304716
2022-06-12 06:35:36,475 ***** Save model *****
2022-06-12 06:36:40,356 ***** Running evaluation *****
2022-06-12 06:36:40,357   Epoch = 1 iter 14999 step
2022-06-12 06:36:40,357   Num examples = 9815
2022-06-12 06:36:40,357   Batch size = 32
2022-06-12 06:36:40,358 ***** Eval results *****
2022-06-12 06:36:40,358   att_loss = 13.7291586049491
2022-06-12 06:36:40,358   global_step = 14999
2022-06-12 06:36:40,358   loss = 14.68960991947532
2022-06-12 06:36:40,359   rep_loss = 0.9604513096319965
2022-06-12 06:36:40,359 ***** Save model *****
2022-06-12 06:37:48,109 ***** Running evaluation *****
2022-06-12 06:37:48,110   Epoch = 3 iter 40499 step
2022-06-12 06:37:48,110   Num examples = 40430
2022-06-12 06:37:48,110   Batch size = 32
2022-06-12 06:37:48,111 ***** Eval results *****
2022-06-12 06:37:48,111   att_loss = 3.810425330559251
2022-06-12 06:37:48,112   global_step = 40499
2022-06-12 06:37:48,112   loss = 4.843432821602791
2022-06-12 06:37:48,112   rep_loss = 1.0330074921537218
2022-06-12 06:37:48,112 ***** Save model *****
2022-06-12 06:38:49,642 ***** Running evaluation *****
2022-06-12 06:38:49,642   Epoch = 1 iter 15499 step
2022-06-12 06:38:49,642   Num examples = 9815
2022-06-12 06:38:49,643   Batch size = 32
2022-06-12 06:38:49,644 ***** Eval results *****
2022-06-12 06:38:49,644   att_loss = 13.703745315804653
2022-06-12 06:38:49,644   global_step = 15499
2022-06-12 06:38:49,644   loss = 14.663179879738053
2022-06-12 06:38:49,644   rep_loss = 0.959434557045998
2022-06-12 06:38:49,644 ***** Save model *****
2022-06-12 06:39:59,862 ***** Running evaluation *****
2022-06-12 06:39:59,863   Epoch = 3 iter 40999 step
2022-06-12 06:39:59,863   Num examples = 40430
2022-06-12 06:39:59,863   Batch size = 32
2022-06-12 06:39:59,864 ***** Eval results *****
2022-06-12 06:39:59,864   att_loss = 3.8106793330462927
2022-06-12 06:39:59,864   global_step = 40999
2022-06-12 06:39:59,864   loss = 4.843440956916427
2022-06-12 06:39:59,864   rep_loss = 1.0327616235759611
2022-06-12 06:39:59,864 ***** Save model *****
2022-06-12 06:41:00,775 ***** Running evaluation *****
2022-06-12 06:41:00,776   Epoch = 1 iter 15999 step
2022-06-12 06:41:00,776   Num examples = 9815
2022-06-12 06:41:00,776   Batch size = 32
2022-06-12 06:41:00,777 ***** Eval results *****
2022-06-12 06:41:00,778   att_loss = 13.695408771222242
2022-06-12 06:41:00,778   global_step = 15999
2022-06-12 06:41:00,778   loss = 14.654017183402065
2022-06-12 06:41:00,778   rep_loss = 0.9586084058324411
2022-06-12 06:41:00,778 ***** Save model *****
2022-06-12 06:42:11,023 ***** Running evaluation *****
2022-06-12 06:42:11,023   Epoch = 3 iter 41499 step
2022-06-12 06:42:11,023   Num examples = 40430
2022-06-12 06:42:11,023   Batch size = 32
2022-06-12 06:42:11,025 ***** Eval results *****
2022-06-12 06:42:11,025   att_loss = 3.811237085525424
2022-06-12 06:42:11,025   global_step = 41499
2022-06-12 06:42:11,025   loss = 4.843873033473224
2022-06-12 06:42:11,025   rep_loss = 1.0326359470362656
2022-06-12 06:42:11,025 ***** Save model *****
2022-06-12 06:43:12,077 ***** Running evaluation *****
2022-06-12 06:43:12,078   Epoch = 1 iter 16499 step
2022-06-12 06:43:12,078   Num examples = 9815
2022-06-12 06:43:12,078   Batch size = 32
2022-06-12 06:43:12,080 ***** Eval results *****
2022-06-12 06:43:12,080   att_loss = 13.684455066101666
2022-06-12 06:43:12,080   global_step = 16499
2022-06-12 06:43:12,080   loss = 14.642048773950524
2022-06-12 06:43:12,080   rep_loss = 0.9575936991224452
2022-06-12 06:43:12,080 ***** Save model *****
2022-06-12 06:44:24,148 ***** Running evaluation *****
2022-06-12 06:44:24,149   Epoch = 3 iter 41999 step
2022-06-12 06:44:24,149   Num examples = 40430
2022-06-12 06:44:24,149   Batch size = 32
2022-06-12 06:44:24,150 ***** Eval results *****
2022-06-12 06:44:24,150   att_loss = 3.8135599483780602
2022-06-12 06:44:24,150   global_step = 41999
2022-06-12 06:44:24,150   loss = 4.845900199125892
2022-06-12 06:44:24,151   rep_loss = 1.0323402500754
2022-06-12 06:44:24,151 ***** Save model *****
2022-06-12 06:45:23,446 ***** Running evaluation *****
2022-06-12 06:45:23,447   Epoch = 1 iter 16999 step
2022-06-12 06:45:23,447   Num examples = 9815
2022-06-12 06:45:23,447   Batch size = 32
2022-06-12 06:45:23,448 ***** Eval results *****
2022-06-12 06:45:23,448   att_loss = 13.668674039558308
2022-06-12 06:45:23,448   global_step = 16999
2022-06-12 06:45:23,448   loss = 14.62539694656172
2022-06-12 06:45:23,449   rep_loss = 0.9567228986073266
2022-06-12 06:45:23,449 ***** Save model *****
2022-06-12 06:46:36,483 ***** Running evaluation *****
2022-06-12 06:46:36,483   Epoch = 3 iter 42499 step
2022-06-12 06:46:36,483   Num examples = 40430
2022-06-12 06:46:36,483   Batch size = 32
2022-06-12 06:46:36,484 ***** Eval results *****
2022-06-12 06:46:36,484   att_loss = 3.8152729267016228
2022-06-12 06:46:36,484   global_step = 42499
2022-06-12 06:46:36,485   loss = 4.847444583781642
2022-06-12 06:46:36,485   rep_loss = 1.0321716571865953
2022-06-12 06:46:36,485 ***** Save model *****
2022-06-12 06:47:34,547 ***** Running evaluation *****
2022-06-12 06:47:34,547   Epoch = 1 iter 17499 step
2022-06-12 06:47:34,547   Num examples = 9815
2022-06-12 06:47:34,547   Batch size = 32
2022-06-12 06:47:34,549 ***** Eval results *****
2022-06-12 06:47:34,549   att_loss = 13.638619795407424
2022-06-12 06:47:34,549   global_step = 17499
2022-06-12 06:47:34,549   loss = 14.594355234410257
2022-06-12 06:47:34,549   rep_loss = 0.9557354308168854
2022-06-12 06:47:34,549 ***** Save model *****
2022-06-12 06:48:48,577 ***** Running evaluation *****
2022-06-12 06:48:48,577   Epoch = 3 iter 42999 step
2022-06-12 06:48:48,578   Num examples = 40430
2022-06-12 06:48:48,578   Batch size = 32
2022-06-12 06:48:48,579 ***** Eval results *****
2022-06-12 06:48:48,579   att_loss = 3.813063330236738
2022-06-12 06:48:48,580   global_step = 42999
2022-06-12 06:48:48,580   loss = 4.844900732542508
2022-06-12 06:48:48,580   rep_loss = 1.0318374016352265
2022-06-12 06:48:48,580 ***** Save model *****
2022-06-12 06:49:43,915 ***** Running evaluation *****
2022-06-12 06:49:43,916   Epoch = 1 iter 17999 step
2022-06-12 06:49:43,916   Num examples = 9815
2022-06-12 06:49:43,916   Batch size = 32
2022-06-12 06:49:43,917 ***** Eval results *****
2022-06-12 06:49:43,917   att_loss = 13.626828728441419
2022-06-12 06:49:43,918   global_step = 17999
2022-06-12 06:49:43,918   loss = 14.581887122639065
2022-06-12 06:49:43,918   rep_loss = 0.9550583854567405
2022-06-12 06:49:43,918 ***** Save model *****
2022-06-12 06:51:01,701 ***** Running evaluation *****
2022-06-12 06:51:01,702   Epoch = 3 iter 43499 step
2022-06-12 06:51:01,702   Num examples = 40430
2022-06-12 06:51:01,702   Batch size = 32
2022-06-12 06:51:01,703 ***** Eval results *****
2022-06-12 06:51:01,703   att_loss = 3.8137639945200146
2022-06-12 06:51:01,703   global_step = 43499
2022-06-12 06:51:01,703   loss = 4.845420451016456
2022-06-12 06:51:01,703   rep_loss = 1.0316564560901473
2022-06-12 06:51:01,703 ***** Save model *****
2022-06-12 06:51:53,417 ***** Running evaluation *****
2022-06-12 06:51:53,418   Epoch = 1 iter 18499 step
2022-06-12 06:51:53,418   Num examples = 9815
2022-06-12 06:51:53,418   Batch size = 32
2022-06-12 06:51:53,419 ***** Eval results *****
2022-06-12 06:51:53,420   att_loss = 13.615568443529869
2022-06-12 06:51:53,420   global_step = 18499
2022-06-12 06:51:53,420   loss = 14.569797676039876
2022-06-12 06:51:53,420   rep_loss = 0.9542292241550212
2022-06-12 06:51:53,420 ***** Save model *****
2022-06-12 06:53:15,450 ***** Running evaluation *****
2022-06-12 06:53:15,450   Epoch = 3 iter 43999 step
2022-06-12 06:53:15,450   Num examples = 40430
2022-06-12 06:53:15,450   Batch size = 32
2022-06-12 06:53:15,451 ***** Eval results *****
2022-06-12 06:53:15,452   att_loss = 3.8121675428310846
2022-06-12 06:53:15,452   global_step = 43999
2022-06-12 06:53:15,452   loss = 4.843503581608423
2022-06-12 06:53:15,452   rep_loss = 1.0313360385000796
2022-06-12 06:53:15,452 ***** Save model *****
2022-06-12 06:54:02,889 ***** Running evaluation *****
2022-06-12 06:54:02,889   Epoch = 1 iter 18999 step
2022-06-12 06:54:02,889   Num examples = 9815
2022-06-12 06:54:02,889   Batch size = 32
2022-06-12 06:54:02,891 ***** Eval results *****
2022-06-12 06:54:02,891   att_loss = 13.596273547403877
2022-06-12 06:54:02,891   global_step = 18999
2022-06-12 06:54:02,891   loss = 14.549689862730954
2022-06-12 06:54:02,891   rep_loss = 0.9534163083371736
2022-06-12 06:54:02,891 ***** Save model *****
2022-06-12 06:55:28,082 ***** Running evaluation *****
2022-06-12 06:55:28,083   Epoch = 3 iter 44499 step
2022-06-12 06:55:28,083   Num examples = 40430
2022-06-12 06:55:28,083   Batch size = 32
2022-06-12 06:55:28,084 ***** Eval results *****
2022-06-12 06:55:28,084   att_loss = 3.811844173897356
2022-06-12 06:55:28,084   global_step = 44499
2022-06-12 06:55:28,084   loss = 4.842848086244666
2022-06-12 06:55:28,084   rep_loss = 1.0310039123415728
2022-06-12 06:55:28,084 ***** Save model *****
2022-06-12 06:56:12,388 ***** Running evaluation *****
2022-06-12 06:56:12,388   Epoch = 1 iter 19499 step
2022-06-12 06:56:12,389   Num examples = 9815
2022-06-12 06:56:12,389   Batch size = 32
2022-06-12 06:56:12,390 ***** Eval results *****
2022-06-12 06:56:12,390   att_loss = 13.577206252751987
2022-06-12 06:56:12,390   global_step = 19499
2022-06-12 06:56:12,390   loss = 14.52977805646929
2022-06-12 06:56:12,390   rep_loss = 0.9525717969717872
2022-06-12 06:56:12,390 ***** Save model *****
2022-06-12 06:57:39,546 ***** Running evaluation *****
2022-06-12 06:57:39,547   Epoch = 3 iter 44999 step
2022-06-12 06:57:39,547   Num examples = 40430
2022-06-12 06:57:39,547   Batch size = 32
2022-06-12 06:57:39,548 ***** Eval results *****
2022-06-12 06:57:39,548   att_loss = 3.811328901295623
2022-06-12 06:57:39,548   global_step = 44999
2022-06-12 06:57:39,548   loss = 4.842087334427846
2022-06-12 06:57:39,548   rep_loss = 1.0307584333128592
2022-06-12 06:57:39,548 ***** Save model *****
2022-06-12 06:58:23,015 ***** Running evaluation *****
2022-06-12 06:58:23,015   Epoch = 1 iter 19999 step
2022-06-12 06:58:23,015   Num examples = 9815
2022-06-12 06:58:23,015   Batch size = 32
2022-06-12 06:58:23,016 ***** Eval results *****
2022-06-12 06:58:23,017   att_loss = 13.56760886823662
2022-06-12 06:58:23,017   global_step = 19999
2022-06-12 06:58:23,017   loss = 14.519549180015026
2022-06-12 06:58:23,017   rep_loss = 0.9519403044975076
2022-06-12 06:58:23,017 ***** Save model *****
2022-06-12 06:59:52,426 ***** Running evaluation *****
2022-06-12 06:59:52,427   Epoch = 4 iter 45499 step
2022-06-12 06:59:52,427   Num examples = 40430
2022-06-12 06:59:52,427   Batch size = 32
2022-06-12 06:59:52,428 ***** Eval results *****
2022-06-12 06:59:52,428   att_loss = 3.583182824285407
2022-06-12 06:59:52,428   global_step = 45499
2022-06-12 06:59:52,428   loss = 4.597428346935072
2022-06-12 06:59:52,429   rep_loss = 1.0142454881417124
2022-06-12 06:59:52,429 ***** Save model *****
2022-06-12 07:00:33,055 ***** Running evaluation *****
2022-06-12 07:00:33,056   Epoch = 1 iter 20499 step
2022-06-12 07:00:33,056   Num examples = 9815
2022-06-12 07:00:33,056   Batch size = 32
2022-06-12 07:00:33,057 ***** Eval results *****
2022-06-12 07:00:33,058   att_loss = 13.552295477226862
2022-06-12 07:00:33,058   global_step = 20499
2022-06-12 07:00:33,058   loss = 14.503370421223694
2022-06-12 07:00:33,058   rep_loss = 0.9510749368903472
2022-06-12 07:00:33,058 ***** Save model *****
2022-06-12 07:02:04,637 ***** Running evaluation *****
2022-06-12 07:02:04,637   Epoch = 4 iter 45999 step
2022-06-12 07:02:04,637   Num examples = 40430
2022-06-12 07:02:04,637   Batch size = 32
2022-06-12 07:02:04,639 ***** Eval results *****
2022-06-12 07:02:04,639   att_loss = 3.584750032608679
2022-06-12 07:02:04,639   global_step = 45999
2022-06-12 07:02:04,639   loss = 4.599349064634025
2022-06-12 07:02:04,639   rep_loss = 1.0145990228377326
2022-06-12 07:02:04,639 ***** Save model *****
2022-06-12 07:02:43,900 ***** Running evaluation *****
2022-06-12 07:02:43,901   Epoch = 1 iter 20999 step
2022-06-12 07:02:43,901   Num examples = 9815
2022-06-12 07:02:43,901   Batch size = 32
2022-06-12 07:02:43,903 ***** Eval results *****
2022-06-12 07:02:43,903   att_loss = 13.53139084102253
2022-06-12 07:02:43,903   global_step = 20999
2022-06-12 07:02:43,903   loss = 14.48170876273532
2022-06-12 07:02:43,903   rep_loss = 0.9503179156075476
2022-06-12 07:02:43,903 ***** Save model *****
2022-06-12 07:04:16,308 ***** Running evaluation *****
2022-06-12 07:04:16,308   Epoch = 4 iter 46499 step
2022-06-12 07:04:16,308   Num examples = 40430
2022-06-12 07:04:16,308   Batch size = 32
2022-06-12 07:04:16,310 ***** Eval results *****
2022-06-12 07:04:16,310   att_loss = 3.588673393672535
2022-06-12 07:04:16,310   global_step = 46499
2022-06-12 07:04:16,310   loss = 4.603730846317055
2022-06-12 07:04:16,310   rep_loss = 1.0150574467366997
2022-06-12 07:04:16,310 ***** Save model *****
2022-06-12 07:04:53,531 ***** Running evaluation *****
2022-06-12 07:04:53,531   Epoch = 1 iter 21499 step
2022-06-12 07:04:53,531   Num examples = 9815
2022-06-12 07:04:53,531   Batch size = 32
2022-06-12 07:04:53,533 ***** Eval results *****
2022-06-12 07:04:53,533   att_loss = 13.513068367817931
2022-06-12 07:04:53,533   global_step = 21499
2022-06-12 07:04:53,533   loss = 14.462586291279687
2022-06-12 07:04:53,533   rep_loss = 0.9495179174224897
2022-06-12 07:04:53,533 ***** Save model *****
2022-06-12 07:06:27,781 ***** Running evaluation *****
2022-06-12 07:06:27,782   Epoch = 4 iter 46999 step
2022-06-12 07:06:27,782   Num examples = 40430
2022-06-12 07:06:27,782   Batch size = 32
2022-06-12 07:06:27,783 ***** Eval results *****
2022-06-12 07:06:27,783   att_loss = 3.590722959711804
2022-06-12 07:06:27,783   global_step = 46999
2022-06-12 07:06:27,783   loss = 4.605310176845598
2022-06-12 07:06:27,783   rep_loss = 1.0145872121503905
2022-06-12 07:06:27,784 ***** Save model *****
2022-06-12 07:07:03,027 ***** Running evaluation *****
2022-06-12 07:07:03,027   Epoch = 1 iter 21999 step
2022-06-12 07:07:03,027   Num examples = 9815
2022-06-12 07:07:03,028   Batch size = 32
2022-06-12 07:07:03,029 ***** Eval results *****
2022-06-12 07:07:03,029   att_loss = 13.49862536101749
2022-06-12 07:07:03,029   global_step = 21999
2022-06-12 07:07:03,029   loss = 14.447449991773619
2022-06-12 07:07:03,029   rep_loss = 0.9488246240162928
2022-06-12 07:07:03,029 ***** Save model *****
2022-06-12 07:08:39,400 ***** Running evaluation *****
2022-06-12 07:08:39,400   Epoch = 4 iter 47499 step
2022-06-12 07:08:39,400   Num examples = 40430
2022-06-12 07:08:39,400   Batch size = 32
2022-06-12 07:08:39,402 ***** Eval results *****
2022-06-12 07:08:39,402   att_loss = 3.594297362649246
2022-06-12 07:08:39,402   global_step = 47499
2022-06-12 07:08:39,402   loss = 4.609230076693969
2022-06-12 07:08:39,402   rep_loss = 1.0149327076089565
2022-06-12 07:08:39,402 ***** Save model *****
2022-06-12 07:09:12,488 ***** Running evaluation *****
2022-06-12 07:09:12,488   Epoch = 1 iter 22499 step
2022-06-12 07:09:12,488   Num examples = 9815
2022-06-12 07:09:12,489   Batch size = 32
2022-06-12 07:09:12,490 ***** Eval results *****
2022-06-12 07:09:12,490   att_loss = 13.481232608907801
2022-06-12 07:09:12,490   global_step = 22499
2022-06-12 07:09:12,490   loss = 14.429353874737647
2022-06-12 07:09:12,490   rep_loss = 0.9481212591397655
2022-06-12 07:09:12,490 ***** Save model *****
2022-06-12 07:10:50,956 ***** Running evaluation *****
2022-06-12 07:10:50,956   Epoch = 4 iter 47999 step
2022-06-12 07:10:50,956   Num examples = 40430
2022-06-12 07:10:50,956   Batch size = 32
2022-06-12 07:10:50,958 ***** Eval results *****
2022-06-12 07:10:50,958   att_loss = 3.606239919011297
2022-06-12 07:10:50,958   global_step = 47999
2022-06-12 07:10:50,958   loss = 4.621387702882077
2022-06-12 07:10:50,958   rep_loss = 1.0151477775766813
2022-06-12 07:10:50,958 ***** Save model *****
2022-06-12 07:11:22,507 ***** Running evaluation *****
2022-06-12 07:11:22,508   Epoch = 1 iter 22999 step
2022-06-12 07:11:22,508   Num examples = 9815
2022-06-12 07:11:22,508   Batch size = 32
2022-06-12 07:11:22,509 ***** Eval results *****
2022-06-12 07:11:22,509   att_loss = 13.471095636920374
2022-06-12 07:11:22,509   global_step = 22999
2022-06-12 07:11:22,510   loss = 14.418624077867342
2022-06-12 07:11:22,510   rep_loss = 0.9475284344520167
2022-06-12 07:11:22,510 ***** Save model *****
2022-06-12 07:13:02,643 ***** Running evaluation *****
2022-06-12 07:13:02,643   Epoch = 4 iter 48499 step
2022-06-12 07:13:02,643   Num examples = 40430
2022-06-12 07:13:02,643   Batch size = 32
2022-06-12 07:13:02,645 ***** Eval results *****
2022-06-12 07:13:02,645   att_loss = 3.6061492898915137
2022-06-12 07:13:02,645   global_step = 48499
2022-06-12 07:13:02,645   loss = 4.620978398052973
2022-06-12 07:13:02,645   rep_loss = 1.0148291028505447
2022-06-12 07:13:02,645 ***** Save model *****
2022-06-12 07:13:32,941 ***** Running evaluation *****
2022-06-12 07:13:32,942   Epoch = 1 iter 23499 step
2022-06-12 07:13:32,942   Num examples = 9815
2022-06-12 07:13:32,942   Batch size = 32
2022-06-12 07:13:32,943 ***** Eval results *****
2022-06-12 07:13:32,943   att_loss = 13.446673036513483
2022-06-12 07:13:32,943   global_step = 23499
2022-06-12 07:13:32,943   loss = 14.393450775645915
2022-06-12 07:13:32,943   rep_loss = 0.946777733139054
2022-06-12 07:13:32,943 ***** Save model *****
2022-06-12 07:15:15,496 ***** Running evaluation *****
2022-06-12 07:15:15,496   Epoch = 4 iter 48999 step
2022-06-12 07:15:15,496   Num examples = 40430
2022-06-12 07:15:15,496   Batch size = 32
2022-06-12 07:15:15,498 ***** Eval results *****
2022-06-12 07:15:15,498   att_loss = 3.6050797465444457
2022-06-12 07:15:15,498   global_step = 48999
2022-06-12 07:15:15,498   loss = 4.619634172055287
2022-06-12 07:15:15,498   rep_loss = 1.014554422512824
2022-06-12 07:15:15,498 ***** Save model *****
2022-06-12 07:15:43,102 ***** Running evaluation *****
2022-06-12 07:15:43,102   Epoch = 1 iter 23999 step
2022-06-12 07:15:43,103   Num examples = 9815
2022-06-12 07:15:43,103   Batch size = 32
2022-06-12 07:15:43,104 ***** Eval results *****
2022-06-12 07:15:43,104   att_loss = 13.429526347743213
2022-06-12 07:15:43,104   global_step = 23999
2022-06-12 07:15:43,104   loss = 14.375682890984404
2022-06-12 07:15:43,104   rep_loss = 0.9461565373711908
2022-06-12 07:15:43,104 ***** Save model *****
2022-06-12 07:17:27,603 ***** Running evaluation *****
2022-06-12 07:17:27,604   Epoch = 4 iter 49499 step
2022-06-12 07:17:27,604   Num examples = 40430
2022-06-12 07:17:27,604   Batch size = 32
2022-06-12 07:17:27,606 ***** Eval results *****
2022-06-12 07:17:27,606   att_loss = 3.606325396497204
2022-06-12 07:17:27,606   global_step = 49499
2022-06-12 07:17:27,606   loss = 4.620763401155835
2022-06-12 07:17:27,606   rep_loss = 1.0144380019742714
2022-06-12 07:17:27,607 ***** Save model *****
2022-06-12 07:17:52,570 ***** Running evaluation *****
2022-06-12 07:17:52,571   Epoch = 1 iter 24499 step
2022-06-12 07:17:52,571   Num examples = 9815
2022-06-12 07:17:52,571   Batch size = 32
2022-06-12 07:17:52,572 ***** Eval results *****
2022-06-12 07:17:52,573   att_loss = 13.411879187204264
2022-06-12 07:17:52,573   global_step = 24499
2022-06-12 07:17:52,573   loss = 14.357390187088477
2022-06-12 07:17:52,573   rep_loss = 0.9455109932744741
2022-06-12 07:17:52,573 ***** Save model *****
2022-06-12 07:19:39,169 ***** Running evaluation *****
2022-06-12 07:19:39,170   Epoch = 4 iter 49999 step
2022-06-12 07:19:39,170   Num examples = 40430
2022-06-12 07:19:39,170   Batch size = 32
2022-06-12 07:19:39,172 ***** Eval results *****
2022-06-12 07:19:39,172   att_loss = 3.612308496868592
2022-06-12 07:19:39,172   global_step = 49999
2022-06-12 07:19:39,172   loss = 4.626767079316823
2022-06-12 07:19:39,172   rep_loss = 1.0144585792035439
2022-06-12 07:19:39,172 ***** Save model *****
2022-06-12 07:20:02,335 ***** Running evaluation *****
2022-06-12 07:20:02,336   Epoch = 2 iter 24999 step
2022-06-12 07:20:02,336   Num examples = 9815
2022-06-12 07:20:02,336   Batch size = 32
2022-06-12 07:20:02,337 ***** Eval results *****
2022-06-12 07:20:02,337   att_loss = 12.661391112162717
2022-06-12 07:20:02,337   global_step = 24999
2022-06-12 07:20:02,337   loss = 13.58923727782602
2022-06-12 07:20:02,337   rep_loss = 0.9278461467515458
2022-06-12 07:20:02,337 ***** Save model *****
2022-06-12 07:21:51,188 ***** Running evaluation *****
2022-06-12 07:21:51,188   Epoch = 4 iter 50499 step
2022-06-12 07:21:51,189   Num examples = 40430
2022-06-12 07:21:51,189   Batch size = 32
2022-06-12 07:21:51,190 ***** Eval results *****
2022-06-12 07:21:51,190   att_loss = 3.61248729594273
2022-06-12 07:21:51,190   global_step = 50499
2022-06-12 07:21:51,190   loss = 4.626555859603167
2022-06-12 07:21:51,191   rep_loss = 1.0140685603114612
2022-06-12 07:21:51,191 ***** Save model *****
2022-06-12 07:22:11,507 ***** Running evaluation *****
2022-06-12 07:22:11,507   Epoch = 2 iter 25499 step
2022-06-12 07:22:11,507   Num examples = 9815
2022-06-12 07:22:11,507   Batch size = 32
2022-06-12 07:22:11,509 ***** Eval results *****
2022-06-12 07:22:11,509   att_loss = 12.612117089697062
2022-06-12 07:22:11,509   global_step = 25499
2022-06-12 07:22:11,509   loss = 13.53835808645346
2022-06-12 07:22:11,509   rep_loss = 0.9262409879122409
2022-06-12 07:22:11,509 ***** Save model *****
2022-06-12 07:24:02,788 ***** Running evaluation *****
2022-06-12 07:24:02,790   Epoch = 4 iter 50999 step
2022-06-12 07:24:02,790   Num examples = 40430
2022-06-12 07:24:02,790   Batch size = 32
2022-06-12 07:24:02,792 ***** Eval results *****
2022-06-12 07:24:02,793   att_loss = 3.618098244154706
2022-06-12 07:24:02,793   global_step = 50999
2022-06-12 07:24:02,793   loss = 4.632056098633172
2022-06-12 07:24:02,793   rep_loss = 1.0139578519512895
2022-06-12 07:24:02,793 ***** Save model *****
2022-06-12 07:24:22,048 ***** Running evaluation *****
2022-06-12 07:24:22,048   Epoch = 2 iter 25999 step
2022-06-12 07:24:22,048   Num examples = 9815
2022-06-12 07:24:22,048   Batch size = 32
2022-06-12 07:24:22,049 ***** Eval results *****
2022-06-12 07:24:22,049   att_loss = 12.604430372778815
2022-06-12 07:24:22,049   global_step = 25999
2022-06-12 07:24:22,050   loss = 13.530631127550574
2022-06-12 07:24:22,050   rep_loss = 0.9262007466308368
2022-06-12 07:24:22,050 ***** Save model *****
2022-06-12 07:26:14,273 ***** Running evaluation *****
2022-06-12 07:26:14,274   Epoch = 4 iter 51499 step
2022-06-12 07:26:14,274   Num examples = 40430
2022-06-12 07:26:14,274   Batch size = 32
2022-06-12 07:26:14,276 ***** Eval results *****
2022-06-12 07:26:14,276   att_loss = 3.6166294331684483
2022-06-12 07:26:14,276   global_step = 51499
2022-06-12 07:26:14,276   loss = 4.630430153268262
2022-06-12 07:26:14,276   rep_loss = 1.0138007176241262
2022-06-12 07:26:14,276 ***** Save model *****
2022-06-12 07:26:32,307 ***** Running evaluation *****
2022-06-12 07:26:32,308   Epoch = 2 iter 26499 step
2022-06-12 07:26:32,308   Num examples = 9815
2022-06-12 07:26:32,308   Batch size = 32
2022-06-12 07:26:32,310 ***** Eval results *****
2022-06-12 07:26:32,310   att_loss = 12.561868462930484
2022-06-12 07:26:32,310   global_step = 26499
2022-06-12 07:26:32,310   loss = 13.487061055143938
2022-06-12 07:26:32,310   rep_loss = 0.925192584385967
2022-06-12 07:26:32,310 ***** Save model *****
2022-06-12 07:28:26,377 ***** Running evaluation *****
2022-06-12 07:28:26,378   Epoch = 4 iter 51999 step
2022-06-12 07:28:26,378   Num examples = 40430
2022-06-12 07:28:26,378   Batch size = 32
2022-06-12 07:28:26,379 ***** Eval results *****
2022-06-12 07:28:26,379   att_loss = 3.617733956480487
2022-06-12 07:28:26,379   global_step = 51999
2022-06-12 07:28:26,379   loss = 4.631351422341913
2022-06-12 07:28:26,379   rep_loss = 1.0136174633013249
2022-06-12 07:28:26,379 ***** Save model *****
2022-06-12 07:28:42,288 ***** Running evaluation *****
2022-06-12 07:28:42,289   Epoch = 2 iter 26999 step
2022-06-12 07:28:42,289   Num examples = 9815
2022-06-12 07:28:42,289   Batch size = 32
2022-06-12 07:28:42,290 ***** Eval results *****
2022-06-12 07:28:42,290   att_loss = 12.564925180201516
2022-06-12 07:28:42,290   global_step = 26999
2022-06-12 07:28:42,290   loss = 13.489874087944232
2022-06-12 07:28:42,290   rep_loss = 0.9249489022358967
2022-06-12 07:28:42,290 ***** Save model *****
2022-06-12 07:30:38,122 ***** Running evaluation *****
2022-06-12 07:30:38,123   Epoch = 4 iter 52499 step
2022-06-12 07:30:38,123   Num examples = 40430
2022-06-12 07:30:38,123   Batch size = 32
2022-06-12 07:30:38,126 ***** Eval results *****
2022-06-12 07:30:38,126   att_loss = 3.6201492531854282
2022-06-12 07:30:38,126   global_step = 52499
2022-06-12 07:30:38,126   loss = 4.633556973918888
2022-06-12 07:30:38,126   rep_loss = 1.0134077182962846
2022-06-12 07:30:38,126 ***** Save model *****
2022-06-12 07:30:51,615 ***** Running evaluation *****
2022-06-12 07:30:51,615   Epoch = 2 iter 27499 step
2022-06-12 07:30:51,616   Num examples = 9815
2022-06-12 07:30:51,616   Batch size = 32
2022-06-12 07:30:51,617 ***** Eval results *****
2022-06-12 07:30:51,617   att_loss = 12.548882385424193
2022-06-12 07:30:51,617   global_step = 27499
2022-06-12 07:30:51,618   loss = 13.473338956965117
2022-06-12 07:30:51,618   rep_loss = 0.9244565707749517
2022-06-12 07:30:51,618 ***** Save model *****
2022-06-12 07:32:50,455 ***** Running evaluation *****
2022-06-12 07:32:50,456   Epoch = 4 iter 52999 step
2022-06-12 07:32:50,456   Num examples = 40430
2022-06-12 07:32:50,456   Batch size = 32
2022-06-12 07:32:50,458 ***** Eval results *****
2022-06-12 07:32:50,458   att_loss = 3.6210832267321873
2022-06-12 07:32:50,458   global_step = 52999
2022-06-12 07:32:50,458   loss = 4.63436593018816
2022-06-12 07:32:50,458   rep_loss = 1.013282700919268
2022-06-12 07:32:50,459 ***** Save model *****
2022-06-12 07:33:02,794 ***** Running evaluation *****
2022-06-12 07:33:02,795   Epoch = 2 iter 27999 step
2022-06-12 07:33:02,795   Num examples = 9815
2022-06-12 07:33:02,795   Batch size = 32
2022-06-12 07:33:02,796 ***** Eval results *****
2022-06-12 07:33:02,797   att_loss = 12.552115952654798
2022-06-12 07:33:02,797   global_step = 27999
2022-06-12 07:33:02,797   loss = 13.476603972556655
2022-06-12 07:33:02,797   rep_loss = 0.9244880163500617
2022-06-12 07:33:02,797 ***** Save model *****
2022-06-12 07:35:02,084 ***** Running evaluation *****
2022-06-12 07:35:02,084   Epoch = 4 iter 53499 step
2022-06-12 07:35:02,085   Num examples = 40430
2022-06-12 07:35:02,085   Batch size = 32
2022-06-12 07:35:02,086 ***** Eval results *****
2022-06-12 07:35:02,087   att_loss = 3.620588140633951
2022-06-12 07:35:02,087   global_step = 53499
2022-06-12 07:35:02,087   loss = 4.633662950667022
2022-06-12 07:35:02,087   rep_loss = 1.0130748076396685
2022-06-12 07:35:02,087 ***** Save model *****
2022-06-12 07:35:13,937 ***** Running evaluation *****
2022-06-12 07:35:13,937   Epoch = 2 iter 28499 step
2022-06-12 07:35:13,937   Num examples = 9815
2022-06-12 07:35:13,937   Batch size = 32
2022-06-12 07:35:13,939 ***** Eval results *****
2022-06-12 07:35:13,939   att_loss = 12.542269444266804
2022-06-12 07:35:13,939   global_step = 28499
2022-06-12 07:35:13,939   loss = 13.466266729929908
2022-06-12 07:35:13,939   rep_loss = 0.9239972822437812
2022-06-12 07:35:13,939 ***** Save model *****
2022-06-12 07:37:14,520 ***** Running evaluation *****
2022-06-12 07:37:14,522   Epoch = 4 iter 53999 step
2022-06-12 07:37:14,522   Num examples = 40430
2022-06-12 07:37:14,522   Batch size = 32
2022-06-12 07:37:14,524 ***** Eval results *****
2022-06-12 07:37:14,524   att_loss = 3.6209870920395093
2022-06-12 07:37:14,524   global_step = 53999
2022-06-12 07:37:14,525   loss = 4.6338693754174
2022-06-12 07:37:14,525   rep_loss = 1.0128822808800793
2022-06-12 07:37:14,525 ***** Save model *****
2022-06-12 07:37:24,756 ***** Running evaluation *****
2022-06-12 07:37:24,756   Epoch = 2 iter 28999 step
2022-06-12 07:37:24,756   Num examples = 9815
2022-06-12 07:37:24,756   Batch size = 32
2022-06-12 07:37:24,758 ***** Eval results *****
2022-06-12 07:37:24,758   att_loss = 12.548403576882345
2022-06-12 07:37:24,758   global_step = 28999
2022-06-12 07:37:24,758   loss = 13.472284612512277
2022-06-12 07:37:24,758   rep_loss = 0.9238810311766358
2022-06-12 07:37:24,758 ***** Save model *****
2022-06-12 07:39:26,671 ***** Running evaluation *****
2022-06-12 07:39:26,671   Epoch = 4 iter 54499 step
2022-06-12 07:39:26,672   Num examples = 40430
2022-06-12 07:39:26,672   Batch size = 32
2022-06-12 07:39:26,673 ***** Eval results *****
2022-06-12 07:39:26,673   att_loss = 3.6194969810473574
2022-06-12 07:39:26,673   global_step = 54499
2022-06-12 07:39:26,674   loss = 4.632144299562232
2022-06-12 07:39:26,674   rep_loss = 1.0126473165851089
2022-06-12 07:39:26,675 ***** Save model *****
2022-06-12 07:39:35,509 ***** Running evaluation *****
2022-06-12 07:39:35,510   Epoch = 2 iter 29499 step
2022-06-12 07:39:35,510   Num examples = 9815
2022-06-12 07:39:35,510   Batch size = 32
2022-06-12 07:39:35,511 ***** Eval results *****
2022-06-12 07:39:35,511   att_loss = 12.545995124723008
2022-06-12 07:39:35,511   global_step = 29499
2022-06-12 07:39:35,511   loss = 13.469724594163923
2022-06-12 07:39:35,512   rep_loss = 0.9237294663626844
2022-06-12 07:39:35,512 ***** Save model *****
2022-06-12 07:41:37,746 ***** Running evaluation *****
2022-06-12 07:41:37,746   Epoch = 4 iter 54999 step
2022-06-12 07:41:37,746   Num examples = 40430
2022-06-12 07:41:37,746   Batch size = 32
2022-06-12 07:41:37,748 ***** Eval results *****
2022-06-12 07:41:37,748   att_loss = 3.619300493285841
2022-06-12 07:41:37,748   global_step = 54999
2022-06-12 07:41:37,748   loss = 4.63169929452498
2022-06-12 07:41:37,748   rep_loss = 1.0123987989473748
2022-06-12 07:41:37,748 ***** Save model *****
2022-06-12 07:41:44,668 ***** Running evaluation *****
2022-06-12 07:41:44,668   Epoch = 2 iter 29999 step
2022-06-12 07:41:44,668   Num examples = 9815
2022-06-12 07:41:44,668   Batch size = 32
2022-06-12 07:41:44,670 ***** Eval results *****
2022-06-12 07:41:44,670   att_loss = 12.54444955193518
2022-06-12 07:41:44,670   global_step = 29999
2022-06-12 07:41:44,670   loss = 13.468062349962231
2022-06-12 07:41:44,670   rep_loss = 0.923612796465119
2022-06-12 07:41:44,670 ***** Save model *****
2022-06-12 07:43:49,600 ***** Running evaluation *****
2022-06-12 07:43:49,601   Epoch = 4 iter 55499 step
2022-06-12 07:43:49,601   Num examples = 40430
2022-06-12 07:43:49,601   Batch size = 32
2022-06-12 07:43:49,602 ***** Eval results *****
2022-06-12 07:43:49,602   att_loss = 3.6193193920073434
2022-06-12 07:43:49,602   global_step = 55499
2022-06-12 07:43:49,602   loss = 4.631588693649805
2022-06-12 07:43:49,602   rep_loss = 1.0122692993877296
2022-06-12 07:43:49,603 ***** Save model *****
2022-06-12 07:43:55,378 ***** Running evaluation *****
2022-06-12 07:43:55,378   Epoch = 2 iter 30499 step
2022-06-12 07:43:55,379   Num examples = 9815
2022-06-12 07:43:55,379   Batch size = 32
2022-06-12 07:43:55,380 ***** Eval results *****
2022-06-12 07:43:55,380   att_loss = 12.534751287418382
2022-06-12 07:43:55,380   global_step = 30499
2022-06-12 07:43:55,380   loss = 13.457929580948468
2022-06-12 07:43:55,380   rep_loss = 0.9231782932799395
2022-06-12 07:43:55,380 ***** Save model *****
2022-06-12 07:46:01,098 ***** Running evaluation *****
2022-06-12 07:46:01,099   Epoch = 4 iter 55999 step
2022-06-12 07:46:01,099   Num examples = 40430
2022-06-12 07:46:01,099   Batch size = 32
2022-06-12 07:46:01,100 ***** Eval results *****
2022-06-12 07:46:01,100   att_loss = 3.618822873709648
2022-06-12 07:46:01,100   global_step = 55999
2022-06-12 07:46:01,100   loss = 4.630929641186913
2022-06-12 07:46:01,100   rep_loss = 1.0121067654373688
2022-06-12 07:46:01,100 ***** Save model *****
2022-06-12 07:46:04,387 ***** Running evaluation *****
2022-06-12 07:46:04,387   Epoch = 2 iter 30999 step
2022-06-12 07:46:04,387   Num examples = 9815
2022-06-12 07:46:04,387   Batch size = 32
2022-06-12 07:46:04,389 ***** Eval results *****
2022-06-12 07:46:04,389   att_loss = 12.52995484204268
2022-06-12 07:46:04,389   global_step = 30999
2022-06-12 07:46:04,389   loss = 13.452773457066792
2022-06-12 07:46:04,389   rep_loss = 0.9228186163810711
2022-06-12 07:46:04,389 ***** Save model *****
2022-06-12 07:48:13,283 ***** Running evaluation *****
2022-06-12 07:48:13,284   Epoch = 4 iter 56499 step
2022-06-12 07:48:13,284   Num examples = 40430
2022-06-12 07:48:13,284   Batch size = 32
2022-06-12 07:48:13,285 ***** Eval results *****
2022-06-12 07:48:13,285   att_loss = 3.619815983738957
2022-06-12 07:48:13,285   global_step = 56499
2022-06-12 07:48:13,285   loss = 4.631697197381939
2022-06-12 07:48:13,285   rep_loss = 1.0118812117389222
2022-06-12 07:48:13,286 ***** Save model *****
2022-06-12 07:48:13,713 ***** Running evaluation *****
2022-06-12 07:48:13,713   Epoch = 2 iter 31499 step
2022-06-12 07:48:13,713   Num examples = 9815
2022-06-12 07:48:13,713   Batch size = 32
2022-06-12 07:48:13,715 ***** Eval results *****
2022-06-12 07:48:13,715   att_loss = 12.523021067013014
2022-06-12 07:48:13,715   global_step = 31499
2022-06-12 07:48:13,715   loss = 13.44561779874314
2022-06-12 07:48:13,715   rep_loss = 0.9225967348829947
2022-06-12 07:48:13,715 ***** Save model *****
2022-06-12 07:50:23,274 ***** Running evaluation *****
2022-06-12 07:50:23,274   Epoch = 2 iter 31999 step
2022-06-12 07:50:23,274   Num examples = 9815
2022-06-12 07:50:23,274   Batch size = 32
2022-06-12 07:50:23,276 ***** Eval results *****
2022-06-12 07:50:23,276   att_loss = 12.522395586446317
2022-06-12 07:50:23,276   global_step = 31999
2022-06-12 07:50:23,276   loss = 13.444659925744475
2022-06-12 07:50:23,276   rep_loss = 0.9222643432387625
2022-06-12 07:50:23,276 ***** Save model *****
2022-06-12 07:50:25,494 ***** Running evaluation *****
2022-06-12 07:50:25,495   Epoch = 5 iter 56999 step
2022-06-12 07:50:25,495   Num examples = 40430
2022-06-12 07:50:25,495   Batch size = 32
2022-06-12 07:50:25,496 ***** Eval results *****
2022-06-12 07:50:25,496   att_loss = 3.4321944217553875
2022-06-12 07:50:25,496   global_step = 56999
2022-06-12 07:50:25,496   loss = 4.431973812564108
2022-06-12 07:50:25,496   rep_loss = 0.9997794024096239
2022-06-12 07:50:25,497 ***** Save model *****
2022-06-12 07:52:33,657 ***** Running evaluation *****
2022-06-12 07:52:33,658   Epoch = 2 iter 32499 step
2022-06-12 07:52:33,658   Num examples = 9815
2022-06-12 07:52:33,658   Batch size = 32
2022-06-12 07:52:33,659 ***** Eval results *****
2022-06-12 07:52:33,659   att_loss = 12.519860468034215
2022-06-12 07:52:33,659   global_step = 32499
2022-06-12 07:52:33,659   loss = 13.441776857763225
2022-06-12 07:52:33,659   rep_loss = 0.9219163945680949
2022-06-12 07:52:33,659 ***** Save model *****
2022-06-12 07:52:37,241 ***** Running evaluation *****
2022-06-12 07:52:37,242   Epoch = 5 iter 57499 step
2022-06-12 07:52:37,242   Num examples = 40430
2022-06-12 07:52:37,242   Batch size = 32
2022-06-12 07:52:37,243 ***** Eval results *****
2022-06-12 07:52:37,243   att_loss = 3.4865680269908466
2022-06-12 07:52:37,243   global_step = 57499
2022-06-12 07:52:37,243   loss = 4.488264818588281
2022-06-12 07:52:37,244   rep_loss = 1.0016967938934527
2022-06-12 07:52:37,244 ***** Save model *****
2022-06-12 07:54:44,457 ***** Running evaluation *****
2022-06-12 07:54:44,457   Epoch = 2 iter 32999 step
2022-06-12 07:54:44,457   Num examples = 9815
2022-06-12 07:54:44,457   Batch size = 32
2022-06-12 07:54:44,458 ***** Eval results *****
2022-06-12 07:54:44,459   att_loss = 12.518899041399461
2022-06-12 07:54:44,459   global_step = 32999
2022-06-12 07:54:44,459   loss = 13.440549295873552
2022-06-12 07:54:44,459   rep_loss = 0.9216502579910275
2022-06-12 07:54:44,459 ***** Save model *****
2022-06-12 07:54:49,874 ***** Running evaluation *****
2022-06-12 07:54:49,874   Epoch = 5 iter 57999 step
2022-06-12 07:54:49,874   Num examples = 40430
2022-06-12 07:54:49,874   Batch size = 32
2022-06-12 07:54:49,876 ***** Eval results *****
2022-06-12 07:54:49,876   att_loss = 3.4652295880363337
2022-06-12 07:54:49,876   global_step = 57999
2022-06-12 07:54:49,876   loss = 4.465011702090164
2022-06-12 07:54:49,876   rep_loss = 0.9997821109413166
2022-06-12 07:54:49,876 ***** Save model *****
2022-06-12 07:56:53,474 ***** Running evaluation *****
2022-06-12 07:56:53,475   Epoch = 2 iter 33499 step
2022-06-12 07:56:53,475   Num examples = 9815
2022-06-12 07:56:53,475   Batch size = 32
2022-06-12 07:56:53,476 ***** Eval results *****
2022-06-12 07:56:53,476   att_loss = 12.509483890846477
2022-06-12 07:56:53,476   global_step = 33499
2022-06-12 07:56:53,476   loss = 13.430719789788435
2022-06-12 07:56:53,477   rep_loss = 0.9212359034138039
2022-06-12 07:56:53,477 ***** Save model *****
2022-06-12 07:57:02,014 ***** Running evaluation *****
2022-06-12 07:57:02,014   Epoch = 5 iter 58499 step
2022-06-12 07:57:02,014   Num examples = 40430
2022-06-12 07:57:02,014   Batch size = 32
2022-06-12 07:57:02,015 ***** Eval results *****
2022-06-12 07:57:02,016   att_loss = 3.4759116784379724
2022-06-12 07:57:02,016   global_step = 58499
2022-06-12 07:57:02,016   loss = 4.475896970368357
2022-06-12 07:57:02,016   rep_loss = 0.9999852880627694
2022-06-12 07:57:02,016 ***** Save model *****
2022-06-12 07:59:02,626 ***** Running evaluation *****
2022-06-12 07:59:02,627   Epoch = 2 iter 33999 step
2022-06-12 07:59:02,627   Num examples = 9815
2022-06-12 07:59:02,627   Batch size = 32
2022-06-12 07:59:02,628 ***** Eval results *****
2022-06-12 07:59:02,628   att_loss = 12.504894105024439
2022-06-12 07:59:02,628   global_step = 33999
2022-06-12 07:59:02,628   loss = 13.425990540317997
2022-06-12 07:59:02,628   rep_loss = 0.92109643978108
2022-06-12 07:59:02,629 ***** Save model *****
2022-06-12 07:59:14,364 ***** Running evaluation *****
2022-06-12 07:59:14,364   Epoch = 5 iter 58999 step
2022-06-12 07:59:14,364   Num examples = 40430
2022-06-12 07:59:14,365   Batch size = 32
2022-06-12 07:59:14,366 ***** Eval results *****
2022-06-12 07:59:14,366   att_loss = 3.4770231229197206
2022-06-12 07:59:14,366   global_step = 58999
2022-06-12 07:59:14,366   loss = 4.477099636312195
2022-06-12 07:59:14,366   rep_loss = 1.000076509287547
2022-06-12 07:59:14,366 ***** Save model *****
2022-06-12 08:01:11,878 ***** Running evaluation *****
2022-06-12 08:01:11,878   Epoch = 2 iter 34499 step
2022-06-12 08:01:11,878   Num examples = 9815
2022-06-12 08:01:11,878   Batch size = 32
2022-06-12 08:01:11,880 ***** Eval results *****
2022-06-12 08:01:11,880   att_loss = 12.498267002130616
2022-06-12 08:01:11,880   global_step = 34499
2022-06-12 08:01:11,880   loss = 13.419079109932078
2022-06-12 08:01:11,880   rep_loss = 0.9208121117463706
2022-06-12 08:01:11,880 ***** Save model *****
2022-06-12 08:01:26,441 ***** Running evaluation *****
2022-06-12 08:01:26,442   Epoch = 5 iter 59499 step
2022-06-12 08:01:26,442   Num examples = 40430
2022-06-12 08:01:26,442   Batch size = 32
2022-06-12 08:01:26,443 ***** Eval results *****
2022-06-12 08:01:26,443   att_loss = 3.4778471132547013
2022-06-12 08:01:26,443   global_step = 59499
2022-06-12 08:01:26,443   loss = 4.477831229663156
2022-06-12 08:01:26,443   rep_loss = 0.9999841117732882
2022-06-12 08:01:26,444 ***** Save model *****
2022-06-12 08:03:20,945 ***** Running evaluation *****
2022-06-12 08:03:20,945   Epoch = 2 iter 34999 step
2022-06-12 08:03:20,945   Num examples = 9815
2022-06-12 08:03:20,945   Batch size = 32
2022-06-12 08:03:20,947 ***** Eval results *****
2022-06-12 08:03:20,947   att_loss = 12.492520257187948
2022-06-12 08:03:20,947   global_step = 34999
2022-06-12 08:03:20,947   loss = 13.412971267032614
2022-06-12 08:03:20,947   rep_loss = 0.9204510134071513
2022-06-12 08:03:20,947 ***** Save model *****
2022-06-12 08:03:37,457 ***** Running evaluation *****
2022-06-12 08:03:37,458   Epoch = 5 iter 59999 step
2022-06-12 08:03:37,458   Num examples = 40430
2022-06-12 08:03:37,458   Batch size = 32
2022-06-12 08:03:37,459 ***** Eval results *****
2022-06-12 08:03:37,459   att_loss = 3.479751242119458
2022-06-12 08:03:37,459   global_step = 59999
2022-06-12 08:03:37,459   loss = 4.479444632192535
2022-06-12 08:03:37,459   rep_loss = 0.9996933848110597
2022-06-12 08:03:37,459 ***** Save model *****
2022-06-12 08:05:31,051 ***** Running evaluation *****
2022-06-12 08:05:31,052   Epoch = 2 iter 35499 step
2022-06-12 08:05:31,052   Num examples = 9815
2022-06-12 08:05:31,052   Batch size = 32
2022-06-12 08:05:31,053 ***** Eval results *****
2022-06-12 08:05:31,053   att_loss = 12.49047510930958
2022-06-12 08:05:31,053   global_step = 35499
2022-06-12 08:05:31,053   loss = 13.410697711496509
2022-06-12 08:05:31,054   rep_loss = 0.9202226057337226
2022-06-12 08:05:31,054 ***** Save model *****
2022-06-12 08:05:49,648 ***** Running evaluation *****
2022-06-12 08:05:49,648   Epoch = 5 iter 60499 step
2022-06-12 08:05:49,648   Num examples = 40430
2022-06-12 08:05:49,648   Batch size = 32
2022-06-12 08:05:49,649 ***** Eval results *****
2022-06-12 08:05:49,650   att_loss = 3.4778032640654213
2022-06-12 08:05:49,650   global_step = 60499
2022-06-12 08:05:49,650   loss = 4.477200275252832
2022-06-12 08:05:49,650   rep_loss = 0.9993970066137463
2022-06-12 08:05:49,650 ***** Save model *****
2022-06-12 08:07:40,356 ***** Running evaluation *****
2022-06-12 08:07:40,356   Epoch = 2 iter 35999 step
2022-06-12 08:07:40,356   Num examples = 9815
2022-06-12 08:07:40,356   Batch size = 32
2022-06-12 08:07:40,358 ***** Eval results *****
2022-06-12 08:07:40,358   att_loss = 12.488526579950184
2022-06-12 08:07:40,358   global_step = 35999
2022-06-12 08:07:40,358   loss = 13.408483606051991
2022-06-12 08:07:40,358   rep_loss = 0.9199570284169039
2022-06-12 08:07:40,358 ***** Save model *****
2022-06-12 08:08:01,499 ***** Running evaluation *****
2022-06-12 08:08:01,499   Epoch = 5 iter 60999 step
2022-06-12 08:08:01,499   Num examples = 40430
2022-06-12 08:08:01,499   Batch size = 32
2022-06-12 08:08:01,501 ***** Eval results *****
2022-06-12 08:08:01,501   att_loss = 3.473977143978837
2022-06-12 08:08:01,501   global_step = 60999
2022-06-12 08:08:01,501   loss = 4.473021565531788
2022-06-12 08:08:01,501   rep_loss = 0.9990444161800569
2022-06-12 08:08:01,501 ***** Save model *****
2022-06-12 08:09:49,658 ***** Running evaluation *****
2022-06-12 08:09:49,658   Epoch = 2 iter 36499 step
2022-06-12 08:09:49,658   Num examples = 9815
2022-06-12 08:09:49,658   Batch size = 32
2022-06-12 08:09:49,660 ***** Eval results *****
2022-06-12 08:09:49,660   att_loss = 12.486043952625412
2022-06-12 08:09:49,660   global_step = 36499
2022-06-12 08:09:49,660   loss = 13.405763900229152
2022-06-12 08:09:49,660   rep_loss = 0.9197199499466512
2022-06-12 08:09:49,660 ***** Save model *****
2022-06-12 08:10:12,992 ***** Running evaluation *****
2022-06-12 08:10:12,992   Epoch = 5 iter 61499 step
2022-06-12 08:10:12,992   Num examples = 40430
2022-06-12 08:10:12,993   Batch size = 32
2022-06-12 08:10:12,994 ***** Eval results *****
2022-06-12 08:10:12,994   att_loss = 3.474659202995801
2022-06-12 08:10:12,994   global_step = 61499
2022-06-12 08:10:12,994   loss = 4.473601803879093
2022-06-12 08:10:12,994   rep_loss = 0.9989425960241479
2022-06-12 08:10:12,995 ***** Save model *****
2022-06-12 08:11:59,080 ***** Running evaluation *****
2022-06-12 08:11:59,080   Epoch = 3 iter 36999 step
2022-06-12 08:11:59,081   Num examples = 9815
2022-06-12 08:11:59,081   Batch size = 32
2022-06-12 08:11:59,082 ***** Eval results *****
2022-06-12 08:11:59,082   att_loss = 12.112084865570068
2022-06-12 08:11:59,082   global_step = 36999
2022-06-12 08:11:59,082   loss = 13.022994790025937
2022-06-12 08:11:59,083   rep_loss = 0.9109099529763703
2022-06-12 08:11:59,083 ***** Save model *****
2022-06-12 08:12:24,931 ***** Running evaluation *****
2022-06-12 08:12:24,932   Epoch = 5 iter 61999 step
2022-06-12 08:12:24,932   Num examples = 40430
2022-06-12 08:12:24,932   Batch size = 32
2022-06-12 08:12:24,933 ***** Eval results *****
2022-06-12 08:12:24,933   att_loss = 3.475407356288878
2022-06-12 08:12:24,933   global_step = 61999
2022-06-12 08:12:24,933   loss = 4.474232307288873
2022-06-12 08:12:24,934   rep_loss = 0.9988249472030785
2022-06-12 08:12:24,934 ***** Save model *****
2022-06-12 08:14:08,883 ***** Running evaluation *****
2022-06-12 08:14:08,884   Epoch = 3 iter 37499 step
2022-06-12 08:14:08,884   Num examples = 9815
2022-06-12 08:14:08,884   Batch size = 32
2022-06-12 08:14:08,885 ***** Eval results *****
2022-06-12 08:14:08,885   att_loss = 12.071316208853318
2022-06-12 08:14:08,885   global_step = 37499
2022-06-12 08:14:08,885   loss = 12.98152103507484
2022-06-12 08:14:08,886   rep_loss = 0.9102048477695566
2022-06-12 08:14:08,886 ***** Save model *****
2022-06-12 08:14:36,751 ***** Running evaluation *****
2022-06-12 08:14:36,751   Epoch = 5 iter 62499 step
2022-06-12 08:14:36,751   Num examples = 40430
2022-06-12 08:14:36,752   Batch size = 32
2022-06-12 08:14:36,753 ***** Eval results *****
2022-06-12 08:14:36,753   att_loss = 3.475278213986846
2022-06-12 08:14:36,753   global_step = 62499
2022-06-12 08:14:36,753   loss = 4.47395082933119
2022-06-12 08:14:36,753   rep_loss = 0.9986726115353024
2022-06-12 08:14:36,753 ***** Save model *****
2022-06-12 08:16:18,858 ***** Running evaluation *****
2022-06-12 08:16:18,858   Epoch = 3 iter 37999 step
2022-06-12 08:16:18,858   Num examples = 9815
2022-06-12 08:16:18,858   Batch size = 32
2022-06-12 08:16:18,860 ***** Eval results *****
2022-06-12 08:16:18,860   att_loss = 12.059580219937818
2022-06-12 08:16:18,860   global_step = 37999
2022-06-12 08:16:18,860   loss = 12.969919937256014
2022-06-12 08:16:18,860   rep_loss = 0.9103397247562119
2022-06-12 08:16:18,860 ***** Save model *****
2022-06-12 08:16:48,332 ***** Running evaluation *****
2022-06-12 08:16:48,332   Epoch = 5 iter 62999 step
2022-06-12 08:16:48,332   Num examples = 40430
2022-06-12 08:16:48,332   Batch size = 32
2022-06-12 08:16:48,334 ***** Eval results *****
2022-06-12 08:16:48,334   att_loss = 3.476465305985038
2022-06-12 08:16:48,334   global_step = 62999
2022-06-12 08:16:48,334   loss = 4.474952114405797
2022-06-12 08:16:48,334   rep_loss = 0.9984868049020592
2022-06-12 08:16:48,334 ***** Save model *****
2022-06-12 08:18:28,884 ***** Running evaluation *****
2022-06-12 08:18:28,885   Epoch = 3 iter 38499 step
2022-06-12 08:18:28,885   Num examples = 9815
2022-06-12 08:18:28,885   Batch size = 32
2022-06-12 08:18:28,887 ***** Eval results *****
2022-06-12 08:18:28,887   att_loss = 12.018067066355533
2022-06-12 08:18:28,887   global_step = 38499
2022-06-12 08:18:28,887   loss = 12.928146283680292
2022-06-12 08:18:28,887   rep_loss = 0.9100792211074987
2022-06-12 08:18:28,888 ***** Save model *****
2022-06-12 08:18:59,921 ***** Running evaluation *****
2022-06-12 08:18:59,922   Epoch = 5 iter 63499 step
2022-06-12 08:18:59,922   Num examples = 40430
2022-06-12 08:18:59,922   Batch size = 32
2022-06-12 08:18:59,923 ***** Eval results *****
2022-06-12 08:18:59,924   att_loss = 3.478758221587699
2022-06-12 08:18:59,924   global_step = 63499
2022-06-12 08:18:59,924   loss = 4.477276458114874
2022-06-12 08:18:59,924   rep_loss = 0.998518233210328
2022-06-12 08:18:59,924 ***** Save model *****
2022-06-12 08:20:38,728 ***** Running evaluation *****
2022-06-12 08:20:38,729   Epoch = 3 iter 38999 step
2022-06-12 08:20:38,729   Num examples = 9815
2022-06-12 08:20:38,729   Batch size = 32
2022-06-12 08:20:38,730 ***** Eval results *****
2022-06-12 08:20:38,730   att_loss = 12.013700343007248
2022-06-12 08:20:38,730   global_step = 38999
2022-06-12 08:20:38,730   loss = 12.923497007619538
2022-06-12 08:20:38,730   rep_loss = 0.9097966690294697
2022-06-12 08:20:38,731 ***** Save model *****
2022-06-12 08:21:10,855 ***** Running evaluation *****
2022-06-12 08:21:10,855   Epoch = 5 iter 63999 step
2022-06-12 08:21:10,855   Num examples = 40430
2022-06-12 08:21:10,855   Batch size = 32
2022-06-12 08:21:10,857 ***** Eval results *****
2022-06-12 08:21:10,857   att_loss = 3.4799373476701043
2022-06-12 08:21:10,857   global_step = 63999
2022-06-12 08:21:10,857   loss = 4.478385082340387
2022-06-12 08:21:10,857   rep_loss = 0.9984477313352909
2022-06-12 08:21:10,857 ***** Save model *****
2022-06-12 08:22:48,292 ***** Running evaluation *****
2022-06-12 08:22:48,293   Epoch = 3 iter 39499 step
2022-06-12 08:22:48,293   Num examples = 9815
2022-06-12 08:22:48,293   Batch size = 32
2022-06-12 08:22:48,294 ***** Eval results *****
2022-06-12 08:22:48,294   att_loss = 12.014507403412773
2022-06-12 08:22:48,294   global_step = 39499
2022-06-12 08:22:48,294   loss = 12.924075499099311
2022-06-12 08:22:48,295   rep_loss = 0.9095680996587004
2022-06-12 08:22:48,295 ***** Save model *****
2022-06-12 08:23:22,771 ***** Running evaluation *****
2022-06-12 08:23:22,771   Epoch = 5 iter 64499 step
2022-06-12 08:23:22,771   Num examples = 40430
2022-06-12 08:23:22,771   Batch size = 32
2022-06-12 08:23:22,773 ***** Eval results *****
2022-06-12 08:23:22,773   att_loss = 3.481189355676915
2022-06-12 08:23:22,773   global_step = 64499
2022-06-12 08:23:22,773   loss = 4.479540238303755
2022-06-12 08:23:22,773   rep_loss = 0.9983508787461863
2022-06-12 08:23:22,773 ***** Save model *****
2022-06-12 08:24:59,212 ***** Running evaluation *****
2022-06-12 08:24:59,213   Epoch = 3 iter 39999 step
2022-06-12 08:24:59,213   Num examples = 9815
2022-06-12 08:24:59,213   Batch size = 32
2022-06-12 08:24:59,214 ***** Eval results *****
2022-06-12 08:24:59,214   att_loss = 12.012879660798555
2022-06-12 08:24:59,214   global_step = 39999
2022-06-12 08:24:59,215   loss = 12.922398357666744
2022-06-12 08:24:59,215   rep_loss = 0.9095187038838003
2022-06-12 08:24:59,215 ***** Save model *****
2022-06-12 08:25:34,561 ***** Running evaluation *****
2022-06-12 08:25:34,561   Epoch = 5 iter 64999 step
2022-06-12 08:25:34,562   Num examples = 40430
2022-06-12 08:25:34,562   Batch size = 32
2022-06-12 08:25:34,563 ***** Eval results *****
2022-06-12 08:25:34,563   att_loss = 3.482091036350804
2022-06-12 08:25:34,563   global_step = 64999
2022-06-12 08:25:34,563   loss = 4.480168647393959
2022-06-12 08:25:34,563   rep_loss = 0.9980776075980958
2022-06-12 08:25:34,563 ***** Save model *****
2022-06-12 08:27:08,644 ***** Running evaluation *****
2022-06-12 08:27:08,644   Epoch = 3 iter 40499 step
2022-06-12 08:27:08,644   Num examples = 9815
2022-06-12 08:27:08,644   Batch size = 32
2022-06-12 08:27:08,646 ***** Eval results *****
2022-06-12 08:27:08,646   att_loss = 12.013014642557112
2022-06-12 08:27:08,646   global_step = 40499
2022-06-12 08:27:08,646   loss = 12.922280997215763
2022-06-12 08:27:08,646   rep_loss = 0.9092663619030577
2022-06-12 08:27:08,646 ***** Save model *****
2022-06-12 08:27:45,541 ***** Running evaluation *****
2022-06-12 08:27:45,541   Epoch = 5 iter 65499 step
2022-06-12 08:27:45,541   Num examples = 40430
2022-06-12 08:27:45,542   Batch size = 32
2022-06-12 08:27:45,543 ***** Eval results *****
2022-06-12 08:27:45,543   att_loss = 3.4839556434965226
2022-06-12 08:27:45,543   global_step = 65499
2022-06-12 08:27:45,543   loss = 4.481936683350456
2022-06-12 08:27:45,543   rep_loss = 0.9979810370904392
2022-06-12 08:27:45,543 ***** Save model *****
2022-06-12 08:29:17,777 ***** Running evaluation *****
2022-06-12 08:29:17,777   Epoch = 3 iter 40999 step
2022-06-12 08:29:17,777   Num examples = 9815
2022-06-12 08:29:17,777   Batch size = 32
2022-06-12 08:29:17,778 ***** Eval results *****
2022-06-12 08:29:17,778   att_loss = 12.009745608774942
2022-06-12 08:29:17,778   global_step = 40999
2022-06-12 08:29:17,778   loss = 12.918895010850216
2022-06-12 08:29:17,779   rep_loss = 0.909149407998717
2022-06-12 08:29:17,779 ***** Save model *****
2022-06-12 08:29:56,646 ***** Running evaluation *****
2022-06-12 08:29:56,646   Epoch = 5 iter 65999 step
2022-06-12 08:29:56,646   Num examples = 40430
2022-06-12 08:29:56,646   Batch size = 32
2022-06-12 08:29:56,648 ***** Eval results *****
2022-06-12 08:29:56,648   att_loss = 3.4818785529877267
2022-06-12 08:29:56,648   global_step = 65999
2022-06-12 08:29:56,648   loss = 4.479666205179054
2022-06-12 08:29:56,648   rep_loss = 0.9977876493312945
2022-06-12 08:29:56,648 ***** Save model *****
2022-06-12 08:31:27,019 ***** Running evaluation *****
2022-06-12 08:31:27,020   Epoch = 3 iter 41499 step
2022-06-12 08:31:27,020   Num examples = 9815
2022-06-12 08:31:27,020   Batch size = 32
2022-06-12 08:31:27,021 ***** Eval results *****
2022-06-12 08:31:27,021   att_loss = 12.013331796896036
2022-06-12 08:31:27,021   global_step = 41499
2022-06-12 08:31:27,021   loss = 12.92213889649929
2022-06-12 08:31:27,021   rep_loss = 0.9088071036990056
2022-06-12 08:31:27,021 ***** Save model *****
2022-06-12 08:32:07,792 ***** Running evaluation *****
2022-06-12 08:32:07,792   Epoch = 5 iter 66499 step
2022-06-12 08:32:07,792   Num examples = 40430
2022-06-12 08:32:07,792   Batch size = 32
2022-06-12 08:32:07,794 ***** Eval results *****
2022-06-12 08:32:07,794   att_loss = 3.483587187650303
2022-06-12 08:32:07,794   global_step = 66499
2022-06-12 08:32:07,794   loss = 4.481285807258391
2022-06-12 08:32:07,795   rep_loss = 0.9976986163650131
2022-06-12 08:32:07,795 ***** Save model *****
2022-06-12 08:33:36,641 ***** Running evaluation *****
2022-06-12 08:33:36,642   Epoch = 3 iter 41999 step
2022-06-12 08:33:36,642   Num examples = 9815
2022-06-12 08:33:36,642   Batch size = 32
2022-06-12 08:33:36,643 ***** Eval results *****
2022-06-12 08:33:36,644   att_loss = 12.010936975938861
2022-06-12 08:33:36,644   global_step = 41999
2022-06-12 08:33:36,644   loss = 12.919472083684978
2022-06-12 08:33:36,644   rep_loss = 0.908535109860898
2022-06-12 08:33:36,644 ***** Save model *****
2022-06-12 08:34:19,065 ***** Running evaluation *****
2022-06-12 08:34:19,065   Epoch = 5 iter 66999 step
2022-06-12 08:34:19,065   Num examples = 40430
2022-06-12 08:34:19,065   Batch size = 32
2022-06-12 08:34:19,066 ***** Eval results *****
2022-06-12 08:34:19,067   att_loss = 3.4839551522586367
2022-06-12 08:34:19,067   global_step = 66999
2022-06-12 08:34:19,067   loss = 4.481566432239105
2022-06-12 08:34:19,067   rep_loss = 0.9976112773376377
2022-06-12 08:34:19,067 ***** Save model *****
2022-06-12 08:35:46,019 ***** Running evaluation *****
2022-06-12 08:35:46,019   Epoch = 3 iter 42499 step
2022-06-12 08:35:46,019   Num examples = 9815
2022-06-12 08:35:46,020   Batch size = 32
2022-06-12 08:35:46,021 ***** Eval results *****
2022-06-12 08:35:46,021   att_loss = 12.012458954904014
2022-06-12 08:35:46,021   global_step = 42499
2022-06-12 08:35:46,021   loss = 12.920838573722826
2022-06-12 08:35:46,021   rep_loss = 0.9083796205589396
2022-06-12 08:35:46,021 ***** Save model *****
2022-06-12 08:36:30,242 ***** Running evaluation *****
2022-06-12 08:36:30,243   Epoch = 5 iter 67499 step
2022-06-12 08:36:30,243   Num examples = 40430
2022-06-12 08:36:30,243   Batch size = 32
2022-06-12 08:36:30,244 ***** Eval results *****
2022-06-12 08:36:30,244   att_loss = 3.4841698358759476
2022-06-12 08:36:30,244   global_step = 67499
2022-06-12 08:36:30,244   loss = 4.481572175699844
2022-06-12 08:36:30,244   rep_loss = 0.9974023369245434
2022-06-12 08:36:30,244 ***** Save model *****
2022-06-12 08:37:55,435 ***** Running evaluation *****
2022-06-12 08:37:55,436   Epoch = 3 iter 42999 step
2022-06-12 08:37:55,436   Num examples = 9815
2022-06-12 08:37:55,436   Batch size = 32
2022-06-12 08:37:55,437 ***** Eval results *****
2022-06-12 08:37:55,437   att_loss = 12.007854525649043
2022-06-12 08:37:55,438   global_step = 42999
2022-06-12 08:37:55,438   loss = 12.916122007786244
2022-06-12 08:37:55,438   rep_loss = 0.9082674845364191
2022-06-12 08:37:55,438 ***** Save model *****
2022-06-12 08:38:42,214 ***** Running evaluation *****
2022-06-12 08:38:42,215   Epoch = 5 iter 67999 step
2022-06-12 08:38:42,215   Num examples = 40430
2022-06-12 08:38:42,215   Batch size = 32
2022-06-12 08:38:42,216 ***** Eval results *****
2022-06-12 08:38:42,217   att_loss = 3.4850057402965
2022-06-12 08:38:42,217   global_step = 67999
2022-06-12 08:38:42,217   loss = 4.482282965800636
2022-06-12 08:38:42,217   rep_loss = 0.9972772226599644
2022-06-12 08:38:42,217 ***** Save model *****
2022-06-12 08:39:42,175 Task finish! 
2022-06-12 08:39:42,176 Task cost 303.1742412166667 minutes, i.e. 5.05290402361111 hours. 
2022-06-12 08:39:44,308 Task start! 
2022-06-12 08:39:44,333 device: cuda n_gpu: 1
2022-06-12 08:39:44,334 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/QQP', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=500, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=6, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/qqp/on_original_data', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/qqp/on_original_data', task_name='qqp', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/qqp/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/qqp/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 08:39:46,606 Writing example 0 of 363846
2022-06-12 08:39:46,607 *** Example ***
2022-06-12 08:39:46,607 guid: train-133273
2022-06-12 08:39:46,607 tokens: [CLS] how is the life of a math student ? could you describe your own experiences ? [SEP] which level of prep ##ration is enough for the exam j ##lp ##t ##5 ? [SEP]
2022-06-12 08:39:46,607 input_ids: 101 2129 2003 1996 2166 1997 1037 8785 3076 1029 2071 2017 6235 2115 2219 6322 1029 102 2029 2504 1997 17463 8156 2003 2438 2005 1996 11360 1046 14277 2102 2629 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 08:39:46,607 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 08:39:46,607 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 08:39:46,607 label: 0
2022-06-12 08:39:46,608 label_id: 0
2022-06-12 08:39:50,862 Writing example 10000 of 363846
2022-06-12 08:39:55,047 Writing example 20000 of 363846
2022-06-12 08:39:59,286 Writing example 30000 of 363846
2022-06-12 08:40:03,640 Writing example 40000 of 363846
2022-06-12 08:40:05,426 ***** Running evaluation *****
2022-06-12 08:40:05,426   Epoch = 3 iter 43499 step
2022-06-12 08:40:05,426   Num examples = 9815
2022-06-12 08:40:05,426   Batch size = 32
2022-06-12 08:40:05,428 ***** Eval results *****
2022-06-12 08:40:05,428   att_loss = 12.014255811524112
2022-06-12 08:40:05,428   global_step = 43499
2022-06-12 08:40:05,428   loss = 12.92237106469303
2022-06-12 08:40:05,428   rep_loss = 0.9081152554065426
2022-06-12 08:40:05,428 ***** Save model *****
2022-06-12 08:40:07,769 Writing example 50000 of 363846
2022-06-12 08:40:11,933 Writing example 60000 of 363846
2022-06-12 08:40:16,378 Writing example 70000 of 363846
2022-06-12 08:40:20,554 Writing example 80000 of 363846
2022-06-12 08:40:24,694 Writing example 90000 of 363846
2022-06-12 08:40:28,860 Writing example 100000 of 363846
2022-06-12 08:40:33,552 Writing example 110000 of 363846
2022-06-12 08:40:37,651 Writing example 120000 of 363846
2022-06-12 08:40:41,770 Writing example 130000 of 363846
2022-06-12 08:40:45,893 Writing example 140000 of 363846
2022-06-12 08:40:50,015 Writing example 150000 of 363846
2022-06-12 08:40:54,199 Writing example 160000 of 363846
2022-06-12 08:40:59,065 Writing example 170000 of 363846
2022-06-12 08:41:03,146 Writing example 180000 of 363846
2022-06-12 08:41:07,240 Writing example 190000 of 363846
2022-06-12 08:41:11,338 Writing example 200000 of 363846
2022-06-12 08:41:15,483 Writing example 210000 of 363846
2022-06-12 08:41:19,675 Writing example 220000 of 363846
2022-06-12 08:41:24,769 Writing example 230000 of 363846
2022-06-12 08:41:28,890 Writing example 240000 of 363846
2022-06-12 08:41:32,991 Writing example 250000 of 363846
2022-06-12 08:41:37,136 Writing example 260000 of 363846
2022-06-12 08:41:41,266 Writing example 270000 of 363846
2022-06-12 08:41:45,442 Writing example 280000 of 363846
2022-06-12 08:41:49,625 Writing example 290000 of 363846
2022-06-12 08:41:53,734 Writing example 300000 of 363846
2022-06-12 08:41:57,830 Writing example 310000 of 363846
2022-06-12 08:42:03,167 Writing example 320000 of 363846
2022-06-12 08:42:07,275 Writing example 330000 of 363846
2022-06-12 08:42:11,459 Writing example 340000 of 363846
2022-06-12 08:42:15,239 ***** Running evaluation *****
2022-06-12 08:42:15,239   Epoch = 3 iter 43999 step
2022-06-12 08:42:15,239   Num examples = 9815
2022-06-12 08:42:15,239   Batch size = 32
2022-06-12 08:42:15,241 ***** Eval results *****
2022-06-12 08:42:15,241   att_loss = 12.013579914306419
2022-06-12 08:42:15,241   global_step = 43999
2022-06-12 08:42:15,241   loss = 12.921530905521053
2022-06-12 08:42:15,241   rep_loss = 0.9079509934790447
2022-06-12 08:42:15,241 ***** Save model *****
2022-06-12 08:42:15,636 Writing example 350000 of 363846
2022-06-12 08:42:19,794 Writing example 360000 of 363846
2022-06-12 08:42:24,184 Writing example 0 of 40430
2022-06-12 08:42:24,185 *** Example ***
2022-06-12 08:42:24,185 guid: dev-201359
2022-06-12 08:42:24,185 tokens: [CLS] why are african - americans so beautiful ? [SEP] why are hispanic ##s so beautiful ? [SEP]
2022-06-12 08:42:24,185 input_ids: 101 2339 2024 3060 1011 4841 2061 3376 1029 102 2339 2024 6696 2015 2061 3376 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 08:42:24,185 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 08:42:24,185 segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 08:42:24,185 label: 0
2022-06-12 08:42:24,185 label_id: 0
2022-06-12 08:42:28,340 Writing example 10000 of 40430
2022-06-12 08:42:32,493 Writing example 20000 of 40430
2022-06-12 08:42:36,616 Writing example 30000 of 40430
2022-06-12 08:42:42,386 Writing example 40000 of 40430
2022-06-12 08:42:42,862 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "qqp",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 08:42:48,034 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/qqp/on_original_data/pytorch_model.bin
2022-06-12 08:42:48,582 loading model...
2022-06-12 08:42:48,781 done!
2022-06-12 08:42:51,886 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 08:42:52,963 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/qqp/on_original_data/pytorch_model.bin
2022-06-12 08:42:53,085 loading model...
2022-06-12 08:42:53,179 done!
2022-06-12 08:42:54,550 ***** Running training *****
2022-06-12 08:42:54,566   Num examples = 363846
2022-06-12 08:42:54,571   Batch size = 32
2022-06-12 08:42:54,581   Num steps = 68220
2022-06-12 08:42:54,597 n: bert.embeddings.word_embeddings.weight
2022-06-12 08:42:54,613 n: bert.embeddings.position_embeddings.weight
2022-06-12 08:42:54,621 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 08:42:54,622 n: bert.embeddings.LayerNorm.weight
2022-06-12 08:42:54,622 n: bert.embeddings.LayerNorm.bias
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 08:42:54,622 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 08:42:54,623 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 08:42:54,624 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 08:42:54,625 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 08:42:54,626 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 08:42:54,626 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 08:42:54,627 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 08:42:54,627 n: bert.pooler.dense.weight
2022-06-12 08:42:54,627 n: bert.pooler.dense.bias
2022-06-12 08:42:54,627 n: classifier.weight
2022-06-12 08:42:54,627 n: classifier.bias
2022-06-12 08:42:54,627 n: fit_denses.0.weight
2022-06-12 08:42:54,627 n: fit_denses.0.bias
2022-06-12 08:42:54,627 n: fit_denses.1.weight
2022-06-12 08:42:54,627 n: fit_denses.1.bias
2022-06-12 08:42:54,627 n: fit_denses.2.weight
2022-06-12 08:42:54,628 n: fit_denses.2.bias
2022-06-12 08:42:54,628 n: fit_denses.3.weight
2022-06-12 08:42:54,628 n: fit_denses.3.bias
2022-06-12 08:42:54,628 n: fit_denses.4.weight
2022-06-12 08:42:54,628 n: fit_denses.4.bias
2022-06-12 08:42:54,628 n: fit_denses.5.weight
2022-06-12 08:42:54,628 n: fit_denses.5.bias
2022-06-12 08:42:54,628 n: fit_denses.6.weight
2022-06-12 08:42:54,628 n: fit_denses.6.bias
2022-06-12 08:42:54,628 Total parameters: 72468738
2022-06-12 08:44:24,984 ***** Running evaluation *****
2022-06-12 08:44:24,984   Epoch = 3 iter 44499 step
2022-06-12 08:44:24,984   Num examples = 9815
2022-06-12 08:44:24,984   Batch size = 32
2022-06-12 08:44:24,986 ***** Eval results *****
2022-06-12 08:44:24,986   att_loss = 12.009380730681825
2022-06-12 08:44:24,986   global_step = 44499
2022-06-12 08:44:24,986   loss = 12.917130480822628
2022-06-12 08:44:24,986   rep_loss = 0.9077497515444516
2022-06-12 08:44:24,986 ***** Save model *****
2022-06-12 08:44:57,467 ***** Running evaluation *****
2022-06-12 08:44:57,468   Epoch = 0 iter 499 step
2022-06-12 08:44:57,468   Num examples = 40430
2022-06-12 08:44:57,468   Batch size = 32
2022-06-12 08:45:32,469 ***** Eval results *****
2022-06-12 08:45:32,469   acc = 0.8988869651249073
2022-06-12 08:45:32,469   acc_and_f1 = 0.8821047463168188
2022-06-12 08:45:32,470   cls_loss = 0.20244175337538692
2022-06-12 08:45:32,470   eval_loss = 0.2799742971016448
2022-06-12 08:45:32,470   f1 = 0.8653225275087304
2022-06-12 08:45:32,470   global_step = 499
2022-06-12 08:45:32,470   loss = 0.20244175337538692
2022-06-12 08:45:32,470 ***** Save model *****
2022-06-12 08:46:34,601 ***** Running evaluation *****
2022-06-12 08:46:34,601   Epoch = 3 iter 44999 step
2022-06-12 08:46:34,601   Num examples = 9815
2022-06-12 08:46:34,602   Batch size = 32
2022-06-12 08:46:34,603 ***** Eval results *****
2022-06-12 08:46:34,603   att_loss = 12.008086176568654
2022-06-12 08:46:34,603   global_step = 44999
2022-06-12 08:46:34,603   loss = 12.915728270439597
2022-06-12 08:46:34,603   rep_loss = 0.9076420951670142
2022-06-12 08:46:34,603 ***** Save model *****
2022-06-12 08:47:36,141 ***** Running evaluation *****
2022-06-12 08:47:36,142   Epoch = 0 iter 999 step
2022-06-12 08:47:36,142   Num examples = 40430
2022-06-12 08:47:36,142   Batch size = 32
2022-06-12 08:48:11,137 ***** Eval results *****
2022-06-12 08:48:11,137   acc = 0.902448676725204
2022-06-12 08:48:11,137   acc_and_f1 = 0.8853372685329997
2022-06-12 08:48:11,137   cls_loss = 0.1523036322406343
2022-06-12 08:48:11,137   eval_loss = 0.2573661020119922
2022-06-12 08:48:11,138   f1 = 0.8682258603407952
2022-06-12 08:48:11,138   global_step = 999
2022-06-12 08:48:11,138   loss = 0.1523036322406343
2022-06-12 08:48:11,138 ***** Save model *****
2022-06-12 08:48:44,457 ***** Running evaluation *****
2022-06-12 08:48:44,458   Epoch = 3 iter 45499 step
2022-06-12 08:48:44,458   Num examples = 9815
2022-06-12 08:48:44,458   Batch size = 32
2022-06-12 08:48:44,459 ***** Eval results *****
2022-06-12 08:48:44,459   att_loss = 12.010405660361814
2022-06-12 08:48:44,459   global_step = 45499
2022-06-12 08:48:44,459   loss = 12.917924638884752
2022-06-12 08:48:44,459   rep_loss = 0.9075189803963072
2022-06-12 08:48:44,459 ***** Save model *****
2022-06-12 08:50:14,708 ***** Running evaluation *****
2022-06-12 08:50:14,708   Epoch = 0 iter 1499 step
2022-06-12 08:50:14,708   Num examples = 40430
2022-06-12 08:50:14,708   Batch size = 32
2022-06-12 08:50:49,683 ***** Eval results *****
2022-06-12 08:50:49,683   acc = 0.9017313875834776
2022-06-12 08:50:49,683   acc_and_f1 = 0.8847129747534759
2022-06-12 08:50:49,683   cls_loss = 0.13416871793314644
2022-06-12 08:50:49,683   eval_loss = 0.25321937912391335
2022-06-12 08:50:49,683   f1 = 0.867694561923474
2022-06-12 08:50:49,683   global_step = 1499
2022-06-12 08:50:49,683   loss = 0.13416871793314644
2022-06-12 08:50:53,660 ***** Running evaluation *****
2022-06-12 08:50:53,660   Epoch = 3 iter 45999 step
2022-06-12 08:50:53,660   Num examples = 9815
2022-06-12 08:50:53,660   Batch size = 32
2022-06-12 08:50:53,662 ***** Eval results *****
2022-06-12 08:50:53,662   att_loss = 12.011940625662277
2022-06-12 08:50:53,662   global_step = 45999
2022-06-12 08:50:53,662   loss = 12.91935949254965
2022-06-12 08:50:53,662   rep_loss = 0.907418868795034
2022-06-12 08:50:53,662 ***** Save model *****
2022-06-12 08:52:53,179 ***** Running evaluation *****
2022-06-12 08:52:53,179   Epoch = 0 iter 1999 step
2022-06-12 08:52:53,179   Num examples = 40430
2022-06-12 08:52:53,180   Batch size = 32
2022-06-12 08:53:03,209 ***** Running evaluation *****
2022-06-12 08:53:03,209   Epoch = 3 iter 46499 step
2022-06-12 08:53:03,210   Num examples = 9815
2022-06-12 08:53:03,210   Batch size = 32
2022-06-12 08:53:03,211 ***** Eval results *****
2022-06-12 08:53:03,211   att_loss = 12.012364191283329
2022-06-12 08:53:03,211   global_step = 46499
2022-06-12 08:53:03,211   loss = 12.919624049678806
2022-06-12 08:53:03,211   rep_loss = 0.9072598596569824
2022-06-12 08:53:03,212 ***** Save model *****
2022-06-12 08:53:28,099 ***** Eval results *****
2022-06-12 08:53:28,099   acc = 0.899307444966609
2022-06-12 08:53:28,099   acc_and_f1 = 0.882654611196645
2022-06-12 08:53:28,099   cls_loss = 0.12569160292250267
2022-06-12 08:53:28,099   eval_loss = 0.25197954370369074
2022-06-12 08:53:28,100   f1 = 0.866001777426681
2022-06-12 08:53:28,100   global_step = 1999
2022-06-12 08:53:28,100   loss = 0.12569160292250267
2022-06-12 08:55:14,359 ***** Running evaluation *****
2022-06-12 08:55:14,360   Epoch = 3 iter 46999 step
2022-06-12 08:55:14,360   Num examples = 9815
2022-06-12 08:55:14,360   Batch size = 32
2022-06-12 08:55:14,361 ***** Eval results *****
2022-06-12 08:55:14,362   att_loss = 12.012725670445816
2022-06-12 08:55:14,362   global_step = 46999
2022-06-12 08:55:14,362   loss = 12.919888310925844
2022-06-12 08:55:14,362   rep_loss = 0.9071626416386497
2022-06-12 08:55:14,362 ***** Save model *****
2022-06-12 08:55:32,503 ***** Running evaluation *****
2022-06-12 08:55:32,503   Epoch = 0 iter 2499 step
2022-06-12 08:55:32,503   Num examples = 40430
2022-06-12 08:55:32,504   Batch size = 32
2022-06-12 08:56:07,471 ***** Eval results *****
2022-06-12 08:56:07,471   acc = 0.9032401681919366
2022-06-12 08:56:07,471   acc_and_f1 = 0.8865285699528735
2022-06-12 08:56:07,471   cls_loss = 0.12005358387161942
2022-06-12 08:56:07,471   eval_loss = 0.25094486302931945
2022-06-12 08:56:07,472   f1 = 0.8698169717138104
2022-06-12 08:56:07,472   global_step = 2499
2022-06-12 08:56:07,472   loss = 0.12005358387161942
2022-06-12 08:56:07,472 ***** Save model *****
2022-06-12 08:57:26,120 ***** Running evaluation *****
2022-06-12 08:57:26,120   Epoch = 3 iter 47499 step
2022-06-12 08:57:26,121   Num examples = 9815
2022-06-12 08:57:26,121   Batch size = 32
2022-06-12 08:57:26,122 ***** Eval results *****
2022-06-12 08:57:26,122   att_loss = 12.015251443913963
2022-06-12 08:57:26,122   global_step = 47499
2022-06-12 08:57:26,122   loss = 12.922312477253895
2022-06-12 08:57:26,123   rep_loss = 0.9070610340092705
2022-06-12 08:57:26,123 ***** Save model *****
2022-06-12 08:58:10,864 ***** Running evaluation *****
2022-06-12 08:58:10,865   Epoch = 0 iter 2999 step
2022-06-12 08:58:10,865   Num examples = 40430
2022-06-12 08:58:10,865   Batch size = 32
2022-06-12 08:58:45,771 ***** Eval results *****
2022-06-12 08:58:45,771   acc = 0.9014593123917882
2022-06-12 08:58:45,771   acc_and_f1 = 0.8849088790349107
2022-06-12 08:58:45,771   cls_loss = 0.11672207072784639
2022-06-12 08:58:45,771   eval_loss = 0.2491557057652198
2022-06-12 08:58:45,771   f1 = 0.8683584456780332
2022-06-12 08:58:45,771   global_step = 2999
2022-06-12 08:58:45,771   loss = 0.11672207072784639
2022-06-12 08:59:35,366 ***** Running evaluation *****
2022-06-12 08:59:35,366   Epoch = 3 iter 47999 step
2022-06-12 08:59:35,366   Num examples = 9815
2022-06-12 08:59:35,366   Batch size = 32
2022-06-12 08:59:35,368 ***** Eval results *****
2022-06-12 08:59:35,368   att_loss = 12.011851961148482
2022-06-12 08:59:35,368   global_step = 47999
2022-06-12 08:59:35,368   loss = 12.918757921123726
2022-06-12 08:59:35,368   rep_loss = 0.90690595974079
2022-06-12 08:59:35,368 ***** Save model *****
2022-06-12 09:00:48,300 ***** Running evaluation *****
2022-06-12 09:00:48,300   Epoch = 0 iter 3499 step
2022-06-12 09:00:48,300   Num examples = 40430
2022-06-12 09:00:48,300   Batch size = 32
2022-06-12 09:01:23,246 ***** Eval results *****
2022-06-12 09:01:23,246   acc = 0.8997031906999753
2022-06-12 09:01:23,246   acc_and_f1 = 0.8825308130675558
2022-06-12 09:01:23,246   cls_loss = 0.11431124717290997
2022-06-12 09:01:23,246   eval_loss = 0.2493799857182216
2022-06-12 09:01:23,246   f1 = 0.8653584354351364
2022-06-12 09:01:23,246   global_step = 3499
2022-06-12 09:01:23,246   loss = 0.11431124717290997
2022-06-12 09:01:46,004 ***** Running evaluation *****
2022-06-12 09:01:46,005   Epoch = 3 iter 48499 step
2022-06-12 09:01:46,005   Num examples = 9815
2022-06-12 09:01:46,005   Batch size = 32
2022-06-12 09:01:46,006 ***** Eval results *****
2022-06-12 09:01:46,006   att_loss = 12.010633495803486
2022-06-12 09:01:46,006   global_step = 48499
2022-06-12 09:01:46,006   loss = 12.917442450963245
2022-06-12 09:01:46,006   rep_loss = 0.9068089546395066
2022-06-12 09:01:46,007 ***** Save model *****
2022-06-12 09:03:26,015 ***** Running evaluation *****
2022-06-12 09:03:26,016   Epoch = 0 iter 3999 step
2022-06-12 09:03:26,016   Num examples = 40430
2022-06-12 09:03:26,016   Batch size = 32
2022-06-12 09:03:56,068 ***** Running evaluation *****
2022-06-12 09:03:56,068   Epoch = 3 iter 48999 step
2022-06-12 09:03:56,068   Num examples = 9815
2022-06-12 09:03:56,068   Batch size = 32
2022-06-12 09:03:56,069 ***** Eval results *****
2022-06-12 09:03:56,069   att_loss = 12.012383389390788
2022-06-12 09:03:56,070   global_step = 48999
2022-06-12 09:03:56,070   loss = 12.919089703064492
2022-06-12 09:03:56,070   rep_loss = 0.9067063133949015
2022-06-12 09:03:56,070 ***** Save model *****
2022-06-12 09:04:00,960 ***** Eval results *****
2022-06-12 09:04:00,960   acc = 0.9031659658669305
2022-06-12 09:04:00,960   acc_and_f1 = 0.8859148961140484
2022-06-12 09:04:00,960   cls_loss = 0.11246957477050801
2022-06-12 09:04:00,960   eval_loss = 0.2529177426248553
2022-06-12 09:04:00,960   f1 = 0.8686638263611661
2022-06-12 09:04:00,960   global_step = 3999
2022-06-12 09:04:00,960   loss = 0.11246957477050801
2022-06-12 09:06:03,692 ***** Running evaluation *****
2022-06-12 09:06:03,692   Epoch = 0 iter 4499 step
2022-06-12 09:06:03,692   Num examples = 40430
2022-06-12 09:06:03,692   Batch size = 32
2022-06-12 09:06:05,490 ***** Running evaluation *****
2022-06-12 09:06:05,490   Epoch = 4 iter 49499 step
2022-06-12 09:06:05,490   Num examples = 9815
2022-06-12 09:06:05,490   Batch size = 32
2022-06-12 09:06:05,492 ***** Eval results *****
2022-06-12 09:06:05,492   att_loss = 11.67697734258261
2022-06-12 09:06:05,492   global_step = 49499
2022-06-12 09:06:05,492   loss = 12.577566188214773
2022-06-12 09:06:05,492   rep_loss = 0.9005888518080654
2022-06-12 09:06:05,492 ***** Save model *****
2022-06-12 09:06:38,598 ***** Eval results *****
2022-06-12 09:06:38,599   acc = 0.9015087806084591
2022-06-12 09:06:38,599   acc_and_f1 = 0.8842724883408263
2022-06-12 09:06:38,599   cls_loss = 0.110904086633745
2022-06-12 09:06:38,599   eval_loss = 0.2570756328221458
2022-06-12 09:06:38,599   f1 = 0.8670361960731935
2022-06-12 09:06:38,599   global_step = 4499
2022-06-12 09:06:38,599   loss = 0.110904086633745
2022-06-12 09:08:14,829 ***** Running evaluation *****
2022-06-12 09:08:14,830   Epoch = 4 iter 49999 step
2022-06-12 09:08:14,830   Num examples = 9815
2022-06-12 09:08:14,830   Batch size = 32
2022-06-12 09:08:14,831 ***** Eval results *****
2022-06-12 09:08:14,831   att_loss = 11.757041774291158
2022-06-12 09:08:14,831   global_step = 49999
2022-06-12 09:08:14,831   loss = 12.657927330725832
2022-06-12 09:08:14,831   rep_loss = 0.900885555196981
2022-06-12 09:08:14,831 ***** Save model *****
2022-06-12 09:08:41,376 ***** Running evaluation *****
2022-06-12 09:08:41,376   Epoch = 0 iter 4999 step
2022-06-12 09:08:41,376   Num examples = 40430
2022-06-12 09:08:41,376   Batch size = 32
2022-06-12 09:09:16,324 ***** Eval results *****
2022-06-12 09:09:16,325   acc = 0.900371011625031
2022-06-12 09:09:16,325   acc_and_f1 = 0.8810376653950192
2022-06-12 09:09:16,325   cls_loss = 0.10990719593931708
2022-06-12 09:09:16,325   eval_loss = 0.27228435294119097
2022-06-12 09:09:16,325   f1 = 0.8617043191650072
2022-06-12 09:09:16,325   global_step = 4999
2022-06-12 09:09:16,325   loss = 0.10990719593931708
2022-06-12 09:10:26,407 ***** Running evaluation *****
2022-06-12 09:10:26,407   Epoch = 4 iter 50499 step
2022-06-12 09:10:26,408   Num examples = 9815
2022-06-12 09:10:26,408   Batch size = 32
2022-06-12 09:10:26,409 ***** Eval results *****
2022-06-12 09:10:26,409   att_loss = 11.770041830042647
2022-06-12 09:10:26,409   global_step = 50499
2022-06-12 09:10:26,410   loss = 12.671253686911648
2022-06-12 09:10:26,410   rep_loss = 0.9012118573323576
2022-06-12 09:10:26,410 ***** Save model *****
2022-06-12 09:11:20,118 ***** Running evaluation *****
2022-06-12 09:11:20,118   Epoch = 0 iter 5499 step
2022-06-12 09:11:20,118   Num examples = 40430
2022-06-12 09:11:20,118   Batch size = 32
2022-06-12 09:11:55,013 ***** Eval results *****
2022-06-12 09:11:55,013   acc = 0.8972297798664358
2022-06-12 09:11:55,013   acc_and_f1 = 0.8801251263094418
2022-06-12 09:11:55,014   cls_loss = 0.10889189826450926
2022-06-12 09:11:55,014   eval_loss = 0.26216230277691177
2022-06-12 09:11:55,014   f1 = 0.8630204727524478
2022-06-12 09:11:55,014   global_step = 5499
2022-06-12 09:11:55,014   loss = 0.10889189826450926
2022-06-12 09:12:36,680 ***** Running evaluation *****
2022-06-12 09:12:36,680   Epoch = 4 iter 50999 step
2022-06-12 09:12:36,680   Num examples = 9815
2022-06-12 09:12:36,680   Batch size = 32
2022-06-12 09:12:36,681 ***** Eval results *****
2022-06-12 09:12:36,681   att_loss = 11.748829889670986
2022-06-12 09:12:36,681   global_step = 50999
2022-06-12 09:12:36,682   loss = 12.649618023307143
2022-06-12 09:12:36,682   rep_loss = 0.9007881280958808
2022-06-12 09:12:36,682 ***** Save model *****
2022-06-12 09:13:57,737 ***** Running evaluation *****
2022-06-12 09:13:57,738   Epoch = 0 iter 5999 step
2022-06-12 09:13:57,738   Num examples = 40430
2022-06-12 09:13:57,738   Batch size = 32
2022-06-12 09:14:32,636 ***** Eval results *****
2022-06-12 09:14:32,636   acc = 0.8983922829581994
2022-06-12 09:14:32,636   acc_and_f1 = 0.8811288031970212
2022-06-12 09:14:32,636   cls_loss = 0.1083041184727282
2022-06-12 09:14:32,636   eval_loss = 0.2678857580640742
2022-06-12 09:14:32,637   f1 = 0.8638653234358431
2022-06-12 09:14:32,637   global_step = 5999
2022-06-12 09:14:32,637   loss = 0.1083041184727282
2022-06-12 09:14:46,942 ***** Running evaluation *****
2022-06-12 09:14:46,942   Epoch = 4 iter 51499 step
2022-06-12 09:14:46,942   Num examples = 9815
2022-06-12 09:14:46,942   Batch size = 32
2022-06-12 09:14:46,944 ***** Eval results *****
2022-06-12 09:14:46,944   att_loss = 11.738259732303659
2022-06-12 09:14:46,944   global_step = 51499
2022-06-12 09:14:46,944   loss = 12.638876301309336
2022-06-12 09:14:46,944   rep_loss = 0.9006165663154476
2022-06-12 09:14:46,944 ***** Save model *****
2022-06-12 09:16:34,961 ***** Running evaluation *****
2022-06-12 09:16:34,961   Epoch = 0 iter 6499 step
2022-06-12 09:16:34,962   Num examples = 40430
2022-06-12 09:16:34,962   Batch size = 32
2022-06-12 09:16:58,163 ***** Running evaluation *****
2022-06-12 09:16:58,164   Epoch = 4 iter 51999 step
2022-06-12 09:16:58,164   Num examples = 9815
2022-06-12 09:16:58,164   Batch size = 32
2022-06-12 09:16:58,166 ***** Eval results *****
2022-06-12 09:16:58,166   att_loss = 11.746478075024195
2022-06-12 09:16:58,166   global_step = 51999
2022-06-12 09:16:58,166   loss = 12.64701718417081
2022-06-12 09:16:58,166   rep_loss = 0.9005391063248518
2022-06-12 09:16:58,166 ***** Save model *****
2022-06-12 09:17:09,869 ***** Eval results *****
2022-06-12 09:17:09,869   acc = 0.8985901558248826
2022-06-12 09:17:09,869   acc_and_f1 = 0.8799399117983605
2022-06-12 09:17:09,869   cls_loss = 0.10799715734799287
2022-06-12 09:17:09,870   eval_loss = 0.2563187476583533
2022-06-12 09:17:09,870   f1 = 0.8612896677718385
2022-06-12 09:17:09,870   global_step = 6499
2022-06-12 09:17:09,870   loss = 0.10799715734799287
2022-06-12 09:19:09,293 ***** Running evaluation *****
2022-06-12 09:19:09,294   Epoch = 4 iter 52499 step
2022-06-12 09:19:09,294   Num examples = 9815
2022-06-12 09:19:09,294   Batch size = 32
2022-06-12 09:19:09,295 ***** Eval results *****
2022-06-12 09:19:09,295   att_loss = 11.747220302291357
2022-06-12 09:19:09,295   global_step = 52499
2022-06-12 09:19:09,295   loss = 12.647855719291274
2022-06-12 09:19:09,295   rep_loss = 0.9006354127761037
2022-06-12 09:19:09,295 ***** Save model *****
2022-06-12 09:19:12,335 ***** Running evaluation *****
2022-06-12 09:19:12,335   Epoch = 0 iter 6999 step
2022-06-12 09:19:12,335   Num examples = 40430
2022-06-12 09:19:12,335   Batch size = 32
2022-06-12 09:19:47,270 ***** Eval results *****
2022-06-12 09:19:47,271   acc = 0.8965619589413801
2022-06-12 09:19:47,271   acc_and_f1 = 0.8783010597919753
2022-06-12 09:19:47,271   cls_loss = 0.10761052227638537
2022-06-12 09:19:47,271   eval_loss = 0.2722260349052899
2022-06-12 09:19:47,271   f1 = 0.8600401606425704
2022-06-12 09:19:47,271   global_step = 6999
2022-06-12 09:19:47,271   loss = 0.10761052227638537
2022-06-12 09:21:20,587 ***** Running evaluation *****
2022-06-12 09:21:20,587   Epoch = 4 iter 52999 step
2022-06-12 09:21:20,587   Num examples = 9815
2022-06-12 09:21:20,587   Batch size = 32
2022-06-12 09:21:20,589 ***** Eval results *****
2022-06-12 09:21:20,589   att_loss = 11.743147187580094
2022-06-12 09:21:20,589   global_step = 52999
2022-06-12 09:21:20,589   loss = 12.643680256079897
2022-06-12 09:21:20,589   rep_loss = 0.9005330646784065
2022-06-12 09:21:20,589 ***** Save model *****
2022-06-12 09:21:49,861 ***** Running evaluation *****
2022-06-12 09:21:49,861   Epoch = 0 iter 7499 step
2022-06-12 09:21:49,861   Num examples = 40430
2022-06-12 09:21:49,861   Batch size = 32
2022-06-12 09:22:24,810 ***** Eval results *****
2022-06-12 09:22:24,810   acc = 0.8896858768241405
2022-06-12 09:22:24,811   acc_and_f1 = 0.8742241531818467
2022-06-12 09:22:24,811   cls_loss = 0.1073118400090471
2022-06-12 09:22:24,811   eval_loss = 0.2745863426170206
2022-06-12 09:22:24,811   f1 = 0.858762429539553
2022-06-12 09:22:24,811   global_step = 7499
2022-06-12 09:22:24,811   loss = 0.1073118400090471
2022-06-12 09:23:30,359 ***** Running evaluation *****
2022-06-12 09:23:30,360   Epoch = 4 iter 53499 step
2022-06-12 09:23:30,360   Num examples = 9815
2022-06-12 09:23:30,360   Batch size = 32
2022-06-12 09:23:30,362 ***** Eval results *****
2022-06-12 09:23:30,362   att_loss = 11.731357263087686
2022-06-12 09:23:30,362   global_step = 53499
2022-06-12 09:23:30,362   loss = 12.631708526611328
2022-06-12 09:23:30,362   rep_loss = 0.9003512608235447
2022-06-12 09:23:30,362 ***** Save model *****
2022-06-12 09:24:27,326 ***** Running evaluation *****
2022-06-12 09:24:27,326   Epoch = 0 iter 7999 step
2022-06-12 09:24:27,326   Num examples = 40430
2022-06-12 09:24:27,326   Batch size = 32
2022-06-12 09:25:02,264 ***** Eval results *****
2022-06-12 09:25:02,265   acc = 0.8980212713331684
2022-06-12 09:25:02,265   acc_and_f1 = 0.879353599004986
2022-06-12 09:25:02,265   cls_loss = 0.10712454359173208
2022-06-12 09:25:02,265   eval_loss = 0.263465446738314
2022-06-12 09:25:02,265   f1 = 0.8606859266768035
2022-06-12 09:25:02,265   global_step = 7999
2022-06-12 09:25:02,265   loss = 0.10712454359173208
2022-06-12 09:25:40,954 ***** Running evaluation *****
2022-06-12 09:25:40,955   Epoch = 4 iter 53999 step
2022-06-12 09:25:40,955   Num examples = 9815
2022-06-12 09:25:40,955   Batch size = 32
2022-06-12 09:25:40,956 ***** Eval results *****
2022-06-12 09:25:40,957   att_loss = 11.732210846277123
2022-06-12 09:25:40,957   global_step = 53999
2022-06-12 09:25:40,957   loss = 12.632357669725545
2022-06-12 09:25:40,957   rep_loss = 0.900146821859773
2022-06-12 09:25:40,957 ***** Save model *****
2022-06-12 09:27:05,163 ***** Running evaluation *****
2022-06-12 09:27:05,164   Epoch = 0 iter 8499 step
2022-06-12 09:27:05,164   Num examples = 40430
2022-06-12 09:27:05,164   Batch size = 32
2022-06-12 09:27:40,060 ***** Eval results *****
2022-06-12 09:27:40,060   acc = 0.9
2022-06-12 09:27:40,060   acc_and_f1 = 0.8816483516483518
2022-06-12 09:27:40,061   cls_loss = 0.1068651134141446
2022-06-12 09:27:40,061   eval_loss = 0.26274554208923084
2022-06-12 09:27:40,061   f1 = 0.8632967032967034
2022-06-12 09:27:40,061   global_step = 8499
2022-06-12 09:27:40,061   loss = 0.1068651134141446
2022-06-12 09:27:51,217 ***** Running evaluation *****
2022-06-12 09:27:51,218   Epoch = 4 iter 54499 step
2022-06-12 09:27:51,218   Num examples = 9815
2022-06-12 09:27:51,218   Batch size = 32
2022-06-12 09:27:51,219 ***** Eval results *****
2022-06-12 09:27:51,219   att_loss = 11.726047076897167
2022-06-12 09:27:51,219   global_step = 54499
2022-06-12 09:27:51,219   loss = 12.625820204469745
2022-06-12 09:27:51,220   rep_loss = 0.8997731249528358
2022-06-12 09:27:51,220 ***** Save model *****
2022-06-12 09:29:43,498 ***** Running evaluation *****
2022-06-12 09:29:43,498   Epoch = 0 iter 8999 step
2022-06-12 09:29:43,499   Num examples = 40430
2022-06-12 09:29:43,499   Batch size = 32
2022-06-12 09:30:00,996 ***** Running evaluation *****
2022-06-12 09:30:00,997   Epoch = 4 iter 54999 step
2022-06-12 09:30:00,997   Num examples = 9815
2022-06-12 09:30:00,997   Batch size = 32
2022-06-12 09:30:00,999 ***** Eval results *****
2022-06-12 09:30:00,999   att_loss = 11.724151747003008
2022-06-12 09:30:00,999   global_step = 54999
2022-06-12 09:30:00,999   loss = 12.623779097756186
2022-06-12 09:30:00,999   rep_loss = 0.8996273480727324
2022-06-12 09:30:00,999 ***** Save model *****
2022-06-12 09:30:18,447 ***** Eval results *****
2022-06-12 09:30:18,447   acc = 0.8981696759831808
2022-06-12 09:30:18,447   acc_and_f1 = 0.8797400510621647
2022-06-12 09:30:18,447   cls_loss = 0.10677609810464275
2022-06-12 09:30:18,447   eval_loss = 0.2632974567322084
2022-06-12 09:30:18,447   f1 = 0.8613104261411487
2022-06-12 09:30:18,447   global_step = 8999
2022-06-12 09:30:18,447   loss = 0.10677609810464275
2022-06-12 09:32:11,147 ***** Running evaluation *****
2022-06-12 09:32:11,148   Epoch = 4 iter 55499 step
2022-06-12 09:32:11,148   Num examples = 9815
2022-06-12 09:32:11,148   Batch size = 32
2022-06-12 09:32:11,149 ***** Eval results *****
2022-06-12 09:32:11,149   att_loss = 11.722230066922338
2022-06-12 09:32:11,149   global_step = 55499
2022-06-12 09:32:11,150   loss = 12.621692221887578
2022-06-12 09:32:11,150   rep_loss = 0.899462152047726
2022-06-12 09:32:11,150 ***** Save model *****
2022-06-12 09:32:21,241 ***** Running evaluation *****
2022-06-12 09:32:21,241   Epoch = 0 iter 9499 step
2022-06-12 09:32:21,242   Num examples = 40430
2022-06-12 09:32:21,242   Batch size = 32
2022-06-12 09:32:56,252 ***** Eval results *****
2022-06-12 09:32:56,253   acc = 0.8967350976997279
2022-06-12 09:32:56,253   acc_and_f1 = 0.8792976352079356
2022-06-12 09:32:56,253   cls_loss = 0.10663821654699579
2022-06-12 09:32:56,253   eval_loss = 0.26470818448976813
2022-06-12 09:32:56,253   f1 = 0.8618601727161433
2022-06-12 09:32:56,253   global_step = 9499
2022-06-12 09:32:56,253   loss = 0.10663821654699579
2022-06-12 09:34:20,754 ***** Running evaluation *****
2022-06-12 09:34:20,754   Epoch = 4 iter 55999 step
2022-06-12 09:34:20,754   Num examples = 9815
2022-06-12 09:34:20,754   Batch size = 32
2022-06-12 09:34:20,756 ***** Eval results *****
2022-06-12 09:34:20,756   att_loss = 11.721966850421435
2022-06-12 09:34:20,756   global_step = 55999
2022-06-12 09:34:20,756   loss = 12.621303762186288
2022-06-12 09:34:20,756   rep_loss = 0.8993369096789081
2022-06-12 09:34:20,756 ***** Save model *****
2022-06-12 09:34:59,658 ***** Running evaluation *****
2022-06-12 09:34:59,658   Epoch = 0 iter 9999 step
2022-06-12 09:34:59,659   Num examples = 40430
2022-06-12 09:34:59,659   Batch size = 32
2022-06-12 09:35:34,639 ***** Eval results *****
2022-06-12 09:35:34,640   acc = 0.8967845659163988
2022-06-12 09:35:34,640   acc_and_f1 = 0.8795331693975909
2022-06-12 09:35:34,640   cls_loss = 0.10653510141578636
2022-06-12 09:35:34,640   eval_loss = 0.26373256991699906
2022-06-12 09:35:34,640   f1 = 0.8622817728787829
2022-06-12 09:35:34,640   global_step = 9999
2022-06-12 09:35:34,640   loss = 0.10653510141578636
2022-06-12 09:36:30,570 ***** Running evaluation *****
2022-06-12 09:36:30,571   Epoch = 4 iter 56499 step
2022-06-12 09:36:30,571   Num examples = 9815
2022-06-12 09:36:30,571   Batch size = 32
2022-06-12 09:36:30,572 ***** Eval results *****
2022-06-12 09:36:30,573   att_loss = 11.725068006110368
2022-06-12 09:36:30,573   global_step = 56499
2022-06-12 09:36:30,573   loss = 12.62430657922458
2022-06-12 09:36:30,573   rep_loss = 0.8992385710804991
2022-06-12 09:36:30,573 ***** Save model *****
2022-06-12 09:37:37,568 ***** Running evaluation *****
2022-06-12 09:37:37,569   Epoch = 0 iter 10499 step
2022-06-12 09:37:37,569   Num examples = 40430
2022-06-12 09:37:37,569   Batch size = 32
2022-06-12 09:38:12,602 ***** Eval results *****
2022-06-12 09:38:12,602   acc = 0.8975265891664606
2022-06-12 09:38:12,603   acc_and_f1 = 0.8782057142841737
2022-06-12 09:38:12,603   cls_loss = 0.10652070712851325
2022-06-12 09:38:12,603   eval_loss = 0.2548092296376494
2022-06-12 09:38:12,603   f1 = 0.858884839401887
2022-06-12 09:38:12,603   global_step = 10499
2022-06-12 09:38:12,603   loss = 0.10652070712851325
2022-06-12 09:38:40,679 ***** Running evaluation *****
2022-06-12 09:38:40,680   Epoch = 4 iter 56999 step
2022-06-12 09:38:40,680   Num examples = 9815
2022-06-12 09:38:40,680   Batch size = 32
2022-06-12 09:38:40,681 ***** Eval results *****
2022-06-12 09:38:40,681   att_loss = 11.726681293420075
2022-06-12 09:38:40,681   global_step = 56999
2022-06-12 09:38:40,681   loss = 12.625913487818146
2022-06-12 09:38:40,681   rep_loss = 0.8992321905348772
2022-06-12 09:38:40,681 ***** Save model *****
2022-06-12 09:40:16,218 ***** Running evaluation *****
2022-06-12 09:40:16,219   Epoch = 0 iter 10999 step
2022-06-12 09:40:16,219   Num examples = 40430
2022-06-12 09:40:16,219   Batch size = 32
2022-06-12 09:40:51,116 ***** Eval results *****
2022-06-12 09:40:51,116   acc = 0.8942122186495177
2022-06-12 09:40:51,116   acc_and_f1 = 0.8766239235035928
2022-06-12 09:40:51,116   cls_loss = 0.10644639614864158
2022-06-12 09:40:51,116   eval_loss = 0.2545442603546183
2022-06-12 09:40:51,116   f1 = 0.8590356283576678
2022-06-12 09:40:51,116   global_step = 10999
2022-06-12 09:40:51,116   loss = 0.10644639614864158
2022-06-12 09:40:51,677 ***** Running evaluation *****
2022-06-12 09:40:51,677   Epoch = 4 iter 57499 step
2022-06-12 09:40:51,678   Num examples = 9815
2022-06-12 09:40:51,678   Batch size = 32
2022-06-12 09:40:51,679 ***** Eval results *****
2022-06-12 09:40:51,679   att_loss = 11.71878025538852
2022-06-12 09:40:51,679   global_step = 57499
2022-06-12 09:40:51,679   loss = 12.617702410919497
2022-06-12 09:40:51,679   rep_loss = 0.8989221522514833
2022-06-12 09:40:51,679 ***** Save model *****
2022-06-12 09:42:53,834 ***** Running evaluation *****
2022-06-12 09:42:53,834   Epoch = 1 iter 11499 step
2022-06-12 09:42:53,834   Num examples = 40430
2022-06-12 09:42:53,835   Batch size = 32
2022-06-12 09:43:01,227 ***** Running evaluation *****
2022-06-12 09:43:01,227   Epoch = 4 iter 57999 step
2022-06-12 09:43:01,227   Num examples = 9815
2022-06-12 09:43:01,227   Batch size = 32
2022-06-12 09:43:01,229 ***** Eval results *****
2022-06-12 09:43:01,229   att_loss = 11.717624478011203
2022-06-12 09:43:01,229   global_step = 57999
2022-06-12 09:43:01,229   loss = 12.616341416941422
2022-06-12 09:43:01,229   rep_loss = 0.8987169356875653
2022-06-12 09:43:01,229 ***** Save model *****
2022-06-12 09:43:28,800 ***** Eval results *****
2022-06-12 09:43:28,800   acc = 0.894607964382884
2022-06-12 09:43:28,800   acc_and_f1 = 0.8766429759229932
2022-06-12 09:43:28,800   cls_loss = 0.10474564637555632
2022-06-12 09:43:28,800   eval_loss = 0.2590813834184804
2022-06-12 09:43:28,800   f1 = 0.8586779874631024
2022-06-12 09:43:28,800   global_step = 11499
2022-06-12 09:43:28,801   loss = 0.10474564637555632
2022-06-12 09:45:12,513 ***** Running evaluation *****
2022-06-12 09:45:12,514   Epoch = 4 iter 58499 step
2022-06-12 09:45:12,514   Num examples = 9815
2022-06-12 09:45:12,514   Batch size = 32
2022-06-12 09:45:12,515 ***** Eval results *****
2022-06-12 09:45:12,515   att_loss = 11.714931486669652
2022-06-12 09:45:12,515   global_step = 58499
2022-06-12 09:45:12,515   loss = 12.613509943228228
2022-06-12 09:45:12,515   rep_loss = 0.8985784534944604
2022-06-12 09:45:12,516 ***** Save model *****
2022-06-12 09:45:31,290 ***** Running evaluation *****
2022-06-12 09:45:31,290   Epoch = 1 iter 11999 step
2022-06-12 09:45:31,290   Num examples = 40430
2022-06-12 09:45:31,290   Batch size = 32
2022-06-12 09:46:06,212 ***** Eval results *****
2022-06-12 09:46:06,212   acc = 0.8979965372248331
2022-06-12 09:46:06,212   acc_and_f1 = 0.8805887403793862
2022-06-12 09:46:06,212   cls_loss = 0.10158056833311939
2022-06-12 09:46:06,212   eval_loss = 0.25629874621223236
2022-06-12 09:46:06,212   f1 = 0.8631809435339394
2022-06-12 09:46:06,212   global_step = 11999
2022-06-12 09:46:06,212   loss = 0.10158056833311939
2022-06-12 09:47:22,770 ***** Running evaluation *****
2022-06-12 09:47:22,770   Epoch = 4 iter 58999 step
2022-06-12 09:47:22,771   Num examples = 9815
2022-06-12 09:47:22,771   Batch size = 32
2022-06-12 09:47:22,772 ***** Eval results *****
2022-06-12 09:47:22,772   att_loss = 11.714483770744643
2022-06-12 09:47:22,772   global_step = 58999
2022-06-12 09:47:22,772   loss = 12.612917708292551
2022-06-12 09:47:22,772   rep_loss = 0.8984339349569243
2022-06-12 09:47:22,772 ***** Save model *****
2022-06-12 09:48:08,970 ***** Running evaluation *****
2022-06-12 09:48:08,971   Epoch = 1 iter 12499 step
2022-06-12 09:48:08,971   Num examples = 40430
2022-06-12 09:48:08,971   Batch size = 32
2022-06-12 09:48:43,897 ***** Eval results *****
2022-06-12 09:48:43,897   acc = 0.8975760573831313
2022-06-12 09:48:43,897   acc_and_f1 = 0.8799572052481961
2022-06-12 09:48:43,897   cls_loss = 0.10145772899854912
2022-06-12 09:48:43,897   eval_loss = 0.2664482502349287
2022-06-12 09:48:43,897   f1 = 0.862338353113261
2022-06-12 09:48:43,898   global_step = 12499
2022-06-12 09:48:43,898   loss = 0.10145772899854912
2022-06-12 09:49:32,210 ***** Running evaluation *****
2022-06-12 09:49:32,210   Epoch = 4 iter 59499 step
2022-06-12 09:49:32,210   Num examples = 9815
2022-06-12 09:49:32,210   Batch size = 32
2022-06-12 09:49:32,211 ***** Eval results *****
2022-06-12 09:49:32,212   att_loss = 11.714018998212444
2022-06-12 09:49:32,212   global_step = 59499
2022-06-12 09:49:32,212   loss = 12.612408439757099
2022-06-12 09:49:32,212   rep_loss = 0.8983894397018624
2022-06-12 09:49:32,212 ***** Save model *****
2022-06-12 09:50:46,496 ***** Running evaluation *****
2022-06-12 09:50:46,496   Epoch = 1 iter 12999 step
2022-06-12 09:50:46,496   Num examples = 40430
2022-06-12 09:50:46,496   Batch size = 32
2022-06-12 09:51:21,420 ***** Eval results *****
2022-06-12 09:51:21,421   acc = 0.898243878308187
2022-06-12 09:51:21,421   acc_and_f1 = 0.8791797426223016
2022-06-12 09:51:21,421   cls_loss = 0.10019096025043356
2022-06-12 09:51:21,421   eval_loss = 0.2667411055763641
2022-06-12 09:51:21,421   f1 = 0.8601156069364162
2022-06-12 09:51:21,421   global_step = 12999
2022-06-12 09:51:21,421   loss = 0.10019096025043356
2022-06-12 09:51:42,088 ***** Running evaluation *****
2022-06-12 09:51:42,089   Epoch = 4 iter 59999 step
2022-06-12 09:51:42,089   Num examples = 9815
2022-06-12 09:51:42,089   Batch size = 32
2022-06-12 09:51:42,090 ***** Eval results *****
2022-06-12 09:51:42,090   att_loss = 11.711684698060298
2022-06-12 09:51:42,090   global_step = 59999
2022-06-12 09:51:42,090   loss = 12.609864463928474
2022-06-12 09:51:42,090   rep_loss = 0.898179764743251
2022-06-12 09:51:42,090 ***** Save model *****
2022-06-12 09:53:25,206 ***** Running evaluation *****
2022-06-12 09:53:25,207   Epoch = 1 iter 13499 step
2022-06-12 09:53:25,207   Num examples = 40430
2022-06-12 09:53:25,207   Batch size = 32
2022-06-12 09:53:51,621 ***** Running evaluation *****
2022-06-12 09:53:51,622   Epoch = 4 iter 60499 step
2022-06-12 09:53:51,622   Num examples = 9815
2022-06-12 09:53:51,622   Batch size = 32
2022-06-12 09:53:51,623 ***** Eval results *****
2022-06-12 09:53:51,623   att_loss = 11.710369380503122
2022-06-12 09:53:51,623   global_step = 60499
2022-06-12 09:53:51,623   loss = 12.608454935455656
2022-06-12 09:53:51,623   rep_loss = 0.8980855528952205
2022-06-12 09:53:51,624 ***** Save model *****
2022-06-12 09:54:00,226 ***** Eval results *****
2022-06-12 09:54:00,226   acc = 0.8973534504081128
2022-06-12 09:54:00,226   acc_and_f1 = 0.8780792837233813
2022-06-12 09:54:00,226   cls_loss = 0.10009759835489426
2022-06-12 09:54:00,226   eval_loss = 0.2672096167667474
2022-06-12 09:54:00,226   f1 = 0.8588051170386499
2022-06-12 09:54:00,226   global_step = 13499
2022-06-12 09:54:00,226   loss = 0.10009759835489426
2022-06-12 09:56:01,816 ***** Running evaluation *****
2022-06-12 09:56:01,816   Epoch = 4 iter 60999 step
2022-06-12 09:56:01,816   Num examples = 9815
2022-06-12 09:56:01,816   Batch size = 32
2022-06-12 09:56:01,818 ***** Eval results *****
2022-06-12 09:56:01,818   att_loss = 11.707671404585236
2022-06-12 09:56:01,818   global_step = 60999
2022-06-12 09:56:01,818   loss = 12.605598723683295
2022-06-12 09:56:01,818   rep_loss = 0.8979273172621461
2022-06-12 09:56:01,818 ***** Save model *****
2022-06-12 09:56:03,340 ***** Running evaluation *****
2022-06-12 09:56:03,340   Epoch = 1 iter 13999 step
2022-06-12 09:56:03,340   Num examples = 40430
2022-06-12 09:56:03,340   Batch size = 32
2022-06-12 09:56:38,285 ***** Eval results *****
2022-06-12 09:56:38,285   acc = 0.8989611674499134
2022-06-12 09:56:38,286   acc_and_f1 = 0.8816617438670691
2022-06-12 09:56:38,286   cls_loss = 0.1003823963809757
2022-06-12 09:56:38,286   eval_loss = 0.26636014064874125
2022-06-12 09:56:38,286   f1 = 0.8643623202842249
2022-06-12 09:56:38,286   global_step = 13999
2022-06-12 09:56:38,286   loss = 0.1003823963809757
2022-06-12 09:58:11,496 ***** Running evaluation *****
2022-06-12 09:58:11,496   Epoch = 5 iter 61499 step
2022-06-12 09:58:11,496   Num examples = 9815
2022-06-12 09:58:11,496   Batch size = 32
2022-06-12 09:58:11,498 ***** Eval results *****
2022-06-12 09:58:11,498   att_loss = 11.597341232829624
2022-06-12 09:58:11,498   global_step = 61499
2022-06-12 09:58:11,498   loss = 12.495555652512444
2022-06-12 09:58:11,498   rep_loss = 0.8982144085069498
2022-06-12 09:58:11,498 ***** Save model *****
2022-06-12 09:58:41,301 ***** Running evaluation *****
2022-06-12 09:58:41,301   Epoch = 1 iter 14499 step
2022-06-12 09:58:41,301   Num examples = 40430
2022-06-12 09:58:41,302   Batch size = 32
2022-06-12 09:59:16,200 ***** Eval results *****
2022-06-12 09:59:16,200   acc = 0.9010140984417512
2022-06-12 09:59:16,200   acc_and_f1 = 0.8833278375106761
2022-06-12 09:59:16,200   cls_loss = 0.10012105730213118
2022-06-12 09:59:16,201   eval_loss = 0.27431135796617623
2022-06-12 09:59:16,201   f1 = 0.8656415765796012
2022-06-12 09:59:16,201   global_step = 14499
2022-06-12 09:59:16,201   loss = 0.10012105730213118
2022-06-12 10:00:20,715 ***** Running evaluation *****
2022-06-12 10:00:20,715   Epoch = 5 iter 61999 step
2022-06-12 10:00:20,715   Num examples = 9815
2022-06-12 10:00:20,715   Batch size = 32
2022-06-12 10:00:20,717 ***** Eval results *****
2022-06-12 10:00:20,717   att_loss = 11.568082656919586
2022-06-12 10:00:20,717   global_step = 61999
2022-06-12 10:00:20,717   loss = 12.46383706234997
2022-06-12 10:00:20,717   rep_loss = 0.8957544076516761
2022-06-12 10:00:20,717 ***** Save model *****
2022-06-12 10:01:20,003 ***** Running evaluation *****
2022-06-12 10:01:20,003   Epoch = 1 iter 14999 step
2022-06-12 10:01:20,003   Num examples = 40430
2022-06-12 10:01:20,004   Batch size = 32
2022-06-12 10:01:55,032 ***** Eval results *****
2022-06-12 10:01:55,033   acc = 0.895795201582983
2022-06-12 10:01:55,033   acc_and_f1 = 0.8757843557810501
2022-06-12 10:01:55,033   cls_loss = 0.10032915928703624
2022-06-12 10:01:55,033   eval_loss = 0.26650418196451153
2022-06-12 10:01:55,033   f1 = 0.8557735099791173
2022-06-12 10:01:55,033   global_step = 14999
2022-06-12 10:01:55,033   loss = 0.10032915928703624
2022-06-12 10:02:30,807 ***** Running evaluation *****
2022-06-12 10:02:30,807   Epoch = 5 iter 62499 step
2022-06-12 10:02:30,807   Num examples = 9815
2022-06-12 10:02:30,808   Batch size = 32
2022-06-12 10:02:30,809 ***** Eval results *****
2022-06-12 10:02:30,809   att_loss = 11.476457148998767
2022-06-12 10:02:30,809   global_step = 62499
2022-06-12 10:02:30,809   loss = 12.370128206439786
2022-06-12 10:02:30,809   rep_loss = 0.8936710618175827
2022-06-12 10:02:30,809 ***** Save model *****
2022-06-12 10:03:59,403 ***** Running evaluation *****
2022-06-12 10:03:59,404   Epoch = 1 iter 15499 step
2022-06-12 10:03:59,404   Num examples = 40430
2022-06-12 10:03:59,405   Batch size = 32
2022-06-12 10:04:34,515 ***** Eval results *****
2022-06-12 10:04:34,516   acc = 0.8984417511748701
2022-06-12 10:04:34,516   acc_and_f1 = 0.8798487055529687
2022-06-12 10:04:34,516   cls_loss = 0.1004218195995181
2022-06-12 10:04:34,516   eval_loss = 0.25444584803750053
2022-06-12 10:04:34,516   f1 = 0.8612556599310671
2022-06-12 10:04:34,516   global_step = 15499
2022-06-12 10:04:34,516   loss = 0.1004218195995181
2022-06-12 10:04:40,193 ***** Running evaluation *****
2022-06-12 10:04:40,194   Epoch = 5 iter 62999 step
2022-06-12 10:04:40,194   Num examples = 9815
2022-06-12 10:04:40,194   Batch size = 32
2022-06-12 10:04:40,196 ***** Eval results *****
2022-06-12 10:04:40,196   att_loss = 11.453054009852908
2022-06-12 10:04:40,196   global_step = 62999
2022-06-12 10:04:40,196   loss = 12.34604629982997
2022-06-12 10:04:40,196   rep_loss = 0.8929922947265806
2022-06-12 10:04:40,197 ***** Save model *****
2022-06-12 10:06:38,435 ***** Running evaluation *****
2022-06-12 10:06:38,435   Epoch = 1 iter 15999 step
2022-06-12 10:06:38,435   Num examples = 40430
2022-06-12 10:06:38,435   Batch size = 32
2022-06-12 10:06:49,783 ***** Running evaluation *****
2022-06-12 10:06:49,784   Epoch = 5 iter 63499 step
2022-06-12 10:06:49,784   Num examples = 9815
2022-06-12 10:06:49,784   Batch size = 32
2022-06-12 10:06:49,785 ***** Eval results *****
2022-06-12 10:06:49,786   att_loss = 11.47681499239224
2022-06-12 10:06:49,786   global_step = 63499
2022-06-12 10:06:49,786   loss = 12.370203465668123
2022-06-12 10:06:49,786   rep_loss = 0.8933884730812773
2022-06-12 10:06:49,786 ***** Save model *****
2022-06-12 10:07:13,341 ***** Eval results *****
2022-06-12 10:07:13,341   acc = 0.9004452139500371
2022-06-12 10:07:13,341   acc_and_f1 = 0.8839369115104423
2022-06-12 10:07:13,341   cls_loss = 0.10048206155601834
2022-06-12 10:07:13,341   eval_loss = 0.25276819095781805
2022-06-12 10:07:13,341   f1 = 0.8674286090708475
2022-06-12 10:07:13,342   global_step = 15999
2022-06-12 10:07:13,342   loss = 0.10048206155601834
2022-06-12 10:08:59,979 ***** Running evaluation *****
2022-06-12 10:08:59,980   Epoch = 5 iter 63999 step
2022-06-12 10:08:59,980   Num examples = 9815
2022-06-12 10:08:59,980   Batch size = 32
2022-06-12 10:08:59,982 ***** Eval results *****
2022-06-12 10:08:59,982   att_loss = 11.476333399583641
2022-06-12 10:08:59,982   global_step = 63999
2022-06-12 10:08:59,982   loss = 12.36967285245702
2022-06-12 10:08:59,982   rep_loss = 0.8933394526479465
2022-06-12 10:08:59,982 ***** Save model *****
2022-06-12 10:09:16,382 ***** Running evaluation *****
2022-06-12 10:09:16,382   Epoch = 1 iter 16499 step
2022-06-12 10:09:16,382   Num examples = 40430
2022-06-12 10:09:16,382   Batch size = 32
2022-06-12 10:09:51,291 ***** Eval results *****
2022-06-12 10:09:51,291   acc = 0.8976502597081375
2022-06-12 10:09:51,292   acc_and_f1 = 0.8792414882316837
2022-06-12 10:09:51,292   cls_loss = 0.10040486269868543
2022-06-12 10:09:51,292   eval_loss = 0.26970738171794323
2022-06-12 10:09:51,292   f1 = 0.8608327167552298
2022-06-12 10:09:51,292   global_step = 16499
2022-06-12 10:09:51,292   loss = 0.10040486269868543
2022-06-12 10:11:10,620 ***** Running evaluation *****
2022-06-12 10:11:10,621   Epoch = 5 iter 64499 step
2022-06-12 10:11:10,621   Num examples = 9815
2022-06-12 10:11:10,621   Batch size = 32
2022-06-12 10:11:10,622 ***** Eval results *****
2022-06-12 10:11:10,622   att_loss = 11.476085443229772
2022-06-12 10:11:10,622   global_step = 64499
2022-06-12 10:11:10,622   loss = 12.369130923244485
2022-06-12 10:11:10,623   rep_loss = 0.893045478839304
2022-06-12 10:11:10,623 ***** Save model *****
2022-06-12 10:11:54,089 ***** Running evaluation *****
2022-06-12 10:11:54,089   Epoch = 1 iter 16999 step
2022-06-12 10:11:54,090   Num examples = 40430
2022-06-12 10:11:54,090   Batch size = 32
2022-06-12 10:12:29,061 ***** Eval results *****
2022-06-12 10:12:29,061   acc = 0.89812020776651
2022-06-12 10:12:29,061   acc_and_f1 = 0.8801643757863762
2022-06-12 10:12:29,061   cls_loss = 0.10053094541384305
2022-06-12 10:12:29,061   eval_loss = 0.2565364428336107
2022-06-12 10:12:29,061   f1 = 0.8622085438062423
2022-06-12 10:12:29,061   global_step = 16999
2022-06-12 10:12:29,061   loss = 0.10053094541384305
2022-06-12 10:13:21,588 ***** Running evaluation *****
2022-06-12 10:13:21,589   Epoch = 5 iter 64999 step
2022-06-12 10:13:21,589   Num examples = 9815
2022-06-12 10:13:21,589   Batch size = 32
2022-06-12 10:13:21,591 ***** Eval results *****
2022-06-12 10:13:21,591   att_loss = 11.474123865791261
2022-06-12 10:13:21,591   global_step = 64999
2022-06-12 10:13:21,591   loss = 12.366849983824595
2022-06-12 10:13:21,591   rep_loss = 0.89272611661028
2022-06-12 10:13:21,591 ***** Save model *****
2022-06-12 10:14:32,079 ***** Running evaluation *****
2022-06-12 10:14:32,079   Epoch = 1 iter 17499 step
2022-06-12 10:14:32,079   Num examples = 40430
2022-06-12 10:14:32,079   Batch size = 32
2022-06-12 10:15:07,150 ***** Eval results *****
2022-06-12 10:15:07,151   acc = 0.8969577046747464
2022-06-12 10:15:07,151   acc_and_f1 = 0.8799455891933317
2022-06-12 10:15:07,151   cls_loss = 0.10061275272185387
2022-06-12 10:15:07,151   eval_loss = 0.26084382047204746
2022-06-12 10:15:07,151   f1 = 0.8629334737119168
2022-06-12 10:15:07,151   global_step = 17499
2022-06-12 10:15:07,151   loss = 0.10061275272185387
2022-06-12 10:15:31,799 ***** Running evaluation *****
2022-06-12 10:15:31,800   Epoch = 5 iter 65499 step
2022-06-12 10:15:31,800   Num examples = 9815
2022-06-12 10:15:31,800   Batch size = 32
2022-06-12 10:15:31,801 ***** Eval results *****
2022-06-12 10:15:31,801   att_loss = 11.472556636485354
2022-06-12 10:15:31,801   global_step = 65499
2022-06-12 10:15:31,801   loss = 12.365076508070972
2022-06-12 10:15:31,801   rep_loss = 0.8925198699171478
2022-06-12 10:15:31,802 ***** Save model *****
2022-06-12 10:17:10,054 ***** Running evaluation *****
2022-06-12 10:17:10,055   Epoch = 1 iter 17999 step
2022-06-12 10:17:10,055   Num examples = 40430
2022-06-12 10:17:10,055   Batch size = 32
2022-06-12 10:17:41,703 ***** Running evaluation *****
2022-06-12 10:17:41,704   Epoch = 5 iter 65999 step
2022-06-12 10:17:41,704   Num examples = 9815
2022-06-12 10:17:41,704   Batch size = 32
2022-06-12 10:17:41,705 ***** Eval results *****
2022-06-12 10:17:41,706   att_loss = 11.471512491696872
2022-06-12 10:17:41,706   global_step = 65999
2022-06-12 10:17:41,706   loss = 12.363940975976133
2022-06-12 10:17:41,706   rep_loss = 0.8924284848824952
2022-06-12 10:17:41,706 ***** Save model *****
2022-06-12 10:17:45,118 ***** Eval results *****
2022-06-12 10:17:45,119   acc = 0.8965619589413801
2022-06-12 10:17:45,119   acc_and_f1 = 0.8766223091485517
2022-06-12 10:17:45,119   cls_loss = 0.10062221351205665
2022-06-12 10:17:45,119   eval_loss = 0.2735575121662379
2022-06-12 10:17:45,119   f1 = 0.8566826593557231
2022-06-12 10:17:45,119   global_step = 17999
2022-06-12 10:17:45,119   loss = 0.10062221351205665
2022-06-12 10:19:48,846 ***** Running evaluation *****
2022-06-12 10:19:48,847   Epoch = 1 iter 18499 step
2022-06-12 10:19:48,847   Num examples = 40430
2022-06-12 10:19:48,847   Batch size = 32
2022-06-12 10:19:52,474 ***** Running evaluation *****
2022-06-12 10:19:52,475   Epoch = 5 iter 66499 step
2022-06-12 10:19:52,475   Num examples = 9815
2022-06-12 10:19:52,475   Batch size = 32
2022-06-12 10:19:52,477 ***** Eval results *****
2022-06-12 10:19:52,477   att_loss = 11.472095834615628
2022-06-12 10:19:52,477   global_step = 66499
2022-06-12 10:19:52,477   loss = 12.364499989601539
2022-06-12 10:19:52,477   rep_loss = 0.8924041549279753
2022-06-12 10:19:52,477 ***** Save model *****
2022-06-12 10:20:23,892 ***** Eval results *****
2022-06-12 10:20:23,892   acc = 0.8989116992332427
2022-06-12 10:20:23,893   acc_and_f1 = 0.8816128165167907
2022-06-12 10:20:23,893   cls_loss = 0.10059581016882604
2022-06-12 10:20:23,893   eval_loss = 0.2654700324475718
2022-06-12 10:20:23,893   f1 = 0.8643139338003387
2022-06-12 10:20:23,893   global_step = 18499
2022-06-12 10:20:23,893   loss = 0.10059581016882604
2022-06-12 10:22:03,103 ***** Running evaluation *****
2022-06-12 10:22:03,104   Epoch = 5 iter 66999 step
2022-06-12 10:22:03,104   Num examples = 9815
2022-06-12 10:22:03,104   Batch size = 32
2022-06-12 10:22:03,105 ***** Eval results *****
2022-06-12 10:22:03,105   att_loss = 11.471582391006908
2022-06-12 10:22:03,105   global_step = 66999
2022-06-12 10:22:03,105   loss = 12.36396381130124
2022-06-12 10:22:03,106   rep_loss = 0.8923814193544279
2022-06-12 10:22:03,106 ***** Save model *****
2022-06-12 10:22:27,195 ***** Running evaluation *****
2022-06-12 10:22:27,196   Epoch = 1 iter 18999 step
2022-06-12 10:22:27,196   Num examples = 40430
2022-06-12 10:22:27,196   Batch size = 32
2022-06-12 10:23:02,314 ***** Eval results *****
2022-06-12 10:23:02,314   acc = 0.9007420232500618
2022-06-12 10:23:02,314   acc_and_f1 = 0.8825494804436891
2022-06-12 10:23:02,314   cls_loss = 0.10052197751330195
2022-06-12 10:23:02,314   eval_loss = 0.25827085111537784
2022-06-12 10:23:02,314   f1 = 0.8643569376373162
2022-06-12 10:23:02,314   global_step = 18999
2022-06-12 10:23:02,314   loss = 0.10052197751330195
2022-06-12 10:24:13,215 ***** Running evaluation *****
2022-06-12 10:24:13,216   Epoch = 5 iter 67499 step
2022-06-12 10:24:13,216   Num examples = 9815
2022-06-12 10:24:13,216   Batch size = 32
2022-06-12 10:24:13,217 ***** Eval results *****
2022-06-12 10:24:13,218   att_loss = 11.46968788242278
2022-06-12 10:24:13,218   global_step = 67499
2022-06-12 10:24:13,218   loss = 12.361985678163668
2022-06-12 10:24:13,218   rep_loss = 0.8922977932864645
2022-06-12 10:24:13,218 ***** Save model *****
2022-06-12 10:25:05,911 ***** Running evaluation *****
2022-06-12 10:25:05,911   Epoch = 1 iter 19499 step
2022-06-12 10:25:05,911   Num examples = 40430
2022-06-12 10:25:05,912   Batch size = 32
2022-06-12 10:25:40,875 ***** Eval results *****
2022-06-12 10:25:40,875   acc = 0.9015582488251298
2022-06-12 10:25:40,875   acc_and_f1 = 0.8852366700287957
2022-06-12 10:25:40,875   cls_loss = 0.10061997481250487
2022-06-12 10:25:40,875   eval_loss = 0.24331623224152513
2022-06-12 10:25:40,875   f1 = 0.8689150912324616
2022-06-12 10:25:40,875   global_step = 19499
2022-06-12 10:25:40,875   loss = 0.10061997481250487
2022-06-12 10:26:22,789 ***** Running evaluation *****
2022-06-12 10:26:22,789   Epoch = 5 iter 67999 step
2022-06-12 10:26:22,789   Num examples = 9815
2022-06-12 10:26:22,789   Batch size = 32
2022-06-12 10:26:22,790 ***** Eval results *****
2022-06-12 10:26:22,790   att_loss = 11.467451626747792
2022-06-12 10:26:22,790   global_step = 67999
2022-06-12 10:26:22,791   loss = 12.359655115275697
2022-06-12 10:26:22,791   rep_loss = 0.8922034871732522
2022-06-12 10:26:22,791 ***** Save model *****
2022-06-12 10:27:44,283 ***** Running evaluation *****
2022-06-12 10:27:44,283   Epoch = 1 iter 19999 step
2022-06-12 10:27:44,283   Num examples = 40430
2022-06-12 10:27:44,283   Batch size = 32
2022-06-12 10:28:19,236 ***** Eval results *****
2022-06-12 10:28:19,236   acc = 0.8995795201582983
2022-06-12 10:28:19,237   acc_and_f1 = 0.8816963748651413
2022-06-12 10:28:19,237   cls_loss = 0.10054050138059809
2022-06-12 10:28:19,237   eval_loss = 0.2534530979221616
2022-06-12 10:28:19,237   f1 = 0.8638132295719843
2022-06-12 10:28:19,237   global_step = 19999
2022-06-12 10:28:19,237   loss = 0.10054050138059809
2022-06-12 10:28:32,693 ***** Running evaluation *****
2022-06-12 10:28:32,693   Epoch = 5 iter 68499 step
2022-06-12 10:28:32,693   Num examples = 9815
2022-06-12 10:28:32,693   Batch size = 32
2022-06-12 10:28:32,695 ***** Eval results *****
2022-06-12 10:28:32,695   att_loss = 11.463304563906265
2022-06-12 10:28:32,695   global_step = 68499
2022-06-12 10:28:32,695   loss = 12.355229547816721
2022-06-12 10:28:32,695   rep_loss = 0.8919249824587201
2022-06-12 10:28:32,695 ***** Save model *****
2022-06-12 10:30:22,652 ***** Running evaluation *****
2022-06-12 10:30:22,652   Epoch = 1 iter 20499 step
2022-06-12 10:30:22,652   Num examples = 40430
2022-06-12 10:30:22,652   Batch size = 32
2022-06-12 10:30:42,094 ***** Running evaluation *****
2022-06-12 10:30:42,095   Epoch = 5 iter 68999 step
2022-06-12 10:30:42,095   Num examples = 9815
2022-06-12 10:30:42,095   Batch size = 32
2022-06-12 10:30:42,096 ***** Eval results *****
2022-06-12 10:30:42,096   att_loss = 11.46888456657755
2022-06-12 10:30:42,096   global_step = 68999
2022-06-12 10:30:42,096   loss = 12.360815956479026
2022-06-12 10:30:42,096   rep_loss = 0.8919313885992824
2022-06-12 10:30:42,097 ***** Save model *****
2022-06-12 10:30:57,657 ***** Eval results *****
2022-06-12 10:30:57,657   acc = 0.8994805837249568
2022-06-12 10:30:57,657   acc_and_f1 = 0.8824466105801894
2022-06-12 10:30:57,657   cls_loss = 0.10058738559407764
2022-06-12 10:30:57,657   eval_loss = 0.25672901238468065
2022-06-12 10:30:57,657   f1 = 0.8654126374354218
2022-06-12 10:30:57,657   global_step = 20499
2022-06-12 10:30:57,657   loss = 0.10058738559407764
2022-06-12 10:32:51,565 ***** Running evaluation *****
2022-06-12 10:32:51,566   Epoch = 5 iter 69499 step
2022-06-12 10:32:51,566   Num examples = 9815
2022-06-12 10:32:51,566   Batch size = 32
2022-06-12 10:32:51,567 ***** Eval results *****
2022-06-12 10:32:51,567   att_loss = 11.470415508875913
2022-06-12 10:32:51,567   global_step = 69499
2022-06-12 10:32:51,568   loss = 12.362274284681309
2022-06-12 10:32:51,568   rep_loss = 0.8918587740342369
2022-06-12 10:32:51,568 ***** Save model *****
2022-06-12 10:33:01,101 ***** Running evaluation *****
2022-06-12 10:33:01,101   Epoch = 1 iter 20999 step
2022-06-12 10:33:01,102   Num examples = 40430
2022-06-12 10:33:01,102   Batch size = 32
2022-06-12 10:33:36,166 ***** Eval results *****
2022-06-12 10:33:36,167   acc = 0.8992579767499381
2022-06-12 10:33:36,167   acc_and_f1 = 0.8822599057060088
2022-06-12 10:33:36,167   cls_loss = 0.1006260759217777
2022-06-12 10:33:36,167   eval_loss = 0.25838141406509035
2022-06-12 10:33:36,167   f1 = 0.8652618346620794
2022-06-12 10:33:36,167   global_step = 20999
2022-06-12 10:33:36,167   loss = 0.1006260759217777
2022-06-12 10:35:01,285 ***** Running evaluation *****
2022-06-12 10:35:01,285   Epoch = 5 iter 69999 step
2022-06-12 10:35:01,285   Num examples = 9815
2022-06-12 10:35:01,285   Batch size = 32
2022-06-12 10:35:01,287 ***** Eval results *****
2022-06-12 10:35:01,287   att_loss = 11.474753023407516
2022-06-12 10:35:01,287   global_step = 69999
2022-06-12 10:35:01,287   loss = 12.366582699806147
2022-06-12 10:35:01,287   rep_loss = 0.8918296749298907
2022-06-12 10:35:01,287 ***** Save model *****
2022-06-12 10:35:39,508 ***** Running evaluation *****
2022-06-12 10:35:39,508   Epoch = 1 iter 21499 step
2022-06-12 10:35:39,508   Num examples = 40430
2022-06-12 10:35:39,508   Batch size = 32
2022-06-12 10:36:14,484 ***** Eval results *****
2022-06-12 10:36:14,484   acc = 0.9012367054167697
2022-06-12 10:36:14,485   acc_and_f1 = 0.8836239565471822
2022-06-12 10:36:14,485   cls_loss = 0.10062565585740499
2022-06-12 10:36:14,485   eval_loss = 0.2607087246112856
2022-06-12 10:36:14,485   f1 = 0.8660112076775947
2022-06-12 10:36:14,485   global_step = 21499
2022-06-12 10:36:14,485   loss = 0.10062565585740499
2022-06-12 10:37:10,994 ***** Running evaluation *****
2022-06-12 10:37:10,994   Epoch = 5 iter 70499 step
2022-06-12 10:37:10,994   Num examples = 9815
2022-06-12 10:37:10,994   Batch size = 32
2022-06-12 10:37:10,995 ***** Eval results *****
2022-06-12 10:37:10,995   att_loss = 11.476798387702264
2022-06-12 10:37:10,996   global_step = 70499
2022-06-12 10:37:10,996   loss = 12.368642442286902
2022-06-12 10:37:10,996   rep_loss = 0.8918440538219788
2022-06-12 10:37:10,996 ***** Save model *****
2022-06-12 10:38:17,812 ***** Running evaluation *****
2022-06-12 10:38:17,813   Epoch = 1 iter 21999 step
2022-06-12 10:38:17,813   Num examples = 40430
2022-06-12 10:38:17,813   Batch size = 32
2022-06-12 10:38:52,763 ***** Eval results *****
2022-06-12 10:38:52,763   acc = 0.9009646302250803
2022-06-12 10:38:52,764   acc_and_f1 = 0.8837712188113072
2022-06-12 10:38:52,764   cls_loss = 0.10060256195265752
2022-06-12 10:38:52,764   eval_loss = 0.2499967140964928
2022-06-12 10:38:52,764   f1 = 0.8665778073975341
2022-06-12 10:38:52,764   global_step = 21999
2022-06-12 10:38:52,764   loss = 0.10060256195265752
2022-06-12 10:39:20,659 ***** Running evaluation *****
2022-06-12 10:39:20,660   Epoch = 5 iter 70999 step
2022-06-12 10:39:20,660   Num examples = 9815
2022-06-12 10:39:20,660   Batch size = 32
2022-06-12 10:39:20,661 ***** Eval results *****
2022-06-12 10:39:20,661   att_loss = 11.480640432885059
2022-06-12 10:39:20,661   global_step = 70999
2022-06-12 10:39:20,661   loss = 12.372382299774467
2022-06-12 10:39:20,661   rep_loss = 0.8917418656594919
2022-06-12 10:39:20,661 ***** Save model *****
2022-06-12 10:40:55,665 ***** Running evaluation *****
2022-06-12 10:40:55,665   Epoch = 1 iter 22499 step
2022-06-12 10:40:55,666   Num examples = 40430
2022-06-12 10:40:55,666   Batch size = 32
2022-06-12 10:41:30,383 ***** Running evaluation *****
2022-06-12 10:41:30,383   Epoch = 5 iter 71499 step
2022-06-12 10:41:30,383   Num examples = 9815
2022-06-12 10:41:30,384   Batch size = 32
2022-06-12 10:41:30,385 ***** Eval results *****
2022-06-12 10:41:30,385   att_loss = 11.477518497440341
2022-06-12 10:41:30,385   global_step = 71499
2022-06-12 10:41:30,386   loss = 12.369081227185223
2022-06-12 10:41:30,386   rep_loss = 0.891562726965603
2022-06-12 10:41:30,386 ***** Save model *****
2022-06-12 10:41:30,769 ***** Eval results *****
2022-06-12 10:41:30,770   acc = 0.8995300519416275
2022-06-12 10:41:30,770   acc_and_f1 = 0.8840963762036125
2022-06-12 10:41:30,770   cls_loss = 0.10055616337486982
2022-06-12 10:41:30,770   eval_loss = 0.24777290622019976
2022-06-12 10:41:30,770   f1 = 0.8686627004655975
2022-06-12 10:41:30,770   global_step = 22499
2022-06-12 10:41:30,770   loss = 0.10055616337486982
2022-06-12 10:43:34,233 ***** Running evaluation *****
2022-06-12 10:43:34,234   Epoch = 2 iter 22999 step
2022-06-12 10:43:34,234   Num examples = 40430
2022-06-12 10:43:34,234   Batch size = 32
2022-06-12 10:43:40,231 ***** Running evaluation *****
2022-06-12 10:43:40,232   Epoch = 5 iter 71999 step
2022-06-12 10:43:40,232   Num examples = 9815
2022-06-12 10:43:40,232   Batch size = 32
2022-06-12 10:43:40,233 ***** Eval results *****
2022-06-12 10:43:40,234   att_loss = 11.476623891977743
2022-06-12 10:43:40,234   global_step = 71999
2022-06-12 10:43:40,234   loss = 12.368043920591512
2022-06-12 10:43:40,234   rep_loss = 0.8914200257410549
2022-06-12 10:43:40,234 ***** Save model *****
2022-06-12 10:44:09,210 ***** Eval results *****
2022-06-12 10:44:09,210   acc = 0.9007667573583972
2022-06-12 10:44:09,210   acc_and_f1 = 0.8825718716817001
2022-06-12 10:44:09,210   cls_loss = 0.09082564419588528
2022-06-12 10:44:09,210   eval_loss = 0.26114491776185983
2022-06-12 10:44:09,210   f1 = 0.864376986005003
2022-06-12 10:44:09,210   global_step = 22999
2022-06-12 10:44:09,210   loss = 0.09082564419588528
2022-06-12 10:45:50,522 ***** Running evaluation *****
2022-06-12 10:45:50,523   Epoch = 5 iter 72499 step
2022-06-12 10:45:50,523   Num examples = 9815
2022-06-12 10:45:50,523   Batch size = 32
2022-06-12 10:45:50,525 ***** Eval results *****
2022-06-12 10:45:50,525   att_loss = 11.478215760279284
2022-06-12 10:45:50,525   global_step = 72499
2022-06-12 10:45:50,525   loss = 12.369590512728914
2022-06-12 10:45:50,525   rep_loss = 0.8913747489409574
2022-06-12 10:45:50,526 ***** Save model *****
2022-06-12 10:46:12,765 ***** Running evaluation *****
2022-06-12 10:46:12,765   Epoch = 2 iter 23499 step
2022-06-12 10:46:12,765   Num examples = 40430
2022-06-12 10:46:12,765   Batch size = 32
2022-06-12 10:46:47,744 ***** Eval results *****
2022-06-12 10:46:47,745   acc = 0.9004204798417017
2022-06-12 10:46:47,745   acc_and_f1 = 0.8818803010206472
2022-06-12 10:46:47,745   cls_loss = 0.09253050057568411
2022-06-12 10:46:47,745   eval_loss = 0.25805061024649045
2022-06-12 10:46:47,745   f1 = 0.8633401221995927
2022-06-12 10:46:47,745   global_step = 23499
2022-06-12 10:46:47,745   loss = 0.09253050057568411
2022-06-12 10:48:00,111 ***** Running evaluation *****
2022-06-12 10:48:00,111   Epoch = 5 iter 72999 step
2022-06-12 10:48:00,111   Num examples = 9815
2022-06-12 10:48:00,111   Batch size = 32
2022-06-12 10:48:00,113 ***** Eval results *****
2022-06-12 10:48:00,113   att_loss = 11.480574730498322
2022-06-12 10:48:00,113   global_step = 72999
2022-06-12 10:48:00,113   loss = 12.371852452711567
2022-06-12 10:48:00,113   rep_loss = 0.891277718998566
2022-06-12 10:48:00,113 ***** Save model *****
2022-06-12 10:48:51,166 ***** Running evaluation *****
2022-06-12 10:48:51,166   Epoch = 2 iter 23999 step
2022-06-12 10:48:51,166   Num examples = 40430
2022-06-12 10:48:51,166   Batch size = 32
2022-06-12 10:49:26,182 ***** Eval results *****
2022-06-12 10:49:26,183   acc = 0.8998021271333169
2022-06-12 10:49:26,183   acc_and_f1 = 0.8826422224575765
2022-06-12 10:49:26,183   cls_loss = 0.09317140069932953
2022-06-12 10:49:26,183   eval_loss = 0.2602195078595482
2022-06-12 10:49:26,183   f1 = 0.8654823177818363
2022-06-12 10:49:26,183   global_step = 23999
2022-06-12 10:49:26,183   loss = 0.09317140069932953
2022-06-12 10:50:09,975 ***** Running evaluation *****
2022-06-12 10:50:09,975   Epoch = 5 iter 73499 step
2022-06-12 10:50:09,975   Num examples = 9815
2022-06-12 10:50:09,976   Batch size = 32
2022-06-12 10:50:09,977 ***** Eval results *****
2022-06-12 10:50:09,977   att_loss = 11.484640130687295
2022-06-12 10:50:09,977   global_step = 73499
2022-06-12 10:50:09,977   loss = 12.37590264653657
2022-06-12 10:50:09,977   rep_loss = 0.8912625122565054
2022-06-12 10:50:09,977 ***** Save model *****
2022-06-12 10:50:45,064 Task finish! 
2022-06-12 10:50:45,066 Task cost 323.22093505 minutes, i.e. 5.387015588055556 hours. 
2022-06-12 10:50:47,621 Task start! 
2022-06-12 10:50:47,645 device: cuda n_gpu: 1
2022-06-12 10:50:47,646 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=500, gpu_id=3, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=128, max_seq_length=128, no_cuda=False, num_train_epochs=6, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/mnli/on_original_data', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/mnli/on_original_data', task_name='mnli', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mnli/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/mnli/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 10:50:53,780 Writing example 0 of 392702
2022-06-12 10:50:53,781 *** Example ***
2022-06-12 10:50:53,782 guid: train-0
2022-06-12 10:50:53,782 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2022-06-12 10:50:53,782 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 10:50:53,782 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 10:50:53,782 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 10:50:53,782 label: neutral
2022-06-12 10:50:53,782 label_id: 2
2022-06-12 10:50:59,520 Writing example 10000 of 392702
2022-06-12 10:51:05,186 Writing example 20000 of 392702
2022-06-12 10:51:10,849 Writing example 30000 of 392702
2022-06-12 10:51:16,791 Writing example 40000 of 392702
2022-06-12 10:51:22,532 Writing example 50000 of 392702
2022-06-12 10:51:28,210 Writing example 60000 of 392702
2022-06-12 10:51:29,803 ***** Running evaluation *****
2022-06-12 10:51:29,803   Epoch = 2 iter 24499 step
2022-06-12 10:51:29,803   Num examples = 40430
2022-06-12 10:51:29,803   Batch size = 32
2022-06-12 10:51:33,930 Writing example 70000 of 392702
2022-06-12 10:51:40,029 Writing example 80000 of 392702
2022-06-12 10:51:45,712 Writing example 90000 of 392702
2022-06-12 10:51:51,361 Writing example 100000 of 392702
2022-06-12 10:51:57,033 Writing example 110000 of 392702
2022-06-12 10:52:02,771 Writing example 120000 of 392702
2022-06-12 10:52:04,796 ***** Eval results *****
2022-06-12 10:52:04,797   acc = 0.9002720751916894
2022-06-12 10:52:04,797   acc_and_f1 = 0.8829808077490757
2022-06-12 10:52:04,797   cls_loss = 0.09338358844111018
2022-06-12 10:52:04,797   eval_loss = 0.25577930891237965
2022-06-12 10:52:04,797   f1 = 0.8656895403064622
2022-06-12 10:52:04,797   global_step = 24499
2022-06-12 10:52:04,797   loss = 0.09338358844111018
2022-06-12 10:52:09,056 Writing example 130000 of 392702
2022-06-12 10:52:14,804 Writing example 140000 of 392702
2022-06-12 10:52:20,553 Writing example 150000 of 392702
2022-06-12 10:52:26,342 Writing example 160000 of 392702
2022-06-12 10:52:32,104 Writing example 170000 of 392702
2022-06-12 10:52:37,861 Writing example 180000 of 392702
2022-06-12 10:52:44,360 Writing example 190000 of 392702
2022-06-12 10:52:50,094 Writing example 200000 of 392702
2022-06-12 10:52:55,818 Writing example 210000 of 392702
2022-06-12 10:53:01,527 Writing example 220000 of 392702
2022-06-12 10:53:07,281 Writing example 230000 of 392702
2022-06-12 10:53:12,927 Writing example 240000 of 392702
2022-06-12 10:53:18,657 Writing example 250000 of 392702
2022-06-12 10:53:25,418 Writing example 260000 of 392702
2022-06-12 10:53:31,208 Writing example 270000 of 392702
2022-06-12 10:53:36,842 Writing example 280000 of 392702
2022-06-12 10:53:42,553 Writing example 290000 of 392702
2022-06-12 10:53:48,236 Writing example 300000 of 392702
2022-06-12 10:53:53,973 Writing example 310000 of 392702
2022-06-12 10:53:59,705 Writing example 320000 of 392702
2022-06-12 10:54:05,425 Writing example 330000 of 392702
2022-06-12 10:54:07,726 ***** Running evaluation *****
2022-06-12 10:54:07,726   Epoch = 2 iter 24999 step
2022-06-12 10:54:07,726   Num examples = 40430
2022-06-12 10:54:07,726   Batch size = 32
2022-06-12 10:54:11,209 Writing example 340000 of 392702
2022-06-12 10:54:18,210 Writing example 350000 of 392702
2022-06-12 10:54:23,937 Writing example 360000 of 392702
2022-06-12 10:54:29,666 Writing example 370000 of 392702
2022-06-12 10:54:35,429 Writing example 380000 of 392702
2022-06-12 10:54:41,161 Writing example 390000 of 392702
2022-06-12 10:54:42,737 ***** Eval results *****
2022-06-12 10:54:42,737   acc = 0.8991590403165965
2022-06-12 10:54:42,737   acc_and_f1 = 0.8828943161934973
2022-06-12 10:54:42,738   cls_loss = 0.0940598919789788
2022-06-12 10:54:42,738   eval_loss = 0.24880557747180515
2022-06-12 10:54:42,738   f1 = 0.8666295920703981
2022-06-12 10:54:42,738   global_step = 24999
2022-06-12 10:54:42,738   loss = 0.0940598919789788
2022-06-12 10:54:45,841 Writing example 0 of 9815
2022-06-12 10:54:45,841 *** Example ***
2022-06-12 10:54:45,841 guid: dev_matched-0
2022-06-12 10:54:45,841 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2022-06-12 10:54:45,842 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 10:54:45,842 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 10:54:45,842 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 10:54:45,842 label: neutral
2022-06-12 10:54:45,842 label_id: 2
2022-06-12 10:54:51,410 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 10:54:56,624 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/mnli/on_original_data/pytorch_model.bin
2022-06-12 10:54:57,192 loading model...
2022-06-12 10:54:57,445 done!
2022-06-12 10:54:57,446 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.dense_fit.weight', 'bert.dense_fit.bias']
2022-06-12 10:55:00,667 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 10:55:01,747 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/mnli/on_original_data/pytorch_model.bin
2022-06-12 10:55:01,922 loading model...
2022-06-12 10:55:01,953 done!
2022-06-12 10:55:03,339 ***** Running training *****
2022-06-12 10:55:03,350   Num examples = 392702
2022-06-12 10:55:03,360   Batch size = 32
2022-06-12 10:55:03,376   Num steps = 73626
2022-06-12 10:55:03,392 n: bert.embeddings.word_embeddings.weight
2022-06-12 10:55:03,402 n: bert.embeddings.position_embeddings.weight
2022-06-12 10:55:03,407 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 10:55:03,407 n: bert.embeddings.LayerNorm.weight
2022-06-12 10:55:03,407 n: bert.embeddings.LayerNorm.bias
2022-06-12 10:55:03,407 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 10:55:03,408 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 10:55:03,408 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 10:55:03,408 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 10:55:03,408 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 10:55:03,408 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 10:55:03,408 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 10:55:03,409 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 10:55:03,410 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 10:55:03,411 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 10:55:03,411 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 10:55:03,411 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 10:55:03,411 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 10:55:03,411 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 10:55:03,411 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 10:55:03,411 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 10:55:03,411 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 10:55:03,411 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 10:55:03,411 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 10:55:03,412 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 10:55:03,413 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 10:55:03,413 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 10:55:03,413 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 10:55:03,413 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 10:55:03,413 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 10:55:03,413 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 10:55:03,413 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 10:55:03,413 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 10:55:03,413 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 10:55:03,414 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 10:55:03,414 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 10:55:03,414 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 10:55:03,414 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 10:55:03,414 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 10:55:03,415 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 10:55:03,415 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 10:55:03,415 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 10:55:03,415 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 10:55:03,416 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 10:55:03,416 n: bert.pooler.dense.weight
2022-06-12 10:55:03,416 n: bert.pooler.dense.bias
2022-06-12 10:55:03,416 n: classifier.weight
2022-06-12 10:55:03,416 n: classifier.bias
2022-06-12 10:55:03,416 n: fit_denses.0.weight
2022-06-12 10:55:03,416 n: fit_denses.0.bias
2022-06-12 10:55:03,416 n: fit_denses.1.weight
2022-06-12 10:55:03,417 n: fit_denses.1.bias
2022-06-12 10:55:03,417 n: fit_denses.2.weight
2022-06-12 10:55:03,417 n: fit_denses.2.bias
2022-06-12 10:55:03,417 n: fit_denses.3.weight
2022-06-12 10:55:03,417 n: fit_denses.3.bias
2022-06-12 10:55:03,417 n: fit_denses.4.weight
2022-06-12 10:55:03,417 n: fit_denses.4.bias
2022-06-12 10:55:03,417 n: fit_denses.5.weight
2022-06-12 10:55:03,417 n: fit_denses.5.bias
2022-06-12 10:55:03,417 n: fit_denses.6.weight
2022-06-12 10:55:03,417 n: fit_denses.6.bias
2022-06-12 10:55:03,417 Total parameters: 72469507
2022-06-12 10:56:45,857 ***** Running evaluation *****
2022-06-12 10:56:45,858   Epoch = 2 iter 25499 step
2022-06-12 10:56:45,858   Num examples = 40430
2022-06-12 10:56:45,858   Batch size = 32
2022-06-12 10:57:01,522 ***** Running evaluation *****
2022-06-12 10:57:01,522   Epoch = 0 iter 499 step
2022-06-12 10:57:01,522   Num examples = 9815
2022-06-12 10:57:01,522   Batch size = 32
2022-06-12 10:57:09,636 ***** Eval results *****
2022-06-12 10:57:09,636   acc = 0.810697911360163
2022-06-12 10:57:09,636   cls_loss = 0.29711000666111886
2022-06-12 10:57:09,636   eval_loss = 0.5769679003701537
2022-06-12 10:57:09,636   global_step = 499
2022-06-12 10:57:09,636   loss = 0.29711000666111886
2022-06-12 10:57:09,637 ***** Save model *****
2022-06-12 10:57:20,835 ***** Eval results *****
2022-06-12 10:57:20,835   acc = 0.8996537224833044
2022-06-12 10:57:20,835   acc_and_f1 = 0.8821492847561485
2022-06-12 10:57:20,835   cls_loss = 0.09441462512107464
2022-06-12 10:57:20,835   eval_loss = 0.25415170626255057
2022-06-12 10:57:20,835   f1 = 0.8646448470289927
2022-06-12 10:57:20,835   global_step = 25499
2022-06-12 10:57:20,835   loss = 0.09441462512107464
2022-06-12 10:59:08,673 ***** Running evaluation *****
2022-06-12 10:59:08,673   Epoch = 0 iter 999 step
2022-06-12 10:59:08,673   Num examples = 9815
2022-06-12 10:59:08,673   Batch size = 32
2022-06-12 10:59:16,783 ***** Eval results *****
2022-06-12 10:59:16,784   acc = 0.8168110035659705
2022-06-12 10:59:16,784   cls_loss = 0.26543693894798215
2022-06-12 10:59:16,784   eval_loss = 0.5137384447484529
2022-06-12 10:59:16,784   global_step = 999
2022-06-12 10:59:16,784   loss = 0.26543693894798215
2022-06-12 10:59:16,784 ***** Save model *****
2022-06-12 10:59:23,862 ***** Running evaluation *****
2022-06-12 10:59:23,862   Epoch = 2 iter 25999 step
2022-06-12 10:59:23,862   Num examples = 40430
2022-06-12 10:59:23,862   Batch size = 32
2022-06-12 10:59:58,822 ***** Eval results *****
2022-06-12 10:59:58,822   acc = 0.8995300519416275
2022-06-12 10:59:58,822   acc_and_f1 = 0.8815007076056618
2022-06-12 10:59:58,822   cls_loss = 0.09455212795133604
2022-06-12 10:59:58,822   eval_loss = 0.26371533310046724
2022-06-12 10:59:58,822   f1 = 0.8634713632696962
2022-06-12 10:59:58,823   global_step = 25999
2022-06-12 10:59:58,823   loss = 0.09455212795133604
2022-06-12 11:01:16,065 ***** Running evaluation *****
2022-06-12 11:01:16,067   Epoch = 0 iter 1499 step
2022-06-12 11:01:16,067   Num examples = 9815
2022-06-12 11:01:16,067   Batch size = 32
2022-06-12 11:01:24,176 ***** Eval results *****
2022-06-12 11:01:24,176   acc = 0.813143148242486
2022-06-12 11:01:24,176   cls_loss = 0.253673766188577
2022-06-12 11:01:24,176   eval_loss = 0.5139915876163333
2022-06-12 11:01:24,177   global_step = 1499
2022-06-12 11:01:24,177   loss = 0.253673766188577
2022-06-12 11:02:01,737 ***** Running evaluation *****
2022-06-12 11:02:01,738   Epoch = 2 iter 26499 step
2022-06-12 11:02:01,738   Num examples = 40430
2022-06-12 11:02:01,738   Batch size = 32
2022-06-12 11:02:36,756 ***** Eval results *****
2022-06-12 11:02:36,757   acc = 0.9013603759584466
2022-06-12 11:02:36,757   acc_and_f1 = 0.8851957873224088
2022-06-12 11:02:36,757   cls_loss = 0.09450131317292892
2022-06-12 11:02:36,757   eval_loss = 0.25720327071067467
2022-06-12 11:02:36,757   f1 = 0.869031198686371
2022-06-12 11:02:36,757   global_step = 26499
2022-06-12 11:02:36,757   loss = 0.09450131317292892
2022-06-12 11:03:23,367 ***** Running evaluation *****
2022-06-12 11:03:23,368   Epoch = 0 iter 1999 step
2022-06-12 11:03:23,368   Num examples = 9815
2022-06-12 11:03:23,368   Batch size = 32
2022-06-12 11:03:31,481 ***** Eval results *****
2022-06-12 11:03:31,481   acc = 0.8147733061640346
2022-06-12 11:03:31,482   cls_loss = 0.2476018655219634
2022-06-12 11:03:31,482   eval_loss = 0.5101236224368652
2022-06-12 11:03:31,482   global_step = 1999
2022-06-12 11:03:31,482   loss = 0.2476018655219634
2022-06-12 11:04:40,059 ***** Running evaluation *****
2022-06-12 11:04:40,060   Epoch = 2 iter 26999 step
2022-06-12 11:04:40,060   Num examples = 40430
2022-06-12 11:04:40,060   Batch size = 32
2022-06-12 11:05:15,066 ***** Eval results *****
2022-06-12 11:05:15,066   acc = 0.9006430868167202
2022-06-12 11:05:15,066   acc_and_f1 = 0.8830959183456027
2022-06-12 11:05:15,066   cls_loss = 0.09454497223425623
2022-06-12 11:05:15,066   eval_loss = 0.2620983758139648
2022-06-12 11:05:15,066   f1 = 0.8655487498744853
2022-06-12 11:05:15,067   global_step = 26999
2022-06-12 11:05:15,067   loss = 0.09454497223425623
2022-06-12 11:05:30,537 ***** Running evaluation *****
2022-06-12 11:05:30,538   Epoch = 0 iter 2499 step
2022-06-12 11:05:30,538   Num examples = 9815
2022-06-12 11:05:30,538   Batch size = 32
2022-06-12 11:05:38,649 ***** Eval results *****
2022-06-12 11:05:38,649   acc = 0.8142638818135507
2022-06-12 11:05:38,649   cls_loss = 0.2439328668092718
2022-06-12 11:05:38,649   eval_loss = 0.5106019567977333
2022-06-12 11:05:38,649   global_step = 2499
2022-06-12 11:05:38,649   loss = 0.2439328668092718
2022-06-12 11:07:18,053 ***** Running evaluation *****
2022-06-12 11:07:18,053   Epoch = 2 iter 27499 step
2022-06-12 11:07:18,053   Num examples = 40430
2022-06-12 11:07:18,054   Batch size = 32
2022-06-12 11:07:37,664 ***** Running evaluation *****
2022-06-12 11:07:37,665   Epoch = 0 iter 2999 step
2022-06-12 11:07:37,665   Num examples = 9815
2022-06-12 11:07:37,665   Batch size = 32
2022-06-12 11:07:45,773 ***** Eval results *****
2022-06-12 11:07:45,773   acc = 0.8147733061640346
2022-06-12 11:07:45,774   cls_loss = 0.24163337385765749
2022-06-12 11:07:45,774   eval_loss = 0.4969505401698308
2022-06-12 11:07:45,774   global_step = 2999
2022-06-12 11:07:45,774   loss = 0.24163337385765749
2022-06-12 11:07:52,978 ***** Eval results *****
2022-06-12 11:07:52,978   acc = 0.9021023992085085
2022-06-12 11:07:52,978   acc_and_f1 = 0.8836281960610545
2022-06-12 11:07:52,978   cls_loss = 0.09456912006922168
2022-06-12 11:07:52,978   eval_loss = 0.2596801341587937
2022-06-12 11:07:52,979   f1 = 0.8651539929136004
2022-06-12 11:07:52,979   global_step = 27499
2022-06-12 11:07:52,979   loss = 0.09456912006922168
2022-06-12 11:09:44,742 ***** Running evaluation *****
2022-06-12 11:09:44,742   Epoch = 0 iter 3499 step
2022-06-12 11:09:44,742   Num examples = 9815
2022-06-12 11:09:44,742   Batch size = 32
2022-06-12 11:09:52,856 ***** Eval results *****
2022-06-12 11:09:52,856   acc = 0.8124299541518084
2022-06-12 11:09:52,856   cls_loss = 0.23991905333366215
2022-06-12 11:09:52,856   eval_loss = 0.4996924777955108
2022-06-12 11:09:52,856   global_step = 3499
2022-06-12 11:09:52,856   loss = 0.23991905333366215
2022-06-12 11:09:57,007 ***** Running evaluation *****
2022-06-12 11:09:57,007   Epoch = 2 iter 27999 step
2022-06-12 11:09:57,008   Num examples = 40430
2022-06-12 11:09:57,008   Batch size = 32
2022-06-12 11:10:31,983 ***** Eval results *****
2022-06-12 11:10:31,983   acc = 0.900371011625031
2022-06-12 11:10:31,983   acc_and_f1 = 0.8809758838193883
2022-06-12 11:10:31,983   cls_loss = 0.09475398177281376
2022-06-12 11:10:31,983   eval_loss = 0.2501537837913331
2022-06-12 11:10:31,983   f1 = 0.8615807560137456
2022-06-12 11:10:31,983   global_step = 27999
2022-06-12 11:10:31,983   loss = 0.09475398177281376
2022-06-12 11:11:51,188 ***** Running evaluation *****
2022-06-12 11:11:51,188   Epoch = 0 iter 3999 step
2022-06-12 11:11:51,189   Num examples = 9815
2022-06-12 11:11:51,189   Batch size = 32
2022-06-12 11:11:59,298 ***** Eval results *****
2022-06-12 11:11:59,298   acc = 0.8157921548650026
2022-06-12 11:11:59,298   cls_loss = 0.23851346759072958
2022-06-12 11:11:59,298   eval_loss = 0.5017109969539829
2022-06-12 11:11:59,299   global_step = 3999
2022-06-12 11:11:59,299   loss = 0.23851346759072958
2022-06-12 11:12:35,605 ***** Running evaluation *****
2022-06-12 11:12:35,605   Epoch = 2 iter 28499 step
2022-06-12 11:12:35,605   Num examples = 40430
2022-06-12 11:12:35,605   Batch size = 32
2022-06-12 11:13:10,586 ***** Eval results *****
2022-06-12 11:13:10,587   acc = 0.9002226069750185
2022-06-12 11:13:10,587   acc_and_f1 = 0.8829585534542159
2022-06-12 11:13:10,587   cls_loss = 0.09499009448357662
2022-06-12 11:13:10,587   eval_loss = 0.25210366604991163
2022-06-12 11:13:10,587   f1 = 0.8656944999334133
2022-06-12 11:13:10,587   global_step = 28499
2022-06-12 11:13:10,587   loss = 0.09499009448357662
2022-06-12 11:13:57,782 ***** Running evaluation *****
2022-06-12 11:13:57,782   Epoch = 0 iter 4499 step
2022-06-12 11:13:57,782   Num examples = 9815
2022-06-12 11:13:57,782   Batch size = 32
2022-06-12 11:14:05,888 ***** Eval results *****
2022-06-12 11:14:05,888   acc = 0.8146714212939379
2022-06-12 11:14:05,889   cls_loss = 0.23737606964963998
2022-06-12 11:14:05,889   eval_loss = 0.49572282874234724
2022-06-12 11:14:05,889   global_step = 4499
2022-06-12 11:14:05,889   loss = 0.23737606964963998
2022-06-12 11:15:13,612 ***** Running evaluation *****
2022-06-12 11:15:13,612   Epoch = 2 iter 28999 step
2022-06-12 11:15:13,612   Num examples = 40430
2022-06-12 11:15:13,613   Batch size = 32
2022-06-12 11:15:48,655 ***** Eval results *****
2022-06-12 11:15:48,655   acc = 0.8998515953499876
2022-06-12 11:15:48,655   acc_and_f1 = 0.8817309649199159
2022-06-12 11:15:48,655   cls_loss = 0.09512239327661087
2022-06-12 11:15:48,655   eval_loss = 0.24935680007727085
2022-06-12 11:15:48,655   f1 = 0.863610334489844
2022-06-12 11:15:48,655   global_step = 28999
2022-06-12 11:15:48,655   loss = 0.09512239327661087
2022-06-12 11:16:04,647 ***** Running evaluation *****
2022-06-12 11:16:04,648   Epoch = 0 iter 4999 step
2022-06-12 11:16:04,648   Num examples = 9815
2022-06-12 11:16:04,648   Batch size = 32
2022-06-12 11:16:12,771 ***** Eval results *****
2022-06-12 11:16:12,772   acc = 0.8120224146714213
2022-06-12 11:16:12,772   cls_loss = 0.23656633753231893
2022-06-12 11:16:12,772   eval_loss = 0.5085150291748854
2022-06-12 11:16:12,772   global_step = 4999
2022-06-12 11:16:12,772   loss = 0.23656633753231893
2022-06-12 11:17:51,882 ***** Running evaluation *****
2022-06-12 11:17:51,883   Epoch = 2 iter 29499 step
2022-06-12 11:17:51,883   Num examples = 40430
2022-06-12 11:17:51,883   Batch size = 32
2022-06-12 11:18:11,383 ***** Running evaluation *****
2022-06-12 11:18:11,384   Epoch = 0 iter 5499 step
2022-06-12 11:18:11,384   Num examples = 9815
2022-06-12 11:18:11,384   Batch size = 32
2022-06-12 11:18:19,505 ***** Eval results *****
2022-06-12 11:18:19,506   acc = 0.8146714212939379
2022-06-12 11:18:19,506   cls_loss = 0.23601729051159348
2022-06-12 11:18:19,506   eval_loss = 0.5106935593320803
2022-06-12 11:18:19,506   global_step = 5499
2022-06-12 11:18:19,506   loss = 0.23601729051159348
2022-06-12 11:18:26,886 ***** Eval results *****
2022-06-12 11:18:26,887   acc = 0.8996784565916399
2022-06-12 11:18:26,887   acc_and_f1 = 0.8833517848494886
2022-06-12 11:18:26,887   cls_loss = 0.0950883854809448
2022-06-12 11:18:26,887   eval_loss = 0.2514339240733534
2022-06-12 11:18:26,887   f1 = 0.8670251131073373
2022-06-12 11:18:26,887   global_step = 29499
2022-06-12 11:18:26,887   loss = 0.0950883854809448
2022-06-12 11:20:18,230 ***** Running evaluation *****
2022-06-12 11:20:18,230   Epoch = 0 iter 5999 step
2022-06-12 11:20:18,230   Num examples = 9815
2022-06-12 11:20:18,230   Batch size = 32
2022-06-12 11:20:26,342 ***** Eval results *****
2022-06-12 11:20:26,342   acc = 0.8118186449312277
2022-06-12 11:20:26,343   cls_loss = 0.23552850714960302
2022-06-12 11:20:26,343   eval_loss = 0.5015001859066929
2022-06-12 11:20:26,343   global_step = 5999
2022-06-12 11:20:26,343   loss = 0.23552850714960302
2022-06-12 11:20:30,466 ***** Running evaluation *****
2022-06-12 11:20:30,466   Epoch = 2 iter 29999 step
2022-06-12 11:20:30,466   Num examples = 40430
2022-06-12 11:20:30,466   Batch size = 32
2022-06-12 11:21:05,481 ***** Eval results *****
2022-06-12 11:21:05,482   acc = 0.9001731387583478
2022-06-12 11:21:05,482   acc_and_f1 = 0.8818417266331475
2022-06-12 11:21:05,482   cls_loss = 0.0951321935074596
2022-06-12 11:21:05,482   eval_loss = 0.2536323055819479
2022-06-12 11:21:05,482   f1 = 0.8635103145079474
2022-06-12 11:21:05,482   global_step = 29999
2022-06-12 11:21:05,482   loss = 0.0951321935074596
2022-06-12 11:22:25,503 ***** Running evaluation *****
2022-06-12 11:22:25,504   Epoch = 0 iter 6499 step
2022-06-12 11:22:25,504   Num examples = 9815
2022-06-12 11:22:25,504   Batch size = 32
2022-06-12 11:22:33,613 ***** Eval results *****
2022-06-12 11:22:33,613   acc = 0.8105960264900662
2022-06-12 11:22:33,613   cls_loss = 0.23512556371431678
2022-06-12 11:22:33,613   eval_loss = 0.5053368362231052
2022-06-12 11:22:33,613   global_step = 6499
2022-06-12 11:22:33,614   loss = 0.23512556371431678
2022-06-12 11:23:08,928 ***** Running evaluation *****
2022-06-12 11:23:08,929   Epoch = 2 iter 30499 step
2022-06-12 11:23:08,929   Num examples = 40430
2022-06-12 11:23:08,929   Batch size = 32
2022-06-12 11:23:43,942 ***** Eval results *****
2022-06-12 11:23:43,942   acc = 0.9018550581251545
2022-06-12 11:23:43,942   acc_and_f1 = 0.8849305894045373
2022-06-12 11:23:43,942   cls_loss = 0.09501895555470896
2022-06-12 11:23:43,942   eval_loss = 0.2564314188689135
2022-06-12 11:23:43,942   f1 = 0.86800612068392
2022-06-12 11:23:43,943   global_step = 30499
2022-06-12 11:23:43,943   loss = 0.09501895555470896
2022-06-12 11:24:32,578 ***** Running evaluation *****
2022-06-12 11:24:32,579   Epoch = 0 iter 6999 step
2022-06-12 11:24:32,579   Num examples = 9815
2022-06-12 11:24:32,579   Batch size = 32
2022-06-12 11:24:40,688 ***** Eval results *****
2022-06-12 11:24:40,688   acc = 0.8171166581762608
2022-06-12 11:24:40,688   cls_loss = 0.23474655094606942
2022-06-12 11:24:40,688   eval_loss = 0.5038508270967279
2022-06-12 11:24:40,689   global_step = 6999
2022-06-12 11:24:40,689   loss = 0.23474655094606942
2022-06-12 11:24:40,689 ***** Save model *****
2022-06-12 11:25:46,793 ***** Running evaluation *****
2022-06-12 11:25:46,793   Epoch = 2 iter 30999 step
2022-06-12 11:25:46,794   Num examples = 40430
2022-06-12 11:25:46,794   Batch size = 32
2022-06-12 11:26:21,717 ***** Eval results *****
2022-06-12 11:26:21,717   acc = 0.9025228790502102
2022-06-12 11:26:21,717   acc_and_f1 = 0.8846792465871088
2022-06-12 11:26:21,718   cls_loss = 0.09496326764222568
2022-06-12 11:26:21,718   eval_loss = 0.24849963627602387
2022-06-12 11:26:21,718   f1 = 0.8668356141240074
2022-06-12 11:26:21,718   global_step = 30999
2022-06-12 11:26:21,718   loss = 0.09496326764222568
2022-06-12 11:26:39,852 ***** Running evaluation *****
2022-06-12 11:26:39,853   Epoch = 0 iter 7499 step
2022-06-12 11:26:39,853   Num examples = 9815
2022-06-12 11:26:39,853   Batch size = 32
2022-06-12 11:26:47,962 ***** Eval results *****
2022-06-12 11:26:47,963   acc = 0.8109016811003565
2022-06-12 11:26:47,963   cls_loss = 0.2345272063879128
2022-06-12 11:26:47,963   eval_loss = 0.5103583953279626
2022-06-12 11:26:47,963   global_step = 7499
2022-06-12 11:26:47,963   loss = 0.2345272063879128
2022-06-12 11:28:25,049 ***** Running evaluation *****
2022-06-12 11:28:25,050   Epoch = 2 iter 31499 step
2022-06-12 11:28:25,050   Num examples = 40430
2022-06-12 11:28:25,050   Batch size = 32
2022-06-12 11:28:47,051 ***** Running evaluation *****
2022-06-12 11:28:47,052   Epoch = 0 iter 7999 step
2022-06-12 11:28:47,052   Num examples = 9815
2022-06-12 11:28:47,052   Batch size = 32
2022-06-12 11:28:55,161 ***** Eval results *****
2022-06-12 11:28:55,161   acc = 0.8133469179826796
2022-06-12 11:28:55,161   cls_loss = 0.2342548348602913
2022-06-12 11:28:55,161   eval_loss = 0.49935085251199307
2022-06-12 11:28:55,161   global_step = 7999
2022-06-12 11:28:55,161   loss = 0.2342548348602913
2022-06-12 11:28:59,990 ***** Eval results *****
2022-06-12 11:28:59,990   acc = 0.9012861736334405
2022-06-12 11:28:59,990   acc_and_f1 = 0.8853431751703967
2022-06-12 11:28:59,991   cls_loss = 0.0949259651538811
2022-06-12 11:28:59,991   eval_loss = 0.24998244219990093
2022-06-12 11:28:59,991   f1 = 0.8694001767073529
2022-06-12 11:28:59,991   global_step = 31499
2022-06-12 11:28:59,991   loss = 0.0949259651538811
2022-06-12 11:30:53,943 ***** Running evaluation *****
2022-06-12 11:30:53,943   Epoch = 0 iter 8499 step
2022-06-12 11:30:53,943   Num examples = 9815
2022-06-12 11:30:53,943   Batch size = 32
2022-06-12 11:31:02,054 ***** Eval results *****
2022-06-12 11:31:02,054   acc = 0.8097809475292919
2022-06-12 11:31:02,054   cls_loss = 0.23410555256936308
2022-06-12 11:31:02,054   eval_loss = 0.5002405950223195
2022-06-12 11:31:02,054   global_step = 8499
2022-06-12 11:31:02,054   loss = 0.23410555256936308
2022-06-12 11:31:03,072 ***** Running evaluation *****
2022-06-12 11:31:03,072   Epoch = 2 iter 31999 step
2022-06-12 11:31:03,072   Num examples = 40430
2022-06-12 11:31:03,073   Batch size = 32
2022-06-12 11:31:38,039 ***** Eval results *****
2022-06-12 11:31:38,039   acc = 0.9005441503833786
2022-06-12 11:31:38,039   acc_and_f1 = 0.8844554647225762
2022-06-12 11:31:38,039   cls_loss = 0.09493281404511024
2022-06-12 11:31:38,039   eval_loss = 0.24972159586127707
2022-06-12 11:31:38,039   f1 = 0.8683667790617736
2022-06-12 11:31:38,039   global_step = 31999
2022-06-12 11:31:38,040   loss = 0.09493281404511024
2022-06-12 11:33:00,886 ***** Running evaluation *****
2022-06-12 11:33:00,886   Epoch = 0 iter 8999 step
2022-06-12 11:33:00,886   Num examples = 9815
2022-06-12 11:33:00,886   Batch size = 32
2022-06-12 11:33:09,010 ***** Eval results *****
2022-06-12 11:33:09,010   acc = 0.8129393785022924
2022-06-12 11:33:09,010   cls_loss = 0.2338479835730895
2022-06-12 11:33:09,010   eval_loss = 0.5023598171988993
2022-06-12 11:33:09,010   global_step = 8999
2022-06-12 11:33:09,010   loss = 0.2338479835730895
2022-06-12 11:33:40,802 ***** Running evaluation *****
2022-06-12 11:33:40,802   Epoch = 2 iter 32499 step
2022-06-12 11:33:40,802   Num examples = 40430
2022-06-12 11:33:40,802   Batch size = 32
2022-06-12 11:34:15,811 ***** Eval results *****
2022-06-12 11:34:15,811   acc = 0.9011377689834281
2022-06-12 11:34:15,812   acc_and_f1 = 0.8824095447693308
2022-06-12 11:34:15,812   cls_loss = 0.09487434886132538
2022-06-12 11:34:15,812   eval_loss = 0.26691538076612015
2022-06-12 11:34:15,812   f1 = 0.8636813205552335
2022-06-12 11:34:15,812   global_step = 32499
2022-06-12 11:34:15,812   loss = 0.09487434886132538
2022-06-12 11:35:07,794 ***** Running evaluation *****
2022-06-12 11:35:07,794   Epoch = 0 iter 9499 step
2022-06-12 11:35:07,794   Num examples = 9815
2022-06-12 11:35:07,794   Batch size = 32
2022-06-12 11:35:15,920 ***** Eval results *****
2022-06-12 11:35:15,921   acc = 0.8116148751910341
2022-06-12 11:35:15,921   cls_loss = 0.23360610311314212
2022-06-12 11:35:15,921   eval_loss = 0.50891302385237
2022-06-12 11:35:15,922   global_step = 9499
2022-06-12 11:35:15,922   loss = 0.23360610311314212
2022-06-12 11:36:18,921 ***** Running evaluation *****
2022-06-12 11:36:18,922   Epoch = 2 iter 32999 step
2022-06-12 11:36:18,922   Num examples = 40430
2022-06-12 11:36:18,922   Batch size = 32
2022-06-12 11:36:53,976 ***** Eval results *****
2022-06-12 11:36:53,976   acc = 0.9019045263418254
2022-06-12 11:36:53,976   acc_and_f1 = 0.8843102413937921
2022-06-12 11:36:53,976   cls_loss = 0.09486547272566327
2022-06-12 11:36:53,976   eval_loss = 0.25524594836361425
2022-06-12 11:36:53,976   f1 = 0.8667159564457588
2022-06-12 11:36:53,976   global_step = 32999
2022-06-12 11:36:53,976   loss = 0.09486547272566327
2022-06-12 11:37:14,828 ***** Running evaluation *****
2022-06-12 11:37:14,828   Epoch = 0 iter 9999 step
2022-06-12 11:37:14,828   Num examples = 9815
2022-06-12 11:37:14,828   Batch size = 32
2022-06-12 11:37:22,940 ***** Eval results *****
2022-06-12 11:37:22,940   acc = 0.81365257259297
2022-06-12 11:37:22,940   cls_loss = 0.23347948512437044
2022-06-12 11:37:22,940   eval_loss = 0.5013693062412622
2022-06-12 11:37:22,940   global_step = 9999
2022-06-12 11:37:22,940   loss = 0.23347948512437044
2022-06-12 11:38:57,313 ***** Running evaluation *****
2022-06-12 11:38:57,314   Epoch = 2 iter 33499 step
2022-06-12 11:38:57,314   Num examples = 40430
2022-06-12 11:38:57,314   Batch size = 32
2022-06-12 11:39:21,971 ***** Running evaluation *****
2022-06-12 11:39:21,972   Epoch = 0 iter 10499 step
2022-06-12 11:39:21,972   Num examples = 9815
2022-06-12 11:39:21,972   Batch size = 32
2022-06-12 11:39:30,087 ***** Eval results *****
2022-06-12 11:39:30,087   acc = 0.8159959246051961
2022-06-12 11:39:30,088   cls_loss = 0.23328180190738876
2022-06-12 11:39:30,088   eval_loss = 0.49733243144878736
2022-06-12 11:39:30,088   global_step = 10499
2022-06-12 11:39:30,088   loss = 0.23328180190738876
2022-06-12 11:39:32,260 ***** Eval results *****
2022-06-12 11:39:32,260   acc = 0.9022508038585209
2022-06-12 11:39:32,260   acc_and_f1 = 0.884494703251084
2022-06-12 11:39:32,261   cls_loss = 0.0949561100448637
2022-06-12 11:39:32,261   eval_loss = 0.25301215607861555
2022-06-12 11:39:32,261   f1 = 0.8667386026436472
2022-06-12 11:39:32,261   global_step = 33499
2022-06-12 11:39:32,261   loss = 0.0949561100448637
2022-06-12 11:41:29,138 ***** Running evaluation *****
2022-06-12 11:41:29,139   Epoch = 0 iter 10999 step
2022-06-12 11:41:29,139   Num examples = 9815
2022-06-12 11:41:29,139   Batch size = 32
2022-06-12 11:41:35,507 ***** Running evaluation *****
2022-06-12 11:41:35,507   Epoch = 2 iter 33999 step
2022-06-12 11:41:35,507   Num examples = 40430
2022-06-12 11:41:35,507   Batch size = 32
2022-06-12 11:41:37,247 ***** Eval results *****
2022-06-12 11:41:37,248   acc = 0.810697911360163
2022-06-12 11:41:37,248   cls_loss = 0.23308833326298667
2022-06-12 11:41:37,248   eval_loss = 0.5109884684559577
2022-06-12 11:41:37,248   global_step = 10999
2022-06-12 11:41:37,248   loss = 0.23308833326298667
2022-06-12 11:42:10,437 ***** Eval results *****
2022-06-12 11:42:10,437   acc = 0.9027949542418996
2022-06-12 11:42:10,438   acc_and_f1 = 0.8845470994928625
2022-06-12 11:42:10,438   cls_loss = 0.09490461536370551
2022-06-12 11:42:10,438   eval_loss = 0.2640105385047419
2022-06-12 11:42:10,438   f1 = 0.8662992447438254
2022-06-12 11:42:10,438   global_step = 33999
2022-06-12 11:42:10,438   loss = 0.09490461536370551
2022-06-12 11:43:35,915 ***** Running evaluation *****
2022-06-12 11:43:35,915   Epoch = 0 iter 11499 step
2022-06-12 11:43:35,916   Num examples = 9815
2022-06-12 11:43:35,916   Batch size = 32
2022-06-12 11:43:44,026 ***** Eval results *****
2022-06-12 11:43:44,026   acc = 0.8154865002547121
2022-06-12 11:43:44,026   cls_loss = 0.232965137143116
2022-06-12 11:43:44,026   eval_loss = 0.4977652857668625
2022-06-12 11:43:44,026   global_step = 11499
2022-06-12 11:43:44,026   loss = 0.232965137143116
2022-06-12 11:44:13,390 ***** Running evaluation *****
2022-06-12 11:44:13,390   Epoch = 3 iter 34499 step
2022-06-12 11:44:13,390   Num examples = 40430
2022-06-12 11:44:13,390   Batch size = 32
2022-06-12 11:44:48,310 ***** Eval results *****
2022-06-12 11:44:48,310   acc = 0.9043532030670295
2022-06-12 11:44:48,310   acc_and_f1 = 0.8878445436396535
2022-06-12 11:44:48,310   cls_loss = 0.09103387938979965
2022-06-12 11:44:48,310   eval_loss = 0.2606823373750984
2022-06-12 11:44:48,310   f1 = 0.8713358842122775
2022-06-12 11:44:48,310   global_step = 34499
2022-06-12 11:44:48,311   loss = 0.09103387938979965
2022-06-12 11:44:48,311 ***** Save model *****
2022-06-12 11:45:42,648 ***** Running evaluation *****
2022-06-12 11:45:42,649   Epoch = 0 iter 11999 step
2022-06-12 11:45:42,649   Num examples = 9815
2022-06-12 11:45:42,649   Batch size = 32
2022-06-12 11:45:50,758 ***** Eval results *****
2022-06-12 11:45:50,759   acc = 0.8182373917473256
2022-06-12 11:45:50,759   cls_loss = 0.2328080927464294
2022-06-12 11:45:50,759   eval_loss = 0.4952332947269713
2022-06-12 11:45:50,759   global_step = 11999
2022-06-12 11:45:50,759   loss = 0.2328080927464294
2022-06-12 11:45:50,759 ***** Save model *****
2022-06-12 11:46:51,841 ***** Running evaluation *****
2022-06-12 11:46:51,842   Epoch = 3 iter 34999 step
2022-06-12 11:46:51,842   Num examples = 40430
2022-06-12 11:46:51,842   Batch size = 32
2022-06-12 11:47:26,793 ***** Eval results *****
2022-06-12 11:47:26,794   acc = 0.9030917635419243
2022-06-12 11:47:26,794   acc_and_f1 = 0.8852694083965795
2022-06-12 11:47:26,794   cls_loss = 0.09069521964870234
2022-06-12 11:47:26,794   eval_loss = 0.25478756592219953
2022-06-12 11:47:26,794   f1 = 0.8674470532512348
2022-06-12 11:47:26,794   global_step = 34999
2022-06-12 11:47:26,794   loss = 0.09069521964870234
2022-06-12 11:47:49,917 ***** Running evaluation *****
2022-06-12 11:47:49,918   Epoch = 1 iter 12499 step
2022-06-12 11:47:49,918   Num examples = 9815
2022-06-12 11:47:49,918   Batch size = 32
2022-06-12 11:47:58,051 ***** Eval results *****
2022-06-12 11:47:58,051   acc = 0.8156902699949058
2022-06-12 11:47:58,051   cls_loss = 0.2259613702955999
2022-06-12 11:47:58,051   eval_loss = 0.49284231468598305
2022-06-12 11:47:58,051   global_step = 12499
2022-06-12 11:47:58,051   loss = 0.2259613702955999
2022-06-12 11:49:29,968 ***** Running evaluation *****
2022-06-12 11:49:29,968   Epoch = 3 iter 35499 step
2022-06-12 11:49:29,968   Num examples = 40430
2022-06-12 11:49:29,968   Batch size = 32
2022-06-12 11:49:57,092 ***** Running evaluation *****
2022-06-12 11:49:57,092   Epoch = 1 iter 12999 step
2022-06-12 11:49:57,092   Num examples = 9815
2022-06-12 11:49:57,092   Batch size = 32
2022-06-12 11:50:04,910 ***** Eval results *****
2022-06-12 11:50:04,911   acc = 0.9005688844917141
2022-06-12 11:50:04,911   acc_and_f1 = 0.8854289956790011
2022-06-12 11:50:04,911   cls_loss = 0.09091199519578032
2022-06-12 11:50:04,911   eval_loss = 0.25850518862716854
2022-06-12 11:50:04,911   f1 = 0.8702891068662881
2022-06-12 11:50:04,911   global_step = 35499
2022-06-12 11:50:04,911   loss = 0.09091199519578032
2022-06-12 11:50:05,225 ***** Eval results *****
2022-06-12 11:50:05,225   acc = 0.8119205298013245
2022-06-12 11:50:05,225   cls_loss = 0.22534899857754892
2022-06-12 11:50:05,225   eval_loss = 0.5016102068587313
2022-06-12 11:50:05,226   global_step = 12999
2022-06-12 11:50:05,226   loss = 0.22534899857754892
2022-06-12 11:52:04,217 ***** Running evaluation *****
2022-06-12 11:52:04,217   Epoch = 1 iter 13499 step
2022-06-12 11:52:04,218   Num examples = 9815
2022-06-12 11:52:04,218   Batch size = 32
2022-06-12 11:52:08,348 ***** Running evaluation *****
2022-06-12 11:52:08,349   Epoch = 3 iter 35999 step
2022-06-12 11:52:08,349   Num examples = 40430
2022-06-12 11:52:08,349   Batch size = 32
2022-06-12 11:52:12,330 ***** Eval results *****
2022-06-12 11:52:12,330   acc = 0.8210901681100357
2022-06-12 11:52:12,331   cls_loss = 0.2255269653415641
2022-06-12 11:52:12,331   eval_loss = 0.4849244061044451
2022-06-12 11:52:12,331   global_step = 13499
2022-06-12 11:52:12,331   loss = 0.2255269653415641
2022-06-12 11:52:12,331 ***** Save model *****
2022-06-12 11:52:43,319 ***** Eval results *****
2022-06-12 11:52:43,319   acc = 0.9019787286668316
2022-06-12 11:52:43,319   acc_and_f1 = 0.8863843628662319
2022-06-12 11:52:43,319   cls_loss = 0.09093592439365487
2022-06-12 11:52:43,319   eval_loss = 0.2527314598342003
2022-06-12 11:52:43,319   f1 = 0.8707899970656322
2022-06-12 11:52:43,320   global_step = 35999
2022-06-12 11:52:43,320   loss = 0.09093592439365487
2022-06-12 11:54:11,729 ***** Running evaluation *****
2022-06-12 11:54:11,729   Epoch = 1 iter 13999 step
2022-06-12 11:54:11,729   Num examples = 9815
2022-06-12 11:54:11,730   Batch size = 32
2022-06-12 11:54:19,839 ***** Eval results *****
2022-06-12 11:54:19,839   acc = 0.8168110035659705
2022-06-12 11:54:19,839   cls_loss = 0.22526460415166286
2022-06-12 11:54:19,839   eval_loss = 0.49625607402782873
2022-06-12 11:54:19,839   global_step = 13999
2022-06-12 11:54:19,839   loss = 0.22526460415166286
2022-06-12 11:54:46,696 ***** Running evaluation *****
2022-06-12 11:54:46,696   Epoch = 3 iter 36499 step
2022-06-12 11:54:46,696   Num examples = 40430
2022-06-12 11:54:46,696   Batch size = 32
2022-06-12 11:55:21,674 ***** Eval results *****
2022-06-12 11:55:21,675   acc = 0.9024981449418749
2022-06-12 11:55:21,675   acc_and_f1 = 0.8864178113696941
2022-06-12 11:55:21,675   cls_loss = 0.09090441905894554
2022-06-12 11:55:21,675   eval_loss = 0.2562723440306756
2022-06-12 11:55:21,675   f1 = 0.8703374777975134
2022-06-12 11:55:21,675   global_step = 36499
2022-06-12 11:55:21,675   loss = 0.09090441905894554
2022-06-12 11:56:19,971 ***** Running evaluation *****
2022-06-12 11:56:19,972   Epoch = 1 iter 14499 step
2022-06-12 11:56:19,972   Num examples = 9815
2022-06-12 11:56:19,972   Batch size = 32
2022-06-12 11:56:28,132 ***** Eval results *****
2022-06-12 11:56:28,133   acc = 0.8130412633723892
2022-06-12 11:56:28,133   cls_loss = 0.22537082933972938
2022-06-12 11:56:28,133   eval_loss = 0.49586490063597405
2022-06-12 11:56:28,133   global_step = 14499
2022-06-12 11:56:28,133   loss = 0.22537082933972938
2022-06-12 11:57:24,723 ***** Running evaluation *****
2022-06-12 11:57:24,723   Epoch = 3 iter 36999 step
2022-06-12 11:57:24,723   Num examples = 40430
2022-06-12 11:57:24,723   Batch size = 32
2022-06-12 11:57:59,696 ***** Eval results *****
2022-06-12 11:57:59,696   acc = 0.9010140984417512
2022-06-12 11:57:59,697   acc_and_f1 = 0.8822461499918383
2022-06-12 11:57:59,697   cls_loss = 0.0910866762262886
2022-06-12 11:57:59,697   eval_loss = 0.2651131599640497
2022-06-12 11:57:59,697   f1 = 0.8634782015419253
2022-06-12 11:57:59,697   global_step = 36999
2022-06-12 11:57:59,697   loss = 0.0910866762262886
2022-06-12 11:58:27,793 ***** Running evaluation *****
2022-06-12 11:58:27,794   Epoch = 1 iter 14999 step
2022-06-12 11:58:27,794   Num examples = 9815
2022-06-12 11:58:27,794   Batch size = 32
2022-06-12 11:58:35,900 ***** Eval results *****
2022-06-12 11:58:35,901   acc = 0.8227203260315843
2022-06-12 11:58:35,901   cls_loss = 0.22546946750052513
2022-06-12 11:58:35,901   eval_loss = 0.4840273578023289
2022-06-12 11:58:35,901   global_step = 14999
2022-06-12 11:58:35,901   loss = 0.22546946750052513
2022-06-12 11:58:35,901 ***** Save model *****
2022-06-12 12:00:02,646 ***** Running evaluation *****
2022-06-12 12:00:02,647   Epoch = 3 iter 37499 step
2022-06-12 12:00:02,647   Num examples = 40430
2022-06-12 12:00:02,647   Batch size = 32
2022-06-12 12:00:35,750 ***** Running evaluation *****
2022-06-12 12:00:35,751   Epoch = 1 iter 15499 step
2022-06-12 12:00:35,751   Num examples = 9815
2022-06-12 12:00:35,751   Batch size = 32
2022-06-12 12:00:37,570 ***** Eval results *****
2022-06-12 12:00:37,570   acc = 0.9015829829334653
2022-06-12 12:00:37,570   acc_and_f1 = 0.8836535034128739
2022-06-12 12:00:37,570   cls_loss = 0.0911553688105509
2022-06-12 12:00:37,570   eval_loss = 0.259032424082508
2022-06-12 12:00:37,570   f1 = 0.8657240238922823
2022-06-12 12:00:37,570   global_step = 37499
2022-06-12 12:00:37,571   loss = 0.0911553688105509
2022-06-12 12:00:43,866 ***** Eval results *****
2022-06-12 12:00:43,866   acc = 0.8229240957717779
2022-06-12 12:00:43,866   cls_loss = 0.225236372276679
2022-06-12 12:00:43,866   eval_loss = 0.4782327217465503
2022-06-12 12:00:43,866   global_step = 15499
2022-06-12 12:00:43,866   loss = 0.225236372276679
2022-06-12 12:00:43,867 ***** Save model *****
2022-06-12 12:02:40,450 ***** Running evaluation *****
2022-06-12 12:02:40,451   Epoch = 3 iter 37999 step
2022-06-12 12:02:40,451   Num examples = 40430
2022-06-12 12:02:40,451   Batch size = 32
2022-06-12 12:02:43,492 ***** Running evaluation *****
2022-06-12 12:02:43,493   Epoch = 1 iter 15999 step
2022-06-12 12:02:43,493   Num examples = 9815
2022-06-12 12:02:43,493   Batch size = 32
2022-06-12 12:02:51,599 ***** Eval results *****
2022-06-12 12:02:51,600   acc = 0.8193581253183903
2022-06-12 12:02:51,600   cls_loss = 0.22519263695493788
2022-06-12 12:02:51,600   eval_loss = 0.4926822202019272
2022-06-12 12:02:51,600   global_step = 15999
2022-06-12 12:02:51,600   loss = 0.22519263695493788
2022-06-12 12:03:15,387 ***** Eval results *****
2022-06-12 12:03:15,387   acc = 0.9014593123917882
2022-06-12 12:03:15,387   acc_and_f1 = 0.8850477780634751
2022-06-12 12:03:15,387   cls_loss = 0.09129818167760642
2022-06-12 12:03:15,387   eval_loss = 0.25900771587945615
2022-06-12 12:03:15,387   f1 = 0.8686362437351621
2022-06-12 12:03:15,388   global_step = 37999
2022-06-12 12:03:15,388   loss = 0.09129818167760642
2022-06-12 12:04:50,441 ***** Running evaluation *****
2022-06-12 12:04:50,441   Epoch = 1 iter 16499 step
2022-06-12 12:04:50,441   Num examples = 9815
2022-06-12 12:04:50,441   Batch size = 32
2022-06-12 12:04:58,563 ***** Eval results *****
2022-06-12 12:04:58,563   acc = 0.8167091186958737
2022-06-12 12:04:58,563   cls_loss = 0.22513137269296393
2022-06-12 12:04:58,563   eval_loss = 0.4928156916986459
2022-06-12 12:04:58,563   global_step = 16499
2022-06-12 12:04:58,563   loss = 0.22513137269296393
2022-06-12 12:05:18,367 ***** Running evaluation *****
2022-06-12 12:05:18,368   Epoch = 3 iter 38499 step
2022-06-12 12:05:18,368   Num examples = 40430
2022-06-12 12:05:18,368   Batch size = 32
2022-06-12 12:05:53,332 ***** Eval results *****
2022-06-12 12:05:53,332   acc = 0.9028444224585703
2022-06-12 12:05:53,332   acc_and_f1 = 0.8864880050532619
2022-06-12 12:05:53,333   cls_loss = 0.09128944419526408
2022-06-12 12:05:53,333   eval_loss = 0.2538093790364794
2022-06-12 12:05:53,333   f1 = 0.8701315876479534
2022-06-12 12:05:53,333   global_step = 38499
2022-06-12 12:05:53,333   loss = 0.09128944419526408
2022-06-12 12:06:57,467 ***** Running evaluation *****
2022-06-12 12:06:57,467   Epoch = 1 iter 16999 step
2022-06-12 12:06:57,467   Num examples = 9815
2022-06-12 12:06:57,467   Batch size = 32
2022-06-12 12:07:05,580 ***** Eval results *****
2022-06-12 12:07:05,581   acc = 0.8204788588894549
2022-06-12 12:07:05,581   cls_loss = 0.22525486287971339
2022-06-12 12:07:05,581   eval_loss = 0.48250114432375285
2022-06-12 12:07:05,581   global_step = 16999
2022-06-12 12:07:05,581   loss = 0.22525486287971339
2022-06-12 12:07:56,216 ***** Running evaluation *****
2022-06-12 12:07:56,216   Epoch = 3 iter 38999 step
2022-06-12 12:07:56,216   Num examples = 40430
2022-06-12 12:07:56,216   Batch size = 32
2022-06-12 12:08:31,167 ***** Eval results *****
2022-06-12 12:08:31,167   acc = 0.9021518674251793
2022-06-12 12:08:31,167   acc_and_f1 = 0.8843912866894624
2022-06-12 12:08:31,167   cls_loss = 0.09129353021598223
2022-06-12 12:08:31,167   eval_loss = 0.2617539804078096
2022-06-12 12:08:31,167   f1 = 0.8666307059537456
2022-06-12 12:08:31,168   global_step = 38999
2022-06-12 12:08:31,168   loss = 0.09129353021598223
2022-06-12 12:09:04,579 ***** Running evaluation *****
2022-06-12 12:09:04,579   Epoch = 1 iter 17499 step
2022-06-12 12:09:04,579   Num examples = 9815
2022-06-12 12:09:04,579   Batch size = 32
2022-06-12 12:09:12,700 ***** Eval results *****
2022-06-12 12:09:12,700   acc = 0.8200713194090677
2022-06-12 12:09:12,700   cls_loss = 0.2252557809167494
2022-06-12 12:09:12,700   eval_loss = 0.4868834884139536
2022-06-12 12:09:12,701   global_step = 17499
2022-06-12 12:09:12,701   loss = 0.2252557809167494
2022-06-12 12:10:34,225 ***** Running evaluation *****
2022-06-12 12:10:34,225   Epoch = 3 iter 39499 step
2022-06-12 12:10:34,225   Num examples = 40430
2022-06-12 12:10:34,226   Batch size = 32
2022-06-12 12:11:09,217 ***** Eval results *****
2022-06-12 12:11:09,218   acc = 0.8992579767499381
2022-06-12 12:11:09,218   acc_and_f1 = 0.8836267196989036
2022-06-12 12:11:09,218   cls_loss = 0.09111053136444021
2022-06-12 12:11:09,218   eval_loss = 0.2549536727321676
2022-06-12 12:11:09,218   f1 = 0.8679954626478691
2022-06-12 12:11:09,218   global_step = 39499
2022-06-12 12:11:09,218   loss = 0.09111053136444021
2022-06-12 12:11:11,665 ***** Running evaluation *****
2022-06-12 12:11:11,666   Epoch = 1 iter 17999 step
2022-06-12 12:11:11,666   Num examples = 9815
2022-06-12 12:11:11,666   Batch size = 32
2022-06-12 12:11:19,781 ***** Eval results *****
2022-06-12 12:11:19,781   acc = 0.8145695364238411
2022-06-12 12:11:19,781   cls_loss = 0.22527600376861556
2022-06-12 12:11:19,781   eval_loss = 0.49305966511028987
2022-06-12 12:11:19,781   global_step = 17999
2022-06-12 12:11:19,781   loss = 0.22527600376861556
2022-06-12 12:13:12,596 ***** Running evaluation *****
2022-06-12 12:13:12,596   Epoch = 3 iter 39999 step
2022-06-12 12:13:12,596   Num examples = 40430
2022-06-12 12:13:12,596   Batch size = 32
2022-06-12 12:13:18,742 ***** Running evaluation *****
2022-06-12 12:13:18,742   Epoch = 1 iter 18499 step
2022-06-12 12:13:18,742   Num examples = 9815
2022-06-12 12:13:18,742   Batch size = 32
2022-06-12 12:13:26,849 ***** Eval results *****
2022-06-12 12:13:26,852   acc = 0.8155883851248089
2022-06-12 12:13:26,852   cls_loss = 0.22530388930906986
2022-06-12 12:13:26,852   eval_loss = 0.5031814428610599
2022-06-12 12:13:26,852   global_step = 18499
2022-06-12 12:13:26,852   loss = 0.22530388930906986
2022-06-12 12:13:47,554 ***** Eval results *****
2022-06-12 12:13:47,555   acc = 0.9007667573583972
2022-06-12 12:13:47,555   acc_and_f1 = 0.8839683343142138
2022-06-12 12:13:47,555   cls_loss = 0.09119383857099507
2022-06-12 12:13:47,555   eval_loss = 0.25981059158515607
2022-06-12 12:13:47,555   f1 = 0.8671699112700305
2022-06-12 12:13:47,555   global_step = 39999
2022-06-12 12:13:47,555   loss = 0.09119383857099507
2022-06-12 12:15:25,867 ***** Running evaluation *****
2022-06-12 12:15:25,867   Epoch = 1 iter 18999 step
2022-06-12 12:15:25,867   Num examples = 9815
2022-06-12 12:15:25,868   Batch size = 32
2022-06-12 12:15:33,977 ***** Eval results *****
2022-06-12 12:15:33,978   acc = 0.817524197656648
2022-06-12 12:15:33,978   cls_loss = 0.2253170902049003
2022-06-12 12:15:33,978   eval_loss = 0.489208534117242
2022-06-12 12:15:33,978   global_step = 18999
2022-06-12 12:15:33,978   loss = 0.2253170902049003
2022-06-12 12:15:51,092 ***** Running evaluation *****
2022-06-12 12:15:51,092   Epoch = 3 iter 40499 step
2022-06-12 12:15:51,092   Num examples = 40430
2022-06-12 12:15:51,092   Batch size = 32
2022-06-12 12:16:26,084 ***** Eval results *****
2022-06-12 12:16:26,084   acc = 0.9026218154835518
2022-06-12 12:16:26,084   acc_and_f1 = 0.8844619927419457
2022-06-12 12:16:26,084   cls_loss = 0.09135281488341351
2022-06-12 12:16:26,084   eval_loss = 0.24999956090498385
2022-06-12 12:16:26,084   f1 = 0.8663021700003396
2022-06-12 12:16:26,084   global_step = 40499
2022-06-12 12:16:26,084   loss = 0.09135281488341351
2022-06-12 12:17:33,002 ***** Running evaluation *****
2022-06-12 12:17:33,003   Epoch = 1 iter 19499 step
2022-06-12 12:17:33,003   Num examples = 9815
2022-06-12 12:17:33,003   Batch size = 32
2022-06-12 12:17:41,114 ***** Eval results *****
2022-06-12 12:17:41,114   acc = 0.8166072338257768
2022-06-12 12:17:41,114   cls_loss = 0.225364144798729
2022-06-12 12:17:41,114   eval_loss = 0.49423909279539063
2022-06-12 12:17:41,114   global_step = 19499
2022-06-12 12:17:41,115   loss = 0.225364144798729
2022-06-12 12:18:29,162 ***** Running evaluation *****
2022-06-12 12:18:29,162   Epoch = 3 iter 40999 step
2022-06-12 12:18:29,163   Num examples = 40430
2022-06-12 12:18:29,163   Batch size = 32
2022-06-12 12:19:04,128 ***** Eval results *****
2022-06-12 12:19:04,128   acc = 0.9021271333168439
2022-06-12 12:19:04,128   acc_and_f1 = 0.8859663403234106
2022-06-12 12:19:04,128   cls_loss = 0.09133755697251353
2022-06-12 12:19:04,129   eval_loss = 0.2518231696626054
2022-06-12 12:19:04,129   f1 = 0.8698055473299774
2022-06-12 12:19:04,129   global_step = 40999
2022-06-12 12:19:04,129   loss = 0.09133755697251353
2022-06-12 12:19:40,089 ***** Running evaluation *****
2022-06-12 12:19:40,089   Epoch = 1 iter 19999 step
2022-06-12 12:19:40,089   Num examples = 9815
2022-06-12 12:19:40,089   Batch size = 32
2022-06-12 12:19:48,197 ***** Eval results *****
2022-06-12 12:19:48,198   acc = 0.8184411614875191
2022-06-12 12:19:48,198   cls_loss = 0.2253617662936449
2022-06-12 12:19:48,198   eval_loss = 0.4881931747695134
2022-06-12 12:19:48,198   global_step = 19999
2022-06-12 12:19:48,198   loss = 0.2253617662936449
2022-06-12 12:21:07,177 ***** Running evaluation *****
2022-06-12 12:21:07,178   Epoch = 3 iter 41499 step
2022-06-12 12:21:07,178   Num examples = 40430
2022-06-12 12:21:07,178   Batch size = 32
2022-06-12 12:21:42,121 ***** Eval results *****
2022-06-12 12:21:42,121   acc = 0.9029928271085828
2022-06-12 12:21:42,121   acc_and_f1 = 0.8860424749695117
2022-06-12 12:21:42,121   cls_loss = 0.09124862591488095
2022-06-12 12:21:42,121   eval_loss = 0.25468947022528493
2022-06-12 12:21:42,121   f1 = 0.8690921228304406
2022-06-12 12:21:42,121   global_step = 41499
2022-06-12 12:21:42,121   loss = 0.09124862591488095
2022-06-12 12:21:47,266 ***** Running evaluation *****
2022-06-12 12:21:47,266   Epoch = 1 iter 20499 step
2022-06-12 12:21:47,266   Num examples = 9815
2022-06-12 12:21:47,266   Batch size = 32
2022-06-12 12:21:55,376 ***** Eval results *****
2022-06-12 12:21:55,377   acc = 0.8188487009679063
2022-06-12 12:21:55,377   cls_loss = 0.2252667384329906
2022-06-12 12:21:55,377   eval_loss = 0.48880645938533135
2022-06-12 12:21:55,377   global_step = 20499
2022-06-12 12:21:55,377   loss = 0.2252667384329906
2022-06-12 12:23:44,977 ***** Running evaluation *****
2022-06-12 12:23:44,977   Epoch = 3 iter 41999 step
2022-06-12 12:23:44,977   Num examples = 40430
2022-06-12 12:23:44,978   Batch size = 32
2022-06-12 12:23:54,555 ***** Running evaluation *****
2022-06-12 12:23:54,555   Epoch = 1 iter 20999 step
2022-06-12 12:23:54,555   Num examples = 9815
2022-06-12 12:23:54,555   Batch size = 32
2022-06-12 12:24:02,688 ***** Eval results *****
2022-06-12 12:24:02,688   acc = 0.8174223127865512
2022-06-12 12:24:02,688   cls_loss = 0.2253069554094488
2022-06-12 12:24:02,688   eval_loss = 0.49204821598258003
2022-06-12 12:24:02,688   global_step = 20999
2022-06-12 12:24:02,688   loss = 0.2253069554094488
2022-06-12 12:24:20,093 ***** Eval results *****
2022-06-12 12:24:20,093   acc = 0.9017313875834776
2022-06-12 12:24:20,094   acc_and_f1 = 0.8856468750517933
2022-06-12 12:24:20,094   cls_loss = 0.09117435693971285
2022-06-12 12:24:20,094   eval_loss = 0.2537211425413814
2022-06-12 12:24:20,094   f1 = 0.869562362520109
2022-06-12 12:24:20,094   global_step = 41999
2022-06-12 12:24:20,094   loss = 0.09117435693971285
2022-06-12 12:26:02,054 ***** Running evaluation *****
2022-06-12 12:26:02,054   Epoch = 1 iter 21499 step
2022-06-12 12:26:02,054   Num examples = 9815
2022-06-12 12:26:02,054   Batch size = 32
2022-06-12 12:26:10,170 ***** Eval results *****
2022-06-12 12:26:10,170   acc = 0.8208863983698421
2022-06-12 12:26:10,170   cls_loss = 0.22528248968504702
2022-06-12 12:26:10,170   eval_loss = 0.48793156071283916
2022-06-12 12:26:10,170   global_step = 21499
2022-06-12 12:26:10,170   loss = 0.22528248968504702
2022-06-12 12:26:23,205 ***** Running evaluation *****
2022-06-12 12:26:23,205   Epoch = 3 iter 42499 step
2022-06-12 12:26:23,205   Num examples = 40430
2022-06-12 12:26:23,205   Batch size = 32
2022-06-12 12:26:58,198 ***** Eval results *****
2022-06-12 12:26:58,198   acc = 0.9031164976502597
2022-06-12 12:26:58,198   acc_and_f1 = 0.8868337040615883
2022-06-12 12:26:58,198   cls_loss = 0.09115058661132451
2022-06-12 12:26:58,198   eval_loss = 0.24597984067859907
2022-06-12 12:26:58,198   f1 = 0.8705509104729171
2022-06-12 12:26:58,198   global_step = 42499
2022-06-12 12:26:58,198   loss = 0.09115058661132451
2022-06-12 12:28:09,094 ***** Running evaluation *****
2022-06-12 12:28:09,094   Epoch = 1 iter 21999 step
2022-06-12 12:28:09,094   Num examples = 9815
2022-06-12 12:28:09,094   Batch size = 32
2022-06-12 12:28:17,210 ***** Eval results *****
2022-06-12 12:28:17,210   acc = 0.8246561385634233
2022-06-12 12:28:17,210   cls_loss = 0.2253455994068645
2022-06-12 12:28:17,210   eval_loss = 0.486423103642386
2022-06-12 12:28:17,210   global_step = 21999
2022-06-12 12:28:17,210   loss = 0.2253455994068645
2022-06-12 12:28:17,211 ***** Save model *****
2022-06-12 12:29:01,442 ***** Running evaluation *****
2022-06-12 12:29:01,442   Epoch = 3 iter 42999 step
2022-06-12 12:29:01,442   Num examples = 40430
2022-06-12 12:29:01,442   Batch size = 32
2022-06-12 12:29:36,461 ***** Eval results *****
2022-06-12 12:29:36,461   acc = 0.9044026712837002
2022-06-12 12:29:36,461   acc_and_f1 = 0.8880434391576697
2022-06-12 12:29:36,462   cls_loss = 0.09113940864756145
2022-06-12 12:29:36,462   eval_loss = 0.24934785312359942
2022-06-12 12:29:36,462   f1 = 0.8716842070316391
2022-06-12 12:29:36,462   global_step = 42999
2022-06-12 12:29:36,462   loss = 0.09113940864756145
2022-06-12 12:29:36,462 ***** Save model *****
2022-06-12 12:30:17,190 ***** Running evaluation *****
2022-06-12 12:30:17,190   Epoch = 1 iter 22499 step
2022-06-12 12:30:17,190   Num examples = 9815
2022-06-12 12:30:17,190   Batch size = 32
2022-06-12 12:30:25,301 ***** Eval results *****
2022-06-12 12:30:25,301   acc = 0.8185430463576159
2022-06-12 12:30:25,301   cls_loss = 0.2253331840562783
2022-06-12 12:30:25,301   eval_loss = 0.4934753729970913
2022-06-12 12:30:25,301   global_step = 22499
2022-06-12 12:30:25,302   loss = 0.2253331840562783
2022-06-12 12:31:40,309 ***** Running evaluation *****
2022-06-12 12:31:40,309   Epoch = 3 iter 43499 step
2022-06-12 12:31:40,309   Num examples = 40430
2022-06-12 12:31:40,309   Batch size = 32
2022-06-12 12:32:15,314 ***** Eval results *****
2022-06-12 12:32:15,314   acc = 0.9030175612169181
2022-06-12 12:32:15,314   acc_and_f1 = 0.8874380593430582
2022-06-12 12:32:15,314   cls_loss = 0.0911003790112562
2022-06-12 12:32:15,314   eval_loss = 0.2531152072633746
2022-06-12 12:32:15,314   f1 = 0.8718585574691983
2022-06-12 12:32:15,315   global_step = 43499
2022-06-12 12:32:15,315   loss = 0.0911003790112562
2022-06-12 12:32:15,315 ***** Save model *****
2022-06-12 12:32:23,977 ***** Running evaluation *****
2022-06-12 12:32:23,978   Epoch = 1 iter 22999 step
2022-06-12 12:32:23,978   Num examples = 9815
2022-06-12 12:32:23,978   Batch size = 32
2022-06-12 12:32:32,092 ***** Eval results *****
2022-06-12 12:32:32,098   acc = 0.8245542536933266
2022-06-12 12:32:32,098   cls_loss = 0.22530328285151857
2022-06-12 12:32:32,098   eval_loss = 0.4760008273761513
2022-06-12 12:32:32,098   global_step = 22999
2022-06-12 12:32:32,098   loss = 0.22530328285151857
2022-06-12 12:34:18,776 ***** Running evaluation *****
2022-06-12 12:34:18,776   Epoch = 3 iter 43999 step
2022-06-12 12:34:18,776   Num examples = 40430
2022-06-12 12:34:18,776   Batch size = 32
2022-06-12 12:34:31,164 ***** Running evaluation *****
2022-06-12 12:34:31,165   Epoch = 1 iter 23499 step
2022-06-12 12:34:31,165   Num examples = 9815
2022-06-12 12:34:31,165   Batch size = 32
2022-06-12 12:34:39,287 ***** Eval results *****
2022-06-12 12:34:39,287   acc = 0.8214977075904228
2022-06-12 12:34:39,287   cls_loss = 0.22525155164054547
2022-06-12 12:34:39,287   eval_loss = 0.48197497014890667
2022-06-12 12:34:39,287   global_step = 23499
2022-06-12 12:34:39,287   loss = 0.22525155164054547
2022-06-12 12:34:53,781 ***** Eval results *****
2022-06-12 12:34:53,782   acc = 0.9026218154835518
2022-06-12 12:34:53,782   acc_and_f1 = 0.8871172015620923
2022-06-12 12:34:53,782   cls_loss = 0.0910507518632737
2022-06-12 12:34:53,782   eval_loss = 0.2506286584524603
2022-06-12 12:34:53,782   f1 = 0.8716125876406327
2022-06-12 12:34:53,782   global_step = 43999
2022-06-12 12:34:53,782   loss = 0.0910507518632737
2022-06-12 12:36:38,267 ***** Running evaluation *****
2022-06-12 12:36:38,268   Epoch = 1 iter 23999 step
2022-06-12 12:36:38,268   Num examples = 9815
2022-06-12 12:36:38,268   Batch size = 32
2022-06-12 12:36:46,381 ***** Eval results *****
2022-06-12 12:36:46,381   acc = 0.8206826286296485
2022-06-12 12:36:46,381   cls_loss = 0.22524485368240946
2022-06-12 12:36:46,381   eval_loss = 0.49368411351104513
2022-06-12 12:36:46,381   global_step = 23999
2022-06-12 12:36:46,381   loss = 0.22524485368240946
2022-06-12 12:36:56,884 ***** Running evaluation *****
2022-06-12 12:36:56,884   Epoch = 3 iter 44499 step
2022-06-12 12:36:56,884   Num examples = 40430
2022-06-12 12:36:56,884   Batch size = 32
2022-06-12 12:37:31,864 ***** Eval results *****
2022-06-12 12:37:31,864   acc = 0.9049715557754143
2022-06-12 12:37:31,864   acc_and_f1 = 0.8874773176508206
2022-06-12 12:37:31,864   cls_loss = 0.09100697369361614
2022-06-12 12:37:31,864   eval_loss = 0.2506913453063491
2022-06-12 12:37:31,864   f1 = 0.8699830795262269
2022-06-12 12:37:31,864   global_step = 44499
2022-06-12 12:37:31,864   loss = 0.09100697369361614
2022-06-12 12:38:45,751 ***** Running evaluation *****
2022-06-12 12:38:45,751   Epoch = 1 iter 24499 step
2022-06-12 12:38:45,751   Num examples = 9815
2022-06-12 12:38:45,751   Batch size = 32
2022-06-12 12:38:53,872 ***** Eval results *****
2022-06-12 12:38:53,872   acc = 0.8209882832399389
2022-06-12 12:38:53,872   cls_loss = 0.2251959900156341
2022-06-12 12:38:53,873   eval_loss = 0.48834961788662096
2022-06-12 12:38:53,873   global_step = 24499
2022-06-12 12:38:53,873   loss = 0.2251959900156341
2022-06-12 12:39:35,142 ***** Running evaluation *****
2022-06-12 12:39:35,143   Epoch = 3 iter 44999 step
2022-06-12 12:39:35,143   Num examples = 40430
2022-06-12 12:39:35,143   Batch size = 32
2022-06-12 12:40:10,115 ***** Eval results *****
2022-06-12 12:40:10,115   acc = 0.9029433588919119
2022-06-12 12:40:10,115   acc_and_f1 = 0.887029716626538
2022-06-12 12:40:10,115   cls_loss = 0.09107012931825055
2022-06-12 12:40:10,115   eval_loss = 0.24517347576697887
2022-06-12 12:40:10,115   f1 = 0.8711160743611641
2022-06-12 12:40:10,115   global_step = 44999
2022-06-12 12:40:10,116   loss = 0.09107012931825055
2022-06-12 12:40:52,769 ***** Running evaluation *****
2022-06-12 12:40:52,769   Epoch = 2 iter 24999 step
2022-06-12 12:40:52,769   Num examples = 9815
2022-06-12 12:40:52,770   Batch size = 32
2022-06-12 12:41:00,884 ***** Eval results *****
2022-06-12 12:41:00,885   acc = 0.82190524707081
2022-06-12 12:41:00,885   cls_loss = 0.221180081660951
2022-06-12 12:41:00,885   eval_loss = 0.49025885707392364
2022-06-12 12:41:00,885   global_step = 24999
2022-06-12 12:41:00,885   loss = 0.221180081660951
2022-06-12 12:42:13,431 ***** Running evaluation *****
2022-06-12 12:42:13,431   Epoch = 4 iter 45499 step
2022-06-12 12:42:13,431   Num examples = 40430
2022-06-12 12:42:13,431   Batch size = 32
2022-06-12 12:42:48,404 ***** Eval results *****
2022-06-12 12:42:48,404   acc = 0.9049715557754143
2022-06-12 12:42:48,405   acc_and_f1 = 0.8877794992566104
2022-06-12 12:42:48,405   cls_loss = 0.08528663021953482
2022-06-12 12:42:48,405   eval_loss = 0.2529522084448298
2022-06-12 12:42:48,405   f1 = 0.8705874427378065
2022-06-12 12:42:48,405   global_step = 45499
2022-06-12 12:42:48,405   loss = 0.08528663021953482
2022-06-12 12:42:59,768 ***** Running evaluation *****
2022-06-12 12:42:59,768   Epoch = 2 iter 25499 step
2022-06-12 12:42:59,768   Num examples = 9815
2022-06-12 12:42:59,768   Batch size = 32
2022-06-12 12:43:07,876 ***** Eval results *****
2022-06-12 12:43:07,876   acc = 0.8230259806418747
2022-06-12 12:43:07,876   cls_loss = 0.22027479682035955
2022-06-12 12:43:07,876   eval_loss = 0.4824720025547941
2022-06-12 12:43:07,877   global_step = 25499
2022-06-12 12:43:07,877   loss = 0.22027479682035955
2022-06-12 12:44:51,724 ***** Running evaluation *****
2022-06-12 12:44:51,725   Epoch = 4 iter 45999 step
2022-06-12 12:44:51,725   Num examples = 40430
2022-06-12 12:44:51,725   Batch size = 32
2022-06-12 12:45:07,004 ***** Running evaluation *****
2022-06-12 12:45:07,005   Epoch = 2 iter 25999 step
2022-06-12 12:45:07,005   Num examples = 9815
2022-06-12 12:45:07,005   Batch size = 32
2022-06-12 12:45:15,158 ***** Eval results *****
2022-06-12 12:45:15,158   acc = 0.8275089149261334
2022-06-12 12:45:15,158   cls_loss = 0.22039691559770946
2022-06-12 12:45:15,158   eval_loss = 0.4778258840307739
2022-06-12 12:45:15,158   global_step = 25999
2022-06-12 12:45:15,158   loss = 0.22039691559770946
2022-06-12 12:45:15,158 ***** Save model *****
2022-06-12 12:45:26,718 ***** Eval results *****
2022-06-12 12:45:26,718   acc = 0.9046747464753896
2022-06-12 12:45:26,718   acc_and_f1 = 0.8883004198946136
2022-06-12 12:45:26,718   cls_loss = 0.08763163349445845
2022-06-12 12:45:26,718   eval_loss = 0.2475154472344048
2022-06-12 12:45:26,719   f1 = 0.8719260933138376
2022-06-12 12:45:26,719   global_step = 45999
2022-06-12 12:45:26,719   loss = 0.08763163349445845
2022-06-12 12:45:26,719 ***** Save model *****
2022-06-12 12:47:14,340 ***** Running evaluation *****
2022-06-12 12:47:14,341   Epoch = 2 iter 26499 step
2022-06-12 12:47:14,341   Num examples = 9815
2022-06-12 12:47:14,341   Batch size = 32
2022-06-12 12:47:22,455 ***** Eval results *****
2022-06-12 12:47:22,455   acc = 0.8214977075904228
2022-06-12 12:47:22,455   cls_loss = 0.2202758535887431
2022-06-12 12:47:22,455   eval_loss = 0.4963497385528266
2022-06-12 12:47:22,455   global_step = 26499
2022-06-12 12:47:22,455   loss = 0.2202758535887431
2022-06-12 12:47:30,831 ***** Running evaluation *****
2022-06-12 12:47:30,831   Epoch = 4 iter 46499 step
2022-06-12 12:47:30,831   Num examples = 40430
2022-06-12 12:47:30,831   Batch size = 32
2022-06-12 12:48:05,803 ***** Eval results *****
2022-06-12 12:48:05,804   acc = 0.9034627751669553
2022-06-12 12:48:05,804   acc_and_f1 = 0.8864703769825346
2022-06-12 12:48:05,804   cls_loss = 0.08750421322801748
2022-06-12 12:48:05,804   eval_loss = 0.24819391841345903
2022-06-12 12:48:05,804   f1 = 0.8694779787981138
2022-06-12 12:48:05,804   global_step = 46499
2022-06-12 12:48:05,804   loss = 0.08750421322801748
2022-06-12 12:49:21,824 ***** Running evaluation *****
2022-06-12 12:49:21,824   Epoch = 2 iter 26999 step
2022-06-12 12:49:21,824   Num examples = 9815
2022-06-12 12:49:21,824   Batch size = 32
2022-06-12 12:49:29,932 ***** Eval results *****
2022-06-12 12:49:29,932   acc = 0.8242485990830362
2022-06-12 12:49:29,932   cls_loss = 0.22053156533544996
2022-06-12 12:49:29,932   eval_loss = 0.4865786199266825
2022-06-12 12:49:29,933   global_step = 26999
2022-06-12 12:49:29,933   loss = 0.22053156533544996
2022-06-12 12:50:09,251 ***** Running evaluation *****
2022-06-12 12:50:09,251   Epoch = 4 iter 46999 step
2022-06-12 12:50:09,251   Num examples = 40430
2022-06-12 12:50:09,251   Batch size = 32
2022-06-12 12:50:44,282 ***** Eval results *****
2022-06-12 12:50:44,283   acc = 0.9015087806084591
2022-06-12 12:50:44,283   acc_and_f1 = 0.8851617171804238
2022-06-12 12:50:44,283   cls_loss = 0.08816787290035541
2022-06-12 12:50:44,283   eval_loss = 0.2538766007224994
2022-06-12 12:50:44,283   f1 = 0.8688146537523886
2022-06-12 12:50:44,283   global_step = 46999
2022-06-12 12:50:44,283   loss = 0.08816787290035541
2022-06-12 12:51:28,862 ***** Running evaluation *****
2022-06-12 12:51:28,862   Epoch = 2 iter 27499 step
2022-06-12 12:51:28,862   Num examples = 9815
2022-06-12 12:51:28,862   Batch size = 32
2022-06-12 12:51:36,975 ***** Eval results *****
2022-06-12 12:51:36,976   acc = 0.8184411614875191
2022-06-12 12:51:36,976   cls_loss = 0.2205226671556625
2022-06-12 12:51:36,976   eval_loss = 0.4940531579019192
2022-06-12 12:51:36,976   global_step = 27499
2022-06-12 12:51:36,976   loss = 0.2205226671556625
2022-06-12 12:52:47,786 ***** Running evaluation *****
2022-06-12 12:52:47,786   Epoch = 4 iter 47499 step
2022-06-12 12:52:47,786   Num examples = 40430
2022-06-12 12:52:47,786   Batch size = 32
2022-06-12 12:53:22,766 ***** Eval results *****
2022-06-12 12:53:22,767   acc = 0.9039079891169923
2022-06-12 12:53:22,767   acc_and_f1 = 0.8878810321941462
2022-06-12 12:53:22,767   cls_loss = 0.08793547226059785
2022-06-12 12:53:22,767   eval_loss = 0.2507473132307818
2022-06-12 12:53:22,767   f1 = 0.8718540752713
2022-06-12 12:53:22,767   global_step = 47499
2022-06-12 12:53:22,767   loss = 0.08793547226059785
2022-06-12 12:53:36,710 ***** Running evaluation *****
2022-06-12 12:53:36,711   Epoch = 2 iter 27999 step
2022-06-12 12:53:36,711   Num examples = 9815
2022-06-12 12:53:36,711   Batch size = 32
2022-06-12 12:53:44,834 ***** Eval results *****
2022-06-12 12:53:44,834   acc = 0.8252674477840041
2022-06-12 12:53:44,835   cls_loss = 0.22071656033866557
2022-06-12 12:53:44,835   eval_loss = 0.478546285134185
2022-06-12 12:53:44,835   global_step = 27999
2022-06-12 12:53:44,835   loss = 0.22071656033866557
2022-06-12 12:55:26,286 ***** Running evaluation *****
2022-06-12 12:55:26,286   Epoch = 4 iter 47999 step
2022-06-12 12:55:26,286   Num examples = 40430
2022-06-12 12:55:26,286   Batch size = 32
2022-06-12 12:55:43,719 ***** Running evaluation *****
2022-06-12 12:55:43,720   Epoch = 2 iter 28499 step
2022-06-12 12:55:43,720   Num examples = 9815
2022-06-12 12:55:43,720   Batch size = 32
2022-06-12 12:55:51,835 ***** Eval results *****
2022-06-12 12:55:51,835   acc = 0.8222109016811003
2022-06-12 12:55:51,836   cls_loss = 0.22074066096102918
2022-06-12 12:55:51,836   eval_loss = 0.48012407983164834
2022-06-12 12:55:51,836   global_step = 28499
2022-06-12 12:55:51,836   loss = 0.22074066096102918
2022-06-12 12:56:01,324 ***** Eval results *****
2022-06-12 12:56:01,324   acc = 0.9044274053920356
2022-06-12 12:56:01,325   acc_and_f1 = 0.8874857477241604
2022-06-12 12:56:01,325   cls_loss = 0.08811564981322195
2022-06-12 12:56:01,325   eval_loss = 0.24680330709235954
2022-06-12 12:56:01,325   f1 = 0.8705440900562853
2022-06-12 12:56:01,325   global_step = 47999
2022-06-12 12:56:01,325   loss = 0.08811564981322195
2022-06-12 12:57:50,415 ***** Running evaluation *****
2022-06-12 12:57:50,415   Epoch = 2 iter 28999 step
2022-06-12 12:57:50,415   Num examples = 9815
2022-06-12 12:57:50,416   Batch size = 32
2022-06-12 12:57:58,533 ***** Eval results *****
2022-06-12 12:57:58,533   acc = 0.8242485990830362
2022-06-12 12:57:58,533   cls_loss = 0.22083237400457734
2022-06-12 12:57:58,533   eval_loss = 0.4786692841239395
2022-06-12 12:57:58,533   global_step = 28999
2022-06-12 12:57:58,534   loss = 0.22083237400457734
2022-06-12 12:58:04,887 ***** Running evaluation *****
2022-06-12 12:58:04,888   Epoch = 4 iter 48499 step
2022-06-12 12:58:04,888   Num examples = 40430
2022-06-12 12:58:04,888   Batch size = 32
2022-06-12 12:58:39,944 ***** Eval results *****
2022-06-12 12:58:39,945   acc = 0.9044274053920356
2022-06-12 12:58:39,945   acc_and_f1 = 0.887606961122984
2022-06-12 12:58:39,945   cls_loss = 0.08823447978557755
2022-06-12 12:58:39,945   eval_loss = 0.24943921394657956
2022-06-12 12:58:39,945   f1 = 0.8707865168539325
2022-06-12 12:58:39,945   global_step = 48499
2022-06-12 12:58:39,945   loss = 0.08823447978557755
2022-06-12 12:59:57,391 ***** Running evaluation *****
2022-06-12 12:59:57,391   Epoch = 2 iter 29499 step
2022-06-12 12:59:57,391   Num examples = 9815
2022-06-12 12:59:57,391   Batch size = 32
2022-06-12 13:00:05,523 ***** Eval results *****
2022-06-12 13:00:05,523   acc = 0.8209882832399389
2022-06-12 13:00:05,523   cls_loss = 0.2207363672059992
2022-06-12 13:00:05,523   eval_loss = 0.4888872560538376
2022-06-12 13:00:05,523   global_step = 29499
2022-06-12 13:00:05,523   loss = 0.2207363672059992
2022-06-12 13:00:43,143 ***** Running evaluation *****
2022-06-12 13:00:43,143   Epoch = 4 iter 48999 step
2022-06-12 13:00:43,143   Num examples = 40430
2022-06-12 13:00:43,143   Batch size = 32
2022-06-12 13:01:18,129 ***** Eval results *****
2022-06-12 13:01:18,130   acc = 0.9040563937670048
2022-06-12 13:01:18,130   acc_and_f1 = 0.8887017202452421
2022-06-12 13:01:18,130   cls_loss = 0.08799566021842793
2022-06-12 13:01:18,130   eval_loss = 0.24692619155320233
2022-06-12 13:01:18,130   f1 = 0.8733470467234793
2022-06-12 13:01:18,130   global_step = 48999
2022-06-12 13:01:18,130   loss = 0.08799566021842793
2022-06-12 13:01:18,130 ***** Save model *****
2022-06-12 13:02:04,565 ***** Running evaluation *****
2022-06-12 13:02:04,566   Epoch = 2 iter 29999 step
2022-06-12 13:02:04,566   Num examples = 9815
2022-06-12 13:02:04,566   Batch size = 32
2022-06-12 13:02:12,672 ***** Eval results *****
2022-06-12 13:02:12,673   acc = 0.8225165562913908
2022-06-12 13:02:12,673   cls_loss = 0.22069703130980256
2022-06-12 13:02:12,673   eval_loss = 0.4803590975968768
2022-06-12 13:02:12,673   global_step = 29999
2022-06-12 13:02:12,673   loss = 0.22069703130980256
2022-06-12 13:03:21,876 ***** Running evaluation *****
2022-06-12 13:03:21,877   Epoch = 4 iter 49499 step
2022-06-12 13:03:21,877   Num examples = 40430
2022-06-12 13:03:21,877   Batch size = 32
2022-06-12 13:03:56,853 ***** Eval results *****
2022-06-12 13:03:56,853   acc = 0.9046005441503834
2022-06-12 13:03:56,853   acc_and_f1 = 0.8890105868006684
2022-06-12 13:03:56,853   cls_loss = 0.0879206100488501
2022-06-12 13:03:56,854   eval_loss = 0.24954470727722383
2022-06-12 13:03:56,854   f1 = 0.8734206294509533
2022-06-12 13:03:56,854   global_step = 49499
2022-06-12 13:03:56,854   loss = 0.0879206100488501
2022-06-12 13:03:56,854 ***** Save model *****
2022-06-12 13:04:11,722 ***** Running evaluation *****
2022-06-12 13:04:11,722   Epoch = 2 iter 30499 step
2022-06-12 13:04:11,722   Num examples = 9815
2022-06-12 13:04:11,722   Batch size = 32
2022-06-12 13:04:19,870 ***** Eval results *****
2022-06-12 13:04:19,870   acc = 0.8248599083036169
2022-06-12 13:04:19,870   cls_loss = 0.22068128231659578
2022-06-12 13:04:19,870   eval_loss = 0.4815180791899902
2022-06-12 13:04:19,870   global_step = 30499
2022-06-12 13:04:19,870   loss = 0.22068128231659578
2022-06-12 13:06:00,638 ***** Running evaluation *****
2022-06-12 13:06:00,638   Epoch = 4 iter 49999 step
2022-06-12 13:06:00,639   Num examples = 40430
2022-06-12 13:06:00,639   Batch size = 32
2022-06-12 13:06:18,844 ***** Running evaluation *****
2022-06-12 13:06:18,845   Epoch = 2 iter 30999 step
2022-06-12 13:06:18,845   Num examples = 9815
2022-06-12 13:06:18,845   Batch size = 32
2022-06-12 13:06:27,009 ***** Eval results *****
2022-06-12 13:06:27,009   acc = 0.8255731023942945
2022-06-12 13:06:27,009   cls_loss = 0.2207036403374017
2022-06-12 13:06:27,010   eval_loss = 0.4799735903351625
2022-06-12 13:06:27,010   global_step = 30999
2022-06-12 13:06:27,010   loss = 0.2207036403374017
2022-06-12 13:06:35,604 ***** Eval results *****
2022-06-12 13:06:35,604   acc = 0.9038090526836507
2022-06-12 13:06:35,604   acc_and_f1 = 0.8874124696936213
2022-06-12 13:06:35,604   cls_loss = 0.08790190282464687
2022-06-12 13:06:35,604   eval_loss = 0.2532478294208575
2022-06-12 13:06:35,604   f1 = 0.8710158867035919
2022-06-12 13:06:35,605   global_step = 49999
2022-06-12 13:06:35,605   loss = 0.08790190282464687
2022-06-12 13:08:26,346 ***** Running evaluation *****
2022-06-12 13:08:26,346   Epoch = 2 iter 31499 step
2022-06-12 13:08:26,346   Num examples = 9815
2022-06-12 13:08:26,346   Batch size = 32
2022-06-12 13:08:34,486 ***** Eval results *****
2022-06-12 13:08:34,486   acc = 0.825776872134488
2022-06-12 13:08:34,486   cls_loss = 0.2206868761137023
2022-06-12 13:08:34,486   eval_loss = 0.4823727924194709
2022-06-12 13:08:34,486   global_step = 31499
2022-06-12 13:08:34,486   loss = 0.2206868761137023
2022-06-12 13:08:38,881 ***** Running evaluation *****
2022-06-12 13:08:38,882   Epoch = 4 iter 50499 step
2022-06-12 13:08:38,882   Num examples = 40430
2022-06-12 13:08:38,882   Batch size = 32
2022-06-12 13:09:13,863 ***** Eval results *****
2022-06-12 13:09:13,863   acc = 0.9052188968587682
2022-06-12 13:09:13,863   acc_and_f1 = 0.8887555525418949
2022-06-12 13:09:13,863   cls_loss = 0.08805704908714539
2022-06-12 13:09:13,863   eval_loss = 0.24869874491256108
2022-06-12 13:09:13,863   f1 = 0.8722922082250216
2022-06-12 13:09:13,863   global_step = 50499
2022-06-12 13:09:13,864   loss = 0.08805704908714539
2022-06-12 13:10:33,939 ***** Running evaluation *****
2022-06-12 13:10:33,939   Epoch = 2 iter 31999 step
2022-06-12 13:10:33,939   Num examples = 9815
2022-06-12 13:10:33,939   Batch size = 32
2022-06-12 13:10:42,063 ***** Eval results *****
2022-06-12 13:10:42,063   acc = 0.8249617931737137
2022-06-12 13:10:42,063   cls_loss = 0.22061991890218216
2022-06-12 13:10:42,063   eval_loss = 0.4767371162723641
2022-06-12 13:10:42,063   global_step = 31999
2022-06-12 13:10:42,063   loss = 0.22061991890218216
2022-06-12 13:11:17,469 ***** Running evaluation *****
2022-06-12 13:11:17,470   Epoch = 4 iter 50999 step
2022-06-12 13:11:17,470   Num examples = 40430
2022-06-12 13:11:17,470   Batch size = 32
2022-06-12 13:11:52,468 ***** Eval results *****
2022-06-12 13:11:52,468   acc = 0.9038585209003216
2022-06-12 13:11:52,468   acc_and_f1 = 0.8877809949453253
2022-06-12 13:11:52,468   cls_loss = 0.08807739730526797
2022-06-12 13:11:52,469   eval_loss = 0.2502331411881561
2022-06-12 13:11:52,469   f1 = 0.871703468990329
2022-06-12 13:11:52,469   global_step = 50999
2022-06-12 13:11:52,469   loss = 0.08807739730526797
2022-06-12 13:12:41,532 ***** Running evaluation *****
2022-06-12 13:12:41,533   Epoch = 2 iter 32499 step
2022-06-12 13:12:41,533   Num examples = 9815
2022-06-12 13:12:41,533   Batch size = 32
2022-06-12 13:12:49,659 ***** Eval results *****
2022-06-12 13:12:49,660   acc = 0.8241467142129394
2022-06-12 13:12:49,660   cls_loss = 0.22059610909450528
2022-06-12 13:12:49,660   eval_loss = 0.4767510935615639
2022-06-12 13:12:49,660   global_step = 32499
2022-06-12 13:12:49,660   loss = 0.22059610909450528
2022-06-12 13:13:55,972 ***** Running evaluation *****
2022-06-12 13:13:55,973   Epoch = 4 iter 51499 step
2022-06-12 13:13:55,973   Num examples = 40430
2022-06-12 13:13:55,973   Batch size = 32
2022-06-12 13:14:30,956 ***** Eval results *****
2022-06-12 13:14:30,956   acc = 0.903883255008657
2022-06-12 13:14:30,956   acc_and_f1 = 0.8877356926685609
2022-06-12 13:14:30,956   cls_loss = 0.08810678074914052
2022-06-12 13:14:30,956   eval_loss = 0.24818672284872942
2022-06-12 13:14:30,956   f1 = 0.8715881303284647
2022-06-12 13:14:30,957   global_step = 51499
2022-06-12 13:14:30,957   loss = 0.08810678074914052
2022-06-12 13:14:49,074 ***** Running evaluation *****
2022-06-12 13:14:49,074   Epoch = 2 iter 32999 step
2022-06-12 13:14:49,074   Num examples = 9815
2022-06-12 13:14:49,074   Batch size = 32
2022-06-12 13:14:57,185 ***** Eval results *****
2022-06-12 13:14:57,185   acc = 0.8221090168110036
2022-06-12 13:14:57,185   cls_loss = 0.22055714805526233
2022-06-12 13:14:57,185   eval_loss = 0.4773015912852769
2022-06-12 13:14:57,185   global_step = 32999
2022-06-12 13:14:57,185   loss = 0.22055714805526233
2022-06-12 13:16:34,546 ***** Running evaluation *****
2022-06-12 13:16:34,546   Epoch = 4 iter 51999 step
2022-06-12 13:16:34,546   Num examples = 40430
2022-06-12 13:16:34,546   Batch size = 32
2022-06-12 13:16:56,242 ***** Running evaluation *****
2022-06-12 13:16:56,243   Epoch = 2 iter 33499 step
2022-06-12 13:16:56,243   Num examples = 9815
2022-06-12 13:16:56,243   Batch size = 32
2022-06-12 13:17:04,355 ***** Eval results *****
2022-06-12 13:17:04,355   acc = 0.8236372898624554
2022-06-12 13:17:04,355   cls_loss = 0.22051296720776797
2022-06-12 13:17:04,355   eval_loss = 0.47173668730530754
2022-06-12 13:17:04,355   global_step = 33499
2022-06-12 13:17:04,355   loss = 0.22051296720776797
2022-06-12 13:17:09,573 ***** Eval results *****
2022-06-12 13:17:09,573   acc = 0.9047242146920603
2022-06-12 13:17:09,573   acc_and_f1 = 0.8882731947944529
2022-06-12 13:17:09,573   cls_loss = 0.0880888215140306
2022-06-12 13:17:09,573   eval_loss = 0.24949471982596796
2022-06-12 13:17:09,573   f1 = 0.8718221748968455
2022-06-12 13:17:09,574   global_step = 51999
2022-06-12 13:17:09,574   loss = 0.0880888215140306
2022-06-12 13:19:03,221 ***** Running evaluation *****
2022-06-12 13:19:03,221   Epoch = 2 iter 33999 step
2022-06-12 13:19:03,222   Num examples = 9815
2022-06-12 13:19:03,222   Batch size = 32
2022-06-12 13:19:11,358 ***** Eval results *****
2022-06-12 13:19:11,359   acc = 0.8208863983698421
2022-06-12 13:19:11,359   cls_loss = 0.2205676268693621
2022-06-12 13:19:11,359   eval_loss = 0.47734832419052187
2022-06-12 13:19:11,359   global_step = 33999
2022-06-12 13:19:11,359   loss = 0.2205676268693621
2022-06-12 13:19:13,109 ***** Running evaluation *****
2022-06-12 13:19:13,110   Epoch = 4 iter 52499 step
2022-06-12 13:19:13,110   Num examples = 40430
2022-06-12 13:19:13,110   Batch size = 32
2022-06-12 13:19:48,102 ***** Eval results *****
2022-06-12 13:19:48,102   acc = 0.9047489488003957
2022-06-12 13:19:48,102   acc_and_f1 = 0.8877928879504242
2022-06-12 13:19:48,102   cls_loss = 0.08814968659355016
2022-06-12 13:19:48,102   eval_loss = 0.24557467940988467
2022-06-12 13:19:48,102   f1 = 0.8708368271004527
2022-06-12 13:19:48,102   global_step = 52499
2022-06-12 13:19:48,103   loss = 0.08814968659355016
2022-06-12 13:21:10,303 ***** Running evaluation *****
2022-06-12 13:21:10,303   Epoch = 2 iter 34499 step
2022-06-12 13:21:10,304   Num examples = 9815
2022-06-12 13:21:10,304   Batch size = 32
2022-06-12 13:21:18,423 ***** Eval results *****
2022-06-12 13:21:18,423   acc = 0.8250636780438105
2022-06-12 13:21:18,423   cls_loss = 0.220580217898329
2022-06-12 13:21:18,423   eval_loss = 0.48332938600440756
2022-06-12 13:21:18,423   global_step = 34499
2022-06-12 13:21:18,423   loss = 0.220580217898329
2022-06-12 13:21:51,463 ***** Running evaluation *****
2022-06-12 13:21:51,464   Epoch = 4 iter 52999 step
2022-06-12 13:21:51,464   Num examples = 40430
2022-06-12 13:21:51,464   Batch size = 32
2022-06-12 13:22:26,428 ***** Eval results *****
2022-06-12 13:22:26,429   acc = 0.9049220875587435
2022-06-12 13:22:26,429   acc_and_f1 = 0.8875899758706367
2022-06-12 13:22:26,429   cls_loss = 0.08823678813094883
2022-06-12 13:22:26,429   eval_loss = 0.24894509653124627
2022-06-12 13:22:26,429   f1 = 0.87025786418253
2022-06-12 13:22:26,429   global_step = 52999
2022-06-12 13:22:26,429   loss = 0.08823678813094883
2022-06-12 13:23:17,704 ***** Running evaluation *****
2022-06-12 13:23:17,704   Epoch = 2 iter 34999 step
2022-06-12 13:23:17,704   Num examples = 9815
2022-06-12 13:23:17,704   Batch size = 32
2022-06-12 13:23:25,823 ***** Eval results *****
2022-06-12 13:23:25,824   acc = 0.8242485990830362
2022-06-12 13:23:25,824   cls_loss = 0.22052989205712625
2022-06-12 13:23:25,824   eval_loss = 0.47576994926999366
2022-06-12 13:23:25,824   global_step = 34999
2022-06-12 13:23:25,824   loss = 0.22052989205712625
2022-06-12 13:24:30,169 ***** Running evaluation *****
2022-06-12 13:24:30,170   Epoch = 4 iter 53499 step
2022-06-12 13:24:30,170   Num examples = 40430
2022-06-12 13:24:30,170   Batch size = 32
2022-06-12 13:25:05,170 ***** Eval results *****
2022-06-12 13:25:05,171   acc = 0.9049962898837497
2022-06-12 13:25:05,171   acc_and_f1 = 0.8886198811800256
2022-06-12 13:25:05,171   cls_loss = 0.08826669937408257
2022-06-12 13:25:05,171   eval_loss = 0.2505220257459043
2022-06-12 13:25:05,171   f1 = 0.8722434724763014
2022-06-12 13:25:05,171   global_step = 53499
2022-06-12 13:25:05,171   loss = 0.08826669937408257
2022-06-12 13:25:24,761 ***** Running evaluation *****
2022-06-12 13:25:24,761   Epoch = 2 iter 35499 step
2022-06-12 13:25:24,761   Num examples = 9815
2022-06-12 13:25:24,761   Batch size = 32
2022-06-12 13:25:32,869 ***** Eval results *****
2022-06-12 13:25:32,869   acc = 0.8261844116148752
2022-06-12 13:25:32,869   cls_loss = 0.22053749960650598
2022-06-12 13:25:32,869   eval_loss = 0.4771485319937479
2022-06-12 13:25:32,869   global_step = 35499
2022-06-12 13:25:32,869   loss = 0.22053749960650598
2022-06-12 13:27:08,798 ***** Running evaluation *****
2022-06-12 13:27:08,798   Epoch = 4 iter 53999 step
2022-06-12 13:27:08,799   Num examples = 40430
2022-06-12 13:27:08,799   Batch size = 32
2022-06-12 13:27:32,191 ***** Running evaluation *****
2022-06-12 13:27:32,192   Epoch = 2 iter 35999 step
2022-06-12 13:27:32,192   Num examples = 9815
2022-06-12 13:27:32,192   Batch size = 32
2022-06-12 13:27:40,314 ***** Eval results *****
2022-06-12 13:27:40,314   acc = 0.8250636780438105
2022-06-12 13:27:40,314   cls_loss = 0.22050323953613557
2022-06-12 13:27:40,314   eval_loss = 0.4710634991478066
2022-06-12 13:27:40,314   global_step = 35999
2022-06-12 13:27:40,314   loss = 0.22050323953613557
2022-06-12 13:27:43,843 ***** Eval results *****
2022-06-12 13:27:43,844   acc = 0.905392035617116
2022-06-12 13:27:43,844   acc_and_f1 = 0.888644977921087
2022-06-12 13:27:43,844   cls_loss = 0.08826903885199593
2022-06-12 13:27:43,844   eval_loss = 0.2471591033118105
2022-06-12 13:27:43,844   f1 = 0.8718979202250579
2022-06-12 13:27:43,844   global_step = 53999
2022-06-12 13:27:43,844   loss = 0.08826903885199593
2022-06-12 13:29:39,306 ***** Running evaluation *****
2022-06-12 13:29:39,306   Epoch = 2 iter 36499 step
2022-06-12 13:29:39,306   Num examples = 9815
2022-06-12 13:29:39,307   Batch size = 32
2022-06-12 13:29:47,381 ***** Running evaluation *****
2022-06-12 13:29:47,381   Epoch = 4 iter 54499 step
2022-06-12 13:29:47,381   Num examples = 40430
2022-06-12 13:29:47,381   Batch size = 32
2022-06-12 13:29:47,414 ***** Eval results *****
2022-06-12 13:29:47,414   acc = 0.8248599083036169
2022-06-12 13:29:47,414   cls_loss = 0.22049307187355072
2022-06-12 13:29:47,414   eval_loss = 0.4775494600157784
2022-06-12 13:29:47,414   global_step = 36499
2022-06-12 13:29:47,415   loss = 0.22049307187355072
2022-06-12 13:30:22,378 ***** Eval results *****
2022-06-12 13:30:22,378   acc = 0.9022013356418501
2022-06-12 13:30:22,378   acc_and_f1 = 0.8870371551832064
2022-06-12 13:30:22,378   cls_loss = 0.08831199730185424
2022-06-12 13:30:22,378   eval_loss = 0.25115423155889577
2022-06-12 13:30:22,378   f1 = 0.8718729747245625
2022-06-12 13:30:22,378   global_step = 54499
2022-06-12 13:30:22,378   loss = 0.08831199730185424
2022-06-12 13:31:46,517 ***** Running evaluation *****
2022-06-12 13:31:46,517   Epoch = 3 iter 36999 step
2022-06-12 13:31:46,517   Num examples = 9815
2022-06-12 13:31:46,517   Batch size = 32
2022-06-12 13:31:54,633 ***** Eval results *****
2022-06-12 13:31:54,633   acc = 0.8227203260315843
2022-06-12 13:31:54,633   cls_loss = 0.2161329317317214
2022-06-12 13:31:54,633   eval_loss = 0.4753443743390447
2022-06-12 13:31:54,633   global_step = 36999
2022-06-12 13:31:54,634   loss = 0.2161329317317214
2022-06-12 13:32:25,606 ***** Running evaluation *****
2022-06-12 13:32:25,607   Epoch = 4 iter 54999 step
2022-06-12 13:32:25,607   Num examples = 40430
2022-06-12 13:32:25,607   Batch size = 32
2022-06-12 13:33:00,746 ***** Eval results *****
2022-06-12 13:33:00,747   acc = 0.9056888449171407
2022-06-12 13:33:00,747   acc_and_f1 = 0.8888356932324498
2022-06-12 13:33:00,747   cls_loss = 0.08834139223152104
2022-06-12 13:33:00,747   eval_loss = 0.25225774503513415
2022-06-12 13:33:00,747   f1 = 0.8719825415477589
2022-06-12 13:33:00,747   global_step = 54999
2022-06-12 13:33:00,747   loss = 0.08834139223152104
2022-06-12 13:33:53,559 ***** Running evaluation *****
2022-06-12 13:33:53,560   Epoch = 3 iter 37499 step
2022-06-12 13:33:53,560   Num examples = 9815
2022-06-12 13:33:53,560   Batch size = 32
2022-06-12 13:34:01,661 ***** Eval results *****
2022-06-12 13:34:01,661   acc = 0.8288334182373918
2022-06-12 13:34:01,662   cls_loss = 0.21690435136285538
2022-06-12 13:34:01,662   eval_loss = 0.4722566718678521
2022-06-12 13:34:01,662   global_step = 37499
2022-06-12 13:34:01,662   loss = 0.21690435136285538
2022-06-12 13:34:01,662 ***** Save model *****
2022-06-12 13:35:04,286 ***** Running evaluation *****
2022-06-12 13:35:04,287   Epoch = 4 iter 55499 step
2022-06-12 13:35:04,287   Num examples = 40430
2022-06-12 13:35:04,287   Batch size = 32
2022-06-12 13:35:39,307 ***** Eval results *****
2022-06-12 13:35:39,307   acc = 0.9050210239920851
2022-06-12 13:35:39,307   acc_and_f1 = 0.8891148836529137
2022-06-12 13:35:39,307   cls_loss = 0.0883271607953978
2022-06-12 13:35:39,308   eval_loss = 0.2479140941846432
2022-06-12 13:35:39,308   f1 = 0.8732087433137423
2022-06-12 13:35:39,308   global_step = 55499
2022-06-12 13:35:39,308   loss = 0.0883271607953978
2022-06-12 13:36:00,462 ***** Running evaluation *****
2022-06-12 13:36:00,463   Epoch = 3 iter 37999 step
2022-06-12 13:36:00,463   Num examples = 9815
2022-06-12 13:36:00,463   Batch size = 32
2022-06-12 13:36:08,559 ***** Eval results *****
2022-06-12 13:36:08,559   acc = 0.8244523688232297
2022-06-12 13:36:08,559   cls_loss = 0.21716750805182963
2022-06-12 13:36:08,560   eval_loss = 0.47992399538767067
2022-06-12 13:36:08,560   global_step = 37999
2022-06-12 13:36:08,560   loss = 0.21716750805182963
2022-06-12 13:37:42,546 ***** Running evaluation *****
2022-06-12 13:37:42,547   Epoch = 4 iter 55999 step
2022-06-12 13:37:42,547   Num examples = 40430
2022-06-12 13:37:42,547   Batch size = 32
2022-06-12 13:38:07,418 ***** Running evaluation *****
2022-06-12 13:38:07,418   Epoch = 3 iter 38499 step
2022-06-12 13:38:07,418   Num examples = 9815
2022-06-12 13:38:07,418   Batch size = 32
2022-06-12 13:38:15,517 ***** Eval results *****
2022-06-12 13:38:15,517   acc = 0.8248599083036169
2022-06-12 13:38:15,517   cls_loss = 0.21726755416053894
2022-06-12 13:38:15,517   eval_loss = 0.473565709804479
2022-06-12 13:38:15,517   global_step = 38499
2022-06-12 13:38:15,518   loss = 0.21726755416053894
2022-06-12 13:38:17,554 ***** Eval results *****
2022-06-12 13:38:17,555   acc = 0.904575810042048
2022-06-12 13:38:17,555   acc_and_f1 = 0.8884982224813415
2022-06-12 13:38:17,555   cls_loss = 0.08840266105603192
2022-06-12 13:38:17,555   eval_loss = 0.2462718342371945
2022-06-12 13:38:17,555   f1 = 0.8724206349206349
2022-06-12 13:38:17,555   global_step = 55999
2022-06-12 13:38:17,555   loss = 0.08840266105603192
2022-06-12 13:40:13,857 ***** Running evaluation *****
2022-06-12 13:40:13,857   Epoch = 3 iter 38999 step
2022-06-12 13:40:13,857   Num examples = 9815
2022-06-12 13:40:13,857   Batch size = 32
2022-06-12 13:40:21,189 ***** Running evaluation *****
2022-06-12 13:40:21,189   Epoch = 4 iter 56499 step
2022-06-12 13:40:21,189   Num examples = 40430
2022-06-12 13:40:21,189   Batch size = 32
2022-06-12 13:40:21,962 ***** Eval results *****
2022-06-12 13:40:21,962   acc = 0.825776872134488
2022-06-12 13:40:21,962   cls_loss = 0.21733910724016253
2022-06-12 13:40:21,962   eval_loss = 0.4752359645568587
2022-06-12 13:40:21,962   global_step = 38999
2022-06-12 13:40:21,962   loss = 0.21733910724016253
2022-06-12 13:40:56,124 ***** Eval results *****
2022-06-12 13:40:56,124   acc = 0.9058372495671531
2022-06-12 13:40:56,124   acc_and_f1 = 0.8897822023483383
2022-06-12 13:40:56,124   cls_loss = 0.08836179827402914
2022-06-12 13:40:56,125   eval_loss = 0.24838959219382156
2022-06-12 13:40:56,125   f1 = 0.8737271551295235
2022-06-12 13:40:56,125   global_step = 56499
2022-06-12 13:40:56,125   loss = 0.08836179827402914
2022-06-12 13:40:56,125 ***** Save model *****
2022-06-12 13:42:20,665 ***** Running evaluation *****
2022-06-12 13:42:20,665   Epoch = 3 iter 39499 step
2022-06-12 13:42:20,665   Num examples = 9815
2022-06-12 13:42:20,665   Batch size = 32
2022-06-12 13:42:28,768 ***** Eval results *****
2022-06-12 13:42:28,768   acc = 0.8235354049923587
2022-06-12 13:42:28,768   cls_loss = 0.21747669851655668
2022-06-12 13:42:28,769   eval_loss = 0.47612273605716343
2022-06-12 13:42:28,769   global_step = 39499
2022-06-12 13:42:28,769   loss = 0.21747669851655668
2022-06-12 13:43:00,060 ***** Running evaluation *****
2022-06-12 13:43:00,061   Epoch = 5 iter 56999 step
2022-06-12 13:43:00,061   Num examples = 40430
2022-06-12 13:43:00,061   Batch size = 32
2022-06-12 13:43:34,993 ***** Eval results *****
2022-06-12 13:43:34,994   acc = 0.9051941627504329
2022-06-12 13:43:34,994   acc_and_f1 = 0.8895397984482267
2022-06-12 13:43:34,994   cls_loss = 0.08618778337868269
2022-06-12 13:43:34,994   eval_loss = 0.24810751525779504
2022-06-12 13:43:34,994   f1 = 0.8738854341460205
2022-06-12 13:43:34,994   global_step = 56999
2022-06-12 13:43:34,994   loss = 0.08618778337868269
2022-06-12 13:43:34,994 ***** Save model *****
2022-06-12 13:44:27,225 ***** Running evaluation *****
2022-06-12 13:44:27,226   Epoch = 3 iter 39999 step
2022-06-12 13:44:27,226   Num examples = 9815
2022-06-12 13:44:27,226   Batch size = 32
2022-06-12 13:44:35,328 ***** Eval results *****
2022-06-12 13:44:35,328   acc = 0.8243504839531329
2022-06-12 13:44:35,328   cls_loss = 0.21753905393178555
2022-06-12 13:44:35,328   eval_loss = 0.4815331015019929
2022-06-12 13:44:35,328   global_step = 39999
2022-06-12 13:44:35,328   loss = 0.21753905393178555
2022-06-12 13:45:38,774 ***** Running evaluation *****
2022-06-12 13:45:38,774   Epoch = 5 iter 57499 step
2022-06-12 13:45:38,775   Num examples = 40430
2022-06-12 13:45:38,775   Batch size = 32
2022-06-12 13:46:13,748 ***** Eval results *****
2022-06-12 13:46:13,748   acc = 0.9056641108088054
2022-06-12 13:46:13,748   acc_and_f1 = 0.8892017584407723
2022-06-12 13:46:13,748   cls_loss = 0.08686067790757343
2022-06-12 13:46:13,748   eval_loss = 0.244193097880063
2022-06-12 13:46:13,749   f1 = 0.8727394060727394
2022-06-12 13:46:13,749   global_step = 57499
2022-06-12 13:46:13,749   loss = 0.08686067790757343
2022-06-12 13:46:33,956 ***** Running evaluation *****
2022-06-12 13:46:33,957   Epoch = 3 iter 40499 step
2022-06-12 13:46:33,957   Num examples = 9815
2022-06-12 13:46:33,957   Batch size = 32
2022-06-12 13:46:42,070 ***** Eval results *****
2022-06-12 13:46:42,070   acc = 0.8258787570045848
2022-06-12 13:46:42,070   cls_loss = 0.21766349179665162
2022-06-12 13:46:42,070   eval_loss = 0.47040688205231285
2022-06-12 13:46:42,071   global_step = 40499
2022-06-12 13:46:42,071   loss = 0.21766349179665162
2022-06-12 13:48:16,920 ***** Running evaluation *****
2022-06-12 13:48:16,921   Epoch = 5 iter 57999 step
2022-06-12 13:48:16,921   Num examples = 40430
2022-06-12 13:48:16,921   Batch size = 32
2022-06-12 13:48:40,527 ***** Running evaluation *****
2022-06-12 13:48:40,528   Epoch = 3 iter 40999 step
2022-06-12 13:48:40,528   Num examples = 9815
2022-06-12 13:48:40,528   Batch size = 32
2022-06-12 13:48:48,632 ***** Eval results *****
2022-06-12 13:48:48,632   acc = 0.8252674477840041
2022-06-12 13:48:48,632   cls_loss = 0.2176689797726705
2022-06-12 13:48:48,632   eval_loss = 0.47576507673589724
2022-06-12 13:48:48,633   global_step = 40999
2022-06-12 13:48:48,633   loss = 0.2176689797726705
2022-06-12 13:48:51,987 ***** Eval results *****
2022-06-12 13:48:51,987   acc = 0.9063813999505318
2022-06-12 13:48:51,987   acc_and_f1 = 0.8897882409314118
2022-06-12 13:48:51,988   cls_loss = 0.08751279153934866
2022-06-12 13:48:51,988   eval_loss = 0.24647785718641185
2022-06-12 13:48:51,988   f1 = 0.8731950819122919
2022-06-12 13:48:51,988   global_step = 57999
2022-06-12 13:48:51,988   loss = 0.08751279153934866
2022-06-12 13:50:46,959 ***** Running evaluation *****
2022-06-12 13:50:46,959   Epoch = 3 iter 41499 step
2022-06-12 13:50:46,959   Num examples = 9815
2022-06-12 13:50:46,959   Batch size = 32
2022-06-12 13:50:55,062 ***** Eval results *****
2022-06-12 13:50:55,063   acc = 0.8237391747325522
2022-06-12 13:50:55,063   cls_loss = 0.2176171556414064
2022-06-12 13:50:55,063   eval_loss = 0.4728743742168532
2022-06-12 13:50:55,063   global_step = 41499
2022-06-12 13:50:55,063   loss = 0.2176171556414064
2022-06-12 13:50:55,446 ***** Running evaluation *****
2022-06-12 13:50:55,446   Epoch = 5 iter 58499 step
2022-06-12 13:50:55,446   Num examples = 40430
2022-06-12 13:50:55,446   Batch size = 32
2022-06-12 13:51:30,356 ***** Eval results *****
2022-06-12 13:51:30,356   acc = 0.9052683650754391
2022-06-12 13:51:30,356   acc_and_f1 = 0.8888008492043862
2022-06-12 13:51:30,356   cls_loss = 0.08700653336820204
2022-06-12 13:51:30,356   eval_loss = 0.25079921566250535
2022-06-12 13:51:30,357   f1 = 0.8723333333333333
2022-06-12 13:51:30,357   global_step = 58499
2022-06-12 13:51:30,357   loss = 0.08700653336820204
2022-06-12 13:52:53,637 ***** Running evaluation *****
2022-06-12 13:52:53,638   Epoch = 3 iter 41999 step
2022-06-12 13:52:53,638   Num examples = 9815
2022-06-12 13:52:53,638   Batch size = 32
2022-06-12 13:53:01,751 ***** Eval results *****
2022-06-12 13:53:01,751   acc = 0.8241467142129394
2022-06-12 13:53:01,752   cls_loss = 0.217634888345033
2022-06-12 13:53:01,752   eval_loss = 0.47662837759679616
2022-06-12 13:53:01,752   global_step = 41999
2022-06-12 13:53:01,752   loss = 0.217634888345033
2022-06-12 13:53:34,166 ***** Running evaluation *****
2022-06-12 13:53:34,166   Epoch = 5 iter 58999 step
2022-06-12 13:53:34,166   Num examples = 40430
2022-06-12 13:53:34,167   Batch size = 32
2022-06-12 13:54:09,159 ***** Eval results *****
2022-06-12 13:54:09,159   acc = 0.9047736829087312
2022-06-12 13:54:09,159   acc_and_f1 = 0.8888177244871904
2022-06-12 13:54:09,159   cls_loss = 0.08731028978986316
2022-06-12 13:54:09,159   eval_loss = 0.2451625476485189
2022-06-12 13:54:09,159   f1 = 0.8728617660656496
2022-06-12 13:54:09,160   global_step = 58999
2022-06-12 13:54:09,160   loss = 0.08731028978986316
2022-06-12 13:55:00,657 ***** Running evaluation *****
2022-06-12 13:55:00,657   Epoch = 3 iter 42499 step
2022-06-12 13:55:00,657   Num examples = 9815
2022-06-12 13:55:00,658   Batch size = 32
2022-06-12 13:55:08,781 ***** Eval results *****
2022-06-12 13:55:08,781   acc = 0.8279164544065206
2022-06-12 13:55:08,782   cls_loss = 0.21770086704982322
2022-06-12 13:55:08,782   eval_loss = 0.47071198832716926
2022-06-12 13:55:08,782   global_step = 42499
2022-06-12 13:55:08,782   loss = 0.21770086704982322
2022-06-12 13:56:12,782 ***** Running evaluation *****
2022-06-12 13:56:12,782   Epoch = 5 iter 59499 step
2022-06-12 13:56:12,782   Num examples = 40430
2022-06-12 13:56:12,782   Batch size = 32
2022-06-12 13:56:47,771 ***** Eval results *****
2022-06-12 13:56:47,772   acc = 0.9065545387088796
2022-06-12 13:56:47,772   acc_and_f1 = 0.8898028069888484
2022-06-12 13:56:47,772   cls_loss = 0.08701293096058817
2022-06-12 13:56:47,772   eval_loss = 0.2484556417028078
2022-06-12 13:56:47,772   f1 = 0.8730510752688172
2022-06-12 13:56:47,772   global_step = 59499
2022-06-12 13:56:47,772   loss = 0.08701293096058817
2022-06-12 13:57:07,776 ***** Running evaluation *****
2022-06-12 13:57:07,777   Epoch = 3 iter 42999 step
2022-06-12 13:57:07,777   Num examples = 9815
2022-06-12 13:57:07,777   Batch size = 32
2022-06-12 13:57:15,877 ***** Eval results *****
2022-06-12 13:57:15,877   acc = 0.8273051451859399
2022-06-12 13:57:15,878   cls_loss = 0.2176370606708018
2022-06-12 13:57:15,878   eval_loss = 0.4770212706408982
2022-06-12 13:57:15,878   global_step = 42999
2022-06-12 13:57:15,878   loss = 0.2176370606708018
2022-06-12 13:58:50,678 ***** Running evaluation *****
2022-06-12 13:58:50,679   Epoch = 5 iter 59999 step
2022-06-12 13:58:50,679   Num examples = 40430
2022-06-12 13:58:50,679   Batch size = 32
2022-06-12 13:59:15,375 ***** Running evaluation *****
2022-06-12 13:59:15,375   Epoch = 3 iter 43499 step
2022-06-12 13:59:15,375   Num examples = 9815
2022-06-12 13:59:15,375   Batch size = 32
2022-06-12 13:59:23,479 ***** Eval results *****
2022-06-12 13:59:23,480   acc = 0.8274070300560367
2022-06-12 13:59:23,480   cls_loss = 0.2177703662160054
2022-06-12 13:59:23,480   eval_loss = 0.4850475030923899
2022-06-12 13:59:23,480   global_step = 43499
2022-06-12 13:59:23,480   loss = 0.2177703662160054
2022-06-12 13:59:25,685 ***** Eval results *****
2022-06-12 13:59:25,685   acc = 0.9067276774672273
2022-06-12 13:59:25,685   acc_and_f1 = 0.8904530249469532
2022-06-12 13:59:25,685   cls_loss = 0.08678098835663857
2022-06-12 13:59:25,685   eval_loss = 0.24836172777687823
2022-06-12 13:59:25,685   f1 = 0.8741783724266792
2022-06-12 13:59:25,685   global_step = 59999
2022-06-12 13:59:25,685   loss = 0.08678098835663857
2022-06-12 13:59:25,685 ***** Save model *****
2022-06-12 14:01:23,005 ***** Running evaluation *****
2022-06-12 14:01:23,005   Epoch = 3 iter 43999 step
2022-06-12 14:01:23,005   Num examples = 9815
2022-06-12 14:01:23,005   Batch size = 32
2022-06-12 14:01:30,155 ***** Running evaluation *****
2022-06-12 14:01:30,155   Epoch = 5 iter 60499 step
2022-06-12 14:01:30,155   Num examples = 40430
2022-06-12 14:01:30,155   Batch size = 32
2022-06-12 14:01:31,109 ***** Eval results *****
2022-06-12 14:01:31,109   acc = 0.8278145695364238
2022-06-12 14:01:31,109   cls_loss = 0.21777400333354507
2022-06-12 14:01:31,109   eval_loss = 0.47146066926201313
2022-06-12 14:01:31,109   global_step = 43999
2022-06-12 14:01:31,109   loss = 0.21777400333354507
2022-06-12 14:02:05,120 ***** Eval results *****
2022-06-12 14:02:05,121   acc = 0.9062329953005194
2022-06-12 14:02:05,121   acc_and_f1 = 0.8894770818369309
2022-06-12 14:02:05,121   cls_loss = 0.08659887062801698
2022-06-12 14:02:05,121   eval_loss = 0.24568474171182164
2022-06-12 14:02:05,121   f1 = 0.8727211683733422
2022-06-12 14:02:05,121   global_step = 60499
2022-06-12 14:02:05,121   loss = 0.08659887062801698
2022-06-12 14:03:30,036 ***** Running evaluation *****
2022-06-12 14:03:30,037   Epoch = 3 iter 44499 step
2022-06-12 14:03:30,037   Num examples = 9815
2022-06-12 14:03:30,037   Batch size = 32
2022-06-12 14:03:38,147 ***** Eval results *****
2022-06-12 14:03:38,147   acc = 0.8283239938869078
2022-06-12 14:03:38,147   cls_loss = 0.21777637539208014
2022-06-12 14:03:38,148   eval_loss = 0.4761202018890008
2022-06-12 14:03:38,148   global_step = 44499
2022-06-12 14:03:38,148   loss = 0.21777637539208014
2022-06-12 14:04:08,969 ***** Running evaluation *****
2022-06-12 14:04:08,969   Epoch = 5 iter 60999 step
2022-06-12 14:04:08,969   Num examples = 40430
2022-06-12 14:04:08,969   Batch size = 32
2022-06-12 14:04:43,929 ***** Eval results *****
2022-06-12 14:04:43,929   acc = 0.9065298046005441
2022-06-12 14:04:43,929   acc_and_f1 = 0.8901658874396943
2022-06-12 14:04:43,929   cls_loss = 0.08666691359040048
2022-06-12 14:04:43,930   eval_loss = 0.2453228363684744
2022-06-12 14:04:43,930   f1 = 0.8738019702788445
2022-06-12 14:04:43,930   global_step = 60999
2022-06-12 14:04:43,930   loss = 0.08666691359040048
2022-06-12 14:05:36,997 ***** Running evaluation *****
2022-06-12 14:05:36,997   Epoch = 3 iter 44999 step
2022-06-12 14:05:36,997   Num examples = 9815
2022-06-12 14:05:36,997   Batch size = 32
2022-06-12 14:05:45,104 ***** Eval results *****
2022-06-12 14:05:45,105   acc = 0.8242485990830362
2022-06-12 14:05:45,105   cls_loss = 0.21775251731636935
2022-06-12 14:05:45,105   eval_loss = 0.4782050967410644
2022-06-12 14:05:45,105   global_step = 44999
2022-06-12 14:05:45,105   loss = 0.21775251731636935
2022-06-12 14:06:47,221 ***** Running evaluation *****
2022-06-12 14:06:47,222   Epoch = 5 iter 61499 step
2022-06-12 14:06:47,222   Num examples = 40430
2022-06-12 14:06:47,222   Batch size = 32
2022-06-12 14:07:22,167 ***** Eval results *****
2022-06-12 14:07:22,167   acc = 0.9059361860004946
2022-06-12 14:07:22,167   acc_and_f1 = 0.8896796922680124
2022-06-12 14:07:22,167   cls_loss = 0.0866808832560362
2022-06-12 14:07:22,167   eval_loss = 0.24577772733460687
2022-06-12 14:07:22,167   f1 = 0.8734231985355301
2022-06-12 14:07:22,167   global_step = 61499
2022-06-12 14:07:22,168   loss = 0.0866808832560362
2022-06-12 14:07:43,780 ***** Running evaluation *****
2022-06-12 14:07:43,780   Epoch = 3 iter 45499 step
2022-06-12 14:07:43,781   Num examples = 9815
2022-06-12 14:07:43,781   Batch size = 32
2022-06-12 14:07:51,885 ***** Eval results *****
2022-06-12 14:07:51,886   acc = 0.8284258787570046
2022-06-12 14:07:51,886   cls_loss = 0.21776823617471627
2022-06-12 14:07:51,886   eval_loss = 0.4741162225749671
2022-06-12 14:07:51,886   global_step = 45499
2022-06-12 14:07:51,886   loss = 0.21776823617471627
2022-06-12 14:09:25,926 ***** Running evaluation *****
2022-06-12 14:09:25,926   Epoch = 5 iter 61999 step
2022-06-12 14:09:25,926   Num examples = 40430
2022-06-12 14:09:25,926   Batch size = 32
2022-06-12 14:09:50,548 ***** Running evaluation *****
2022-06-12 14:09:50,548   Epoch = 3 iter 45999 step
2022-06-12 14:09:50,548   Num examples = 9815
2022-06-12 14:09:50,548   Batch size = 32
2022-06-12 14:09:58,661 ***** Eval results *****
2022-06-12 14:09:58,661   acc = 0.8276107997962303
2022-06-12 14:09:58,661   cls_loss = 0.2177107019437935
2022-06-12 14:09:58,661   eval_loss = 0.47292442110151733
2022-06-12 14:09:58,661   global_step = 45999
2022-06-12 14:09:58,662   loss = 0.2177107019437935
2022-06-12 14:10:00,848 ***** Eval results *****
2022-06-12 14:10:00,849   acc = 0.9065298046005441
2022-06-12 14:10:00,849   acc_and_f1 = 0.8901321547978496
2022-06-12 14:10:00,849   cls_loss = 0.08664090866445454
2022-06-12 14:10:00,849   eval_loss = 0.2472630449632607
2022-06-12 14:10:00,849   f1 = 0.8737345049951551
2022-06-12 14:10:00,849   global_step = 61999
2022-06-12 14:10:00,849   loss = 0.08664090866445454
2022-06-12 14:11:57,618 ***** Running evaluation *****
2022-06-12 14:11:57,618   Epoch = 3 iter 46499 step
2022-06-12 14:11:57,618   Num examples = 9815
2022-06-12 14:11:57,619   Batch size = 32
2022-06-12 14:12:03,791 ***** Running evaluation *****
2022-06-12 14:12:03,791   Epoch = 5 iter 62499 step
2022-06-12 14:12:03,791   Num examples = 40430
2022-06-12 14:12:03,791   Batch size = 32
2022-06-12 14:12:05,731 ***** Eval results *****
2022-06-12 14:12:05,732   acc = 0.823841059602649
2022-06-12 14:12:05,732   cls_loss = 0.2177048555932802
2022-06-12 14:12:05,732   eval_loss = 0.4765884807327283
2022-06-12 14:12:05,732   global_step = 46499
2022-06-12 14:12:05,732   loss = 0.2177048555932802
2022-06-12 14:12:38,775 ***** Eval results *****
2022-06-12 14:12:38,775   acc = 0.9064061340588672
2022-06-12 14:12:38,775   acc_and_f1 = 0.8901994040257706
2022-06-12 14:12:38,775   cls_loss = 0.08671090194510662
2022-06-12 14:12:38,775   eval_loss = 0.24401909950678533
2022-06-12 14:12:38,775   f1 = 0.8739926739926741
2022-06-12 14:12:38,775   global_step = 62499
2022-06-12 14:12:38,776   loss = 0.08671090194510662
2022-06-12 14:14:04,306 ***** Running evaluation *****
2022-06-12 14:14:04,306   Epoch = 3 iter 46999 step
2022-06-12 14:14:04,307   Num examples = 9815
2022-06-12 14:14:04,307   Batch size = 32
2022-06-12 14:14:12,413 ***** Eval results *****
2022-06-12 14:14:12,413   acc = 0.8230259806418747
2022-06-12 14:14:12,413   cls_loss = 0.2177523015911444
2022-06-12 14:14:12,413   eval_loss = 0.4793709970840802
2022-06-12 14:14:12,413   global_step = 46999
2022-06-12 14:14:12,413   loss = 0.2177523015911444
2022-06-12 14:14:41,616 ***** Running evaluation *****
2022-06-12 14:14:41,617   Epoch = 5 iter 62999 step
2022-06-12 14:14:41,617   Num examples = 40430
2022-06-12 14:14:41,617   Batch size = 32
2022-06-12 14:15:16,574 ***** Eval results *****
2022-06-12 14:15:16,574   acc = 0.9064803363838734
2022-06-12 14:15:16,574   acc_and_f1 = 0.8905806607178721
2022-06-12 14:15:16,574   cls_loss = 0.08684921546483702
2022-06-12 14:15:16,574   eval_loss = 0.24397735470976634
2022-06-12 14:15:16,574   f1 = 0.874680985051871
2022-06-12 14:15:16,574   global_step = 62999
2022-06-12 14:15:16,574   loss = 0.08684921546483702
2022-06-12 14:15:16,574 ***** Save model *****
2022-06-12 14:16:10,975 ***** Running evaluation *****
2022-06-12 14:16:10,975   Epoch = 3 iter 47499 step
2022-06-12 14:16:10,975   Num examples = 9815
2022-06-12 14:16:10,976   Batch size = 32
2022-06-12 14:16:19,077 ***** Eval results *****
2022-06-12 14:16:19,077   acc = 0.828222109016811
2022-06-12 14:16:19,077   cls_loss = 0.21772389456242094
2022-06-12 14:16:19,077   eval_loss = 0.47033084108697476
2022-06-12 14:16:19,078   global_step = 47499
2022-06-12 14:16:19,078   loss = 0.21772389456242094
2022-06-12 14:17:20,525 ***** Running evaluation *****
2022-06-12 14:17:20,526   Epoch = 5 iter 63499 step
2022-06-12 14:17:20,526   Num examples = 40430
2022-06-12 14:17:20,526   Batch size = 32
2022-06-12 14:17:55,489 ***** Eval results *****
2022-06-12 14:17:55,490   acc = 0.9071234232005936
2022-06-12 14:17:55,490   acc_and_f1 = 0.8900145801059662
2022-06-12 14:17:55,490   cls_loss = 0.08690803181080285
2022-06-12 14:17:55,490   eval_loss = 0.24786671128012003
2022-06-12 14:17:55,490   f1 = 0.8729057370113388
2022-06-12 14:17:55,490   global_step = 63499
2022-06-12 14:17:55,490   loss = 0.08690803181080285
2022-06-12 14:18:17,740 ***** Running evaluation *****
2022-06-12 14:18:17,741   Epoch = 3 iter 47999 step
2022-06-12 14:18:17,741   Num examples = 9815
2022-06-12 14:18:17,741   Batch size = 32
2022-06-12 14:18:25,852 ***** Eval results *****
2022-06-12 14:18:25,853   acc = 0.8237391747325522
2022-06-12 14:18:25,853   cls_loss = 0.21771185470009968
2022-06-12 14:18:25,853   eval_loss = 0.4751780468011912
2022-06-12 14:18:25,853   global_step = 47999
2022-06-12 14:18:25,853   loss = 0.21771185470009968
2022-06-12 14:19:58,537 ***** Running evaluation *****
2022-06-12 14:19:58,538   Epoch = 5 iter 63999 step
2022-06-12 14:19:58,538   Num examples = 40430
2022-06-12 14:19:58,538   Batch size = 32
2022-06-12 14:20:24,519 ***** Running evaluation *****
2022-06-12 14:20:24,519   Epoch = 3 iter 48499 step
2022-06-12 14:20:24,519   Num examples = 9815
2022-06-12 14:20:24,519   Batch size = 32
2022-06-12 14:20:32,624 ***** Eval results *****
2022-06-12 14:20:32,624   acc = 0.8267957208354559
2022-06-12 14:20:32,624   cls_loss = 0.21770924924605517
2022-06-12 14:20:32,624   eval_loss = 0.4742652730172931
2022-06-12 14:20:32,624   global_step = 48499
2022-06-12 14:20:32,625   loss = 0.21770924924605517
2022-06-12 14:20:33,453 ***** Eval results *****
2022-06-12 14:20:33,453   acc = 0.9068760821172397
2022-06-12 14:20:33,453   acc_and_f1 = 0.8904339244549851
2022-06-12 14:20:33,454   cls_loss = 0.08689972202339562
2022-06-12 14:20:33,454   eval_loss = 0.24521199170547195
2022-06-12 14:20:33,454   f1 = 0.8739917667927306
2022-06-12 14:20:33,454   global_step = 63999
2022-06-12 14:20:33,454   loss = 0.08689972202339562
2022-06-12 14:22:31,404 ***** Running evaluation *****
2022-06-12 14:22:31,404   Epoch = 3 iter 48999 step
2022-06-12 14:22:31,404   Num examples = 9815
2022-06-12 14:22:31,404   Batch size = 32
2022-06-12 14:22:37,141 ***** Running evaluation *****
2022-06-12 14:22:37,142   Epoch = 5 iter 64499 step
2022-06-12 14:22:37,142   Num examples = 40430
2022-06-12 14:22:37,142   Batch size = 32
2022-06-12 14:22:39,506 ***** Eval results *****
2022-06-12 14:22:39,507   acc = 0.8264900662251655
2022-06-12 14:22:39,507   cls_loss = 0.2176882078458127
2022-06-12 14:22:39,507   eval_loss = 0.47298091902406675
2022-06-12 14:22:39,507   global_step = 48999
2022-06-12 14:22:39,507   loss = 0.2176882078458127
2022-06-12 14:23:12,056 ***** Eval results *****
2022-06-12 14:23:12,056   acc = 0.906331931733861
2022-06-12 14:23:12,056   acc_and_f1 = 0.8900261479336883
2022-06-12 14:23:12,057   cls_loss = 0.08682307534544183
2022-06-12 14:23:12,057   eval_loss = 0.2474345948231772
2022-06-12 14:23:12,057   f1 = 0.8737203641335156
2022-06-12 14:23:12,057   global_step = 64499
2022-06-12 14:23:12,057   loss = 0.08682307534544183
2022-06-12 14:24:38,231 ***** Running evaluation *****
2022-06-12 14:24:38,231   Epoch = 4 iter 49499 step
2022-06-12 14:24:38,231   Num examples = 9815
2022-06-12 14:24:38,231   Batch size = 32
2022-06-12 14:24:46,335 ***** Eval results *****
2022-06-12 14:24:46,336   acc = 0.8253693326541008
2022-06-12 14:24:46,336   cls_loss = 0.21615345370338623
2022-06-12 14:24:46,336   eval_loss = 0.47213792558213397
2022-06-12 14:24:46,336   global_step = 49499
2022-06-12 14:24:46,336   loss = 0.21615345370338623
2022-06-12 14:25:15,502 ***** Running evaluation *****
2022-06-12 14:25:15,503   Epoch = 5 iter 64999 step
2022-06-12 14:25:15,503   Num examples = 40430
2022-06-12 14:25:15,503   Batch size = 32
2022-06-12 14:25:50,415 ***** Eval results *****
2022-06-12 14:25:50,415   acc = 0.9071728914172644
2022-06-12 14:25:50,415   acc_and_f1 = 0.8906525489383333
2022-06-12 14:25:50,415   cls_loss = 0.08674229705492065
2022-06-12 14:25:50,416   eval_loss = 0.24531090649174928
2022-06-12 14:25:50,416   f1 = 0.8741322064594023
2022-06-12 14:25:50,416   global_step = 64999
2022-06-12 14:25:50,416   loss = 0.08674229705492065
2022-06-12 14:26:45,057 ***** Running evaluation *****
2022-06-12 14:26:45,057   Epoch = 4 iter 49999 step
2022-06-12 14:26:45,057   Num examples = 9815
2022-06-12 14:26:45,057   Batch size = 32
2022-06-12 14:26:53,158 ***** Eval results *****
2022-06-12 14:26:53,158   acc = 0.8266938359653592
2022-06-12 14:26:53,158   cls_loss = 0.21585319781563972
2022-06-12 14:26:53,158   eval_loss = 0.4719000797512477
2022-06-12 14:26:53,158   global_step = 49999
2022-06-12 14:26:53,159   loss = 0.21585319781563972
2022-06-12 14:27:54,348 ***** Running evaluation *****
2022-06-12 14:27:54,349   Epoch = 5 iter 65499 step
2022-06-12 14:27:54,349   Num examples = 40430
2022-06-12 14:27:54,349   Batch size = 32
2022-06-12 14:28:29,305 ***** Eval results *****
2022-06-12 14:28:29,305   acc = 0.9060845906505071
2022-06-12 14:28:29,305   acc_and_f1 = 0.8898411235152732
2022-06-12 14:28:29,305   cls_loss = 0.0866627159071549
2022-06-12 14:28:29,305   eval_loss = 0.24611688588003192
2022-06-12 14:28:29,306   f1 = 0.8735976563800393
2022-06-12 14:28:29,306   global_step = 65499
2022-06-12 14:28:29,306   loss = 0.0866627159071549
2022-06-12 14:28:51,846 ***** Running evaluation *****
2022-06-12 14:28:51,846   Epoch = 4 iter 50499 step
2022-06-12 14:28:51,846   Num examples = 9815
2022-06-12 14:28:51,847   Batch size = 32
2022-06-12 14:28:59,953 ***** Eval results *****
2022-06-12 14:28:59,953   acc = 0.8277126846663271
2022-06-12 14:28:59,953   cls_loss = 0.21585308302838896
2022-06-12 14:28:59,953   eval_loss = 0.46977844290702275
2022-06-12 14:28:59,953   global_step = 50499
2022-06-12 14:28:59,953   loss = 0.21585308302838896
2022-06-12 14:30:32,329 ***** Running evaluation *****
2022-06-12 14:30:32,329   Epoch = 5 iter 65999 step
2022-06-12 14:30:32,330   Num examples = 40430
2022-06-12 14:30:32,330   Batch size = 32
2022-06-12 14:30:58,493 ***** Running evaluation *****
2022-06-12 14:30:58,493   Epoch = 4 iter 50999 step
2022-06-12 14:30:58,493   Num examples = 9815
2022-06-12 14:30:58,493   Batch size = 32
2022-06-12 14:31:06,597 ***** Eval results *****
2022-06-12 14:31:06,598   acc = 0.8271013754457464
2022-06-12 14:31:06,598   cls_loss = 0.215787995904295
2022-06-12 14:31:06,598   eval_loss = 0.4743864743057214
2022-06-12 14:31:06,598   global_step = 50999
2022-06-12 14:31:06,598   loss = 0.215787995904295
2022-06-12 14:31:07,341 ***** Eval results *****
2022-06-12 14:31:07,341   acc = 0.9063813999505318
2022-06-12 14:31:07,341   acc_and_f1 = 0.8900715991514657
2022-06-12 14:31:07,341   cls_loss = 0.08671037924549972
2022-06-12 14:31:07,341   eval_loss = 0.2441586620539804
2022-06-12 14:31:07,341   f1 = 0.8737617983523996
2022-06-12 14:31:07,342   global_step = 65999
2022-06-12 14:31:07,342   loss = 0.08671037924549972
2022-06-12 14:33:05,360 ***** Running evaluation *****
2022-06-12 14:33:05,361   Epoch = 4 iter 51499 step
2022-06-12 14:33:05,361   Num examples = 9815
2022-06-12 14:33:05,361   Batch size = 32
2022-06-12 14:33:10,877 ***** Running evaluation *****
2022-06-12 14:33:10,877   Epoch = 5 iter 66499 step
2022-06-12 14:33:10,877   Num examples = 40430
2022-06-12 14:33:10,877   Batch size = 32
2022-06-12 14:33:13,459 ***** Eval results *****
2022-06-12 14:33:13,459   acc = 0.8276107997962303
2022-06-12 14:33:13,459   cls_loss = 0.21582345744097456
2022-06-12 14:33:13,459   eval_loss = 0.472897687059272
2022-06-12 14:33:13,459   global_step = 51499
2022-06-12 14:33:13,459   loss = 0.21582345744097456
2022-06-12 14:33:45,859 ***** Eval results *****
2022-06-12 14:33:45,859   acc = 0.9059609201088301
2022-06-12 14:33:45,859   acc_and_f1 = 0.8894443637977305
2022-06-12 14:33:45,859   cls_loss = 0.08667151947977852
2022-06-12 14:33:45,859   eval_loss = 0.24509460475926487
2022-06-12 14:33:45,859   f1 = 0.872927807486631
2022-06-12 14:33:45,860   global_step = 66499
2022-06-12 14:33:45,860   loss = 0.08667151947977852
2022-06-12 14:35:12,040 ***** Running evaluation *****
2022-06-12 14:35:12,040   Epoch = 4 iter 51999 step
2022-06-12 14:35:12,040   Num examples = 9815
2022-06-12 14:35:12,040   Batch size = 32
2022-06-12 14:35:20,142 ***** Eval results *****
2022-06-12 14:35:20,142   acc = 0.8255731023942945
2022-06-12 14:35:20,142   cls_loss = 0.21567840548056477
2022-06-12 14:35:20,142   eval_loss = 0.47096977404740425
2022-06-12 14:35:20,142   global_step = 51999
2022-06-12 14:35:20,142   loss = 0.21567840548056477
2022-06-12 14:35:49,879 ***** Running evaluation *****
2022-06-12 14:35:49,879   Epoch = 5 iter 66999 step
2022-06-12 14:35:49,879   Num examples = 40430
2022-06-12 14:35:49,879   Batch size = 32
2022-06-12 14:36:24,841 ***** Eval results *****
2022-06-12 14:36:24,842   acc = 0.9058125154588177
2022-06-12 14:36:24,842   acc_and_f1 = 0.8895367576295619
2022-06-12 14:36:24,842   cls_loss = 0.08664830779097552
2022-06-12 14:36:24,842   eval_loss = 0.24444034144641902
2022-06-12 14:36:24,842   f1 = 0.8732609998003061
2022-06-12 14:36:24,842   global_step = 66999
2022-06-12 14:36:24,842   loss = 0.08664830779097552
2022-06-12 14:37:18,894 ***** Running evaluation *****
2022-06-12 14:37:18,894   Epoch = 4 iter 52499 step
2022-06-12 14:37:18,894   Num examples = 9815
2022-06-12 14:37:18,894   Batch size = 32
2022-06-12 14:37:27,016 ***** Eval results *****
2022-06-12 14:37:27,016   acc = 0.8279164544065206
2022-06-12 14:37:27,016   cls_loss = 0.21552459033394278
2022-06-12 14:37:27,017   eval_loss = 0.46821167793258395
2022-06-12 14:37:27,017   global_step = 52499
2022-06-12 14:37:27,017   loss = 0.21552459033394278
2022-06-12 14:38:27,811 ***** Running evaluation *****
2022-06-12 14:38:27,811   Epoch = 5 iter 67499 step
2022-06-12 14:38:27,812   Num examples = 40430
2022-06-12 14:38:27,812   Batch size = 32
2022-06-12 14:39:02,826 ***** Eval results *****
2022-06-12 14:39:02,827   acc = 0.9056888449171407
2022-06-12 14:39:02,827   acc_and_f1 = 0.889279590664485
2022-06-12 14:39:02,827   cls_loss = 0.0866438292165616
2022-06-12 14:39:02,827   eval_loss = 0.2454142045329879
2022-06-12 14:39:02,827   f1 = 0.8728703364118294
2022-06-12 14:39:02,827   global_step = 67499
2022-06-12 14:39:02,827   loss = 0.0866438292165616
2022-06-12 14:39:25,759 ***** Running evaluation *****
2022-06-12 14:39:25,759   Epoch = 4 iter 52999 step
2022-06-12 14:39:25,759   Num examples = 9815
2022-06-12 14:39:25,759   Batch size = 32
2022-06-12 14:39:33,863 ***** Eval results *****
2022-06-12 14:39:33,863   acc = 0.8241467142129394
2022-06-12 14:39:33,863   cls_loss = 0.21561039363059048
2022-06-12 14:39:33,863   eval_loss = 0.476865607868965
2022-06-12 14:39:33,863   global_step = 52999
2022-06-12 14:39:33,863   loss = 0.21561039363059048
2022-06-12 14:41:06,329 ***** Running evaluation *****
2022-06-12 14:41:06,329   Epoch = 5 iter 67999 step
2022-06-12 14:41:06,330   Num examples = 40430
2022-06-12 14:41:06,330   Batch size = 32
2022-06-12 14:41:32,428 ***** Running evaluation *****
2022-06-12 14:41:32,428   Epoch = 4 iter 53499 step
2022-06-12 14:41:32,428   Num examples = 9815
2022-06-12 14:41:32,428   Batch size = 32
2022-06-12 14:41:40,530 ***** Eval results *****
2022-06-12 14:41:40,530   acc = 0.8271013754457464
2022-06-12 14:41:40,531   cls_loss = 0.2155499412951356
2022-06-12 14:41:40,531   eval_loss = 0.46588485462269486
2022-06-12 14:41:40,531   global_step = 53499
2022-06-12 14:41:40,531   loss = 0.2155499412951356
2022-06-12 14:41:41,265 ***** Eval results *****
2022-06-12 14:41:41,265   acc = 0.905763047242147
2022-06-12 14:41:41,265   acc_and_f1 = 0.8894196007747813
2022-06-12 14:41:41,265   cls_loss = 0.0866773854321306
2022-06-12 14:41:41,266   eval_loss = 0.2441384541533911
2022-06-12 14:41:41,266   f1 = 0.8730761543074157
2022-06-12 14:41:41,266   global_step = 67999
2022-06-12 14:41:41,266   loss = 0.0866773854321306
2022-06-12 14:42:36,113 **************S*************
task_name = qqp
best_metirc = 0.874680985051871
**************E*************

2022-06-12 14:42:37,162 Task finish! 
2022-06-12 14:42:37,163 Task cost 362.8809034166667 minutes, i.e. 6.048015060277778 hours. 
2022-06-12 14:42:39,308 Task start! 
2022-06-12 14:42:39,331 device: cuda n_gpu: 1
2022-06-12 14:42:39,332 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=100, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=64, max_seq_length=128, no_cuda=False, num_train_epochs=15, output_dir='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/sst-2/on_original_data', pred_distill=False, seed=42, student_model='../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D', task_name='sst-2', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/sst-2/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/sst-2/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 14:42:39,604 Writing example 0 of 67349
2022-06-12 14:42:39,604 *** Example ***
2022-06-12 14:42:39,604 guid: train-1
2022-06-12 14:42:39,605 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2022-06-12 14:42:39,605 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 14:42:39,605 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 14:42:39,605 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 14:42:39,605 label: 0
2022-06-12 14:42:39,605 label_id: 0
2022-06-12 14:42:41,575 Writing example 10000 of 67349
2022-06-12 14:42:43,481 Writing example 20000 of 67349
2022-06-12 14:42:45,511 Writing example 30000 of 67349
2022-06-12 14:42:47,401 Writing example 40000 of 67349
2022-06-12 14:42:49,482 Writing example 50000 of 67349
2022-06-12 14:42:51,367 Writing example 60000 of 67349
2022-06-12 14:42:53,489 Writing example 0 of 872
2022-06-12 14:42:53,489 *** Example ***
2022-06-12 14:42:53,489 guid: dev-1
2022-06-12 14:42:53,490 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2022-06-12 14:42:53,490 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 14:42:53,490 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 14:42:53,490 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 14:42:53,490 label: 1
2022-06-12 14:42:53,490 label_id: 1
2022-06-12 14:42:53,808 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "sst-2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 14:42:58,922 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/sst-2/on_original_data/pytorch_model.bin
2022-06-12 14:43:00,911 loading model...
2022-06-12 14:43:01,282 done!
2022-06-12 14:43:05,490 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 14:43:06,577 Loading model ../tiny_bert/model/distilled_pretrained_model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin
2022-06-12 14:43:06,704 loading model...
2022-06-12 14:43:06,731 done!
2022-06-12 14:43:06,731 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2022-06-12 14:43:06,731 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2022-06-12 14:43:08,160 ***** Running training *****
2022-06-12 14:43:08,176   Num examples = 67349
2022-06-12 14:43:08,187   Batch size = 32
2022-06-12 14:43:08,202   Num steps = 31560
2022-06-12 14:43:08,208 n: bert.embeddings.word_embeddings.weight
2022-06-12 14:43:08,214 n: bert.embeddings.position_embeddings.weight
2022-06-12 14:43:08,224 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 14:43:08,226 n: bert.embeddings.LayerNorm.weight
2022-06-12 14:43:08,226 n: bert.embeddings.LayerNorm.bias
2022-06-12 14:43:08,226 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 14:43:08,226 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 14:43:08,226 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 14:43:08,226 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 14:43:08,226 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 14:43:08,227 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 14:43:08,228 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 14:43:08,228 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 14:43:08,229 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 14:43:08,229 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 14:43:08,229 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 14:43:08,229 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 14:43:08,229 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 14:43:08,229 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 14:43:08,229 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 14:43:08,229 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 14:43:08,229 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 14:43:08,230 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 14:43:08,231 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 14:43:08,231 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 14:43:08,231 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 14:43:08,231 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 14:43:08,232 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 14:43:08,233 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 14:43:08,233 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 14:43:08,233 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 14:43:08,234 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 14:43:08,234 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 14:43:08,234 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 14:43:08,234 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 14:43:08,234 n: bert.pooler.dense.weight
2022-06-12 14:43:08,234 n: bert.pooler.dense.bias
2022-06-12 14:43:08,234 n: classifier.weight
2022-06-12 14:43:08,234 n: classifier.bias
2022-06-12 14:43:08,234 n: fit_denses.0.weight
2022-06-12 14:43:08,234 n: fit_denses.0.bias
2022-06-12 14:43:08,234 n: fit_denses.1.weight
2022-06-12 14:43:08,234 n: fit_denses.1.bias
2022-06-12 14:43:08,234 n: fit_denses.2.weight
2022-06-12 14:43:08,235 n: fit_denses.2.bias
2022-06-12 14:43:08,235 n: fit_denses.3.weight
2022-06-12 14:43:08,235 n: fit_denses.3.bias
2022-06-12 14:43:08,235 n: fit_denses.4.weight
2022-06-12 14:43:08,235 n: fit_denses.4.bias
2022-06-12 14:43:08,235 n: fit_denses.5.weight
2022-06-12 14:43:08,235 n: fit_denses.5.bias
2022-06-12 14:43:08,235 n: fit_denses.6.weight
2022-06-12 14:43:08,235 n: fit_denses.6.bias
2022-06-12 14:43:08,235 Total parameters: 72468738
2022-06-12 14:43:34,085 ***** Running evaluation *****
2022-06-12 14:43:34,086   Epoch = 0 iter 99 step
2022-06-12 14:43:34,086   Num examples = 872
2022-06-12 14:43:34,086   Batch size = 32
2022-06-12 14:43:34,087 ***** Eval results *****
2022-06-12 14:43:34,087   att_loss = 1.4634725830771707
2022-06-12 14:43:34,087   global_step = 99
2022-06-12 14:43:34,087   loss = 3.155024266002154
2022-06-12 14:43:34,087   rep_loss = 1.691551685333252
2022-06-12 14:43:34,087 ***** Save model *****
2022-06-12 14:43:39,048 ***** Running evaluation *****
2022-06-12 14:43:39,048   Epoch = 4 iter 53999 step
2022-06-12 14:43:39,048   Num examples = 9815
2022-06-12 14:43:39,048   Batch size = 32
2022-06-12 14:43:47,157 ***** Eval results *****
2022-06-12 14:43:47,157   acc = 0.8268976057055527
2022-06-12 14:43:47,157   cls_loss = 0.2156374651017776
2022-06-12 14:43:47,157   eval_loss = 0.4690307777751152
2022-06-12 14:43:47,157   global_step = 53999
2022-06-12 14:43:47,157   loss = 0.2156374651017776
2022-06-12 14:44:00,511 ***** Running evaluation *****
2022-06-12 14:44:00,511   Epoch = 0 iter 199 step
2022-06-12 14:44:00,511   Num examples = 872
2022-06-12 14:44:00,511   Batch size = 32
2022-06-12 14:44:00,512 ***** Eval results *****
2022-06-12 14:44:00,512   att_loss = 1.366638549308681
2022-06-12 14:44:00,512   global_step = 199
2022-06-12 14:44:00,512   loss = 2.85915299755844
2022-06-12 14:44:00,512   rep_loss = 1.4925144407617388
2022-06-12 14:44:00,512 ***** Save model *****
2022-06-12 14:44:27,089 ***** Running evaluation *****
2022-06-12 14:44:27,089   Epoch = 0 iter 299 step
2022-06-12 14:44:27,089   Num examples = 872
2022-06-12 14:44:27,089   Batch size = 32
2022-06-12 14:44:27,090 ***** Eval results *****
2022-06-12 14:44:27,090   att_loss = 1.3031694187368437
2022-06-12 14:44:27,091   global_step = 299
2022-06-12 14:44:27,091   loss = 2.6955819413016076
2022-06-12 14:44:27,091   rep_loss = 1.3924125177804443
2022-06-12 14:44:27,091 ***** Save model *****
2022-06-12 14:44:53,743 ***** Running evaluation *****
2022-06-12 14:44:53,743   Epoch = 0 iter 399 step
2022-06-12 14:44:53,743   Num examples = 872
2022-06-12 14:44:53,743   Batch size = 32
2022-06-12 14:44:53,744 ***** Eval results *****
2022-06-12 14:44:53,745   att_loss = 1.2587263702151172
2022-06-12 14:44:53,745   global_step = 399
2022-06-12 14:44:53,745   loss = 2.5903517016790865
2022-06-12 14:44:53,745   rep_loss = 1.3316253226502497
2022-06-12 14:44:53,745 ***** Save model *****
2022-06-12 14:45:20,325 ***** Running evaluation *****
2022-06-12 14:45:20,325   Epoch = 0 iter 499 step
2022-06-12 14:45:20,325   Num examples = 872
2022-06-12 14:45:20,325   Batch size = 32
2022-06-12 14:45:20,326 ***** Eval results *****
2022-06-12 14:45:20,326   att_loss = 1.221299813840098
2022-06-12 14:45:20,326   global_step = 499
2022-06-12 14:45:20,327   loss = 2.5075535912790854
2022-06-12 14:45:20,327   rep_loss = 1.2862537709887853
2022-06-12 14:45:20,327 ***** Save model *****
2022-06-12 14:45:45,472 ***** Running evaluation *****
2022-06-12 14:45:45,473   Epoch = 4 iter 54499 step
2022-06-12 14:45:45,473   Num examples = 9815
2022-06-12 14:45:45,473   Batch size = 32
2022-06-12 14:45:46,855 ***** Running evaluation *****
2022-06-12 14:45:46,856   Epoch = 0 iter 599 step
2022-06-12 14:45:46,856   Num examples = 872
2022-06-12 14:45:46,856   Batch size = 32
2022-06-12 14:45:46,857 ***** Eval results *****
2022-06-12 14:45:46,858   att_loss = 1.1913828015924495
2022-06-12 14:45:46,858   global_step = 599
2022-06-12 14:45:46,858   loss = 2.4436598415167783
2022-06-12 14:45:46,858   rep_loss = 1.2522770349489827
2022-06-12 14:45:46,858 ***** Save model *****
2022-06-12 14:45:53,576 ***** Eval results *****
2022-06-12 14:45:53,576   acc = 0.8248599083036169
2022-06-12 14:45:53,577   cls_loss = 0.2156447807452465
2022-06-12 14:45:53,577   eval_loss = 0.47519958135747753
2022-06-12 14:45:53,577   global_step = 54499
2022-06-12 14:45:53,577   loss = 0.2156447807452465
2022-06-12 14:46:13,512 ***** Running evaluation *****
2022-06-12 14:46:13,512   Epoch = 0 iter 699 step
2022-06-12 14:46:13,512   Num examples = 872
2022-06-12 14:46:13,512   Batch size = 32
2022-06-12 14:46:13,514 ***** Eval results *****
2022-06-12 14:46:13,514   att_loss = 1.1649786532351558
2022-06-12 14:46:13,514   global_step = 699
2022-06-12 14:46:13,514   loss = 2.387364961558657
2022-06-12 14:46:13,514   rep_loss = 1.2223863023545098
2022-06-12 14:46:13,514 ***** Save model *****
2022-06-12 14:46:40,158 ***** Running evaluation *****
2022-06-12 14:46:40,159   Epoch = 0 iter 799 step
2022-06-12 14:46:40,159   Num examples = 872
2022-06-12 14:46:40,159   Batch size = 32
2022-06-12 14:46:40,160 ***** Eval results *****
2022-06-12 14:46:40,160   att_loss = 1.1407931134459075
2022-06-12 14:46:40,161   global_step = 799
2022-06-12 14:46:40,161   loss = 2.341433941348175
2022-06-12 14:46:40,161   rep_loss = 1.200640821785145
2022-06-12 14:46:40,161 ***** Save model *****
2022-06-12 14:47:06,741 ***** Running evaluation *****
2022-06-12 14:47:06,741   Epoch = 0 iter 899 step
2022-06-12 14:47:06,742   Num examples = 872
2022-06-12 14:47:06,742   Batch size = 32
2022-06-12 14:47:06,743 ***** Eval results *****
2022-06-12 14:47:06,743   att_loss = 1.1218856644709994
2022-06-12 14:47:06,743   global_step = 899
2022-06-12 14:47:06,743   loss = 2.3045717573271975
2022-06-12 14:47:06,743   rep_loss = 1.1826860854967816
2022-06-12 14:47:06,743 ***** Save model *****
2022-06-12 14:47:33,441 ***** Running evaluation *****
2022-06-12 14:47:33,441   Epoch = 0 iter 999 step
2022-06-12 14:47:33,441   Num examples = 872
2022-06-12 14:47:33,441   Batch size = 32
2022-06-12 14:47:33,442 ***** Eval results *****
2022-06-12 14:47:33,442   att_loss = 1.1049086970609945
2022-06-12 14:47:33,443   global_step = 999
2022-06-12 14:47:33,443   loss = 2.2705417036174893
2022-06-12 14:47:33,443   rep_loss = 1.1656330006497282
2022-06-12 14:47:33,443 ***** Save model *****
2022-06-12 14:47:52,081 ***** Running evaluation *****
2022-06-12 14:47:52,081   Epoch = 4 iter 54999 step
2022-06-12 14:47:52,081   Num examples = 9815
2022-06-12 14:47:52,081   Batch size = 32
2022-06-12 14:48:00,140 ***** Running evaluation *****
2022-06-12 14:48:00,141   Epoch = 0 iter 1099 step
2022-06-12 14:48:00,141   Num examples = 872
2022-06-12 14:48:00,141   Batch size = 32
2022-06-12 14:48:00,142 ***** Eval results *****
2022-06-12 14:48:00,142   att_loss = 1.0926573753248463
2022-06-12 14:48:00,142   global_step = 1099
2022-06-12 14:48:00,142   loss = 2.244524629469673
2022-06-12 14:48:00,143   rep_loss = 1.1518672491551747
2022-06-12 14:48:00,143 ***** Save model *****
2022-06-12 14:48:00,198 ***** Eval results *****
2022-06-12 14:48:00,198   acc = 0.8303616912888436
2022-06-12 14:48:00,199   cls_loss = 0.21560345751848245
2022-06-12 14:48:00,199   eval_loss = 0.47011621480850135
2022-06-12 14:48:00,199   global_step = 54999
2022-06-12 14:48:00,199   loss = 0.21560345751848245
2022-06-12 14:48:00,199 ***** Save model *****
2022-06-12 14:48:26,931 ***** Running evaluation *****
2022-06-12 14:48:26,931   Epoch = 0 iter 1199 step
2022-06-12 14:48:26,931   Num examples = 872
2022-06-12 14:48:26,931   Batch size = 32
2022-06-12 14:48:26,932 ***** Eval results *****
2022-06-12 14:48:26,932   att_loss = 1.0792434433781175
2022-06-12 14:48:26,932   global_step = 1199
2022-06-12 14:48:26,932   loss = 2.2179541650466663
2022-06-12 14:48:26,932   rep_loss = 1.1387107168464883
2022-06-12 14:48:26,932 ***** Save model *****
2022-06-12 14:48:53,336 ***** Running evaluation *****
2022-06-12 14:48:53,336   Epoch = 0 iter 1299 step
2022-06-12 14:48:53,337   Num examples = 872
2022-06-12 14:48:53,337   Batch size = 32
2022-06-12 14:48:53,338 ***** Eval results *****
2022-06-12 14:48:53,338   att_loss = 1.0683936388608215
2022-06-12 14:48:53,338   global_step = 1299
2022-06-12 14:48:53,338   loss = 2.1956912046950814
2022-06-12 14:48:53,338   rep_loss = 1.1272975613834126
2022-06-12 14:48:53,338 ***** Save model *****
2022-06-12 14:49:19,892 ***** Running evaluation *****
2022-06-12 14:49:19,892   Epoch = 0 iter 1399 step
2022-06-12 14:49:19,892   Num examples = 872
2022-06-12 14:49:19,892   Batch size = 32
2022-06-12 14:49:19,893 ***** Eval results *****
2022-06-12 14:49:19,894   att_loss = 1.0582161108198296
2022-06-12 14:49:19,894   global_step = 1399
2022-06-12 14:49:19,894   loss = 2.1741600526580647
2022-06-12 14:49:19,894   rep_loss = 1.1159439374925069
2022-06-12 14:49:19,894 ***** Save model *****
2022-06-12 14:49:46,463 ***** Running evaluation *****
2022-06-12 14:49:46,463   Epoch = 0 iter 1499 step
2022-06-12 14:49:46,463   Num examples = 872
2022-06-12 14:49:46,463   Batch size = 32
2022-06-12 14:49:46,464 ***** Eval results *****
2022-06-12 14:49:46,465   att_loss = 1.0476557558739161
2022-06-12 14:49:46,465   global_step = 1499
2022-06-12 14:49:46,465   loss = 2.1543111700149598
2022-06-12 14:49:46,465   rep_loss = 1.106655410085224
2022-06-12 14:49:46,465 ***** Save model *****
2022-06-12 14:49:59,286 ***** Running evaluation *****
2022-06-12 14:49:59,287   Epoch = 4 iter 55499 step
2022-06-12 14:49:59,287   Num examples = 9815
2022-06-12 14:49:59,287   Batch size = 32
2022-06-12 14:50:07,399 ***** Eval results *****
2022-06-12 14:50:07,399   acc = 0.8290371879775853
2022-06-12 14:50:07,399   cls_loss = 0.2156466063190417
2022-06-12 14:50:07,399   eval_loss = 0.4684899854155239
2022-06-12 14:50:07,399   global_step = 55499
2022-06-12 14:50:07,399   loss = 0.2156466063190417
2022-06-12 14:50:13,060 ***** Running evaluation *****
2022-06-12 14:50:13,061   Epoch = 0 iter 1599 step
2022-06-12 14:50:13,061   Num examples = 872
2022-06-12 14:50:13,061   Batch size = 32
2022-06-12 14:50:13,062 ***** Eval results *****
2022-06-12 14:50:13,062   att_loss = 1.0389124875220752
2022-06-12 14:50:13,062   global_step = 1599
2022-06-12 14:50:13,063   loss = 2.136928359443803
2022-06-12 14:50:13,063   rep_loss = 1.0980158681568315
2022-06-12 14:50:13,063 ***** Save model *****
2022-06-12 14:50:39,628 ***** Running evaluation *****
2022-06-12 14:50:39,629   Epoch = 0 iter 1699 step
2022-06-12 14:50:39,629   Num examples = 872
2022-06-12 14:50:39,629   Batch size = 32
2022-06-12 14:50:39,630 ***** Eval results *****
2022-06-12 14:50:39,630   att_loss = 1.029177931808878
2022-06-12 14:50:39,630   global_step = 1699
2022-06-12 14:50:39,630   loss = 2.118677666146591
2022-06-12 14:50:39,630   rep_loss = 1.0894997307593293
2022-06-12 14:50:39,631 ***** Save model *****
2022-06-12 14:51:06,189 ***** Running evaluation *****
2022-06-12 14:51:06,189   Epoch = 0 iter 1799 step
2022-06-12 14:51:06,189   Num examples = 872
2022-06-12 14:51:06,189   Batch size = 32
2022-06-12 14:51:06,191 ***** Eval results *****
2022-06-12 14:51:06,191   att_loss = 1.01977263593952
2022-06-12 14:51:06,191   global_step = 1799
2022-06-12 14:51:06,191   loss = 2.1012467735008507
2022-06-12 14:51:06,191   rep_loss = 1.0814741340658942
2022-06-12 14:51:06,191 ***** Save model *****
2022-06-12 14:51:32,820 ***** Running evaluation *****
2022-06-12 14:51:32,820   Epoch = 0 iter 1899 step
2022-06-12 14:51:32,820   Num examples = 872
2022-06-12 14:51:32,820   Batch size = 32
2022-06-12 14:51:32,821 ***** Eval results *****
2022-06-12 14:51:32,821   att_loss = 1.0124022189317847
2022-06-12 14:51:32,821   global_step = 1899
2022-06-12 14:51:32,822   loss = 2.0872531323887413
2022-06-12 14:51:32,822   rep_loss = 1.0748509103966863
2022-06-12 14:51:32,822 ***** Save model *****
2022-06-12 14:51:59,382 ***** Running evaluation *****
2022-06-12 14:51:59,382   Epoch = 0 iter 1999 step
2022-06-12 14:51:59,382   Num examples = 872
2022-06-12 14:51:59,382   Batch size = 32
2022-06-12 14:51:59,384 ***** Eval results *****
2022-06-12 14:51:59,384   att_loss = 1.0029565661087103
2022-06-12 14:51:59,384   global_step = 1999
2022-06-12 14:51:59,384   loss = 2.0707279842695394
2022-06-12 14:51:59,384   rep_loss = 1.0677714155518216
2022-06-12 14:51:59,384 ***** Save model *****
2022-06-12 14:52:05,816 ***** Running evaluation *****
2022-06-12 14:52:05,817   Epoch = 4 iter 55999 step
2022-06-12 14:52:05,817   Num examples = 9815
2022-06-12 14:52:05,817   Batch size = 32
2022-06-12 14:52:13,916 ***** Eval results *****
2022-06-12 14:52:13,916   acc = 0.8272032603158431
2022-06-12 14:52:13,916   cls_loss = 0.21560187303580328
2022-06-12 14:52:13,916   eval_loss = 0.4731230136334702
2022-06-12 14:52:13,916   global_step = 55999
2022-06-12 14:52:13,916   loss = 0.21560187303580328
2022-06-12 14:52:25,980 ***** Running evaluation *****
2022-06-12 14:52:25,980   Epoch = 0 iter 2099 step
2022-06-12 14:52:25,980   Num examples = 872
2022-06-12 14:52:25,980   Batch size = 32
2022-06-12 14:52:25,981 ***** Eval results *****
2022-06-12 14:52:25,981   att_loss = 0.996026977591426
2022-06-12 14:52:25,982   global_step = 2099
2022-06-12 14:52:25,982   loss = 2.057232994133657
2022-06-12 14:52:25,982   rep_loss = 1.0612060138871409
2022-06-12 14:52:25,982 ***** Save model *****
2022-06-12 14:52:52,621 ***** Running evaluation *****
2022-06-12 14:52:52,621   Epoch = 1 iter 2199 step
2022-06-12 14:52:52,621   Num examples = 872
2022-06-12 14:52:52,621   Batch size = 32
2022-06-12 14:52:52,622 ***** Eval results *****
2022-06-12 14:52:52,622   att_loss = 0.8180941788773788
2022-06-12 14:52:52,622   global_step = 2199
2022-06-12 14:52:52,622   loss = 1.7213433729974847
2022-06-12 14:52:52,622   rep_loss = 0.9032491847088463
2022-06-12 14:52:52,623 ***** Save model *****
2022-06-12 14:53:19,160 ***** Running evaluation *****
2022-06-12 14:53:19,160   Epoch = 1 iter 2299 step
2022-06-12 14:53:19,160   Num examples = 872
2022-06-12 14:53:19,160   Batch size = 32
2022-06-12 14:53:19,161 ***** Eval results *****
2022-06-12 14:53:19,161   att_loss = 0.8133168553694701
2022-06-12 14:53:19,162   global_step = 2299
2022-06-12 14:53:19,162   loss = 1.7106921440515763
2022-06-12 14:53:19,162   rep_loss = 0.8973752819574796
2022-06-12 14:53:19,162 ***** Save model *****
2022-06-12 14:53:45,899 ***** Running evaluation *****
2022-06-12 14:53:45,899   Epoch = 1 iter 2399 step
2022-06-12 14:53:45,899   Num examples = 872
2022-06-12 14:53:45,899   Batch size = 32
2022-06-12 14:53:45,900 ***** Eval results *****
2022-06-12 14:53:45,900   att_loss = 0.8106610871977725
2022-06-12 14:53:45,900   global_step = 2399
2022-06-12 14:53:45,900   loss = 1.708461818856708
2022-06-12 14:53:45,900   rep_loss = 0.897800727415893
2022-06-12 14:53:45,900 ***** Save model *****
2022-06-12 14:54:12,625 ***** Running evaluation *****
2022-06-12 14:54:12,625   Epoch = 1 iter 2499 step
2022-06-12 14:54:12,626   Num examples = 872
2022-06-12 14:54:12,626   Batch size = 32
2022-06-12 14:54:12,627 ***** Eval results *****
2022-06-12 14:54:12,627   att_loss = 0.805860746661319
2022-06-12 14:54:12,627   global_step = 2499
2022-06-12 14:54:12,627   loss = 1.7022179479840436
2022-06-12 14:54:12,627   rep_loss = 0.8963571983047679
2022-06-12 14:54:12,627 ***** Save model *****
2022-06-12 14:54:13,187 ***** Running evaluation *****
2022-06-12 14:54:13,188   Epoch = 4 iter 56499 step
2022-06-12 14:54:13,188   Num examples = 9815
2022-06-12 14:54:13,188   Batch size = 32
2022-06-12 14:54:21,304 ***** Eval results *****
2022-06-12 14:54:21,305   acc = 0.8299541518084564
2022-06-12 14:54:21,305   cls_loss = 0.21562302890896234
2022-06-12 14:54:21,305   eval_loss = 0.4662016559792652
2022-06-12 14:54:21,305   global_step = 56499
2022-06-12 14:54:21,305   loss = 0.21562302890896234
2022-06-12 14:54:39,317 ***** Running evaluation *****
2022-06-12 14:54:39,318   Epoch = 1 iter 2599 step
2022-06-12 14:54:39,318   Num examples = 872
2022-06-12 14:54:39,318   Batch size = 32
2022-06-12 14:54:39,319 ***** Eval results *****
2022-06-12 14:54:39,319   att_loss = 0.8051543079241358
2022-06-12 14:54:39,319   global_step = 2599
2022-06-12 14:54:39,319   loss = 1.700887228021718
2022-06-12 14:54:39,319   rep_loss = 0.8957329179301406
2022-06-12 14:54:39,319 ***** Save model *****
2022-06-12 14:55:05,976 ***** Running evaluation *****
2022-06-12 14:55:05,977   Epoch = 1 iter 2699 step
2022-06-12 14:55:05,977   Num examples = 872
2022-06-12 14:55:05,977   Batch size = 32
2022-06-12 14:55:05,978 ***** Eval results *****
2022-06-12 14:55:05,978   att_loss = 0.8066742402665755
2022-06-12 14:55:05,978   global_step = 2699
2022-06-12 14:55:05,978   loss = 1.7033802150678234
2022-06-12 14:55:05,978   rep_loss = 0.8967059733486977
2022-06-12 14:55:05,978 ***** Save model *****
2022-06-12 14:55:32,527 ***** Running evaluation *****
2022-06-12 14:55:32,528   Epoch = 1 iter 2799 step
2022-06-12 14:55:32,528   Num examples = 872
2022-06-12 14:55:32,528   Batch size = 32
2022-06-12 14:55:32,529 ***** Eval results *****
2022-06-12 14:55:32,529   att_loss = 0.8038150428010405
2022-06-12 14:55:32,529   global_step = 2799
2022-06-12 14:55:32,529   loss = 1.6984285284289353
2022-06-12 14:55:32,529   rep_loss = 0.894613483655367
2022-06-12 14:55:32,529 ***** Save model *****
2022-06-12 14:55:59,153 ***** Running evaluation *****
2022-06-12 14:55:59,153   Epoch = 1 iter 2899 step
2022-06-12 14:55:59,153   Num examples = 872
2022-06-12 14:55:59,153   Batch size = 32
2022-06-12 14:55:59,155 ***** Eval results *****
2022-06-12 14:55:59,155   att_loss = 0.8000832531437184
2022-06-12 14:55:59,155   global_step = 2899
2022-06-12 14:55:59,155   loss = 1.6931081984777872
2022-06-12 14:55:59,155   rep_loss = 0.8930249441344783
2022-06-12 14:55:59,155 ***** Save model *****
2022-06-12 14:56:20,196 ***** Running evaluation *****
2022-06-12 14:56:20,197   Epoch = 4 iter 56999 step
2022-06-12 14:56:20,197   Num examples = 9815
2022-06-12 14:56:20,197   Batch size = 32
2022-06-12 14:56:25,728 ***** Running evaluation *****
2022-06-12 14:56:25,728   Epoch = 1 iter 2999 step
2022-06-12 14:56:25,728   Num examples = 872
2022-06-12 14:56:25,728   Batch size = 32
2022-06-12 14:56:25,729 ***** Eval results *****
2022-06-12 14:56:25,729   att_loss = 0.7980099655729432
2022-06-12 14:56:25,729   global_step = 2999
2022-06-12 14:56:25,729   loss = 1.6898164627938297
2022-06-12 14:56:25,729   rep_loss = 0.8918064956558484
2022-06-12 14:56:25,729 ***** Save model *****
2022-06-12 14:56:28,307 ***** Eval results *****
2022-06-12 14:56:28,307   acc = 0.8295466123280693
2022-06-12 14:56:28,307   cls_loss = 0.21566463215058918
2022-06-12 14:56:28,307   eval_loss = 0.47136180932824695
2022-06-12 14:56:28,307   global_step = 56999
2022-06-12 14:56:28,307   loss = 0.21566463215058918
2022-06-12 14:56:52,425 ***** Running evaluation *****
2022-06-12 14:56:52,426   Epoch = 1 iter 3099 step
2022-06-12 14:56:52,426   Num examples = 872
2022-06-12 14:56:52,426   Batch size = 32
2022-06-12 14:56:52,427 ***** Eval results *****
2022-06-12 14:56:52,427   att_loss = 0.7976853614476458
2022-06-12 14:56:52,427   global_step = 3099
2022-06-12 14:56:52,427   loss = 1.6887900798165019
2022-06-12 14:56:52,427   rep_loss = 0.891104716871252
2022-06-12 14:56:52,428 ***** Save model *****
2022-06-12 14:57:19,109 ***** Running evaluation *****
2022-06-12 14:57:19,109   Epoch = 1 iter 3199 step
2022-06-12 14:57:19,109   Num examples = 872
2022-06-12 14:57:19,109   Batch size = 32
2022-06-12 14:57:19,111 ***** Eval results *****
2022-06-12 14:57:19,111   att_loss = 0.7947596851549192
2022-06-12 14:57:19,111   global_step = 3199
2022-06-12 14:57:19,111   loss = 1.6845242250999903
2022-06-12 14:57:19,111   rep_loss = 0.8897645389652687
2022-06-12 14:57:19,111 ***** Save model *****
2022-06-12 14:57:45,698 ***** Running evaluation *****
2022-06-12 14:57:45,698   Epoch = 1 iter 3299 step
2022-06-12 14:57:45,698   Num examples = 872
2022-06-12 14:57:45,698   Batch size = 32
2022-06-12 14:57:45,700 ***** Eval results *****
2022-06-12 14:57:45,700   att_loss = 0.7915476856620741
2022-06-12 14:57:45,700   global_step = 3299
2022-06-12 14:57:45,700   loss = 1.6793778962171226
2022-06-12 14:57:45,700   rep_loss = 0.8878302096821773
2022-06-12 14:57:45,700 ***** Save model *****
2022-06-12 14:58:12,234 ***** Running evaluation *****
2022-06-12 14:58:12,235   Epoch = 1 iter 3399 step
2022-06-12 14:58:12,235   Num examples = 872
2022-06-12 14:58:12,235   Batch size = 32
2022-06-12 14:58:12,236 ***** Eval results *****
2022-06-12 14:58:12,236   att_loss = 0.7875419384264117
2022-06-12 14:58:12,236   global_step = 3399
2022-06-12 14:58:12,236   loss = 1.6729362297242212
2022-06-12 14:58:12,236   rep_loss = 0.8853942911597292
2022-06-12 14:58:12,236 ***** Save model *****
2022-06-12 14:58:26,962 ***** Running evaluation *****
2022-06-12 14:58:26,962   Epoch = 4 iter 57499 step
2022-06-12 14:58:26,962   Num examples = 9815
2022-06-12 14:58:26,962   Batch size = 32
2022-06-12 14:58:35,068 ***** Eval results *****
2022-06-12 14:58:35,068   acc = 0.8283239938869078
2022-06-12 14:58:35,068   cls_loss = 0.21564660376016398
2022-06-12 14:58:35,068   eval_loss = 0.4696557921773059
2022-06-12 14:58:35,068   global_step = 57499
2022-06-12 14:58:35,068   loss = 0.21564660376016398
2022-06-12 14:58:38,763 ***** Running evaluation *****
2022-06-12 14:58:38,763   Epoch = 1 iter 3499 step
2022-06-12 14:58:38,764   Num examples = 872
2022-06-12 14:58:38,764   Batch size = 32
2022-06-12 14:58:38,765 ***** Eval results *****
2022-06-12 14:58:38,765   att_loss = 0.7850227358093399
2022-06-12 14:58:38,765   global_step = 3499
2022-06-12 14:58:38,765   loss = 1.669058425366665
2022-06-12 14:58:38,765   rep_loss = 0.8840356893436883
2022-06-12 14:58:38,766 ***** Save model *****
2022-06-12 14:59:05,319 ***** Running evaluation *****
2022-06-12 14:59:05,319   Epoch = 1 iter 3599 step
2022-06-12 14:59:05,319   Num examples = 872
2022-06-12 14:59:05,319   Batch size = 32
2022-06-12 14:59:05,320 ***** Eval results *****
2022-06-12 14:59:05,320   att_loss = 0.7832076197483866
2022-06-12 14:59:05,320   global_step = 3599
2022-06-12 14:59:05,320   loss = 1.6660893447423062
2022-06-12 14:59:05,320   rep_loss = 0.8828817248344422
2022-06-12 14:59:05,320 ***** Save model *****
2022-06-12 14:59:31,959 ***** Running evaluation *****
2022-06-12 14:59:31,959   Epoch = 1 iter 3699 step
2022-06-12 14:59:31,959   Num examples = 872
2022-06-12 14:59:31,960   Batch size = 32
2022-06-12 14:59:31,961 ***** Eval results *****
2022-06-12 14:59:31,961   att_loss = 0.7811157626044414
2022-06-12 14:59:31,961   global_step = 3699
2022-06-12 14:59:31,961   loss = 1.6624115194272846
2022-06-12 14:59:31,961   rep_loss = 0.8812957561501888
2022-06-12 14:59:31,961 ***** Save model *****
2022-06-12 14:59:58,474 ***** Running evaluation *****
2022-06-12 14:59:58,474   Epoch = 1 iter 3799 step
2022-06-12 14:59:58,474   Num examples = 872
2022-06-12 14:59:58,474   Batch size = 32
2022-06-12 14:59:58,475 ***** Eval results *****
2022-06-12 14:59:58,475   att_loss = 0.7798103208562969
2022-06-12 14:59:58,475   global_step = 3799
2022-06-12 14:59:58,475   loss = 1.6601280166687866
2022-06-12 14:59:58,475   rep_loss = 0.8803176951267726
2022-06-12 14:59:58,476 ***** Save model *****
2022-06-12 15:00:24,872 ***** Running evaluation *****
2022-06-12 15:00:24,872   Epoch = 1 iter 3899 step
2022-06-12 15:00:24,872   Num examples = 872
2022-06-12 15:00:24,872   Batch size = 32
2022-06-12 15:00:24,873 ***** Eval results *****
2022-06-12 15:00:24,873   att_loss = 0.7774576442985481
2022-06-12 15:00:24,874   global_step = 3899
2022-06-12 15:00:24,874   loss = 1.6563351129090886
2022-06-12 15:00:24,874   rep_loss = 0.8788774683614959
2022-06-12 15:00:24,874 ***** Save model *****
2022-06-12 15:00:33,855 ***** Running evaluation *****
2022-06-12 15:00:33,855   Epoch = 4 iter 57999 step
2022-06-12 15:00:33,855   Num examples = 9815
2022-06-12 15:00:33,855   Batch size = 32
2022-06-12 15:00:41,964 ***** Eval results *****
2022-06-12 15:00:41,964   acc = 0.8309730005094244
2022-06-12 15:00:41,964   cls_loss = 0.21567308817495473
2022-06-12 15:00:41,964   eval_loss = 0.4646440811382443
2022-06-12 15:00:41,964   global_step = 57999
2022-06-12 15:00:41,964   loss = 0.21567308817495473
2022-06-12 15:00:41,964 ***** Save model *****
2022-06-12 15:00:51,476 ***** Running evaluation *****
2022-06-12 15:00:51,477   Epoch = 1 iter 3999 step
2022-06-12 15:00:51,477   Num examples = 872
2022-06-12 15:00:51,477   Batch size = 32
2022-06-12 15:00:51,478 ***** Eval results *****
2022-06-12 15:00:51,479   att_loss = 0.7757433594531309
2022-06-12 15:00:51,479   global_step = 3999
2022-06-12 15:00:51,479   loss = 1.6529362145703197
2022-06-12 15:00:51,479   rep_loss = 0.8771928545352966
2022-06-12 15:00:51,479 ***** Save model *****
2022-06-12 15:01:17,991 ***** Running evaluation *****
2022-06-12 15:01:17,991   Epoch = 1 iter 4099 step
2022-06-12 15:01:17,991   Num examples = 872
2022-06-12 15:01:17,991   Batch size = 32
2022-06-12 15:01:17,992 ***** Eval results *****
2022-06-12 15:01:17,992   att_loss = 0.7745491130310193
2022-06-12 15:01:17,992   global_step = 4099
2022-06-12 15:01:17,993   loss = 1.6507484970833723
2022-06-12 15:01:17,993   rep_loss = 0.8761993833353048
2022-06-12 15:01:17,993 ***** Save model *****
2022-06-12 15:01:44,569 ***** Running evaluation *****
2022-06-12 15:01:44,569   Epoch = 1 iter 4199 step
2022-06-12 15:01:44,569   Num examples = 872
2022-06-12 15:01:44,570   Batch size = 32
2022-06-12 15:01:44,570 ***** Eval results *****
2022-06-12 15:01:44,571   att_loss = 0.7741227900213729
2022-06-12 15:01:44,571   global_step = 4199
2022-06-12 15:01:44,571   loss = 1.6492741107371656
2022-06-12 15:01:44,571   rep_loss = 0.8751513197769129
2022-06-12 15:01:44,571 ***** Save model *****
2022-06-12 15:02:11,203 ***** Running evaluation *****
2022-06-12 15:02:11,203   Epoch = 2 iter 4299 step
2022-06-12 15:02:11,203   Num examples = 872
2022-06-12 15:02:11,203   Batch size = 32
2022-06-12 15:02:11,204 ***** Eval results *****
2022-06-12 15:02:11,204   att_loss = 0.7313372171842135
2022-06-12 15:02:11,204   global_step = 4299
2022-06-12 15:02:11,204   loss = 1.5658937116245648
2022-06-12 15:02:11,204   rep_loss = 0.8345564957503434
2022-06-12 15:02:11,205 ***** Save model *****
2022-06-12 15:02:37,821 ***** Running evaluation *****
2022-06-12 15:02:37,821   Epoch = 2 iter 4399 step
2022-06-12 15:02:37,821   Num examples = 872
2022-06-12 15:02:37,821   Batch size = 32
2022-06-12 15:02:37,823 ***** Eval results *****
2022-06-12 15:02:37,823   att_loss = 0.7287644712712752
2022-06-12 15:02:37,823   global_step = 4399
2022-06-12 15:02:37,823   loss = 1.5586654596927902
2022-06-12 15:02:37,823   rep_loss = 0.8299009859249854
2022-06-12 15:02:37,823 ***** Save model *****
2022-06-12 15:02:41,180 ***** Running evaluation *****
2022-06-12 15:02:41,180   Epoch = 4 iter 58499 step
2022-06-12 15:02:41,180   Num examples = 9815
2022-06-12 15:02:41,180   Batch size = 32
2022-06-12 15:02:49,286 ***** Eval results *****
2022-06-12 15:02:49,286   acc = 0.8274070300560367
2022-06-12 15:02:49,286   cls_loss = 0.21571868126118873
2022-06-12 15:02:49,286   eval_loss = 0.4706007142214511
2022-06-12 15:02:49,286   global_step = 58499
2022-06-12 15:02:49,286   loss = 0.21571868126118873
2022-06-12 15:03:04,358 ***** Running evaluation *****
2022-06-12 15:03:04,358   Epoch = 2 iter 4499 step
2022-06-12 15:03:04,358   Num examples = 872
2022-06-12 15:03:04,358   Batch size = 32
2022-06-12 15:03:04,359 ***** Eval results *****
2022-06-12 15:03:04,359   att_loss = 0.7283940343922356
2022-06-12 15:03:04,359   global_step = 4499
2022-06-12 15:03:04,359   loss = 1.5578945879264385
2022-06-12 15:03:04,359   rep_loss = 0.8295005510762795
2022-06-12 15:03:04,359 ***** Save model *****
2022-06-12 15:03:30,867 ***** Running evaluation *****
2022-06-12 15:03:30,868   Epoch = 2 iter 4599 step
2022-06-12 15:03:30,868   Num examples = 872
2022-06-12 15:03:30,868   Batch size = 32
2022-06-12 15:03:30,869 ***** Eval results *****
2022-06-12 15:03:30,869   att_loss = 0.7273268434397705
2022-06-12 15:03:30,869   global_step = 4599
2022-06-12 15:03:30,869   loss = 1.5551014422150828
2022-06-12 15:03:30,869   rep_loss = 0.8277745952691569
2022-06-12 15:03:30,869 ***** Save model *****
2022-06-12 15:03:57,496 ***** Running evaluation *****
2022-06-12 15:03:57,496   Epoch = 2 iter 4699 step
2022-06-12 15:03:57,496   Num examples = 872
2022-06-12 15:03:57,496   Batch size = 32
2022-06-12 15:03:57,497 ***** Eval results *****
2022-06-12 15:03:57,498   att_loss = 0.7237657943471143
2022-06-12 15:03:57,498   global_step = 4699
2022-06-12 15:03:57,498   loss = 1.5511809223295472
2022-06-12 15:03:57,498   rep_loss = 0.8274151259187282
2022-06-12 15:03:57,498 ***** Save model *****
2022-06-12 15:04:24,036 ***** Running evaluation *****
2022-06-12 15:04:24,036   Epoch = 2 iter 4799 step
2022-06-12 15:04:24,036   Num examples = 872
2022-06-12 15:04:24,036   Batch size = 32
2022-06-12 15:04:24,037 ***** Eval results *****
2022-06-12 15:04:24,037   att_loss = 0.7209285010541151
2022-06-12 15:04:24,037   global_step = 4799
2022-06-12 15:04:24,037   loss = 1.546467294144348
2022-06-12 15:04:24,037   rep_loss = 0.825538791375717
2022-06-12 15:04:24,038 ***** Save model *****
2022-06-12 15:04:47,940 ***** Running evaluation *****
2022-06-12 15:04:47,940   Epoch = 4 iter 58999 step
2022-06-12 15:04:47,940   Num examples = 9815
2022-06-12 15:04:47,940   Batch size = 32
2022-06-12 15:04:50,847 ***** Running evaluation *****
2022-06-12 15:04:50,847   Epoch = 2 iter 4899 step
2022-06-12 15:04:50,848   Num examples = 872
2022-06-12 15:04:50,848   Batch size = 32
2022-06-12 15:04:50,849 ***** Eval results *****
2022-06-12 15:04:50,849   att_loss = 0.7159823949102383
2022-06-12 15:04:50,849   global_step = 4899
2022-06-12 15:04:50,849   loss = 1.5391689340561068
2022-06-12 15:04:50,849   rep_loss = 0.8231865380676369
2022-06-12 15:04:50,850 ***** Save model *****
2022-06-12 15:04:56,049 ***** Eval results *****
2022-06-12 15:04:56,049   acc = 0.8286296484971981
2022-06-12 15:04:56,049   cls_loss = 0.21574795792725612
2022-06-12 15:04:56,049   eval_loss = 0.4707140546862388
2022-06-12 15:04:56,049   global_step = 58999
2022-06-12 15:04:56,049   loss = 0.21574795792725612
2022-06-12 15:05:17,599 ***** Running evaluation *****
2022-06-12 15:05:17,599   Epoch = 2 iter 4999 step
2022-06-12 15:05:17,599   Num examples = 872
2022-06-12 15:05:17,599   Batch size = 32
2022-06-12 15:05:17,600 ***** Eval results *****
2022-06-12 15:05:17,600   att_loss = 0.7148558269861224
2022-06-12 15:05:17,600   global_step = 4999
2022-06-12 15:05:17,600   loss = 1.5380255475207014
2022-06-12 15:05:17,600   rep_loss = 0.8231697196303367
2022-06-12 15:05:17,600 ***** Save model *****
2022-06-12 15:05:44,433 ***** Running evaluation *****
2022-06-12 15:05:44,433   Epoch = 2 iter 5099 step
2022-06-12 15:05:44,433   Num examples = 872
2022-06-12 15:05:44,433   Batch size = 32
2022-06-12 15:05:44,434 ***** Eval results *****
2022-06-12 15:05:44,434   att_loss = 0.713774516894226
2022-06-12 15:05:44,434   global_step = 5099
2022-06-12 15:05:44,435   loss = 1.5368018148991662
2022-06-12 15:05:44,435   rep_loss = 0.8230272971018396
2022-06-12 15:05:44,435 ***** Save model *****
2022-06-12 15:06:11,210 ***** Running evaluation *****
2022-06-12 15:06:11,210   Epoch = 2 iter 5199 step
2022-06-12 15:06:11,210   Num examples = 872
2022-06-12 15:06:11,211   Batch size = 32
2022-06-12 15:06:11,211 ***** Eval results *****
2022-06-12 15:06:11,211   att_loss = 0.7143725740500584
2022-06-12 15:06:11,211   global_step = 5199
2022-06-12 15:06:11,212   loss = 1.5371039007554501
2022-06-12 15:06:11,212   rep_loss = 0.822731326434735
2022-06-12 15:06:11,212 ***** Save model *****
2022-06-12 15:06:38,048 ***** Running evaluation *****
2022-06-12 15:06:38,048   Epoch = 2 iter 5299 step
2022-06-12 15:06:38,049   Num examples = 872
2022-06-12 15:06:38,049   Batch size = 32
2022-06-12 15:06:38,050 ***** Eval results *****
2022-06-12 15:06:38,050   att_loss = 0.7126187233887715
2022-06-12 15:06:38,050   global_step = 5299
2022-06-12 15:06:38,051   loss = 1.534451285269586
2022-06-12 15:06:38,051   rep_loss = 0.821832561853498
2022-06-12 15:06:38,051 ***** Save model *****
2022-06-12 15:06:54,692 ***** Running evaluation *****
2022-06-12 15:06:54,693   Epoch = 4 iter 59499 step
2022-06-12 15:06:54,693   Num examples = 9815
2022-06-12 15:06:54,693   Batch size = 32
2022-06-12 15:07:02,811 ***** Eval results *****
2022-06-12 15:07:02,812   acc = 0.8311767702496179
2022-06-12 15:07:02,812   cls_loss = 0.2157371177775781
2022-06-12 15:07:02,812   eval_loss = 0.46923243276265236
2022-06-12 15:07:02,812   global_step = 59499
2022-06-12 15:07:02,812   loss = 0.2157371177775781
2022-06-12 15:07:02,812 ***** Save model *****
2022-06-12 15:07:04,807 ***** Running evaluation *****
2022-06-12 15:07:04,808   Epoch = 2 iter 5399 step
2022-06-12 15:07:04,808   Num examples = 872
2022-06-12 15:07:04,808   Batch size = 32
2022-06-12 15:07:04,809 ***** Eval results *****
2022-06-12 15:07:04,809   att_loss = 0.711174776306881
2022-06-12 15:07:04,809   global_step = 5399
2022-06-12 15:07:04,809   loss = 1.532255802506864
2022-06-12 15:07:04,809   rep_loss = 0.8210810263751436
2022-06-12 15:07:04,809 ***** Save model *****
2022-06-12 15:07:31,453 ***** Running evaluation *****
2022-06-12 15:07:31,454   Epoch = 2 iter 5499 step
2022-06-12 15:07:31,454   Num examples = 872
2022-06-12 15:07:31,454   Batch size = 32
2022-06-12 15:07:31,455 ***** Eval results *****
2022-06-12 15:07:31,455   att_loss = 0.7098192956491924
2022-06-12 15:07:31,455   global_step = 5499
2022-06-12 15:07:31,455   loss = 1.5300296996565035
2022-06-12 15:07:31,455   rep_loss = 0.8202104044459201
2022-06-12 15:07:31,455 ***** Save model *****
2022-06-12 15:07:58,107 ***** Running evaluation *****
2022-06-12 15:07:58,107   Epoch = 2 iter 5599 step
2022-06-12 15:07:58,107   Num examples = 872
2022-06-12 15:07:58,108   Batch size = 32
2022-06-12 15:07:58,109 ***** Eval results *****
2022-06-12 15:07:58,109   att_loss = 0.7089262304333455
2022-06-12 15:07:58,109   global_step = 5599
2022-06-12 15:07:58,109   loss = 1.5281802120181316
2022-06-12 15:07:58,109   rep_loss = 0.8192539815847861
2022-06-12 15:07:58,109 ***** Save model *****
2022-06-12 15:08:24,899 ***** Running evaluation *****
2022-06-12 15:08:24,900   Epoch = 2 iter 5699 step
2022-06-12 15:08:24,900   Num examples = 872
2022-06-12 15:08:24,900   Batch size = 32
2022-06-12 15:08:24,901 ***** Eval results *****
2022-06-12 15:08:24,901   att_loss = 0.70932421881748
2022-06-12 15:08:24,901   global_step = 5699
2022-06-12 15:08:24,901   loss = 1.528567709193623
2022-06-12 15:08:24,901   rep_loss = 0.8192434901762616
2022-06-12 15:08:24,901 ***** Save model *****
2022-06-12 15:08:51,595 ***** Running evaluation *****
2022-06-12 15:08:51,596   Epoch = 2 iter 5799 step
2022-06-12 15:08:51,596   Num examples = 872
2022-06-12 15:08:51,596   Batch size = 32
2022-06-12 15:08:51,597 ***** Eval results *****
2022-06-12 15:08:51,597   att_loss = 0.7080057619653506
2022-06-12 15:08:51,597   global_step = 5799
2022-06-12 15:08:51,597   loss = 1.5260290580302345
2022-06-12 15:08:51,597   rep_loss = 0.8180232961023476
2022-06-12 15:08:51,598 ***** Save model *****
2022-06-12 15:09:02,809 ***** Running evaluation *****
2022-06-12 15:09:02,810   Epoch = 4 iter 59999 step
2022-06-12 15:09:02,810   Num examples = 9815
2022-06-12 15:09:02,810   Batch size = 32
2022-06-12 15:09:10,917 ***** Eval results *****
2022-06-12 15:09:10,917   acc = 0.8273051451859399
2022-06-12 15:09:10,917   cls_loss = 0.21570405232021692
2022-06-12 15:09:10,917   eval_loss = 0.4699314804049967
2022-06-12 15:09:10,918   global_step = 59999
2022-06-12 15:09:10,918   loss = 0.21570405232021692
2022-06-12 15:09:18,217 ***** Running evaluation *****
2022-06-12 15:09:18,217   Epoch = 2 iter 5899 step
2022-06-12 15:09:18,217   Num examples = 872
2022-06-12 15:09:18,218   Batch size = 32
2022-06-12 15:09:18,219 ***** Eval results *****
2022-06-12 15:09:18,219   att_loss = 0.7063170606077669
2022-06-12 15:09:18,219   global_step = 5899
2022-06-12 15:09:18,219   loss = 1.5239834434389716
2022-06-12 15:09:18,219   rep_loss = 0.8176663829898214
2022-06-12 15:09:18,219 ***** Save model *****
2022-06-12 15:09:44,984 ***** Running evaluation *****
2022-06-12 15:09:44,984   Epoch = 2 iter 5999 step
2022-06-12 15:09:44,984   Num examples = 872
2022-06-12 15:09:44,984   Batch size = 32
2022-06-12 15:09:44,986 ***** Eval results *****
2022-06-12 15:09:44,986   att_loss = 0.7054481578898257
2022-06-12 15:09:44,986   global_step = 5999
2022-06-12 15:09:44,986   loss = 1.5221318107686983
2022-06-12 15:09:44,986   rep_loss = 0.8166836525793517
2022-06-12 15:09:44,986 ***** Save model *****
2022-06-12 15:10:11,676 ***** Running evaluation *****
2022-06-12 15:10:11,677   Epoch = 2 iter 6099 step
2022-06-12 15:10:11,677   Num examples = 872
2022-06-12 15:10:11,677   Batch size = 32
2022-06-12 15:10:11,678 ***** Eval results *****
2022-06-12 15:10:11,678   att_loss = 0.7037755228866666
2022-06-12 15:10:11,678   global_step = 6099
2022-06-12 15:10:11,678   loss = 1.5195065998388055
2022-06-12 15:10:11,678   rep_loss = 0.815731076794538
2022-06-12 15:10:11,678 ***** Save model *****
2022-06-12 15:10:38,288 ***** Running evaluation *****
2022-06-12 15:10:38,289   Epoch = 2 iter 6199 step
2022-06-12 15:10:38,289   Num examples = 872
2022-06-12 15:10:38,289   Batch size = 32
2022-06-12 15:10:38,290 ***** Eval results *****
2022-06-12 15:10:38,290   att_loss = 0.7028107220865505
2022-06-12 15:10:38,290   global_step = 6199
2022-06-12 15:10:38,291   loss = 1.518016616822726
2022-06-12 15:10:38,291   rep_loss = 0.8152058944218366
2022-06-12 15:10:38,291 ***** Save model *****
2022-06-12 15:11:05,002 ***** Running evaluation *****
2022-06-12 15:11:05,002   Epoch = 2 iter 6299 step
2022-06-12 15:11:05,002   Num examples = 872
2022-06-12 15:11:05,002   Batch size = 32
2022-06-12 15:11:05,003 ***** Eval results *****
2022-06-12 15:11:05,003   att_loss = 0.7025186261424945
2022-06-12 15:11:05,003   global_step = 6299
2022-06-12 15:11:05,004   loss = 1.5170941967989258
2022-06-12 15:11:05,004   rep_loss = 0.8145755705281572
2022-06-12 15:11:05,004 ***** Save model *****
2022-06-12 15:11:09,702 ***** Running evaluation *****
2022-06-12 15:11:09,703   Epoch = 4 iter 60499 step
2022-06-12 15:11:09,703   Num examples = 9815
2022-06-12 15:11:09,703   Batch size = 32
2022-06-12 15:11:17,817 ***** Eval results *****
2022-06-12 15:11:17,818   acc = 0.8259806418746816
2022-06-12 15:11:17,818   cls_loss = 0.21566813175765975
2022-06-12 15:11:17,818   eval_loss = 0.46940889605093467
2022-06-12 15:11:17,818   global_step = 60499
2022-06-12 15:11:17,818   loss = 0.21566813175765975
2022-06-12 15:11:31,702 ***** Running evaluation *****
2022-06-12 15:11:31,702   Epoch = 3 iter 6399 step
2022-06-12 15:11:31,702   Num examples = 872
2022-06-12 15:11:31,702   Batch size = 32
2022-06-12 15:11:31,703 ***** Eval results *****
2022-06-12 15:11:31,703   att_loss = 0.6777345760800373
2022-06-12 15:11:31,703   global_step = 6399
2022-06-12 15:11:31,703   loss = 1.4595853364330598
2022-06-12 15:11:31,704   rep_loss = 0.7818507531593586
2022-06-12 15:11:31,704 ***** Save model *****
2022-06-12 15:11:58,322 ***** Running evaluation *****
2022-06-12 15:11:58,323   Epoch = 3 iter 6499 step
2022-06-12 15:11:58,323   Num examples = 872
2022-06-12 15:11:58,323   Batch size = 32
2022-06-12 15:11:58,324 ***** Eval results *****
2022-06-12 15:11:58,324   att_loss = 0.6747660622558492
2022-06-12 15:11:58,324   global_step = 6499
2022-06-12 15:11:58,324   loss = 1.4608408996765627
2022-06-12 15:11:58,324   rep_loss = 0.7860748337551872
2022-06-12 15:11:58,325 ***** Save model *****
2022-06-12 15:12:24,804 ***** Running evaluation *****
2022-06-12 15:12:24,805   Epoch = 3 iter 6599 step
2022-06-12 15:12:24,805   Num examples = 872
2022-06-12 15:12:24,805   Batch size = 32
2022-06-12 15:12:24,806 ***** Eval results *****
2022-06-12 15:12:24,806   att_loss = 0.6816138588800663
2022-06-12 15:12:24,806   global_step = 6599
2022-06-12 15:12:24,806   loss = 1.469047691763901
2022-06-12 15:12:24,806   rep_loss = 0.7874338288340419
2022-06-12 15:12:24,806 ***** Save model *****
2022-06-12 15:12:51,467 ***** Running evaluation *****
2022-06-12 15:12:51,468   Epoch = 3 iter 6699 step
2022-06-12 15:12:51,468   Num examples = 872
2022-06-12 15:12:51,468   Batch size = 32
2022-06-12 15:12:51,470 ***** Eval results *****
2022-06-12 15:12:51,470   att_loss = 0.6839401606441469
2022-06-12 15:12:51,470   global_step = 6699
2022-06-12 15:12:51,470   loss = 1.47184197261968
2022-06-12 15:12:51,470   rep_loss = 0.7879018092032243
2022-06-12 15:12:51,470 ***** Save model *****
2022-06-12 15:13:16,824 ***** Running evaluation *****
2022-06-12 15:13:16,824   Epoch = 4 iter 60999 step
2022-06-12 15:13:16,824   Num examples = 9815
2022-06-12 15:13:16,824   Batch size = 32
2022-06-12 15:13:18,126 ***** Running evaluation *****
2022-06-12 15:13:18,126   Epoch = 3 iter 6799 step
2022-06-12 15:13:18,126   Num examples = 872
2022-06-12 15:13:18,126   Batch size = 32
2022-06-12 15:13:18,127 ***** Eval results *****
2022-06-12 15:13:18,127   att_loss = 0.6791215267269518
2022-06-12 15:13:18,127   global_step = 6799
2022-06-12 15:13:18,127   loss = 1.4655142528075702
2022-06-12 15:13:18,127   rep_loss = 0.7863927247343122
2022-06-12 15:13:18,128 ***** Save model *****
2022-06-12 15:13:24,946 ***** Eval results *****
2022-06-12 15:13:24,946   acc = 0.8283239938869078
2022-06-12 15:13:24,946   cls_loss = 0.21567148535355668
2022-06-12 15:13:24,946   eval_loss = 0.46814698364330815
2022-06-12 15:13:24,947   global_step = 60999
2022-06-12 15:13:24,947   loss = 0.21567148535355668
2022-06-12 15:13:44,715 ***** Running evaluation *****
2022-06-12 15:13:44,716   Epoch = 3 iter 6899 step
2022-06-12 15:13:44,716   Num examples = 872
2022-06-12 15:13:44,716   Batch size = 32
2022-06-12 15:13:44,717 ***** Eval results *****
2022-06-12 15:13:44,717   att_loss = 0.6770890495236931
2022-06-12 15:13:44,717   global_step = 6899
2022-06-12 15:13:44,717   loss = 1.4623210653535754
2022-06-12 15:13:44,718   rep_loss = 0.7852320153221767
2022-06-12 15:13:44,718 ***** Save model *****
2022-06-12 15:14:11,259 ***** Running evaluation *****
2022-06-12 15:14:11,259   Epoch = 3 iter 6999 step
2022-06-12 15:14:11,259   Num examples = 872
2022-06-12 15:14:11,259   Batch size = 32
2022-06-12 15:14:11,260 ***** Eval results *****
2022-06-12 15:14:11,260   att_loss = 0.676661551606013
2022-06-12 15:14:11,260   global_step = 6999
2022-06-12 15:14:11,260   loss = 1.4622025597390476
2022-06-12 15:14:11,260   rep_loss = 0.7855410082197953
2022-06-12 15:14:11,260 ***** Save model *****
2022-06-12 15:14:38,013 ***** Running evaluation *****
2022-06-12 15:14:38,014   Epoch = 3 iter 7099 step
2022-06-12 15:14:38,014   Num examples = 872
2022-06-12 15:14:38,014   Batch size = 32
2022-06-12 15:14:38,015 ***** Eval results *****
2022-06-12 15:14:38,015   att_loss = 0.6763801704973853
2022-06-12 15:14:38,015   global_step = 7099
2022-06-12 15:14:38,015   loss = 1.4617817583653645
2022-06-12 15:14:38,015   rep_loss = 0.7854015884738713
2022-06-12 15:14:38,015 ***** Save model *****
2022-06-12 15:15:04,734 ***** Running evaluation *****
2022-06-12 15:15:04,734   Epoch = 3 iter 7199 step
2022-06-12 15:15:04,734   Num examples = 872
2022-06-12 15:15:04,734   Batch size = 32
2022-06-12 15:15:04,736 ***** Eval results *****
2022-06-12 15:15:04,736   att_loss = 0.6746468218431935
2022-06-12 15:15:04,736   global_step = 7199
2022-06-12 15:15:04,736   loss = 1.4581809855636374
2022-06-12 15:15:04,736   rep_loss = 0.7835341636868448
2022-06-12 15:15:04,736 ***** Save model *****
2022-06-12 15:15:24,276 ***** Running evaluation *****
2022-06-12 15:15:24,276   Epoch = 5 iter 61499 step
2022-06-12 15:15:24,276   Num examples = 9815
2022-06-12 15:15:24,277   Batch size = 32
2022-06-12 15:15:31,381 ***** Running evaluation *****
2022-06-12 15:15:31,381   Epoch = 3 iter 7299 step
2022-06-12 15:15:31,381   Num examples = 872
2022-06-12 15:15:31,381   Batch size = 32
2022-06-12 15:15:31,382 ***** Eval results *****
2022-06-12 15:15:31,382   att_loss = 0.6729842618545621
2022-06-12 15:15:31,382   global_step = 7299
2022-06-12 15:15:31,382   loss = 1.4552965706484176
2022-06-12 15:15:31,382   rep_loss = 0.7823123079785943
2022-06-12 15:15:31,383 ***** Save model *****
2022-06-12 15:15:32,401 ***** Eval results *****
2022-06-12 15:15:32,402   acc = 0.8295466123280693
2022-06-12 15:15:32,402   cls_loss = 0.21512466120637125
2022-06-12 15:15:32,402   eval_loss = 0.4688851702776328
2022-06-12 15:15:32,402   global_step = 61499
2022-06-12 15:15:32,402   loss = 0.21512466120637125
2022-06-12 15:15:57,922 ***** Running evaluation *****
2022-06-12 15:15:57,923   Epoch = 3 iter 7399 step
2022-06-12 15:15:57,923   Num examples = 872
2022-06-12 15:15:57,923   Batch size = 32
2022-06-12 15:15:57,924 ***** Eval results *****
2022-06-12 15:15:57,924   att_loss = 0.6712223713071168
2022-06-12 15:15:57,924   global_step = 7399
2022-06-12 15:15:57,925   loss = 1.4528760988995255
2022-06-12 15:15:57,925   rep_loss = 0.7816537262763907
2022-06-12 15:15:57,925 ***** Save model *****
2022-06-12 15:16:24,493 ***** Running evaluation *****
2022-06-12 15:16:24,494   Epoch = 3 iter 7499 step
2022-06-12 15:16:24,494   Num examples = 872
2022-06-12 15:16:24,494   Batch size = 32
2022-06-12 15:16:24,495 ***** Eval results *****
2022-06-12 15:16:24,495   att_loss = 0.6694001006346946
2022-06-12 15:16:24,495   global_step = 7499
2022-06-12 15:16:24,495   loss = 1.4502926604930553
2022-06-12 15:16:24,495   rep_loss = 0.7808925587285338
2022-06-12 15:16:24,495 ***** Save model *****
2022-06-12 15:16:51,035 ***** Running evaluation *****
2022-06-12 15:16:51,036   Epoch = 3 iter 7599 step
2022-06-12 15:16:51,036   Num examples = 872
2022-06-12 15:16:51,036   Batch size = 32
2022-06-12 15:16:51,037 ***** Eval results *****
2022-06-12 15:16:51,038   att_loss = 0.6679201461172141
2022-06-12 15:16:51,038   global_step = 7599
2022-06-12 15:16:51,038   loss = 1.4485236904652616
2022-06-12 15:16:51,038   rep_loss = 0.780603543444947
2022-06-12 15:16:51,038 ***** Save model *****
2022-06-12 15:17:17,596 ***** Running evaluation *****
2022-06-12 15:17:17,596   Epoch = 3 iter 7699 step
2022-06-12 15:17:17,596   Num examples = 872
2022-06-12 15:17:17,596   Batch size = 32
2022-06-12 15:17:17,597 ***** Eval results *****
2022-06-12 15:17:17,597   att_loss = 0.6682151510033061
2022-06-12 15:17:17,598   global_step = 7699
2022-06-12 15:17:17,598   loss = 1.4490626169437115
2022-06-12 15:17:17,598   rep_loss = 0.7808474654676937
2022-06-12 15:17:17,598 ***** Save model *****
2022-06-12 15:17:31,196 ***** Running evaluation *****
2022-06-12 15:17:31,197   Epoch = 5 iter 61999 step
2022-06-12 15:17:31,197   Num examples = 9815
2022-06-12 15:17:31,197   Batch size = 32
2022-06-12 15:17:39,302 ***** Eval results *****
2022-06-12 15:17:39,303   acc = 0.8303616912888436
2022-06-12 15:17:39,303   cls_loss = 0.21502531023684496
2022-06-12 15:17:39,303   eval_loss = 0.46450385087281953
2022-06-12 15:17:39,303   global_step = 61999
2022-06-12 15:17:39,303   loss = 0.21502531023684496
2022-06-12 15:17:44,178 ***** Running evaluation *****
2022-06-12 15:17:44,179   Epoch = 3 iter 7799 step
2022-06-12 15:17:44,179   Num examples = 872
2022-06-12 15:17:44,179   Batch size = 32
2022-06-12 15:17:44,180 ***** Eval results *****
2022-06-12 15:17:44,180   att_loss = 0.6677985844511111
2022-06-12 15:17:44,180   global_step = 7799
2022-06-12 15:17:44,180   loss = 1.447973615784892
2022-06-12 15:17:44,180   rep_loss = 0.7801750308327332
2022-06-12 15:17:44,180 ***** Save model *****
2022-06-12 15:18:10,715 ***** Running evaluation *****
2022-06-12 15:18:10,715   Epoch = 3 iter 7899 step
2022-06-12 15:18:10,715   Num examples = 872
2022-06-12 15:18:10,715   Batch size = 32
2022-06-12 15:18:10,716 ***** Eval results *****
2022-06-12 15:18:10,716   att_loss = 0.665820303001566
2022-06-12 15:18:10,716   global_step = 7899
2022-06-12 15:18:10,716   loss = 1.4451056192262262
2022-06-12 15:18:10,716   rep_loss = 0.7792853160556488
2022-06-12 15:18:10,717 ***** Save model *****
2022-06-12 15:18:37,294 ***** Running evaluation *****
2022-06-12 15:18:37,295   Epoch = 3 iter 7999 step
2022-06-12 15:18:37,295   Num examples = 872
2022-06-12 15:18:37,295   Batch size = 32
2022-06-12 15:18:37,296 ***** Eval results *****
2022-06-12 15:18:37,296   att_loss = 0.6645611462574706
2022-06-12 15:18:37,296   global_step = 7999
2022-06-12 15:18:37,296   loss = 1.4434099513253447
2022-06-12 15:18:37,296   rep_loss = 0.7788488051208717
2022-06-12 15:18:37,296 ***** Save model *****
2022-06-12 15:19:03,815 ***** Running evaluation *****
2022-06-12 15:19:03,816   Epoch = 3 iter 8099 step
2022-06-12 15:19:03,816   Num examples = 872
2022-06-12 15:19:03,816   Batch size = 32
2022-06-12 15:19:03,817 ***** Eval results *****
2022-06-12 15:19:03,817   att_loss = 0.6648451915718553
2022-06-12 15:19:03,817   global_step = 8099
2022-06-12 15:19:03,817   loss = 1.443356266752861
2022-06-12 15:19:03,817   rep_loss = 0.7785110754478425
2022-06-12 15:19:03,817 ***** Save model *****
2022-06-12 15:19:30,335 ***** Running evaluation *****
2022-06-12 15:19:30,335   Epoch = 3 iter 8199 step
2022-06-12 15:19:30,335   Num examples = 872
2022-06-12 15:19:30,335   Batch size = 32
2022-06-12 15:19:30,336 ***** Eval results *****
2022-06-12 15:19:30,337   att_loss = 0.6641079670200287
2022-06-12 15:19:30,337   global_step = 8199
2022-06-12 15:19:30,337   loss = 1.4423886420048289
2022-06-12 15:19:30,337   rep_loss = 0.7782806749374196
2022-06-12 15:19:30,337 ***** Save model *****
2022-06-12 15:19:38,143 ***** Running evaluation *****
2022-06-12 15:19:38,143   Epoch = 5 iter 62499 step
2022-06-12 15:19:38,143   Num examples = 9815
2022-06-12 15:19:38,143   Batch size = 32
2022-06-12 15:19:46,251 ***** Eval results *****
2022-06-12 15:19:46,251   acc = 0.8290371879775853
2022-06-12 15:19:46,251   cls_loss = 0.21475530562961434
2022-06-12 15:19:46,251   eval_loss = 0.4701409786939621
2022-06-12 15:19:46,251   global_step = 62499
2022-06-12 15:19:46,251   loss = 0.21475530562961434
2022-06-12 15:19:56,860 ***** Running evaluation *****
2022-06-12 15:19:56,860   Epoch = 3 iter 8299 step
2022-06-12 15:19:56,860   Num examples = 872
2022-06-12 15:19:56,860   Batch size = 32
2022-06-12 15:19:56,862 ***** Eval results *****
2022-06-12 15:19:56,862   att_loss = 0.663499938583758
2022-06-12 15:19:56,862   global_step = 8299
2022-06-12 15:19:56,862   loss = 1.4415025085315791
2022-06-12 15:19:56,862   rep_loss = 0.778002570037813
2022-06-12 15:19:56,862 ***** Save model *****
2022-06-12 15:20:23,374 ***** Running evaluation *****
2022-06-12 15:20:23,374   Epoch = 3 iter 8399 step
2022-06-12 15:20:23,375   Num examples = 872
2022-06-12 15:20:23,375   Batch size = 32
2022-06-12 15:20:23,376 ***** Eval results *****
2022-06-12 15:20:23,376   att_loss = 0.6633180388007144
2022-06-12 15:20:23,376   global_step = 8399
2022-06-12 15:20:23,376   loss = 1.441078557143328
2022-06-12 15:20:23,376   rep_loss = 0.7777605187281731
2022-06-12 15:20:23,376 ***** Save model *****
2022-06-12 15:20:50,036 ***** Running evaluation *****
2022-06-12 15:20:50,037   Epoch = 4 iter 8499 step
2022-06-12 15:20:50,037   Num examples = 872
2022-06-12 15:20:50,037   Batch size = 32
2022-06-12 15:20:50,038 ***** Eval results *****
2022-06-12 15:20:50,038   att_loss = 0.6235113955405821
2022-06-12 15:20:50,038   global_step = 8499
2022-06-12 15:20:50,038   loss = 1.3711664576128304
2022-06-12 15:20:50,038   rep_loss = 0.7476550663810179
2022-06-12 15:20:50,038 ***** Save model *****
2022-06-12 15:21:16,640 ***** Running evaluation *****
2022-06-12 15:21:16,640   Epoch = 4 iter 8599 step
2022-06-12 15:21:16,641   Num examples = 872
2022-06-12 15:21:16,641   Batch size = 32
2022-06-12 15:21:16,642 ***** Eval results *****
2022-06-12 15:21:16,642   att_loss = 0.6443247386340887
2022-06-12 15:21:16,642   global_step = 8599
2022-06-12 15:21:16,642   loss = 1.3994989551481654
2022-06-12 15:21:16,642   rep_loss = 0.75517421895689
2022-06-12 15:21:16,642 ***** Save model *****
2022-06-12 15:21:43,243 ***** Running evaluation *****
2022-06-12 15:21:43,243   Epoch = 4 iter 8699 step
2022-06-12 15:21:43,243   Num examples = 872
2022-06-12 15:21:43,243   Batch size = 32
2022-06-12 15:21:43,244 ***** Eval results *****
2022-06-12 15:21:43,244   att_loss = 0.6413168530160883
2022-06-12 15:21:43,244   global_step = 8699
2022-06-12 15:21:43,244   loss = 1.3964302131228228
2022-06-12 15:21:43,244   rep_loss = 0.7551133632659912
2022-06-12 15:21:43,244 ***** Save model *****
2022-06-12 15:21:45,397 ***** Running evaluation *****
2022-06-12 15:21:45,398   Epoch = 5 iter 62999 step
2022-06-12 15:21:45,398   Num examples = 9815
2022-06-12 15:21:45,398   Batch size = 32
2022-06-12 15:21:53,541 ***** Eval results *****
2022-06-12 15:21:53,543   acc = 0.8280183392766174
2022-06-12 15:21:53,544   cls_loss = 0.2143734318279002
2022-06-12 15:21:53,544   eval_loss = 0.4701024334671443
2022-06-12 15:21:53,544   global_step = 62999
2022-06-12 15:21:53,544   loss = 0.2143734318279002
2022-06-12 15:22:09,787 ***** Running evaluation *****
2022-06-12 15:22:09,788   Epoch = 4 iter 8799 step
2022-06-12 15:22:09,788   Num examples = 872
2022-06-12 15:22:09,788   Batch size = 32
2022-06-12 15:22:09,789 ***** Eval results *****
2022-06-12 15:22:09,789   att_loss = 0.6433532490898362
2022-06-12 15:22:09,789   global_step = 8799
2022-06-12 15:22:09,789   loss = 1.399773959391422
2022-06-12 15:22:09,790   rep_loss = 0.7564207122469073
2022-06-12 15:22:09,790 ***** Save model *****
2022-06-12 15:22:36,355 ***** Running evaluation *****
2022-06-12 15:22:36,356   Epoch = 4 iter 8899 step
2022-06-12 15:22:36,356   Num examples = 872
2022-06-12 15:22:36,356   Batch size = 32
2022-06-12 15:22:36,357 ***** Eval results *****
2022-06-12 15:22:36,357   att_loss = 0.6464294461113079
2022-06-12 15:22:36,357   global_step = 8899
2022-06-12 15:22:36,357   loss = 1.4026781392146834
2022-06-12 15:22:36,357   rep_loss = 0.7562486942757237
2022-06-12 15:22:36,357 ***** Save model *****
2022-06-12 15:23:02,763 ***** Running evaluation *****
2022-06-12 15:23:02,763   Epoch = 4 iter 8999 step
2022-06-12 15:23:02,763   Num examples = 872
2022-06-12 15:23:02,763   Batch size = 32
2022-06-12 15:23:02,764 ***** Eval results *****
2022-06-12 15:23:02,764   att_loss = 0.6460127762006037
2022-06-12 15:23:02,764   global_step = 8999
2022-06-12 15:23:02,765   loss = 1.4028297899926712
2022-06-12 15:23:02,765   rep_loss = 0.7568170145077321
2022-06-12 15:23:02,765 ***** Save model *****
2022-06-12 15:23:29,395 ***** Running evaluation *****
2022-06-12 15:23:29,395   Epoch = 4 iter 9099 step
2022-06-12 15:23:29,395   Num examples = 872
2022-06-12 15:23:29,395   Batch size = 32
2022-06-12 15:23:29,396 ***** Eval results *****
2022-06-12 15:23:29,396   att_loss = 0.6448152289121776
2022-06-12 15:23:29,396   global_step = 9099
2022-06-12 15:23:29,396   loss = 1.401779672411839
2022-06-12 15:23:29,396   rep_loss = 0.7569644436305648
2022-06-12 15:23:29,396 ***** Save model *****
2022-06-12 15:23:52,339 ***** Running evaluation *****
2022-06-12 15:23:52,339   Epoch = 5 iter 63499 step
2022-06-12 15:23:52,339   Num examples = 9815
2022-06-12 15:23:52,340   Batch size = 32
2022-06-12 15:23:56,013 ***** Running evaluation *****
2022-06-12 15:23:56,014   Epoch = 4 iter 9199 step
2022-06-12 15:23:56,014   Num examples = 872
2022-06-12 15:23:56,014   Batch size = 32
2022-06-12 15:23:56,015 ***** Eval results *****
2022-06-12 15:23:56,015   att_loss = 0.6436961270383491
2022-06-12 15:23:56,015   global_step = 9199
2022-06-12 15:23:56,015   loss = 1.400120756635264
2022-06-12 15:23:56,016   rep_loss = 0.7564246302059022
2022-06-12 15:23:56,016 ***** Save model *****
2022-06-12 15:24:00,444 ***** Eval results *****
2022-06-12 15:24:00,444   acc = 0.8281202241467143
2022-06-12 15:24:00,444   cls_loss = 0.21434443389802282
2022-06-12 15:24:00,444   eval_loss = 0.4691725901361397
2022-06-12 15:24:00,444   global_step = 63499
2022-06-12 15:24:00,444   loss = 0.21434443389802282
2022-06-12 15:24:22,478 ***** Running evaluation *****
2022-06-12 15:24:22,478   Epoch = 4 iter 9299 step
2022-06-12 15:24:22,478   Num examples = 872
2022-06-12 15:24:22,478   Batch size = 32
2022-06-12 15:24:22,479 ***** Eval results *****
2022-06-12 15:24:22,479   att_loss = 0.6439014868652564
2022-06-12 15:24:22,479   global_step = 9299
2022-06-12 15:24:22,479   loss = 1.3999053667787786
2022-06-12 15:24:22,479   rep_loss = 0.7560038799472736
2022-06-12 15:24:22,479 ***** Save model *****
2022-06-12 15:24:49,026 ***** Running evaluation *****
2022-06-12 15:24:49,027   Epoch = 4 iter 9399 step
2022-06-12 15:24:49,027   Num examples = 872
2022-06-12 15:24:49,027   Batch size = 32
2022-06-12 15:24:49,028 ***** Eval results *****
2022-06-12 15:24:49,028   att_loss = 0.6444471046434657
2022-06-12 15:24:49,028   global_step = 9399
2022-06-12 15:24:49,028   loss = 1.4001902672912194
2022-06-12 15:24:49,028   rep_loss = 0.7557431630418842
2022-06-12 15:24:49,028 ***** Save model *****
2022-06-12 15:25:15,590 ***** Running evaluation *****
2022-06-12 15:25:15,591   Epoch = 4 iter 9499 step
2022-06-12 15:25:15,591   Num examples = 872
2022-06-12 15:25:15,591   Batch size = 32
2022-06-12 15:25:15,592 ***** Eval results *****
2022-06-12 15:25:15,592   att_loss = 0.6422599000393628
2022-06-12 15:25:15,592   global_step = 9499
2022-06-12 15:25:15,592   loss = 1.3975943110105844
2022-06-12 15:25:15,592   rep_loss = 0.7553344114115144
2022-06-12 15:25:15,593 ***** Save model *****
2022-06-12 15:25:42,182 ***** Running evaluation *****
2022-06-12 15:25:42,183   Epoch = 4 iter 9599 step
2022-06-12 15:25:42,183   Num examples = 872
2022-06-12 15:25:42,183   Batch size = 32
2022-06-12 15:25:42,184 ***** Eval results *****
2022-06-12 15:25:42,184   att_loss = 0.6407955573558405
2022-06-12 15:25:42,184   global_step = 9599
2022-06-12 15:25:42,184   loss = 1.3957740979384086
2022-06-12 15:25:42,184   rep_loss = 0.7549785406329524
2022-06-12 15:25:42,184 ***** Save model *****
2022-06-12 15:25:59,356 ***** Running evaluation *****
2022-06-12 15:25:59,357   Epoch = 5 iter 63999 step
2022-06-12 15:25:59,357   Num examples = 9815
2022-06-12 15:25:59,357   Batch size = 32
2022-06-12 15:26:07,463 ***** Eval results *****
2022-06-12 15:26:07,463   acc = 0.8280183392766174
2022-06-12 15:26:07,463   cls_loss = 0.21438875505855692
2022-06-12 15:26:07,464   eval_loss = 0.4671617902160079
2022-06-12 15:26:07,464   global_step = 63999
2022-06-12 15:26:07,464   loss = 0.21438875505855692
2022-06-12 15:26:08,827 ***** Running evaluation *****
2022-06-12 15:26:08,827   Epoch = 4 iter 9699 step
2022-06-12 15:26:08,827   Num examples = 872
2022-06-12 15:26:08,827   Batch size = 32
2022-06-12 15:26:08,829 ***** Eval results *****
2022-06-12 15:26:08,829   att_loss = 0.6407159678144373
2022-06-12 15:26:08,829   global_step = 9699
2022-06-12 15:26:08,829   loss = 1.3954863138525913
2022-06-12 15:26:08,829   rep_loss = 0.754770345782639
2022-06-12 15:26:08,829 ***** Save model *****
2022-06-12 15:26:35,547 ***** Running evaluation *****
2022-06-12 15:26:35,547   Epoch = 4 iter 9799 step
2022-06-12 15:26:35,548   Num examples = 872
2022-06-12 15:26:35,548   Batch size = 32
2022-06-12 15:26:35,549 ***** Eval results *****
2022-06-12 15:26:35,549   att_loss = 0.6406662561328016
2022-06-12 15:26:35,550   global_step = 9799
2022-06-12 15:26:35,550   loss = 1.3952152472169868
2022-06-12 15:26:35,550   rep_loss = 0.7545489908040475
2022-06-12 15:26:35,550 ***** Save model *****
2022-06-12 15:27:02,197 ***** Running evaluation *****
2022-06-12 15:27:02,198   Epoch = 4 iter 9899 step
2022-06-12 15:27:02,198   Num examples = 872
2022-06-12 15:27:02,198   Batch size = 32
2022-06-12 15:27:02,199 ***** Eval results *****
2022-06-12 15:27:02,200   att_loss = 0.6401511786639972
2022-06-12 15:27:02,200   global_step = 9899
2022-06-12 15:27:02,200   loss = 1.3945386817305778
2022-06-12 15:27:02,200   rep_loss = 0.7543875026043733
2022-06-12 15:27:02,200 ***** Save model *****
2022-06-12 15:27:28,851 ***** Running evaluation *****
2022-06-12 15:27:28,851   Epoch = 4 iter 9999 step
2022-06-12 15:27:28,851   Num examples = 872
2022-06-12 15:27:28,851   Batch size = 32
2022-06-12 15:27:28,853 ***** Eval results *****
2022-06-12 15:27:28,853   att_loss = 0.6384032148390123
2022-06-12 15:27:28,853   global_step = 9999
2022-06-12 15:27:28,853   loss = 1.3919742927472951
2022-06-12 15:27:28,853   rep_loss = 0.7535710774941001
2022-06-12 15:27:28,853 ***** Save model *****
2022-06-12 15:27:55,484 ***** Running evaluation *****
2022-06-12 15:27:55,485   Epoch = 4 iter 10099 step
2022-06-12 15:27:55,485   Num examples = 872
2022-06-12 15:27:55,485   Batch size = 32
2022-06-12 15:27:55,486 ***** Eval results *****
2022-06-12 15:27:55,486   att_loss = 0.637115643180982
2022-06-12 15:27:55,486   global_step = 10099
2022-06-12 15:27:55,486   loss = 1.3902932919707953
2022-06-12 15:27:55,486   rep_loss = 0.7531776486304426
2022-06-12 15:27:55,486 ***** Save model *****
2022-06-12 15:28:07,030 ***** Running evaluation *****
2022-06-12 15:28:07,030   Epoch = 5 iter 64499 step
2022-06-12 15:28:07,030   Num examples = 9815
2022-06-12 15:28:07,030   Batch size = 32
2022-06-12 15:28:15,143 ***** Eval results *****
2022-06-12 15:28:15,143   acc = 0.8304635761589404
2022-06-12 15:28:15,143   cls_loss = 0.214445798503799
2022-06-12 15:28:15,143   eval_loss = 0.4670085487614237
2022-06-12 15:28:15,143   global_step = 64499
2022-06-12 15:28:15,144   loss = 0.214445798503799
2022-06-12 15:28:22,067 ***** Running evaluation *****
2022-06-12 15:28:22,067   Epoch = 4 iter 10199 step
2022-06-12 15:28:22,067   Num examples = 872
2022-06-12 15:28:22,067   Batch size = 32
2022-06-12 15:28:22,068 ***** Eval results *****
2022-06-12 15:28:22,068   att_loss = 0.6364498490606608
2022-06-12 15:28:22,069   global_step = 10199
2022-06-12 15:28:22,069   loss = 1.3894323427491286
2022-06-12 15:28:22,069   rep_loss = 0.7529824936717532
2022-06-12 15:28:22,069 ***** Save model *****
2022-06-12 15:28:48,770 ***** Running evaluation *****
2022-06-12 15:28:48,770   Epoch = 4 iter 10299 step
2022-06-12 15:28:48,771   Num examples = 872
2022-06-12 15:28:48,771   Batch size = 32
2022-06-12 15:28:48,772 ***** Eval results *****
2022-06-12 15:28:48,772   att_loss = 0.6366462822582135
2022-06-12 15:28:48,772   global_step = 10299
2022-06-12 15:28:48,772   loss = 1.389693517983751
2022-06-12 15:28:48,772   rep_loss = 0.7530472356147483
2022-06-12 15:28:48,772 ***** Save model *****
2022-06-12 15:29:15,343 ***** Running evaluation *****
2022-06-12 15:29:15,343   Epoch = 4 iter 10399 step
2022-06-12 15:29:15,343   Num examples = 872
2022-06-12 15:29:15,343   Batch size = 32
2022-06-12 15:29:15,345 ***** Eval results *****
2022-06-12 15:29:15,345   att_loss = 0.6359809196422151
2022-06-12 15:29:15,345   global_step = 10399
2022-06-12 15:29:15,345   loss = 1.3887022733688354
2022-06-12 15:29:15,345   rep_loss = 0.752721353606389
2022-06-12 15:29:15,345 ***** Save model *****
2022-06-12 15:29:42,113 ***** Running evaluation *****
2022-06-12 15:29:42,114   Epoch = 4 iter 10499 step
2022-06-12 15:29:42,114   Num examples = 872
2022-06-12 15:29:42,114   Batch size = 32
2022-06-12 15:29:42,115 ***** Eval results *****
2022-06-12 15:29:42,115   att_loss = 0.6358656162861959
2022-06-12 15:29:42,115   global_step = 10499
2022-06-12 15:29:42,115   loss = 1.3885981838576105
2022-06-12 15:29:42,115   rep_loss = 0.7527325677287962
2022-06-12 15:29:42,115 ***** Save model *****
2022-06-12 15:30:08,746 ***** Running evaluation *****
2022-06-12 15:30:08,746   Epoch = 5 iter 10599 step
2022-06-12 15:30:08,746   Num examples = 872
2022-06-12 15:30:08,746   Batch size = 32
2022-06-12 15:30:08,747 ***** Eval results *****
2022-06-12 15:30:08,747   att_loss = 0.6406023675882364
2022-06-12 15:30:08,747   global_step = 10599
2022-06-12 15:30:08,747   loss = 1.383297696898255
2022-06-12 15:30:08,747   rep_loss = 0.7426953315734863
2022-06-12 15:30:08,748 ***** Save model *****
2022-06-12 15:30:14,087 ***** Running evaluation *****
2022-06-12 15:30:14,087   Epoch = 5 iter 64999 step
2022-06-12 15:30:14,087   Num examples = 9815
2022-06-12 15:30:14,087   Batch size = 32
2022-06-12 15:30:22,205 ***** Eval results *****
2022-06-12 15:30:22,206   acc = 0.8294447274579725
2022-06-12 15:30:22,206   cls_loss = 0.214565013834857
2022-06-12 15:30:22,206   eval_loss = 0.46916190527549395
2022-06-12 15:30:22,206   global_step = 64999
2022-06-12 15:30:22,206   loss = 0.214565013834857
2022-06-12 15:30:35,280 ***** Running evaluation *****
2022-06-12 15:30:35,280   Epoch = 5 iter 10699 step
2022-06-12 15:30:35,280   Num examples = 872
2022-06-12 15:30:35,281   Batch size = 32
2022-06-12 15:30:35,282 ***** Eval results *****
2022-06-12 15:30:35,282   att_loss = 0.6276796195427132
2022-06-12 15:30:35,282   global_step = 10699
2022-06-12 15:30:35,282   loss = 1.3662645257385084
2022-06-12 15:30:35,282   rep_loss = 0.7385849070282622
2022-06-12 15:30:35,282 ***** Save model *****
2022-06-12 15:31:01,971 ***** Running evaluation *****
2022-06-12 15:31:01,971   Epoch = 5 iter 10799 step
2022-06-12 15:31:01,971   Num examples = 872
2022-06-12 15:31:01,971   Batch size = 32
2022-06-12 15:31:01,973 ***** Eval results *****
2022-06-12 15:31:01,973   att_loss = 0.6175836055509506
2022-06-12 15:31:01,973   global_step = 10799
2022-06-12 15:31:01,973   loss = 1.3535700733089104
2022-06-12 15:31:01,973   rep_loss = 0.7359864686125068
2022-06-12 15:31:01,973 ***** Save model *****
2022-06-12 15:31:28,579 ***** Running evaluation *****
2022-06-12 15:31:28,580   Epoch = 5 iter 10899 step
2022-06-12 15:31:28,580   Num examples = 872
2022-06-12 15:31:28,580   Batch size = 32
2022-06-12 15:31:28,581 ***** Eval results *****
2022-06-12 15:31:28,581   att_loss = 0.6188321003498691
2022-06-12 15:31:28,581   global_step = 10899
2022-06-12 15:31:28,581   loss = 1.3565307909077264
2022-06-12 15:31:28,581   rep_loss = 0.7376986915014665
2022-06-12 15:31:28,581 ***** Save model *****
2022-06-12 15:31:55,154 ***** Running evaluation *****
2022-06-12 15:31:55,155   Epoch = 5 iter 10999 step
2022-06-12 15:31:55,155   Num examples = 872
2022-06-12 15:31:55,155   Batch size = 32
2022-06-12 15:31:55,156 ***** Eval results *****
2022-06-12 15:31:55,157   att_loss = 0.6229135357387877
2022-06-12 15:31:55,157   global_step = 10999
2022-06-12 15:31:55,157   loss = 1.3612047239733638
2022-06-12 15:31:55,157   rep_loss = 0.7382911877990516
2022-06-12 15:31:55,157 ***** Save model *****
2022-06-12 15:32:21,178 ***** Running evaluation *****
2022-06-12 15:32:21,179   Epoch = 5 iter 65499 step
2022-06-12 15:32:21,179   Num examples = 9815
2022-06-12 15:32:21,179   Batch size = 32
2022-06-12 15:32:21,657 ***** Running evaluation *****
2022-06-12 15:32:21,657   Epoch = 5 iter 11099 step
2022-06-12 15:32:21,657   Num examples = 872
2022-06-12 15:32:21,657   Batch size = 32
2022-06-12 15:32:21,658 ***** Eval results *****
2022-06-12 15:32:21,658   att_loss = 0.6219164138733826
2022-06-12 15:32:21,659   global_step = 11099
2022-06-12 15:32:21,659   loss = 1.3604453443863231
2022-06-12 15:32:21,659   rep_loss = 0.7385289305644126
2022-06-12 15:32:21,659 ***** Save model *****
2022-06-12 15:32:29,287 ***** Eval results *****
2022-06-12 15:32:29,287   acc = 0.8317880794701987
2022-06-12 15:32:29,287   cls_loss = 0.21455032201519805
2022-06-12 15:32:29,287   eval_loss = 0.4678679827467238
2022-06-12 15:32:29,287   global_step = 65499
2022-06-12 15:32:29,287   loss = 0.21455032201519805
2022-06-12 15:32:29,287 ***** Save model *****
2022-06-12 15:32:48,156 ***** Running evaluation *****
2022-06-12 15:32:48,157   Epoch = 5 iter 11199 step
2022-06-12 15:32:48,157   Num examples = 872
2022-06-12 15:32:48,157   Batch size = 32
2022-06-12 15:32:48,158 ***** Eval results *****
2022-06-12 15:32:48,158   att_loss = 0.6196940686987145
2022-06-12 15:32:48,158   global_step = 11199
2022-06-12 15:32:48,158   loss = 1.357754871490602
2022-06-12 15:32:48,158   rep_loss = 0.7380608027918876
2022-06-12 15:32:48,158 ***** Save model *****
2022-06-12 15:33:14,953 ***** Running evaluation *****
2022-06-12 15:33:14,953   Epoch = 5 iter 11299 step
2022-06-12 15:33:14,953   Num examples = 872
2022-06-12 15:33:14,953   Batch size = 32
2022-06-12 15:33:14,955 ***** Eval results *****
2022-06-12 15:33:14,955   att_loss = 0.6176182984617158
2022-06-12 15:33:14,955   global_step = 11299
2022-06-12 15:33:14,955   loss = 1.3548145684412416
2022-06-12 15:33:14,955   rep_loss = 0.7371962702473259
2022-06-12 15:33:14,955 ***** Save model *****
2022-06-12 15:33:41,570 ***** Running evaluation *****
2022-06-12 15:33:41,570   Epoch = 5 iter 11399 step
2022-06-12 15:33:41,570   Num examples = 872
2022-06-12 15:33:41,570   Batch size = 32
2022-06-12 15:33:41,571 ***** Eval results *****
2022-06-12 15:33:41,571   att_loss = 0.618246062081286
2022-06-12 15:33:41,571   global_step = 11399
2022-06-12 15:33:41,571   loss = 1.3557909891195807
2022-06-12 15:33:41,572   rep_loss = 0.73754492785201
2022-06-12 15:33:41,572 ***** Save model *****
2022-06-12 15:34:08,129 ***** Running evaluation *****
2022-06-12 15:34:08,130   Epoch = 5 iter 11499 step
2022-06-12 15:34:08,130   Num examples = 872
2022-06-12 15:34:08,130   Batch size = 32
2022-06-12 15:34:08,131 ***** Eval results *****
2022-06-12 15:34:08,131   att_loss = 0.6167203231778403
2022-06-12 15:34:08,131   global_step = 11499
2022-06-12 15:34:08,131   loss = 1.3539749885603891
2022-06-12 15:34:08,131   rep_loss = 0.73725466635668
2022-06-12 15:34:08,132 ***** Save model *****
2022-06-12 15:34:28,684 ***** Running evaluation *****
2022-06-12 15:34:28,684   Epoch = 5 iter 65999 step
2022-06-12 15:34:28,684   Num examples = 9815
2022-06-12 15:34:28,684   Batch size = 32
2022-06-12 15:34:34,661 ***** Running evaluation *****
2022-06-12 15:34:34,661   Epoch = 5 iter 11599 step
2022-06-12 15:34:34,661   Num examples = 872
2022-06-12 15:34:34,661   Batch size = 32
2022-06-12 15:34:34,662 ***** Eval results *****
2022-06-12 15:34:34,662   att_loss = 0.6161057394173988
2022-06-12 15:34:34,662   global_step = 11599
2022-06-12 15:34:34,662   loss = 1.353089916253112
2022-06-12 15:34:34,662   rep_loss = 0.7369841765871304
2022-06-12 15:34:34,662 ***** Save model *****
2022-06-12 15:34:36,806 ***** Eval results *****
2022-06-12 15:34:36,806   acc = 0.8315843097300051
2022-06-12 15:34:36,806   cls_loss = 0.21463867704625483
2022-06-12 15:34:36,806   eval_loss = 0.46547701357824406
2022-06-12 15:34:36,806   global_step = 65999
2022-06-12 15:34:36,806   loss = 0.21463867704625483
2022-06-12 15:35:01,161 ***** Running evaluation *****
2022-06-12 15:35:01,161   Epoch = 5 iter 11699 step
2022-06-12 15:35:01,162   Num examples = 872
2022-06-12 15:35:01,162   Batch size = 32
2022-06-12 15:35:01,163 ***** Eval results *****
2022-06-12 15:35:01,163   att_loss = 0.6159272772008024
2022-06-12 15:35:01,163   global_step = 11699
2022-06-12 15:35:01,163   loss = 1.352843823121502
2022-06-12 15:35:01,163   rep_loss = 0.7369165459965324
2022-06-12 15:35:01,163 ***** Save model *****
2022-06-12 15:35:27,626 ***** Running evaluation *****
2022-06-12 15:35:27,626   Epoch = 5 iter 11799 step
2022-06-12 15:35:27,626   Num examples = 872
2022-06-12 15:35:27,626   Batch size = 32
2022-06-12 15:35:27,627 ***** Eval results *****
2022-06-12 15:35:27,627   att_loss = 0.6156312985239409
2022-06-12 15:35:27,627   global_step = 11799
2022-06-12 15:35:27,628   loss = 1.3523061913470908
2022-06-12 15:35:27,628   rep_loss = 0.7366748930794639
2022-06-12 15:35:27,628 ***** Save model *****
2022-06-12 15:35:54,137 ***** Running evaluation *****
2022-06-12 15:35:54,138   Epoch = 5 iter 11899 step
2022-06-12 15:35:54,138   Num examples = 872
2022-06-12 15:35:54,138   Batch size = 32
2022-06-12 15:35:54,139 ***** Eval results *****
2022-06-12 15:35:54,139   att_loss = 0.6156558619408266
2022-06-12 15:35:54,139   global_step = 11899
2022-06-12 15:35:54,139   loss = 1.3526021507531512
2022-06-12 15:35:54,139   rep_loss = 0.7369462893093903
2022-06-12 15:35:54,140 ***** Save model *****
2022-06-12 15:36:20,729 ***** Running evaluation *****
2022-06-12 15:36:20,729   Epoch = 5 iter 11999 step
2022-06-12 15:36:20,729   Num examples = 872
2022-06-12 15:36:20,730   Batch size = 32
2022-06-12 15:36:20,730 ***** Eval results *****
2022-06-12 15:36:20,731   att_loss = 0.6152705920959664
2022-06-12 15:36:20,731   global_step = 11999
2022-06-12 15:36:20,731   loss = 1.3520571995297341
2022-06-12 15:36:20,731   rep_loss = 0.7367866081188787
2022-06-12 15:36:20,731 ***** Save model *****
2022-06-12 15:36:35,548 ***** Running evaluation *****
2022-06-12 15:36:35,549   Epoch = 5 iter 66499 step
2022-06-12 15:36:35,549   Num examples = 9815
2022-06-12 15:36:35,549   Batch size = 32
2022-06-12 15:36:43,664 ***** Eval results *****
2022-06-12 15:36:43,664   acc = 0.8323993886907795
2022-06-12 15:36:43,664   cls_loss = 0.2146121492901959
2022-06-12 15:36:43,664   eval_loss = 0.466915922366835
2022-06-12 15:36:43,665   global_step = 66499
2022-06-12 15:36:43,665   loss = 0.2146121492901959
2022-06-12 15:36:43,665 ***** Save model *****
2022-06-12 15:36:47,334 ***** Running evaluation *****
2022-06-12 15:36:47,334   Epoch = 5 iter 12099 step
2022-06-12 15:36:47,334   Num examples = 872
2022-06-12 15:36:47,334   Batch size = 32
2022-06-12 15:36:47,336 ***** Eval results *****
2022-06-12 15:36:47,336   att_loss = 0.6148430705070496
2022-06-12 15:36:47,336   global_step = 12099
2022-06-12 15:36:47,336   loss = 1.3511465065384152
2022-06-12 15:36:47,336   rep_loss = 0.7363034367485843
2022-06-12 15:36:47,336 ***** Save model *****
2022-06-12 15:37:13,963 ***** Running evaluation *****
2022-06-12 15:37:13,964   Epoch = 5 iter 12199 step
2022-06-12 15:37:13,964   Num examples = 872
2022-06-12 15:37:13,964   Batch size = 32
2022-06-12 15:37:13,965 ***** Eval results *****
2022-06-12 15:37:13,965   att_loss = 0.6152971200534032
2022-06-12 15:37:13,965   global_step = 12199
2022-06-12 15:37:13,965   loss = 1.3516834140746916
2022-06-12 15:37:13,965   rep_loss = 0.7363862945892897
2022-06-12 15:37:13,966 ***** Save model *****
2022-06-12 15:37:40,693 ***** Running evaluation *****
2022-06-12 15:37:40,694   Epoch = 5 iter 12299 step
2022-06-12 15:37:40,694   Num examples = 872
2022-06-12 15:37:40,694   Batch size = 32
2022-06-12 15:37:40,695 ***** Eval results *****
2022-06-12 15:37:40,695   att_loss = 0.6160061671668188
2022-06-12 15:37:40,695   global_step = 12299
2022-06-12 15:37:40,695   loss = 1.3524406103018363
2022-06-12 15:37:40,695   rep_loss = 0.7364344435705772
2022-06-12 15:37:40,695 ***** Save model *****
2022-06-12 15:38:07,380 ***** Running evaluation *****
2022-06-12 15:38:07,381   Epoch = 5 iter 12399 step
2022-06-12 15:38:07,381   Num examples = 872
2022-06-12 15:38:07,381   Batch size = 32
2022-06-12 15:38:07,382 ***** Eval results *****
2022-06-12 15:38:07,382   att_loss = 0.6156253627540584
2022-06-12 15:38:07,382   global_step = 12399
2022-06-12 15:38:07,382   loss = 1.351523536326341
2022-06-12 15:38:07,382   rep_loss = 0.7358981741749904
2022-06-12 15:38:07,382 ***** Save model *****
2022-06-12 15:38:34,231 ***** Running evaluation *****
2022-06-12 15:38:34,231   Epoch = 5 iter 12499 step
2022-06-12 15:38:34,231   Num examples = 872
2022-06-12 15:38:34,231   Batch size = 32
2022-06-12 15:38:34,232 ***** Eval results *****
2022-06-12 15:38:34,232   att_loss = 0.6157241556275789
2022-06-12 15:38:34,232   global_step = 12499
2022-06-12 15:38:34,232   loss = 1.351554866690055
2022-06-12 15:38:34,232   rep_loss = 0.7358307116497882
2022-06-12 15:38:34,232 ***** Save model *****
2022-06-12 15:38:42,866 ***** Running evaluation *****
2022-06-12 15:38:42,867   Epoch = 5 iter 66999 step
2022-06-12 15:38:42,867   Num examples = 9815
2022-06-12 15:38:42,867   Batch size = 32
2022-06-12 15:38:50,981 ***** Eval results *****
2022-06-12 15:38:50,981   acc = 0.8309730005094244
2022-06-12 15:38:50,982   cls_loss = 0.21454492752314205
2022-06-12 15:38:50,982   eval_loss = 0.4676818143855477
2022-06-12 15:38:50,982   global_step = 66999
2022-06-12 15:38:50,982   loss = 0.21454492752314205
2022-06-12 15:39:01,055 ***** Running evaluation *****
2022-06-12 15:39:01,055   Epoch = 5 iter 12599 step
2022-06-12 15:39:01,055   Num examples = 872
2022-06-12 15:39:01,055   Batch size = 32
2022-06-12 15:39:01,057 ***** Eval results *****
2022-06-12 15:39:01,057   att_loss = 0.6158781610554718
2022-06-12 15:39:01,057   global_step = 12599
2022-06-12 15:39:01,057   loss = 1.3516846653817651
2022-06-12 15:39:01,057   rep_loss = 0.7358065045126474
2022-06-12 15:39:01,057 ***** Save model *****
2022-06-12 15:39:27,823 ***** Running evaluation *****
2022-06-12 15:39:27,823   Epoch = 6 iter 12699 step
2022-06-12 15:39:27,823   Num examples = 872
2022-06-12 15:39:27,823   Batch size = 32
2022-06-12 15:39:27,824 ***** Eval results *****
2022-06-12 15:39:27,825   att_loss = 0.5988967883586883
2022-06-12 15:39:27,825   global_step = 12699
2022-06-12 15:39:27,825   loss = 1.3258324718475343
2022-06-12 15:39:27,825   rep_loss = 0.7269356870651245
2022-06-12 15:39:27,825 ***** Save model *****
2022-06-12 15:39:54,479 ***** Running evaluation *****
2022-06-12 15:39:54,480   Epoch = 6 iter 12799 step
2022-06-12 15:39:54,480   Num examples = 872
2022-06-12 15:39:54,480   Batch size = 32
2022-06-12 15:39:54,481 ***** Eval results *****
2022-06-12 15:39:54,481   att_loss = 0.6063885853971753
2022-06-12 15:39:54,482   global_step = 12799
2022-06-12 15:39:54,482   loss = 1.333113113130842
2022-06-12 15:39:54,482   rep_loss = 0.7267245258603777
2022-06-12 15:39:54,482 ***** Save model *****
2022-06-12 15:40:21,018 ***** Running evaluation *****
2022-06-12 15:40:21,018   Epoch = 6 iter 12899 step
2022-06-12 15:40:21,018   Num examples = 872
2022-06-12 15:40:21,018   Batch size = 32
2022-06-12 15:40:21,019 ***** Eval results *****
2022-06-12 15:40:21,019   att_loss = 0.6064969904856249
2022-06-12 15:40:21,019   global_step = 12899
2022-06-12 15:40:21,019   loss = 1.3327153916792436
2022-06-12 15:40:21,020   rep_loss = 0.72621840021827
2022-06-12 15:40:21,020 ***** Save model *****
2022-06-12 15:40:47,591 ***** Running evaluation *****
2022-06-12 15:40:47,591   Epoch = 6 iter 12999 step
2022-06-12 15:40:47,591   Num examples = 872
2022-06-12 15:40:47,591   Batch size = 32
2022-06-12 15:40:47,592 ***** Eval results *****
2022-06-12 15:40:47,592   att_loss = 0.6103814256191253
2022-06-12 15:40:47,592   global_step = 12999
2022-06-12 15:40:47,592   loss = 1.3370408296585083
2022-06-12 15:40:47,592   rep_loss = 0.7266594034830729
2022-06-12 15:40:47,592 ***** Save model *****
2022-06-12 15:40:50,081 ***** Running evaluation *****
2022-06-12 15:40:50,082   Epoch = 5 iter 67499 step
2022-06-12 15:40:50,082   Num examples = 9815
2022-06-12 15:40:50,082   Batch size = 32
2022-06-12 15:40:58,205 ***** Eval results *****
2022-06-12 15:40:58,205   acc = 0.8321956189505858
2022-06-12 15:40:58,206   cls_loss = 0.214527114238687
2022-06-12 15:40:58,206   eval_loss = 0.46614855450217035
2022-06-12 15:40:58,206   global_step = 67499
2022-06-12 15:40:58,206   loss = 0.214527114238687
2022-06-12 15:41:14,159 ***** Running evaluation *****
2022-06-12 15:41:14,159   Epoch = 6 iter 13099 step
2022-06-12 15:41:14,159   Num examples = 872
2022-06-12 15:41:14,159   Batch size = 32
2022-06-12 15:41:14,160 ***** Eval results *****
2022-06-12 15:41:14,160   att_loss = 0.6095230752543399
2022-06-12 15:41:14,160   global_step = 13099
2022-06-12 15:41:14,160   loss = 1.3351005596863597
2022-06-12 15:41:14,160   rep_loss = 0.7255774855613708
2022-06-12 15:41:14,161 ***** Save model *****
2022-06-12 15:41:40,706 ***** Running evaluation *****
2022-06-12 15:41:40,706   Epoch = 6 iter 13199 step
2022-06-12 15:41:40,707   Num examples = 872
2022-06-12 15:41:40,707   Batch size = 32
2022-06-12 15:41:40,707 ***** Eval results *****
2022-06-12 15:41:40,708   att_loss = 0.6092022467177847
2022-06-12 15:41:40,708   global_step = 13199
2022-06-12 15:41:40,708   loss = 1.3339508292986
2022-06-12 15:41:40,708   rep_loss = 0.7247485836692479
2022-06-12 15:41:40,708 ***** Save model *****
2022-06-12 15:42:07,279 ***** Running evaluation *****
2022-06-12 15:42:07,280   Epoch = 6 iter 13299 step
2022-06-12 15:42:07,280   Num examples = 872
2022-06-12 15:42:07,280   Batch size = 32
2022-06-12 15:42:07,281 ***** Eval results *****
2022-06-12 15:42:07,281   att_loss = 0.608524984077171
2022-06-12 15:42:07,281   global_step = 13299
2022-06-12 15:42:07,281   loss = 1.3331423876020643
2022-06-12 15:42:07,281   rep_loss = 0.7246174041430156
2022-06-12 15:42:07,281 ***** Save model *****
2022-06-12 15:42:33,864 ***** Running evaluation *****
2022-06-12 15:42:33,864   Epoch = 6 iter 13399 step
2022-06-12 15:42:33,864   Num examples = 872
2022-06-12 15:42:33,864   Batch size = 32
2022-06-12 15:42:33,865 ***** Eval results *****
2022-06-12 15:42:33,865   att_loss = 0.6101696378953996
2022-06-12 15:42:33,866   global_step = 13399
2022-06-12 15:42:33,866   loss = 1.3354429278835174
2022-06-12 15:42:33,866   rep_loss = 0.7252732909110284
2022-06-12 15:42:33,866 ***** Save model *****
2022-06-12 15:42:56,981 ***** Running evaluation *****
2022-06-12 15:42:56,982   Epoch = 5 iter 67999 step
2022-06-12 15:42:56,982   Num examples = 9815
2022-06-12 15:42:56,982   Batch size = 32
2022-06-12 15:43:00,504 ***** Running evaluation *****
2022-06-12 15:43:00,505   Epoch = 6 iter 13499 step
2022-06-12 15:43:00,505   Num examples = 872
2022-06-12 15:43:00,505   Batch size = 32
2022-06-12 15:43:00,506 ***** Eval results *****
2022-06-12 15:43:00,507   att_loss = 0.6098878584248679
2022-06-12 15:43:00,507   global_step = 13499
2022-06-12 15:43:00,507   loss = 1.3350845251083374
2022-06-12 15:43:00,507   rep_loss = 0.7251966669218881
2022-06-12 15:43:00,507 ***** Save model *****
2022-06-12 15:43:05,093 ***** Eval results *****
2022-06-12 15:43:05,093   acc = 0.8315843097300051
2022-06-12 15:43:05,093   cls_loss = 0.21457900368641616
2022-06-12 15:43:05,093   eval_loss = 0.4667442925881097
2022-06-12 15:43:05,093   global_step = 67999
2022-06-12 15:43:05,093   loss = 0.21457900368641616
2022-06-12 15:43:27,125 ***** Running evaluation *****
2022-06-12 15:43:27,126   Epoch = 6 iter 13599 step
2022-06-12 15:43:27,126   Num examples = 872
2022-06-12 15:43:27,126   Batch size = 32
2022-06-12 15:43:27,127 ***** Eval results *****
2022-06-12 15:43:27,127   att_loss = 0.6087851796700404
2022-06-12 15:43:27,127   global_step = 13599
2022-06-12 15:43:27,127   loss = 1.3336671786430554
2022-06-12 15:43:27,127   rep_loss = 0.7248819993092463
2022-06-12 15:43:27,127 ***** Save model *****
2022-06-12 15:43:53,733 ***** Running evaluation *****
2022-06-12 15:43:53,733   Epoch = 6 iter 13699 step
2022-06-12 15:43:53,733   Num examples = 872
2022-06-12 15:43:53,733   Batch size = 32
2022-06-12 15:43:53,734 ***** Eval results *****
2022-06-12 15:43:53,734   att_loss = 0.6059874895284342
2022-06-12 15:43:53,734   global_step = 13699
2022-06-12 15:43:53,735   loss = 1.3305899099970973
2022-06-12 15:43:53,735   rep_loss = 0.7246024197755858
2022-06-12 15:43:53,735 ***** Save model *****
2022-06-12 15:44:20,349 ***** Running evaluation *****
2022-06-12 15:44:20,349   Epoch = 6 iter 13799 step
2022-06-12 15:44:20,350   Num examples = 872
2022-06-12 15:44:20,350   Batch size = 32
2022-06-12 15:44:20,351 ***** Eval results *****
2022-06-12 15:44:20,351   att_loss = 0.6056991796544258
2022-06-12 15:44:20,351   global_step = 13799
2022-06-12 15:44:20,351   loss = 1.3302802415604287
2022-06-12 15:44:20,351   rep_loss = 0.7245810607646374
2022-06-12 15:44:20,352 ***** Save model *****
2022-06-12 15:44:46,933 ***** Running evaluation *****
2022-06-12 15:44:46,933   Epoch = 6 iter 13899 step
2022-06-12 15:44:46,933   Num examples = 872
2022-06-12 15:44:46,933   Batch size = 32
2022-06-12 15:44:46,934 ***** Eval results *****
2022-06-12 15:44:46,934   att_loss = 0.6056177132269915
2022-06-12 15:44:46,934   global_step = 13899
2022-06-12 15:44:46,934   loss = 1.3301345411001466
2022-06-12 15:44:46,934   rep_loss = 0.7245168264239442
2022-06-12 15:44:46,934 ***** Save model *****
2022-06-12 15:45:03,921 ***** Running evaluation *****
2022-06-12 15:45:03,921   Epoch = 5 iter 68499 step
2022-06-12 15:45:03,921   Num examples = 9815
2022-06-12 15:45:03,921   Batch size = 32
2022-06-12 15:45:12,034 ***** Eval results *****
2022-06-12 15:45:12,034   acc = 0.8316861946001018
2022-06-12 15:45:12,034   cls_loss = 0.2145474474389035
2022-06-12 15:45:12,034   eval_loss = 0.46499626425850277
2022-06-12 15:45:12,035   global_step = 68499
2022-06-12 15:45:12,035   loss = 0.2145474474389035
2022-06-12 15:45:13,621 ***** Running evaluation *****
2022-06-12 15:45:13,621   Epoch = 6 iter 13999 step
2022-06-12 15:45:13,621   Num examples = 872
2022-06-12 15:45:13,621   Batch size = 32
2022-06-12 15:45:13,623 ***** Eval results *****
2022-06-12 15:45:13,623   att_loss = 0.6039045880707827
2022-06-12 15:45:13,623   global_step = 13999
2022-06-12 15:45:13,623   loss = 1.3279386601014571
2022-06-12 15:45:13,623   rep_loss = 0.7240340710119768
2022-06-12 15:45:13,623 ***** Save model *****
2022-06-12 15:45:40,269 ***** Running evaluation *****
2022-06-12 15:45:40,269   Epoch = 6 iter 14099 step
2022-06-12 15:45:40,270   Num examples = 872
2022-06-12 15:45:40,270   Batch size = 32
2022-06-12 15:45:40,271 ***** Eval results *****
2022-06-12 15:45:40,271   att_loss = 0.6027952783794727
2022-06-12 15:45:40,271   global_step = 14099
2022-06-12 15:45:40,271   loss = 1.3262874609737072
2022-06-12 15:45:40,271   rep_loss = 0.7234921817052162
2022-06-12 15:45:40,271 ***** Save model *****
2022-06-12 15:46:07,056 ***** Running evaluation *****
2022-06-12 15:46:07,056   Epoch = 6 iter 14199 step
2022-06-12 15:46:07,056   Num examples = 872
2022-06-12 15:46:07,056   Batch size = 32
2022-06-12 15:46:07,058 ***** Eval results *****
2022-06-12 15:46:07,058   att_loss = 0.6025256882016621
2022-06-12 15:46:07,058   global_step = 14199
2022-06-12 15:46:07,058   loss = 1.325717932648129
2022-06-12 15:46:07,058   rep_loss = 0.7231922434246729
2022-06-12 15:46:07,058 ***** Save model *****
2022-06-12 15:46:33,652 ***** Running evaluation *****
2022-06-12 15:46:33,652   Epoch = 6 iter 14299 step
2022-06-12 15:46:33,652   Num examples = 872
2022-06-12 15:46:33,652   Batch size = 32
2022-06-12 15:46:33,653 ***** Eval results *****
2022-06-12 15:46:33,653   att_loss = 0.6016870628719899
2022-06-12 15:46:33,653   global_step = 14299
2022-06-12 15:46:33,653   loss = 1.3243852658414128
2022-06-12 15:46:33,653   rep_loss = 0.7226982019196695
2022-06-12 15:46:33,653 ***** Save model *****
2022-06-12 15:47:00,244 ***** Running evaluation *****
2022-06-12 15:47:00,245   Epoch = 6 iter 14399 step
2022-06-12 15:47:00,245   Num examples = 872
2022-06-12 15:47:00,245   Batch size = 32
2022-06-12 15:47:00,246 ***** Eval results *****
2022-06-12 15:47:00,246   att_loss = 0.6009834565579052
2022-06-12 15:47:00,246   global_step = 14399
2022-06-12 15:47:00,246   loss = 1.3233893576810059
2022-06-12 15:47:00,246   rep_loss = 0.7224058999477978
2022-06-12 15:47:00,246 ***** Save model *****
2022-06-12 15:47:10,766 ***** Running evaluation *****
2022-06-12 15:47:10,766   Epoch = 5 iter 68999 step
2022-06-12 15:47:10,766   Num examples = 9815
2022-06-12 15:47:10,766   Batch size = 32
2022-06-12 15:47:18,875 ***** Eval results *****
2022-06-12 15:47:18,876   acc = 0.8310748853795211
2022-06-12 15:47:18,876   cls_loss = 0.21448589357864725
2022-06-12 15:47:18,876   eval_loss = 0.4661611374406939
2022-06-12 15:47:18,876   global_step = 68999
2022-06-12 15:47:18,876   loss = 0.21448589357864725
2022-06-12 15:47:26,894 ***** Running evaluation *****
2022-06-12 15:47:26,894   Epoch = 6 iter 14499 step
2022-06-12 15:47:26,894   Num examples = 872
2022-06-12 15:47:26,894   Batch size = 32
2022-06-12 15:47:26,895 ***** Eval results *****
2022-06-12 15:47:26,895   att_loss = 0.6011298698743185
2022-06-12 15:47:26,895   global_step = 14499
2022-06-12 15:47:26,896   loss = 1.3234678336143493
2022-06-12 15:47:26,896   rep_loss = 0.7223379625320434
2022-06-12 15:47:26,896 ***** Save model *****
2022-06-12 15:47:53,472 ***** Running evaluation *****
2022-06-12 15:47:53,473   Epoch = 6 iter 14599 step
2022-06-12 15:47:53,473   Num examples = 872
2022-06-12 15:47:53,473   Batch size = 32
2022-06-12 15:47:53,474 ***** Eval results *****
2022-06-12 15:47:53,474   att_loss = 0.6012401824359652
2022-06-12 15:47:53,474   global_step = 14599
2022-06-12 15:47:53,474   loss = 1.323478226329707
2022-06-12 15:47:53,474   rep_loss = 0.7222380427167385
2022-06-12 15:47:53,474 ***** Save model *****
2022-06-12 15:48:19,936 ***** Running evaluation *****
2022-06-12 15:48:19,937   Epoch = 6 iter 14699 step
2022-06-12 15:48:19,937   Num examples = 872
2022-06-12 15:48:19,937   Batch size = 32
2022-06-12 15:48:19,939 ***** Eval results *****
2022-06-12 15:48:19,939   att_loss = 0.6003320745100458
2022-06-12 15:48:19,939   global_step = 14699
2022-06-12 15:48:19,939   loss = 1.322451054073242
2022-06-12 15:48:19,939   rep_loss = 0.7221189780407641
2022-06-12 15:48:19,939 ***** Save model *****
2022-06-12 15:48:46,666 ***** Running evaluation *****
2022-06-12 15:48:46,667   Epoch = 7 iter 14799 step
2022-06-12 15:48:46,667   Num examples = 872
2022-06-12 15:48:46,667   Batch size = 32
2022-06-12 15:48:46,668 ***** Eval results *****
2022-06-12 15:48:46,668   att_loss = 0.5857760738319074
2022-06-12 15:48:46,668   global_step = 14799
2022-06-12 15:48:46,668   loss = 1.304029859287638
2022-06-12 15:48:46,668   rep_loss = 0.7182537904927428
2022-06-12 15:48:46,669 ***** Save model *****
2022-06-12 15:49:13,480 ***** Running evaluation *****
2022-06-12 15:49:13,480   Epoch = 7 iter 14899 step
2022-06-12 15:49:13,480   Num examples = 872
2022-06-12 15:49:13,480   Batch size = 32
2022-06-12 15:49:13,482 ***** Eval results *****
2022-06-12 15:49:13,482   att_loss = 0.5794693794515398
2022-06-12 15:49:13,482   global_step = 14899
2022-06-12 15:49:13,482   loss = 1.2898700481269791
2022-06-12 15:49:13,482   rep_loss = 0.7104006744267648
2022-06-12 15:49:13,482 ***** Save model *****
2022-06-12 15:49:18,117 ***** Running evaluation *****
2022-06-12 15:49:18,118   Epoch = 5 iter 69499 step
2022-06-12 15:49:18,118   Num examples = 9815
2022-06-12 15:49:18,118   Batch size = 32
2022-06-12 15:49:26,229 ***** Eval results *****
2022-06-12 15:49:26,229   acc = 0.8323993886907795
2022-06-12 15:49:26,229   cls_loss = 0.2144375560834477
2022-06-12 15:49:26,229   eval_loss = 0.46761031599308844
2022-06-12 15:49:26,230   global_step = 69499
2022-06-12 15:49:26,230   loss = 0.2144375560834477
2022-06-12 15:49:40,184 ***** Running evaluation *****
2022-06-12 15:49:40,185   Epoch = 7 iter 14999 step
2022-06-12 15:49:40,185   Num examples = 872
2022-06-12 15:49:40,185   Batch size = 32
2022-06-12 15:49:40,187 ***** Eval results *****
2022-06-12 15:49:40,187   att_loss = 0.5821068326485553
2022-06-12 15:49:40,187   global_step = 14999
2022-06-12 15:49:40,187   loss = 1.292446008907473
2022-06-12 15:49:40,187   rep_loss = 0.7103391804378411
2022-06-12 15:49:40,188 ***** Save model *****
2022-06-12 15:50:06,738 ***** Running evaluation *****
2022-06-12 15:50:06,738   Epoch = 7 iter 15099 step
2022-06-12 15:50:06,738   Num examples = 872
2022-06-12 15:50:06,738   Batch size = 32
2022-06-12 15:50:06,739 ***** Eval results *****
2022-06-12 15:50:06,739   att_loss = 0.582512503284007
2022-06-12 15:50:06,739   global_step = 15099
2022-06-12 15:50:06,740   loss = 1.2930488043396942
2022-06-12 15:50:06,740   rep_loss = 0.7105363040278864
2022-06-12 15:50:06,740 ***** Save model *****
2022-06-12 15:50:33,304 ***** Running evaluation *****
2022-06-12 15:50:33,304   Epoch = 7 iter 15199 step
2022-06-12 15:50:33,304   Num examples = 872
2022-06-12 15:50:33,304   Batch size = 32
2022-06-12 15:50:33,305 ***** Eval results *****
2022-06-12 15:50:33,305   att_loss = 0.5848536628067114
2022-06-12 15:50:33,305   global_step = 15199
2022-06-12 15:50:33,305   loss = 1.2955245554067527
2022-06-12 15:50:33,306   rep_loss = 0.7106708944982784
2022-06-12 15:50:33,306 ***** Save model *****
2022-06-12 15:50:59,934 ***** Running evaluation *****
2022-06-12 15:50:59,934   Epoch = 7 iter 15299 step
2022-06-12 15:50:59,934   Num examples = 872
2022-06-12 15:50:59,934   Batch size = 32
2022-06-12 15:50:59,935 ***** Eval results *****
2022-06-12 15:50:59,935   att_loss = 0.5841187846744124
2022-06-12 15:50:59,936   global_step = 15299
2022-06-12 15:50:59,936   loss = 1.2947302918926846
2022-06-12 15:50:59,936   rep_loss = 0.7106115083143297
2022-06-12 15:50:59,936 ***** Save model *****
2022-06-12 15:51:25,276 ***** Running evaluation *****
2022-06-12 15:51:25,276   Epoch = 5 iter 69999 step
2022-06-12 15:51:25,276   Num examples = 9815
2022-06-12 15:51:25,276   Batch size = 32
2022-06-12 15:51:26,759 ***** Running evaluation *****
2022-06-12 15:51:26,759   Epoch = 7 iter 15399 step
2022-06-12 15:51:26,759   Num examples = 872
2022-06-12 15:51:26,759   Batch size = 32
2022-06-12 15:51:26,760 ***** Eval results *****
2022-06-12 15:51:26,760   att_loss = 0.5882367991388466
2022-06-12 15:51:26,761   global_step = 15399
2022-06-12 15:51:26,761   loss = 1.2994120698394434
2022-06-12 15:51:26,761   rep_loss = 0.7111752718109665
2022-06-12 15:51:26,761 ***** Save model *****
2022-06-12 15:51:33,393 ***** Eval results *****
2022-06-12 15:51:33,393   acc = 0.8322975038206827
2022-06-12 15:51:33,393   cls_loss = 0.21443008808164837
2022-06-12 15:51:33,393   eval_loss = 0.4661708621714713
2022-06-12 15:51:33,394   global_step = 69999
2022-06-12 15:51:33,394   loss = 0.21443008808164837
2022-06-12 15:51:53,451 ***** Running evaluation *****
2022-06-12 15:51:53,452   Epoch = 7 iter 15499 step
2022-06-12 15:51:53,452   Num examples = 872
2022-06-12 15:51:53,452   Batch size = 32
2022-06-12 15:51:53,453 ***** Eval results *****
2022-06-12 15:51:53,454   att_loss = 0.5862894592040863
2022-06-12 15:51:53,454   global_step = 15499
2022-06-12 15:51:53,454   loss = 1.2967633995409915
2022-06-12 15:51:53,454   rep_loss = 0.7104739402209429
2022-06-12 15:51:53,454 ***** Save model *****
2022-06-12 15:52:20,196 ***** Running evaluation *****
2022-06-12 15:52:20,197   Epoch = 7 iter 15599 step
2022-06-12 15:52:20,197   Num examples = 872
2022-06-12 15:52:20,197   Batch size = 32
2022-06-12 15:52:20,198 ***** Eval results *****
2022-06-12 15:52:20,199   att_loss = 0.5860806099309166
2022-06-12 15:52:20,199   global_step = 15599
2022-06-12 15:52:20,199   loss = 1.2973220173171138
2022-06-12 15:52:20,199   rep_loss = 0.7112414080705216
2022-06-12 15:52:20,199 ***** Save model *****
2022-06-12 15:52:46,961 ***** Running evaluation *****
2022-06-12 15:52:46,962   Epoch = 7 iter 15699 step
2022-06-12 15:52:46,962   Num examples = 872
2022-06-12 15:52:46,962   Batch size = 32
2022-06-12 15:52:46,963 ***** Eval results *****
2022-06-12 15:52:46,963   att_loss = 0.5864295964690365
2022-06-12 15:52:46,964   global_step = 15699
2022-06-12 15:52:46,964   loss = 1.2976930520193215
2022-06-12 15:52:46,964   rep_loss = 0.7112634554582079
2022-06-12 15:52:46,964 ***** Save model *****
2022-06-12 15:53:13,552 ***** Running evaluation *****
2022-06-12 15:53:13,552   Epoch = 7 iter 15799 step
2022-06-12 15:53:13,552   Num examples = 872
2022-06-12 15:53:13,552   Batch size = 32
2022-06-12 15:53:13,554 ***** Eval results *****
2022-06-12 15:53:13,554   att_loss = 0.5874496243135825
2022-06-12 15:53:13,554   global_step = 15799
2022-06-12 15:53:13,554   loss = 1.2989542843246995
2022-06-12 15:53:13,554   rep_loss = 0.711504659398931
2022-06-12 15:53:13,554 ***** Save model *****
2022-06-12 15:53:32,248 ***** Running evaluation *****
2022-06-12 15:53:32,249   Epoch = 5 iter 70499 step
2022-06-12 15:53:32,249   Num examples = 9815
2022-06-12 15:53:32,249   Batch size = 32
2022-06-12 15:53:40,101 ***** Running evaluation *****
2022-06-12 15:53:40,102   Epoch = 7 iter 15899 step
2022-06-12 15:53:40,102   Num examples = 872
2022-06-12 15:53:40,102   Batch size = 32
2022-06-12 15:53:40,103 ***** Eval results *****
2022-06-12 15:53:40,103   att_loss = 0.5876468632029835
2022-06-12 15:53:40,103   global_step = 15899
2022-06-12 15:53:40,103   loss = 1.2987419429444331
2022-06-12 15:53:40,103   rep_loss = 0.7110950787997877
2022-06-12 15:53:40,103 ***** Save model *****
2022-06-12 15:53:40,365 ***** Eval results *****
2022-06-12 15:53:40,365   acc = 0.8323993886907795
2022-06-12 15:53:40,365   cls_loss = 0.21447051818419838
2022-06-12 15:53:40,365   eval_loss = 0.46670808456231405
2022-06-12 15:53:40,365   global_step = 70499
2022-06-12 15:53:40,365   loss = 0.21447051818419838
2022-06-12 15:54:06,665 ***** Running evaluation *****
2022-06-12 15:54:06,665   Epoch = 7 iter 15999 step
2022-06-12 15:54:06,665   Num examples = 872
2022-06-12 15:54:06,665   Batch size = 32
2022-06-12 15:54:06,667 ***** Eval results *****
2022-06-12 15:54:06,667   att_loss = 0.5873198105208429
2022-06-12 15:54:06,667   global_step = 15999
2022-06-12 15:54:06,667   loss = 1.2984163266387538
2022-06-12 15:54:06,667   rep_loss = 0.7110965158599836
2022-06-12 15:54:06,667 ***** Save model *****
2022-06-12 15:54:33,247 ***** Running evaluation *****
2022-06-12 15:54:33,247   Epoch = 7 iter 16099 step
2022-06-12 15:54:33,247   Num examples = 872
2022-06-12 15:54:33,247   Batch size = 32
2022-06-12 15:54:33,248 ***** Eval results *****
2022-06-12 15:54:33,248   att_loss = 0.5859052915585293
2022-06-12 15:54:33,248   global_step = 16099
2022-06-12 15:54:33,248   loss = 1.2964596883560253
2022-06-12 15:54:33,249   rep_loss = 0.7105543966453324
2022-06-12 15:54:33,249 ***** Save model *****
2022-06-12 15:54:59,898 ***** Running evaluation *****
2022-06-12 15:54:59,898   Epoch = 7 iter 16199 step
2022-06-12 15:54:59,898   Num examples = 872
2022-06-12 15:54:59,898   Batch size = 32
2022-06-12 15:54:59,899 ***** Eval results *****
2022-06-12 15:54:59,899   att_loss = 0.5860037182075972
2022-06-12 15:54:59,899   global_step = 16199
2022-06-12 15:54:59,900   loss = 1.2967586481271351
2022-06-12 15:54:59,900   rep_loss = 0.7107549297169388
2022-06-12 15:54:59,900 ***** Save model *****
2022-06-12 15:55:26,356 ***** Running evaluation *****
2022-06-12 15:55:26,357   Epoch = 7 iter 16299 step
2022-06-12 15:55:26,357   Num examples = 872
2022-06-12 15:55:26,357   Batch size = 32
2022-06-12 15:55:26,358 ***** Eval results *****
2022-06-12 15:55:26,358   att_loss = 0.5864641944706478
2022-06-12 15:55:26,358   global_step = 16299
2022-06-12 15:55:26,358   loss = 1.2972463851594227
2022-06-12 15:55:26,358   rep_loss = 0.7107821904421612
2022-06-12 15:55:26,358 ***** Save model *****
2022-06-12 15:55:39,263 ***** Running evaluation *****
2022-06-12 15:55:39,264   Epoch = 5 iter 70999 step
2022-06-12 15:55:39,264   Num examples = 9815
2022-06-12 15:55:39,264   Batch size = 32
2022-06-12 15:55:47,392 ***** Eval results *****
2022-06-12 15:55:47,393   acc = 0.8319918492103923
2022-06-12 15:55:47,393   cls_loss = 0.21449837208331893
2022-06-12 15:55:47,393   eval_loss = 0.4652473139258083
2022-06-12 15:55:47,393   global_step = 70999
2022-06-12 15:55:47,393   loss = 0.21449837208331893
2022-06-12 15:55:52,895 ***** Running evaluation *****
2022-06-12 15:55:52,895   Epoch = 7 iter 16399 step
2022-06-12 15:55:52,895   Num examples = 872
2022-06-12 15:55:52,895   Batch size = 32
2022-06-12 15:55:52,896 ***** Eval results *****
2022-06-12 15:55:52,896   att_loss = 0.5863730586588133
2022-06-12 15:55:52,896   global_step = 16399
2022-06-12 15:55:52,896   loss = 1.2970800891670191
2022-06-12 15:55:52,897   rep_loss = 0.7107070303120205
2022-06-12 15:55:52,897 ***** Save model *****
2022-06-12 15:56:19,528 ***** Running evaluation *****
2022-06-12 15:56:19,529   Epoch = 7 iter 16499 step
2022-06-12 15:56:19,529   Num examples = 872
2022-06-12 15:56:19,529   Batch size = 32
2022-06-12 15:56:19,530 ***** Eval results *****
2022-06-12 15:56:19,531   att_loss = 0.5865057955550178
2022-06-12 15:56:19,531   global_step = 16499
2022-06-12 15:56:19,531   loss = 1.297369929204076
2022-06-12 15:56:19,531   rep_loss = 0.7108641333124989
2022-06-12 15:56:19,531 ***** Save model *****
2022-06-12 15:56:46,171 ***** Running evaluation *****
2022-06-12 15:56:46,172   Epoch = 7 iter 16599 step
2022-06-12 15:56:46,172   Num examples = 872
2022-06-12 15:56:46,172   Batch size = 32
2022-06-12 15:56:46,173 ***** Eval results *****
2022-06-12 15:56:46,173   att_loss = 0.5863842755030464
2022-06-12 15:56:46,173   global_step = 16599
2022-06-12 15:56:46,173   loss = 1.29699879387749
2022-06-12 15:56:46,173   rep_loss = 0.7106145181992295
2022-06-12 15:56:46,173 ***** Save model *****
2022-06-12 15:57:12,820 ***** Running evaluation *****
2022-06-12 15:57:12,820   Epoch = 7 iter 16699 step
2022-06-12 15:57:12,820   Num examples = 872
2022-06-12 15:57:12,821   Batch size = 32
2022-06-12 15:57:12,822 ***** Eval results *****
2022-06-12 15:57:12,822   att_loss = 0.5867929981179434
2022-06-12 15:57:12,822   global_step = 16699
2022-06-12 15:57:12,822   loss = 1.2972808376234355
2022-06-12 15:57:12,822   rep_loss = 0.7104878395054922
2022-06-12 15:57:12,823 ***** Save model *****
2022-06-12 15:57:39,453 ***** Running evaluation *****
2022-06-12 15:57:39,453   Epoch = 7 iter 16799 step
2022-06-12 15:57:39,453   Num examples = 872
2022-06-12 15:57:39,453   Batch size = 32
2022-06-12 15:57:39,454 ***** Eval results *****
2022-06-12 15:57:39,454   att_loss = 0.5872331620816849
2022-06-12 15:57:39,454   global_step = 16799
2022-06-12 15:57:39,454   loss = 1.2976345194819352
2022-06-12 15:57:39,454   rep_loss = 0.7104013573139085
2022-06-12 15:57:39,454 ***** Save model *****
2022-06-12 15:57:46,240 ***** Running evaluation *****
2022-06-12 15:57:46,240   Epoch = 5 iter 71499 step
2022-06-12 15:57:46,241   Num examples = 9815
2022-06-12 15:57:46,241   Batch size = 32
2022-06-12 15:57:54,349 ***** Eval results *****
2022-06-12 15:57:54,349   acc = 0.8330106979113602
2022-06-12 15:57:54,349   cls_loss = 0.2144364815716535
2022-06-12 15:57:54,349   eval_loss = 0.46565590638679477
2022-06-12 15:57:54,349   global_step = 71499
2022-06-12 15:57:54,349   loss = 0.2144364815716535
2022-06-12 15:57:54,349 ***** Save model *****
2022-06-12 15:58:06,058 ***** Running evaluation *****
2022-06-12 15:58:06,059   Epoch = 8 iter 16899 step
2022-06-12 15:58:06,059   Num examples = 872
2022-06-12 15:58:06,059   Batch size = 32
2022-06-12 15:58:06,060 ***** Eval results *****
2022-06-12 15:58:06,060   att_loss = 0.5854234708778894
2022-06-12 15:58:06,060   global_step = 16899
2022-06-12 15:58:06,060   loss = 1.2888553614047036
2022-06-12 15:58:06,060   rep_loss = 0.7034318874131388
2022-06-12 15:58:06,060 ***** Save model *****
2022-06-12 15:58:32,710 ***** Running evaluation *****
2022-06-12 15:58:32,710   Epoch = 8 iter 16999 step
2022-06-12 15:58:32,710   Num examples = 872
2022-06-12 15:58:32,710   Batch size = 32
2022-06-12 15:58:32,712 ***** Eval results *****
2022-06-12 15:58:32,712   att_loss = 0.582220671776526
2022-06-12 15:58:32,712   global_step = 16999
2022-06-12 15:58:32,712   loss = 1.2844069307435773
2022-06-12 15:58:32,712   rep_loss = 0.7021862571824811
2022-06-12 15:58:32,712 ***** Save model *****
2022-06-12 15:58:59,330 ***** Running evaluation *****
2022-06-12 15:58:59,330   Epoch = 8 iter 17099 step
2022-06-12 15:58:59,330   Num examples = 872
2022-06-12 15:58:59,330   Batch size = 32
2022-06-12 15:58:59,331 ***** Eval results *****
2022-06-12 15:58:59,331   att_loss = 0.5773981963650564
2022-06-12 15:58:59,331   global_step = 17099
2022-06-12 15:58:59,331   loss = 1.278063795316532
2022-06-12 15:58:59,331   rep_loss = 0.7006656007373824
2022-06-12 15:58:59,332 ***** Save model *****
2022-06-12 15:59:26,099 ***** Running evaluation *****
2022-06-12 15:59:26,100   Epoch = 8 iter 17199 step
2022-06-12 15:59:26,100   Num examples = 872
2022-06-12 15:59:26,100   Batch size = 32
2022-06-12 15:59:26,101 ***** Eval results *****
2022-06-12 15:59:26,101   att_loss = 0.5761987094333452
2022-06-12 15:59:26,101   global_step = 17199
2022-06-12 15:59:26,102   loss = 1.2769864662791468
2022-06-12 15:59:26,102   rep_loss = 0.7007877576578541
2022-06-12 15:59:26,102 ***** Save model *****
2022-06-12 15:59:52,822 ***** Running evaluation *****
2022-06-12 15:59:52,823   Epoch = 8 iter 17299 step
2022-06-12 15:59:52,823   Num examples = 872
2022-06-12 15:59:52,823   Batch size = 32
2022-06-12 15:59:52,824 ***** Eval results *****
2022-06-12 15:59:52,824   att_loss = 0.5775063372270964
2022-06-12 15:59:52,824   global_step = 17299
2022-06-12 15:59:52,824   loss = 1.2788302616337892
2022-06-12 15:59:52,824   rep_loss = 0.7013239254277575
2022-06-12 15:59:52,824 ***** Save model *****
2022-06-12 15:59:53,557 ***** Running evaluation *****
2022-06-12 15:59:53,557   Epoch = 5 iter 71999 step
2022-06-12 15:59:53,557   Num examples = 9815
2022-06-12 15:59:53,558   Batch size = 32
2022-06-12 16:00:01,666 ***** Eval results *****
2022-06-12 16:00:01,667   acc = 0.8322975038206827
2022-06-12 16:00:01,667   cls_loss = 0.21443242483941383
2022-06-12 16:00:01,667   eval_loss = 0.4660044154243283
2022-06-12 16:00:01,667   global_step = 71999
2022-06-12 16:00:01,667   loss = 0.21443242483941383
2022-06-12 16:00:19,412 ***** Running evaluation *****
2022-06-12 16:00:19,413   Epoch = 8 iter 17399 step
2022-06-12 16:00:19,413   Num examples = 872
2022-06-12 16:00:19,413   Batch size = 32
2022-06-12 16:00:19,414 ***** Eval results *****
2022-06-12 16:00:19,414   att_loss = 0.5767350804448338
2022-06-12 16:00:19,415   global_step = 17399
2022-06-12 16:00:19,415   loss = 1.2789547774526808
2022-06-12 16:00:19,415   rep_loss = 0.7022196979539524
2022-06-12 16:00:19,415 ***** Save model *****
2022-06-12 16:00:46,016 ***** Running evaluation *****
2022-06-12 16:00:46,017   Epoch = 8 iter 17499 step
2022-06-12 16:00:46,017   Num examples = 872
2022-06-12 16:00:46,017   Batch size = 32
2022-06-12 16:00:46,019 ***** Eval results *****
2022-06-12 16:00:46,019   att_loss = 0.5767566851679532
2022-06-12 16:00:46,019   global_step = 17499
2022-06-12 16:00:46,019   loss = 1.2790836198576565
2022-06-12 16:00:46,019   rep_loss = 0.7023269351811959
2022-06-12 16:00:46,020 ***** Save model *****
2022-06-12 16:01:12,715 ***** Running evaluation *****
2022-06-12 16:01:12,715   Epoch = 8 iter 17599 step
2022-06-12 16:01:12,716   Num examples = 872
2022-06-12 16:01:12,716   Batch size = 32
2022-06-12 16:01:12,717 ***** Eval results *****
2022-06-12 16:01:12,717   att_loss = 0.5786522029664712
2022-06-12 16:01:12,717   global_step = 17599
2022-06-12 16:01:12,717   loss = 1.2818132495165182
2022-06-12 16:01:12,717   rep_loss = 0.7031610469774597
2022-06-12 16:01:12,717 ***** Save model *****
2022-06-12 16:01:39,305 ***** Running evaluation *****
2022-06-12 16:01:39,306   Epoch = 8 iter 17699 step
2022-06-12 16:01:39,306   Num examples = 872
2022-06-12 16:01:39,306   Batch size = 32
2022-06-12 16:01:39,307 ***** Eval results *****
2022-06-12 16:01:39,307   att_loss = 0.5786730161037688
2022-06-12 16:01:39,307   global_step = 17699
2022-06-12 16:01:39,307   loss = 1.2818898042318043
2022-06-12 16:01:39,308   rep_loss = 0.7032167881967837
2022-06-12 16:01:39,308 ***** Save model *****
2022-06-12 16:02:00,610 ***** Running evaluation *****
2022-06-12 16:02:00,610   Epoch = 5 iter 72499 step
2022-06-12 16:02:00,611   Num examples = 9815
2022-06-12 16:02:00,611   Batch size = 32
2022-06-12 16:02:05,789 ***** Running evaluation *****
2022-06-12 16:02:05,790   Epoch = 8 iter 17799 step
2022-06-12 16:02:05,790   Num examples = 872
2022-06-12 16:02:05,790   Batch size = 32
2022-06-12 16:02:05,791 ***** Eval results *****
2022-06-12 16:02:05,791   att_loss = 0.5779687994946615
2022-06-12 16:02:05,791   global_step = 17799
2022-06-12 16:02:05,791   loss = 1.2818838192117252
2022-06-12 16:02:05,792   rep_loss = 0.7039150201177153
2022-06-12 16:02:05,792 ***** Save model *****
2022-06-12 16:02:08,716 ***** Eval results *****
2022-06-12 16:02:08,716   acc = 0.8321956189505858
2022-06-12 16:02:08,716   cls_loss = 0.2144041097210152
2022-06-12 16:02:08,716   eval_loss = 0.46549703447748864
2022-06-12 16:02:08,716   global_step = 72499
2022-06-12 16:02:08,717   loss = 0.2144041097210152
2022-06-12 16:02:32,323 ***** Running evaluation *****
2022-06-12 16:02:32,324   Epoch = 8 iter 17899 step
2022-06-12 16:02:32,324   Num examples = 872
2022-06-12 16:02:32,324   Batch size = 32
2022-06-12 16:02:32,325 ***** Eval results *****
2022-06-12 16:02:32,325   att_loss = 0.5765456583808266
2022-06-12 16:02:32,325   global_step = 17899
2022-06-12 16:02:32,325   loss = 1.2800254733515546
2022-06-12 16:02:32,325   rep_loss = 0.7034798153338303
2022-06-12 16:02:32,325 ***** Save model *****
2022-06-12 16:02:58,879 ***** Running evaluation *****
2022-06-12 16:02:58,880   Epoch = 8 iter 17999 step
2022-06-12 16:02:58,880   Num examples = 872
2022-06-12 16:02:58,880   Batch size = 32
2022-06-12 16:02:58,881 ***** Eval results *****
2022-06-12 16:02:58,881   att_loss = 0.5748576476161257
2022-06-12 16:02:58,881   global_step = 17999
2022-06-12 16:02:58,881   loss = 1.2775930769441468
2022-06-12 16:02:58,881   rep_loss = 0.702735429506784
2022-06-12 16:02:58,881 ***** Save model *****
2022-06-12 16:03:25,375 ***** Running evaluation *****
2022-06-12 16:03:25,375   Epoch = 8 iter 18099 step
2022-06-12 16:03:25,375   Num examples = 872
2022-06-12 16:03:25,375   Batch size = 32
2022-06-12 16:03:25,376 ***** Eval results *****
2022-06-12 16:03:25,376   att_loss = 0.5739638327658412
2022-06-12 16:03:25,376   global_step = 18099
2022-06-12 16:03:25,376   loss = 1.2763990127579283
2022-06-12 16:03:25,376   rep_loss = 0.7024351799685652
2022-06-12 16:03:25,376 ***** Save model *****
2022-06-12 16:03:51,913 ***** Running evaluation *****
2022-06-12 16:03:51,914   Epoch = 8 iter 18199 step
2022-06-12 16:03:51,914   Num examples = 872
2022-06-12 16:03:51,914   Batch size = 32
2022-06-12 16:03:51,915 ***** Eval results *****
2022-06-12 16:03:51,915   att_loss = 0.5742351536822301
2022-06-12 16:03:51,915   global_step = 18199
2022-06-12 16:03:51,915   loss = 1.2771012325224078
2022-06-12 16:03:51,915   rep_loss = 0.7028660786003638
2022-06-12 16:03:51,916 ***** Save model *****
2022-06-12 16:04:07,369 ***** Running evaluation *****
2022-06-12 16:04:07,369   Epoch = 5 iter 72999 step
2022-06-12 16:04:07,370   Num examples = 9815
2022-06-12 16:04:07,370   Batch size = 32
2022-06-12 16:04:15,488 ***** Eval results *****
2022-06-12 16:04:15,488   acc = 0.8323993886907795
2022-06-12 16:04:15,488   cls_loss = 0.21438250576162904
2022-06-12 16:04:15,488   eval_loss = 0.46494626765919045
2022-06-12 16:04:15,488   global_step = 72999
2022-06-12 16:04:15,488   loss = 0.21438250576162904
2022-06-12 16:04:18,518 ***** Running evaluation *****
2022-06-12 16:04:18,518   Epoch = 8 iter 18299 step
2022-06-12 16:04:18,518   Num examples = 872
2022-06-12 16:04:18,518   Batch size = 32
2022-06-12 16:04:18,519 ***** Eval results *****
2022-06-12 16:04:18,519   att_loss = 0.5748279283830074
2022-06-12 16:04:18,519   global_step = 18299
2022-06-12 16:04:18,519   loss = 1.2777594613963923
2022-06-12 16:04:18,519   rep_loss = 0.7029315326273969
2022-06-12 16:04:18,519 ***** Save model *****
2022-06-12 16:04:45,249 ***** Running evaluation *****
2022-06-12 16:04:45,249   Epoch = 8 iter 18399 step
2022-06-12 16:04:45,249   Num examples = 872
2022-06-12 16:04:45,249   Batch size = 32
2022-06-12 16:04:45,250 ***** Eval results *****
2022-06-12 16:04:45,251   att_loss = 0.5748127099808874
2022-06-12 16:04:45,251   global_step = 18399
2022-06-12 16:04:45,251   loss = 1.277615008874539
2022-06-12 16:04:45,251   rep_loss = 0.7028022984372025
2022-06-12 16:04:45,251 ***** Save model *****
2022-06-12 16:05:11,872 ***** Running evaluation *****
2022-06-12 16:05:11,872   Epoch = 8 iter 18499 step
2022-06-12 16:05:11,872   Num examples = 872
2022-06-12 16:05:11,872   Batch size = 32
2022-06-12 16:05:11,873 ***** Eval results *****
2022-06-12 16:05:11,873   att_loss = 0.575415876377251
2022-06-12 16:05:11,873   global_step = 18499
2022-06-12 16:05:11,873   loss = 1.2784222917565344
2022-06-12 16:05:11,873   rep_loss = 0.7030064150396048
2022-06-12 16:05:11,874 ***** Save model *****
2022-06-12 16:05:38,383 ***** Running evaluation *****
2022-06-12 16:05:38,383   Epoch = 8 iter 18599 step
2022-06-12 16:05:38,383   Num examples = 872
2022-06-12 16:05:38,383   Batch size = 32
2022-06-12 16:05:38,384 ***** Eval results *****
2022-06-12 16:05:38,384   att_loss = 0.5759705614254309
2022-06-12 16:05:38,384   global_step = 18599
2022-06-12 16:05:38,384   loss = 1.2792525065100389
2022-06-12 16:05:38,384   rep_loss = 0.7032819445954923
2022-06-12 16:05:38,384 ***** Save model *****
2022-06-12 16:06:04,901 ***** Running evaluation *****
2022-06-12 16:06:04,901   Epoch = 8 iter 18699 step
2022-06-12 16:06:04,901   Num examples = 872
2022-06-12 16:06:04,901   Batch size = 32
2022-06-12 16:06:04,902 ***** Eval results *****
2022-06-12 16:06:04,902   att_loss = 0.5755001409971976
2022-06-12 16:06:04,903   global_step = 18699
2022-06-12 16:06:04,903   loss = 1.278477836434191
2022-06-12 16:06:04,903   rep_loss = 0.7029776947346357
2022-06-12 16:06:04,903 ***** Save model *****
2022-06-12 16:06:14,309 ***** Running evaluation *****
2022-06-12 16:06:14,309   Epoch = 5 iter 73499 step
2022-06-12 16:06:14,309   Num examples = 9815
2022-06-12 16:06:14,309   Batch size = 32
2022-06-12 16:06:22,423 ***** Eval results *****
2022-06-12 16:06:22,423   acc = 0.832603158430973
2022-06-12 16:06:22,423   cls_loss = 0.21438832305818403
2022-06-12 16:06:22,423   eval_loss = 0.4655973300677557
2022-06-12 16:06:22,423   global_step = 73499
2022-06-12 16:06:22,423   loss = 0.21438832305818403
2022-06-12 16:06:31,492 ***** Running evaluation *****
2022-06-12 16:06:31,493   Epoch = 8 iter 18799 step
2022-06-12 16:06:31,493   Num examples = 872
2022-06-12 16:06:31,493   Batch size = 32
2022-06-12 16:06:31,494 ***** Eval results *****
2022-06-12 16:06:31,494   att_loss = 0.5766006428000281
2022-06-12 16:06:31,494   global_step = 18799
2022-06-12 16:06:31,494   loss = 1.279562109184556
2022-06-12 16:06:31,494   rep_loss = 0.7029614654300052
2022-06-12 16:06:31,495 ***** Save model *****
2022-06-12 16:06:52,701 Writing example 0 of 9832
2022-06-12 16:06:52,702 *** Example ***
2022-06-12 16:06:52,702 guid: dev_matched-0
2022-06-12 16:06:52,702 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-06-12 16:06:52,702 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 16:06:52,702 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 16:06:52,702 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 16:06:52,703 label: contradiction
2022-06-12 16:06:52,703 label_id: 0
2022-06-12 16:06:58,184 ***** Running evaluation *****
2022-06-12 16:06:58,184   Epoch = 8 iter 18899 step
2022-06-12 16:06:58,184   Num examples = 872
2022-06-12 16:06:58,184   Batch size = 32
2022-06-12 16:06:58,185 ***** Eval results *****
2022-06-12 16:06:58,186   att_loss = 0.5772613422935229
2022-06-12 16:06:58,186   global_step = 18899
2022-06-12 16:06:58,186   loss = 1.2802869785103062
2022-06-12 16:06:58,186   rep_loss = 0.7030256357121479
2022-06-12 16:06:58,186 ***** Save model *****
2022-06-12 16:06:58,717 ***** Running mm evaluation *****
2022-06-12 16:06:58,719   Num examples = 9832
2022-06-12 16:06:58,719   Batch size = 32
2022-06-12 16:07:07,205 ***** Eval results *****
2022-06-12 16:07:07,205   acc = 0.8302481692432873
2022-06-12 16:07:07,205   eval_loss = 0.48631168263299124
2022-06-12 16:07:07,206   global_step = 73626
2022-06-12 16:07:07,206 **************S*************
task_name = mnli
best_metirc = 0.8330106979113602
**************E*************

2022-06-12 16:07:08,380 Task finish! 
2022-06-12 16:07:08,381 Task cost 316.34599210000005 minutes, i.e. 5.272433208333333 hours. 
2022-06-12 16:07:25,062 ***** Running evaluation *****
2022-06-12 16:07:25,062   Epoch = 9 iter 18999 step
2022-06-12 16:07:25,062   Num examples = 872
2022-06-12 16:07:25,062   Batch size = 32
2022-06-12 16:07:25,063 ***** Eval results *****
2022-06-12 16:07:25,063   att_loss = 0.5795198612742953
2022-06-12 16:07:25,063   global_step = 18999
2022-06-12 16:07:25,063   loss = 1.277785991865491
2022-06-12 16:07:25,063   rep_loss = 0.6982661220762465
2022-06-12 16:07:25,064 ***** Save model *****
2022-06-12 16:07:51,570 ***** Running evaluation *****
2022-06-12 16:07:51,571   Epoch = 9 iter 19099 step
2022-06-12 16:07:51,571   Num examples = 872
2022-06-12 16:07:51,571   Batch size = 32
2022-06-12 16:07:51,572 ***** Eval results *****
2022-06-12 16:07:51,572   att_loss = 0.5787715248169343
2022-06-12 16:07:51,572   global_step = 19099
2022-06-12 16:07:51,572   loss = 1.2760475215736342
2022-06-12 16:07:51,572   rep_loss = 0.6972759867007016
2022-06-12 16:07:51,573 ***** Save model *****
2022-06-12 16:08:18,099 ***** Running evaluation *****
2022-06-12 16:08:18,100   Epoch = 9 iter 19199 step
2022-06-12 16:08:18,100   Num examples = 872
2022-06-12 16:08:18,100   Batch size = 32
2022-06-12 16:08:18,101 ***** Eval results *****
2022-06-12 16:08:18,101   att_loss = 0.5756788305909915
2022-06-12 16:08:18,101   global_step = 19199
2022-06-12 16:08:18,102   loss = 1.2724409905676606
2022-06-12 16:08:18,102   rep_loss = 0.6967621522711257
2022-06-12 16:08:18,102 ***** Save model *****
2022-06-12 16:08:44,708 ***** Running evaluation *****
2022-06-12 16:08:44,708   Epoch = 9 iter 19299 step
2022-06-12 16:08:44,708   Num examples = 872
2022-06-12 16:08:44,709   Batch size = 32
2022-06-12 16:08:44,710 ***** Eval results *****
2022-06-12 16:08:44,710   att_loss = 0.5712681223045696
2022-06-12 16:08:44,710   global_step = 19299
2022-06-12 16:08:44,710   loss = 1.266022306171659
2022-06-12 16:08:44,710   rep_loss = 0.6947541772169844
2022-06-12 16:08:44,710 ***** Save model *****
2022-06-12 16:09:11,274 ***** Running evaluation *****
2022-06-12 16:09:11,274   Epoch = 9 iter 19399 step
2022-06-12 16:09:11,275   Num examples = 872
2022-06-12 16:09:11,275   Batch size = 32
2022-06-12 16:09:11,276 ***** Eval results *****
2022-06-12 16:09:11,276   att_loss = 0.5735168304479406
2022-06-12 16:09:11,276   global_step = 19399
2022-06-12 16:09:11,276   loss = 1.269414838652889
2022-06-12 16:09:11,276   rep_loss = 0.6958980041497727
2022-06-12 16:09:11,276 ***** Save model *****
2022-06-12 16:09:37,902 ***** Running evaluation *****
2022-06-12 16:09:37,902   Epoch = 9 iter 19499 step
2022-06-12 16:09:37,902   Num examples = 872
2022-06-12 16:09:37,902   Batch size = 32
2022-06-12 16:09:37,903 ***** Eval results *****
2022-06-12 16:09:37,903   att_loss = 0.573619895517085
2022-06-12 16:09:37,903   global_step = 19499
2022-06-12 16:09:37,903   loss = 1.2689218781557643
2022-06-12 16:09:37,903   rep_loss = 0.695301979092044
2022-06-12 16:09:37,904 ***** Save model *****
2022-06-12 16:10:04,571 ***** Running evaluation *****
2022-06-12 16:10:04,571   Epoch = 9 iter 19599 step
2022-06-12 16:10:04,572   Num examples = 872
2022-06-12 16:10:04,572   Batch size = 32
2022-06-12 16:10:04,573 ***** Eval results *****
2022-06-12 16:10:04,573   att_loss = 0.5702028420625949
2022-06-12 16:10:04,573   global_step = 19599
2022-06-12 16:10:04,573   loss = 1.26478126128155
2022-06-12 16:10:04,573   rep_loss = 0.694578415488046
2022-06-12 16:10:04,573 ***** Save model *****
2022-06-12 16:10:31,097 ***** Running evaluation *****
2022-06-12 16:10:31,097   Epoch = 9 iter 19699 step
2022-06-12 16:10:31,097   Num examples = 872
2022-06-12 16:10:31,097   Batch size = 32
2022-06-12 16:10:31,098 ***** Eval results *****
2022-06-12 16:10:31,099   att_loss = 0.5710935148155393
2022-06-12 16:10:31,099   global_step = 19699
2022-06-12 16:10:31,099   loss = 1.2660482503171049
2022-06-12 16:10:31,099   rep_loss = 0.6949547317518632
2022-06-12 16:10:31,099 ***** Save model *****
2022-06-12 16:10:57,725 ***** Running evaluation *****
2022-06-12 16:10:57,725   Epoch = 9 iter 19799 step
2022-06-12 16:10:57,725   Num examples = 872
2022-06-12 16:10:57,725   Batch size = 32
2022-06-12 16:10:57,726 ***** Eval results *****
2022-06-12 16:10:57,726   att_loss = 0.5695086682036191
2022-06-12 16:10:57,727   global_step = 19799
2022-06-12 16:10:57,727   loss = 1.2642677674287992
2022-06-12 16:10:57,727   rep_loss = 0.6947590960135742
2022-06-12 16:10:57,727 ***** Save model *****
2022-06-12 16:11:24,289 ***** Running evaluation *****
2022-06-12 16:11:24,290   Epoch = 9 iter 19899 step
2022-06-12 16:11:24,290   Num examples = 872
2022-06-12 16:11:24,290   Batch size = 32
2022-06-12 16:11:24,291 ***** Eval results *****
2022-06-12 16:11:24,291   att_loss = 0.5694540113922468
2022-06-12 16:11:24,291   global_step = 19899
2022-06-12 16:11:24,291   loss = 1.2647623301542683
2022-06-12 16:11:24,291   rep_loss = 0.6953083161005473
2022-06-12 16:11:24,292 ***** Save model *****
2022-06-12 16:11:50,865 ***** Running evaluation *****
2022-06-12 16:11:50,865   Epoch = 9 iter 19999 step
2022-06-12 16:11:50,865   Num examples = 872
2022-06-12 16:11:50,866   Batch size = 32
2022-06-12 16:11:50,867 ***** Eval results *****
2022-06-12 16:11:50,867   att_loss = 0.5690314265376313
2022-06-12 16:11:50,867   global_step = 19999
2022-06-12 16:11:50,867   loss = 1.2644918761159156
2022-06-12 16:11:50,867   rep_loss = 0.6954604470270037
2022-06-12 16:11:50,867 ***** Save model *****
2022-06-12 16:12:17,355 ***** Running evaluation *****
2022-06-12 16:12:17,355   Epoch = 9 iter 20099 step
2022-06-12 16:12:17,355   Num examples = 872
2022-06-12 16:12:17,355   Batch size = 32
2022-06-12 16:12:17,356 ***** Eval results *****
2022-06-12 16:12:17,357   att_loss = 0.5685908465629391
2022-06-12 16:12:17,357   global_step = 20099
2022-06-12 16:12:17,357   loss = 1.2639578050923368
2022-06-12 16:12:17,357   rep_loss = 0.6953669556337293
2022-06-12 16:12:17,357 ***** Save model *****
2022-06-12 16:12:43,917 ***** Running evaluation *****
2022-06-12 16:12:43,917   Epoch = 9 iter 20199 step
2022-06-12 16:12:43,917   Num examples = 872
2022-06-12 16:12:43,917   Batch size = 32
2022-06-12 16:12:43,918 ***** Eval results *****
2022-06-12 16:12:43,918   att_loss = 0.5690162252171683
2022-06-12 16:12:43,919   global_step = 20199
2022-06-12 16:12:43,919   loss = 1.2643977178058639
2022-06-12 16:12:43,919   rep_loss = 0.6953814900402785
2022-06-12 16:12:43,919 ***** Save model *****
2022-06-12 16:13:10,483 ***** Running evaluation *****
2022-06-12 16:13:10,483   Epoch = 9 iter 20299 step
2022-06-12 16:13:10,483   Num examples = 872
2022-06-12 16:13:10,483   Batch size = 32
2022-06-12 16:13:10,484 ***** Eval results *****
2022-06-12 16:13:10,484   att_loss = 0.5692904488793735
2022-06-12 16:13:10,484   global_step = 20299
2022-06-12 16:13:10,484   loss = 1.264862903408154
2022-06-12 16:13:10,484   rep_loss = 0.6955724520361433
2022-06-12 16:13:10,485 ***** Save model *****
2022-06-12 16:13:36,905 ***** Running evaluation *****
2022-06-12 16:13:36,906   Epoch = 9 iter 20399 step
2022-06-12 16:13:36,906   Num examples = 872
2022-06-12 16:13:36,906   Batch size = 32
2022-06-12 16:13:36,907 ***** Eval results *****
2022-06-12 16:13:36,907   att_loss = 0.5680680745730071
2022-06-12 16:13:36,907   global_step = 20399
2022-06-12 16:13:36,907   loss = 1.2629385846409176
2022-06-12 16:13:36,907   rep_loss = 0.6948705072771256
2022-06-12 16:13:36,907 ***** Save model *****
2022-06-12 16:14:03,586 ***** Running evaluation *****
2022-06-12 16:14:03,587   Epoch = 9 iter 20499 step
2022-06-12 16:14:03,587   Num examples = 872
2022-06-12 16:14:03,587   Batch size = 32
2022-06-12 16:14:03,588 ***** Eval results *****
2022-06-12 16:14:03,588   att_loss = 0.5679093695037729
2022-06-12 16:14:03,588   global_step = 20499
2022-06-12 16:14:03,588   loss = 1.2630738262099024
2022-06-12 16:14:03,589   rep_loss = 0.6951644540748303
2022-06-12 16:14:03,589 ***** Save model *****
2022-06-12 16:14:30,154 ***** Running evaluation *****
2022-06-12 16:14:30,154   Epoch = 9 iter 20599 step
2022-06-12 16:14:30,154   Num examples = 872
2022-06-12 16:14:30,154   Batch size = 32
2022-06-12 16:14:30,155 ***** Eval results *****
2022-06-12 16:14:30,156   att_loss = 0.567301639486205
2022-06-12 16:14:30,156   global_step = 20599
2022-06-12 16:14:30,156   loss = 1.2619550963947734
2022-06-12 16:14:30,156   rep_loss = 0.6946534542025247
2022-06-12 16:14:30,156 ***** Save model *****
2022-06-12 16:14:56,708 ***** Running evaluation *****
2022-06-12 16:14:56,708   Epoch = 9 iter 20699 step
2022-06-12 16:14:56,708   Num examples = 872
2022-06-12 16:14:56,708   Batch size = 32
2022-06-12 16:14:56,710 ***** Eval results *****
2022-06-12 16:14:56,710   att_loss = 0.5669645275741021
2022-06-12 16:14:56,710   global_step = 20699
2022-06-12 16:14:56,710   loss = 1.2615167430971268
2022-06-12 16:14:56,710   rep_loss = 0.6945522128014286
2022-06-12 16:14:56,710 ***** Save model *****
2022-06-12 16:15:23,383 ***** Running evaluation *****
2022-06-12 16:15:23,384   Epoch = 9 iter 20799 step
2022-06-12 16:15:23,384   Num examples = 872
2022-06-12 16:15:23,384   Batch size = 32
2022-06-12 16:15:23,385 ***** Eval results *****
2022-06-12 16:15:23,385   att_loss = 0.5670428855631598
2022-06-12 16:15:23,385   global_step = 20799
2022-06-12 16:15:23,385   loss = 1.2614280528044484
2022-06-12 16:15:23,385   rep_loss = 0.6943851648577426
2022-06-12 16:15:23,386 ***** Save model *****
2022-06-12 16:15:49,890 ***** Running evaluation *****
2022-06-12 16:15:49,891   Epoch = 9 iter 20899 step
2022-06-12 16:15:49,891   Num examples = 872
2022-06-12 16:15:49,891   Batch size = 32
2022-06-12 16:15:49,892 ***** Eval results *****
2022-06-12 16:15:49,892   att_loss = 0.5670435516523331
2022-06-12 16:15:49,892   global_step = 20899
2022-06-12 16:15:49,892   loss = 1.261449431984387
2022-06-12 16:15:49,892   rep_loss = 0.6944058780699316
2022-06-12 16:15:49,892 ***** Save model *****
2022-06-12 16:16:16,437 ***** Running evaluation *****
2022-06-12 16:16:16,438   Epoch = 9 iter 20999 step
2022-06-12 16:16:16,438   Num examples = 872
2022-06-12 16:16:16,438   Batch size = 32
2022-06-12 16:16:16,439 ***** Eval results *****
2022-06-12 16:16:16,439   att_loss = 0.5664510431505787
2022-06-12 16:16:16,439   global_step = 20999
2022-06-12 16:16:16,439   loss = 1.2609449732783231
2022-06-12 16:16:16,439   rep_loss = 0.6944939278597053
2022-06-12 16:16:16,439 ***** Save model *****
2022-06-12 16:16:43,002 ***** Running evaluation *****
2022-06-12 16:16:43,003   Epoch = 10 iter 21099 step
2022-06-12 16:16:43,003   Num examples = 872
2022-06-12 16:16:43,003   Batch size = 32
2022-06-12 16:16:43,004 ***** Eval results *****
2022-06-12 16:16:43,004   att_loss = 0.564540124040539
2022-06-12 16:16:43,004   global_step = 21099
2022-06-12 16:16:43,004   loss = 1.2532588829428464
2022-06-12 16:16:43,004   rep_loss = 0.6887187594074314
2022-06-12 16:16:43,005 ***** Save model *****
2022-06-12 16:17:09,645 ***** Running evaluation *****
2022-06-12 16:17:09,645   Epoch = 10 iter 21199 step
2022-06-12 16:17:09,645   Num examples = 872
2022-06-12 16:17:09,645   Batch size = 32
2022-06-12 16:17:09,646 ***** Eval results *****
2022-06-12 16:17:09,647   att_loss = 0.5630498974578185
2022-06-12 16:17:09,647   global_step = 21199
2022-06-12 16:17:09,647   loss = 1.2520026740787913
2022-06-12 16:17:09,647   rep_loss = 0.6889527754963569
2022-06-12 16:17:09,647 ***** Save model *****
2022-06-12 16:17:36,416 ***** Running evaluation *****
2022-06-12 16:17:36,416   Epoch = 10 iter 21299 step
2022-06-12 16:17:36,416   Num examples = 872
2022-06-12 16:17:36,416   Batch size = 32
2022-06-12 16:17:36,417 ***** Eval results *****
2022-06-12 16:17:36,417   att_loss = 0.5626439583347571
2022-06-12 16:17:36,417   global_step = 21299
2022-06-12 16:17:36,418   loss = 1.2517319563272837
2022-06-12 16:17:36,418   rep_loss = 0.6890879998335967
2022-06-12 16:17:36,418 ***** Save model *****
2022-06-12 16:18:03,227 ***** Running evaluation *****
2022-06-12 16:18:03,228   Epoch = 10 iter 21399 step
2022-06-12 16:18:03,228   Num examples = 872
2022-06-12 16:18:03,228   Batch size = 32
2022-06-12 16:18:03,230 ***** Eval results *****
2022-06-12 16:18:03,230   att_loss = 0.5630102523688154
2022-06-12 16:18:03,230   global_step = 21399
2022-06-12 16:18:03,230   loss = 1.2514595339557255
2022-06-12 16:18:03,230   rep_loss = 0.68844928399434
2022-06-12 16:18:03,230 ***** Save model *****
2022-06-12 16:18:29,831 ***** Running evaluation *****
2022-06-12 16:18:29,831   Epoch = 10 iter 21499 step
2022-06-12 16:18:29,831   Num examples = 872
2022-06-12 16:18:29,831   Batch size = 32
2022-06-12 16:18:29,833 ***** Eval results *****
2022-06-12 16:18:29,833   att_loss = 0.5647669037832413
2022-06-12 16:18:29,833   global_step = 21499
2022-06-12 16:18:29,833   loss = 1.2537753165157792
2022-06-12 16:18:29,833   rep_loss = 0.6890084153946189
2022-06-12 16:18:29,833 ***** Save model *****
2022-06-12 16:18:56,351 ***** Running evaluation *****
2022-06-12 16:18:56,351   Epoch = 10 iter 21599 step
2022-06-12 16:18:56,351   Num examples = 872
2022-06-12 16:18:56,351   Batch size = 32
2022-06-12 16:18:56,352 ***** Eval results *****
2022-06-12 16:18:56,352   att_loss = 0.5620996961120203
2022-06-12 16:18:56,353   global_step = 21599
2022-06-12 16:18:56,353   loss = 1.2499790740780838
2022-06-12 16:18:56,353   rep_loss = 0.6878793803651773
2022-06-12 16:18:56,353 ***** Save model *****
2022-06-12 16:19:22,816 ***** Running evaluation *****
2022-06-12 16:19:22,817   Epoch = 10 iter 21699 step
2022-06-12 16:19:22,817   Num examples = 872
2022-06-12 16:19:22,817   Batch size = 32
2022-06-12 16:19:22,818 ***** Eval results *****
2022-06-12 16:19:22,818   att_loss = 0.5591640781288987
2022-06-12 16:19:22,818   global_step = 21699
2022-06-12 16:19:22,818   loss = 1.2471247687144418
2022-06-12 16:19:22,818   rep_loss = 0.6879606932537328
2022-06-12 16:19:22,818 ***** Save model *****
2022-06-12 16:19:49,291 ***** Running evaluation *****
2022-06-12 16:19:49,292   Epoch = 10 iter 21799 step
2022-06-12 16:19:49,292   Num examples = 872
2022-06-12 16:19:49,292   Batch size = 32
2022-06-12 16:19:49,293 ***** Eval results *****
2022-06-12 16:19:49,293   att_loss = 0.5606639824601501
2022-06-12 16:19:49,293   global_step = 21799
2022-06-12 16:19:49,293   loss = 1.2486000805504238
2022-06-12 16:19:49,294   rep_loss = 0.6879360992289657
2022-06-12 16:19:49,294 ***** Save model *****
2022-06-12 16:20:15,778 ***** Running evaluation *****
2022-06-12 16:20:15,779   Epoch = 10 iter 21899 step
2022-06-12 16:20:15,779   Num examples = 872
2022-06-12 16:20:15,779   Batch size = 32
2022-06-12 16:20:15,780 ***** Eval results *****
2022-06-12 16:20:15,780   att_loss = 0.5604318331783947
2022-06-12 16:20:15,780   global_step = 21899
2022-06-12 16:20:15,780   loss = 1.2483669834447824
2022-06-12 16:20:15,780   rep_loss = 0.6879351507521064
2022-06-12 16:20:15,781 ***** Save model *****
2022-06-12 16:20:42,352 ***** Running evaluation *****
2022-06-12 16:20:42,353   Epoch = 10 iter 21999 step
2022-06-12 16:20:42,353   Num examples = 872
2022-06-12 16:20:42,353   Batch size = 32
2022-06-12 16:20:42,354 ***** Eval results *****
2022-06-12 16:20:42,354   att_loss = 0.560681812674957
2022-06-12 16:20:42,354   global_step = 21999
2022-06-12 16:20:42,354   loss = 1.2488021541685954
2022-06-12 16:20:42,354   rep_loss = 0.6881203410896445
2022-06-12 16:20:42,354 ***** Save model *****
2022-06-12 16:21:08,963 ***** Running evaluation *****
2022-06-12 16:21:08,964   Epoch = 10 iter 22099 step
2022-06-12 16:21:08,964   Num examples = 872
2022-06-12 16:21:08,964   Batch size = 32
2022-06-12 16:21:08,965 ***** Eval results *****
2022-06-12 16:21:08,966   att_loss = 0.5613626537174409
2022-06-12 16:21:08,966   global_step = 22099
2022-06-12 16:21:08,966   loss = 1.2499277451558424
2022-06-12 16:21:08,966   rep_loss = 0.6885650904815753
2022-06-12 16:21:08,966 ***** Save model *****
2022-06-12 16:21:35,506 ***** Running evaluation *****
2022-06-12 16:21:35,506   Epoch = 10 iter 22199 step
2022-06-12 16:21:35,506   Num examples = 872
2022-06-12 16:21:35,506   Batch size = 32
2022-06-12 16:21:35,507 ***** Eval results *****
2022-06-12 16:21:35,507   att_loss = 0.5613364181043363
2022-06-12 16:21:35,507   global_step = 22199
2022-06-12 16:21:35,508   loss = 1.2500888996334092
2022-06-12 16:21:35,508   rep_loss = 0.6887524811433655
2022-06-12 16:21:35,508 ***** Save model *****
2022-06-12 16:22:02,028 ***** Running evaluation *****
2022-06-12 16:22:02,029   Epoch = 10 iter 22299 step
2022-06-12 16:22:02,029   Num examples = 872
2022-06-12 16:22:02,029   Batch size = 32
2022-06-12 16:22:02,030 ***** Eval results *****
2022-06-12 16:22:02,030   att_loss = 0.5608588947816157
2022-06-12 16:22:02,030   global_step = 22299
2022-06-12 16:22:02,030   loss = 1.2495008039038933
2022-06-12 16:22:02,030   rep_loss = 0.6886419084358064
2022-06-12 16:22:02,031 ***** Save model *****
2022-06-12 16:22:28,672 ***** Running evaluation *****
2022-06-12 16:22:28,673   Epoch = 10 iter 22399 step
2022-06-12 16:22:28,673   Num examples = 872
2022-06-12 16:22:28,673   Batch size = 32
2022-06-12 16:22:28,674 ***** Eval results *****
2022-06-12 16:22:28,674   att_loss = 0.5605925646137015
2022-06-12 16:22:28,674   global_step = 22399
2022-06-12 16:22:28,674   loss = 1.2491373932037044
2022-06-12 16:22:28,675   rep_loss = 0.6885448283268477
2022-06-12 16:22:28,675 ***** Save model *****
2022-06-12 16:22:55,141 ***** Running evaluation *****
2022-06-12 16:22:55,141   Epoch = 10 iter 22499 step
2022-06-12 16:22:55,141   Num examples = 872
2022-06-12 16:22:55,141   Batch size = 32
2022-06-12 16:22:55,142 ***** Eval results *****
2022-06-12 16:22:55,143   att_loss = 0.5607453168582394
2022-06-12 16:22:55,143   global_step = 22499
2022-06-12 16:22:55,143   loss = 1.249357503803729
2022-06-12 16:22:55,143   rep_loss = 0.68861218635312
2022-06-12 16:22:55,143 ***** Save model *****
2022-06-12 16:23:21,669 ***** Running evaluation *****
2022-06-12 16:23:21,670   Epoch = 10 iter 22599 step
2022-06-12 16:23:21,670   Num examples = 872
2022-06-12 16:23:21,670   Batch size = 32
2022-06-12 16:23:21,671 ***** Eval results *****
2022-06-12 16:23:21,671   att_loss = 0.5597936446235454
2022-06-12 16:23:21,671   global_step = 22599
2022-06-12 16:23:21,671   loss = 1.2481730841840972
2022-06-12 16:23:21,671   rep_loss = 0.6883794386620855
2022-06-12 16:23:21,671 ***** Save model *****
2022-06-12 16:23:48,159 ***** Running evaluation *****
2022-06-12 16:23:48,160   Epoch = 10 iter 22699 step
2022-06-12 16:23:48,160   Num examples = 872
2022-06-12 16:23:48,160   Batch size = 32
2022-06-12 16:23:48,161 ***** Eval results *****
2022-06-12 16:23:48,161   att_loss = 0.5595311966374956
2022-06-12 16:23:48,161   global_step = 22699
2022-06-12 16:23:48,161   loss = 1.2476483205127888
2022-06-12 16:23:48,161   rep_loss = 0.6881171228154156
2022-06-12 16:23:48,161 ***** Save model *****
2022-06-12 16:24:14,980 ***** Running evaluation *****
2022-06-12 16:24:14,980   Epoch = 10 iter 22799 step
2022-06-12 16:24:14,980   Num examples = 872
2022-06-12 16:24:14,980   Batch size = 32
2022-06-12 16:24:14,981 ***** Eval results *****
2022-06-12 16:24:14,981   att_loss = 0.5598045179219053
2022-06-12 16:24:14,981   global_step = 22799
2022-06-12 16:24:14,981   loss = 1.2478257279087022
2022-06-12 16:24:14,981   rep_loss = 0.6880212091396586
2022-06-12 16:24:14,982 ***** Save model *****
2022-06-12 16:24:41,560 ***** Running evaluation *****
2022-06-12 16:24:41,560   Epoch = 10 iter 22899 step
2022-06-12 16:24:41,560   Num examples = 872
2022-06-12 16:24:41,560   Batch size = 32
2022-06-12 16:24:41,562 ***** Eval results *****
2022-06-12 16:24:41,562   att_loss = 0.5590995144561903
2022-06-12 16:24:41,562   global_step = 22899
2022-06-12 16:24:41,562   loss = 1.2472069189713937
2022-06-12 16:24:41,562   rep_loss = 0.6881074036495092
2022-06-12 16:24:41,562 ***** Save model *****
2022-06-12 16:25:08,206 ***** Running evaluation *****
2022-06-12 16:25:08,207   Epoch = 10 iter 22999 step
2022-06-12 16:25:08,207   Num examples = 872
2022-06-12 16:25:08,207   Batch size = 32
2022-06-12 16:25:08,208 ***** Eval results *****
2022-06-12 16:25:08,208   att_loss = 0.5585403254654043
2022-06-12 16:25:08,208   global_step = 22999
2022-06-12 16:25:08,208   loss = 1.2465540870468856
2022-06-12 16:25:08,208   rep_loss = 0.6880137607295517
2022-06-12 16:25:08,208 ***** Save model *****
2022-06-12 16:25:34,722 ***** Running evaluation *****
2022-06-12 16:25:34,723   Epoch = 10 iter 23099 step
2022-06-12 16:25:34,723   Num examples = 872
2022-06-12 16:25:34,723   Batch size = 32
2022-06-12 16:25:34,724 ***** Eval results *****
2022-06-12 16:25:34,724   att_loss = 0.5587799163800296
2022-06-12 16:25:34,724   global_step = 23099
2022-06-12 16:25:34,724   loss = 1.2468699934761636
2022-06-12 16:25:34,724   rep_loss = 0.6880900763145288
2022-06-12 16:25:34,724 ***** Save model *****
2022-06-12 16:26:01,261 ***** Running evaluation *****
2022-06-12 16:26:01,262   Epoch = 11 iter 23199 step
2022-06-12 16:26:01,262   Num examples = 872
2022-06-12 16:26:01,262   Batch size = 32
2022-06-12 16:26:01,263 ***** Eval results *****
2022-06-12 16:26:01,263   att_loss = 0.5560615127736872
2022-06-12 16:26:01,263   global_step = 23199
2022-06-12 16:26:01,263   loss = 1.2411672429604963
2022-06-12 16:26:01,263   rep_loss = 0.6851057280193675
2022-06-12 16:26:01,263 ***** Save model *****
2022-06-12 16:26:27,885 ***** Running evaluation *****
2022-06-12 16:26:27,886   Epoch = 11 iter 23299 step
2022-06-12 16:26:27,886   Num examples = 872
2022-06-12 16:26:27,886   Batch size = 32
2022-06-12 16:26:27,887 ***** Eval results *****
2022-06-12 16:26:27,887   att_loss = 0.553564977453601
2022-06-12 16:26:27,887   global_step = 23299
2022-06-12 16:26:27,887   loss = 1.2391167498403979
2022-06-12 16:26:27,887   rep_loss = 0.6855517718099778
2022-06-12 16:26:27,887 ***** Save model *****
2022-06-12 16:26:54,527 ***** Running evaluation *****
2022-06-12 16:26:54,527   Epoch = 11 iter 23399 step
2022-06-12 16:26:54,527   Num examples = 872
2022-06-12 16:26:54,527   Batch size = 32
2022-06-12 16:26:54,528 ***** Eval results *****
2022-06-12 16:26:54,528   att_loss = 0.5534886336794087
2022-06-12 16:26:54,529   global_step = 23399
2022-06-12 16:26:54,529   loss = 1.2385118390999588
2022-06-12 16:26:54,529   rep_loss = 0.6850232037843442
2022-06-12 16:26:54,529 ***** Save model *****
2022-06-12 16:27:21,326 ***** Running evaluation *****
2022-06-12 16:27:21,326   Epoch = 11 iter 23499 step
2022-06-12 16:27:21,326   Num examples = 872
2022-06-12 16:27:21,326   Batch size = 32
2022-06-12 16:27:21,328 ***** Eval results *****
2022-06-12 16:27:21,328   att_loss = 0.5550499610497918
2022-06-12 16:27:21,328   global_step = 23499
2022-06-12 16:27:21,328   loss = 1.239871978927666
2022-06-12 16:27:21,328   rep_loss = 0.6848220168704718
2022-06-12 16:27:21,328 ***** Save model *****
2022-06-12 16:27:48,010 ***** Running evaluation *****
2022-06-12 16:27:48,011   Epoch = 11 iter 23599 step
2022-06-12 16:27:48,011   Num examples = 872
2022-06-12 16:27:48,011   Batch size = 32
2022-06-12 16:27:48,012 ***** Eval results *****
2022-06-12 16:27:48,012   att_loss = 0.5571408258689629
2022-06-12 16:27:48,012   global_step = 23599
2022-06-12 16:27:48,012   loss = 1.2419067906809378
2022-06-12 16:27:48,012   rep_loss = 0.6847659650739732
2022-06-12 16:27:48,013 ***** Save model *****
2022-06-12 16:28:14,708 ***** Running evaluation *****
2022-06-12 16:28:14,709   Epoch = 11 iter 23699 step
2022-06-12 16:28:14,709   Num examples = 872
2022-06-12 16:28:14,709   Batch size = 32
2022-06-12 16:28:14,710 ***** Eval results *****
2022-06-12 16:28:14,710   att_loss = 0.556929775240185
2022-06-12 16:28:14,710   global_step = 23699
2022-06-12 16:28:14,710   loss = 1.2416715391047366
2022-06-12 16:28:14,710   rep_loss = 0.6847417632738749
2022-06-12 16:28:14,711 ***** Save model *****
2022-06-12 16:28:41,461 ***** Running evaluation *****
2022-06-12 16:28:41,461   Epoch = 11 iter 23799 step
2022-06-12 16:28:41,461   Num examples = 872
2022-06-12 16:28:41,461   Batch size = 32
2022-06-12 16:28:41,463 ***** Eval results *****
2022-06-12 16:28:41,463   att_loss = 0.5555562044372996
2022-06-12 16:28:41,463   global_step = 23799
2022-06-12 16:28:41,463   loss = 1.239898549600412
2022-06-12 16:28:41,463   rep_loss = 0.6843423440256191
2022-06-12 16:28:41,463 ***** Save model *****
2022-06-12 16:29:08,130 ***** Running evaluation *****
2022-06-12 16:29:08,131   Epoch = 11 iter 23899 step
2022-06-12 16:29:08,131   Num examples = 872
2022-06-12 16:29:08,131   Batch size = 32
2022-06-12 16:29:08,132 ***** Eval results *****
2022-06-12 16:29:08,132   att_loss = 0.5544781982898712
2022-06-12 16:29:08,132   global_step = 23899
2022-06-12 16:29:08,132   loss = 1.2382635851569523
2022-06-12 16:29:08,133   rep_loss = 0.6837853867486613
2022-06-12 16:29:08,133 ***** Save model *****
2022-06-12 16:29:34,727 ***** Running evaluation *****
2022-06-12 16:29:34,727   Epoch = 11 iter 23999 step
2022-06-12 16:29:34,728   Num examples = 872
2022-06-12 16:29:34,728   Batch size = 32
2022-06-12 16:29:34,728 ***** Eval results *****
2022-06-12 16:29:34,729   att_loss = 0.5530491426325681
2022-06-12 16:29:34,729   global_step = 23999
2022-06-12 16:29:34,729   loss = 1.2365835421963742
2022-06-12 16:29:34,729   rep_loss = 0.6835343998078017
2022-06-12 16:29:34,729 ***** Save model *****
2022-06-12 16:30:01,283 ***** Running evaluation *****
2022-06-12 16:30:01,284   Epoch = 11 iter 24099 step
2022-06-12 16:30:01,284   Num examples = 872
2022-06-12 16:30:01,284   Batch size = 32
2022-06-12 16:30:01,285 ***** Eval results *****
2022-06-12 16:30:01,286   att_loss = 0.5514682601571708
2022-06-12 16:30:01,286   global_step = 24099
2022-06-12 16:30:01,286   loss = 1.234406727708447
2022-06-12 16:30:01,286   rep_loss = 0.6829384677073094
2022-06-12 16:30:01,286 ***** Save model *****
2022-06-12 16:30:27,890 ***** Running evaluation *****
2022-06-12 16:30:27,890   Epoch = 11 iter 24199 step
2022-06-12 16:30:27,890   Num examples = 872
2022-06-12 16:30:27,890   Batch size = 32
2022-06-12 16:30:27,891 ***** Eval results *****
2022-06-12 16:30:27,891   att_loss = 0.5516496357759593
2022-06-12 16:30:27,891   global_step = 24199
2022-06-12 16:30:27,891   loss = 1.2347755450773013
2022-06-12 16:30:27,891   rep_loss = 0.6831259098098177
2022-06-12 16:30:27,892 ***** Save model *****
2022-06-12 16:30:54,409 ***** Running evaluation *****
2022-06-12 16:30:54,410   Epoch = 11 iter 24299 step
2022-06-12 16:30:54,410   Num examples = 872
2022-06-12 16:30:54,410   Batch size = 32
2022-06-12 16:30:54,411 ***** Eval results *****
2022-06-12 16:30:54,411   att_loss = 0.5531159872874553
2022-06-12 16:30:54,411   global_step = 24299
2022-06-12 16:30:54,411   loss = 1.2364328614045015
2022-06-12 16:30:54,411   rep_loss = 0.6833168742460606
2022-06-12 16:30:54,411 ***** Save model *****
2022-06-12 16:31:21,006 ***** Running evaluation *****
2022-06-12 16:31:21,007   Epoch = 11 iter 24399 step
2022-06-12 16:31:21,007   Num examples = 872
2022-06-12 16:31:21,007   Batch size = 32
2022-06-12 16:31:21,008 ***** Eval results *****
2022-06-12 16:31:21,008   att_loss = 0.5528063479885162
2022-06-12 16:31:21,008   global_step = 24399
2022-06-12 16:31:21,008   loss = 1.2359787004877372
2022-06-12 16:31:21,008   rep_loss = 0.6831723524279804
2022-06-12 16:31:21,008 ***** Save model *****
2022-06-12 16:31:47,482 ***** Running evaluation *****
2022-06-12 16:31:47,483   Epoch = 11 iter 24499 step
2022-06-12 16:31:47,483   Num examples = 872
2022-06-12 16:31:47,483   Batch size = 32
2022-06-12 16:31:47,484 ***** Eval results *****
2022-06-12 16:31:47,484   att_loss = 0.552032802597623
2022-06-12 16:31:47,484   global_step = 24499
2022-06-12 16:31:47,484   loss = 1.2350800123601822
2022-06-12 16:31:47,484   rep_loss = 0.6830472099825025
2022-06-12 16:31:47,484 ***** Save model *****
2022-06-12 16:32:13,918 ***** Running evaluation *****
2022-06-12 16:32:13,919   Epoch = 11 iter 24599 step
2022-06-12 16:32:13,919   Num examples = 872
2022-06-12 16:32:13,919   Batch size = 32
2022-06-12 16:32:13,920 ***** Eval results *****
2022-06-12 16:32:13,920   att_loss = 0.5522136960242622
2022-06-12 16:32:13,920   global_step = 24599
2022-06-12 16:32:13,920   loss = 1.2354561058516356
2022-06-12 16:32:13,920   rep_loss = 0.6832424103599234
2022-06-12 16:32:13,921 ***** Save model *****
2022-06-12 16:32:40,561 ***** Running evaluation *****
2022-06-12 16:32:40,561   Epoch = 11 iter 24699 step
2022-06-12 16:32:40,562   Num examples = 872
2022-06-12 16:32:40,562   Batch size = 32
2022-06-12 16:32:40,563 ***** Eval results *****
2022-06-12 16:32:40,563   att_loss = 0.5515857532093379
2022-06-12 16:32:40,563   global_step = 24699
2022-06-12 16:32:40,563   loss = 1.2344692940497322
2022-06-12 16:32:40,563   rep_loss = 0.6828835413003658
2022-06-12 16:32:40,563 ***** Save model *****
2022-06-12 16:33:07,169 ***** Running evaluation *****
2022-06-12 16:33:07,170   Epoch = 11 iter 24799 step
2022-06-12 16:33:07,170   Num examples = 872
2022-06-12 16:33:07,170   Batch size = 32
2022-06-12 16:33:07,171 ***** Eval results *****
2022-06-12 16:33:07,171   att_loss = 0.5516934054315631
2022-06-12 16:33:07,171   global_step = 24799
2022-06-12 16:33:07,171   loss = 1.2344513247380444
2022-06-12 16:33:07,171   rep_loss = 0.6827579198286973
2022-06-12 16:33:07,171 ***** Save model *****
2022-06-12 16:33:33,703 ***** Running evaluation *****
2022-06-12 16:33:33,703   Epoch = 11 iter 24899 step
2022-06-12 16:33:33,703   Num examples = 872
2022-06-12 16:33:33,703   Batch size = 32
2022-06-12 16:33:33,705 ***** Eval results *****
2022-06-12 16:33:33,705   att_loss = 0.551183540274275
2022-06-12 16:33:33,705   global_step = 24899
2022-06-12 16:33:33,705   loss = 1.2337108902781777
2022-06-12 16:33:33,705   rep_loss = 0.6825273503944744
2022-06-12 16:33:33,705 ***** Save model *****
2022-06-12 16:34:00,234 ***** Running evaluation *****
2022-06-12 16:34:00,235   Epoch = 11 iter 24999 step
2022-06-12 16:34:00,235   Num examples = 872
2022-06-12 16:34:00,235   Batch size = 32
2022-06-12 16:34:00,236 ***** Eval results *****
2022-06-12 16:34:00,236   att_loss = 0.5517664948724351
2022-06-12 16:34:00,236   global_step = 24999
2022-06-12 16:34:00,236   loss = 1.2343114436476057
2022-06-12 16:34:00,236   rep_loss = 0.6825449494660061
2022-06-12 16:34:00,236 ***** Save model *****
2022-06-12 16:34:26,854 ***** Running evaluation *****
2022-06-12 16:34:26,855   Epoch = 11 iter 25099 step
2022-06-12 16:34:26,855   Num examples = 872
2022-06-12 16:34:26,855   Batch size = 32
2022-06-12 16:34:26,856 ***** Eval results *****
2022-06-12 16:34:26,856   att_loss = 0.550634881724482
2022-06-12 16:34:26,856   global_step = 25099
2022-06-12 16:34:26,856   loss = 1.2328989405461284
2022-06-12 16:34:26,856   rep_loss = 0.6822640599192256
2022-06-12 16:34:26,857 ***** Save model *****
2022-06-12 16:34:53,436 ***** Running evaluation *****
2022-06-12 16:34:53,436   Epoch = 11 iter 25199 step
2022-06-12 16:34:53,436   Num examples = 872
2022-06-12 16:34:53,436   Batch size = 32
2022-06-12 16:34:53,438 ***** Eval results *****
2022-06-12 16:34:53,438   att_loss = 0.5513725280326648
2022-06-12 16:34:53,438   global_step = 25199
2022-06-12 16:34:53,438   loss = 1.2337861081689523
2022-06-12 16:34:53,438   rep_loss = 0.6824135811369495
2022-06-12 16:34:53,438 ***** Save model *****
2022-06-12 16:35:19,970 ***** Running evaluation *****
2022-06-12 16:35:19,970   Epoch = 12 iter 25299 step
2022-06-12 16:35:19,970   Num examples = 872
2022-06-12 16:35:19,971   Batch size = 32
2022-06-12 16:35:19,972 ***** Eval results *****
2022-06-12 16:35:19,972   att_loss = 0.5493930256834217
2022-06-12 16:35:19,972   global_step = 25299
2022-06-12 16:35:19,972   loss = 1.2317241056292665
2022-06-12 16:35:19,972   rep_loss = 0.6823310816989225
2022-06-12 16:35:19,972 ***** Save model *****
2022-06-12 16:35:46,575 ***** Running evaluation *****
2022-06-12 16:35:46,575   Epoch = 12 iter 25399 step
2022-06-12 16:35:46,576   Num examples = 872
2022-06-12 16:35:46,576   Batch size = 32
2022-06-12 16:35:46,577 ***** Eval results *****
2022-06-12 16:35:46,577   att_loss = 0.5375727059825367
2022-06-12 16:35:46,577   global_step = 25399
2022-06-12 16:35:46,577   loss = 1.214140060326911
2022-06-12 16:35:46,577   rep_loss = 0.6765673594759
2022-06-12 16:35:46,577 ***** Save model *****
2022-06-12 16:36:13,121 ***** Running evaluation *****
2022-06-12 16:36:13,122   Epoch = 12 iter 25499 step
2022-06-12 16:36:13,122   Num examples = 872
2022-06-12 16:36:13,122   Batch size = 32
2022-06-12 16:36:13,123 ***** Eval results *****
2022-06-12 16:36:13,123   att_loss = 0.5482114091574908
2022-06-12 16:36:13,123   global_step = 25499
2022-06-12 16:36:13,123   loss = 1.2289840878243465
2022-06-12 16:36:13,123   rep_loss = 0.680772682110152
2022-06-12 16:36:13,123 ***** Save model *****
2022-06-12 16:36:39,639 ***** Running evaluation *****
2022-06-12 16:36:39,640   Epoch = 12 iter 25599 step
2022-06-12 16:36:39,640   Num examples = 872
2022-06-12 16:36:39,640   Batch size = 32
2022-06-12 16:36:39,641 ***** Eval results *****
2022-06-12 16:36:39,642   att_loss = 0.5443518278096137
2022-06-12 16:36:39,642   global_step = 25599
2022-06-12 16:36:39,642   loss = 1.2232804106511281
2022-06-12 16:36:39,642   rep_loss = 0.6789285853038146
2022-06-12 16:36:39,642 ***** Save model *****
2022-06-12 16:37:06,133 ***** Running evaluation *****
2022-06-12 16:37:06,134   Epoch = 12 iter 25699 step
2022-06-12 16:37:06,134   Num examples = 872
2022-06-12 16:37:06,134   Batch size = 32
2022-06-12 16:37:06,135 ***** Eval results *****
2022-06-12 16:37:06,135   att_loss = 0.5401101620525056
2022-06-12 16:37:06,135   global_step = 25699
2022-06-12 16:37:06,136   loss = 1.2180817272604965
2022-06-12 16:37:06,136   rep_loss = 0.6779715667278433
2022-06-12 16:37:06,136 ***** Save model *****
2022-06-12 16:37:32,703 ***** Running evaluation *****
2022-06-12 16:37:32,704   Epoch = 12 iter 25799 step
2022-06-12 16:37:32,704   Num examples = 872
2022-06-12 16:37:32,704   Batch size = 32
2022-06-12 16:37:32,705 ***** Eval results *****
2022-06-12 16:37:32,705   att_loss = 0.5425780131033674
2022-06-12 16:37:32,705   global_step = 25799
2022-06-12 16:37:32,705   loss = 1.2204987171990036
2022-06-12 16:37:32,705   rep_loss = 0.6779207049610394
2022-06-12 16:37:32,705 ***** Save model *****
2022-06-12 16:37:59,353 ***** Running evaluation *****
2022-06-12 16:37:59,353   Epoch = 12 iter 25899 step
2022-06-12 16:37:59,353   Num examples = 872
2022-06-12 16:37:59,353   Batch size = 32
2022-06-12 16:37:59,355 ***** Eval results *****
2022-06-12 16:37:59,355   att_loss = 0.5429292315253831
2022-06-12 16:37:59,355   global_step = 25899
2022-06-12 16:37:59,355   loss = 1.2213977162372864
2022-06-12 16:37:59,355   rep_loss = 0.6784684837505381
2022-06-12 16:37:59,355 ***** Save model *****
2022-06-12 16:38:26,000 ***** Running evaluation *****
2022-06-12 16:38:26,000   Epoch = 12 iter 25999 step
2022-06-12 16:38:26,001   Num examples = 872
2022-06-12 16:38:26,001   Batch size = 32
2022-06-12 16:38:26,002 ***** Eval results *****
2022-06-12 16:38:26,002   att_loss = 0.5435921610512842
2022-06-12 16:38:26,002   global_step = 25999
2022-06-12 16:38:26,002   loss = 1.2214829147893802
2022-06-12 16:38:26,002   rep_loss = 0.6778907536984125
2022-06-12 16:38:26,002 ***** Save model *****
2022-06-12 16:38:52,761 ***** Running evaluation *****
2022-06-12 16:38:52,761   Epoch = 12 iter 26099 step
2022-06-12 16:38:52,761   Num examples = 872
2022-06-12 16:38:52,761   Batch size = 32
2022-06-12 16:38:52,763 ***** Eval results *****
2022-06-12 16:38:52,763   att_loss = 0.5460445817853253
2022-06-12 16:38:52,763   global_step = 26099
2022-06-12 16:38:52,763   loss = 1.2240003688495111
2022-06-12 16:38:52,763   rep_loss = 0.6779557865739009
2022-06-12 16:38:52,764 ***** Save model *****
2022-06-12 16:39:19,362 ***** Running evaluation *****
2022-06-12 16:39:19,362   Epoch = 12 iter 26199 step
2022-06-12 16:39:19,362   Num examples = 872
2022-06-12 16:39:19,362   Batch size = 32
2022-06-12 16:39:19,364 ***** Eval results *****
2022-06-12 16:39:19,364   att_loss = 0.5452593566240698
2022-06-12 16:39:19,364   global_step = 26199
2022-06-12 16:39:19,364   loss = 1.223384675041985
2022-06-12 16:39:19,364   rep_loss = 0.6781253172270758
2022-06-12 16:39:19,364 ***** Save model *****
2022-06-12 16:39:45,977 ***** Running evaluation *****
2022-06-12 16:39:45,977   Epoch = 12 iter 26299 step
2022-06-12 16:39:45,978   Num examples = 872
2022-06-12 16:39:45,978   Batch size = 32
2022-06-12 16:39:45,979 ***** Eval results *****
2022-06-12 16:39:45,979   att_loss = 0.5458901089560294
2022-06-12 16:39:45,979   global_step = 26299
2022-06-12 16:39:45,979   loss = 1.2238020919029424
2022-06-12 16:39:45,979   rep_loss = 0.6779119819260914
2022-06-12 16:39:45,979 ***** Save model *****
2022-06-12 16:40:12,551 ***** Running evaluation *****
2022-06-12 16:40:12,551   Epoch = 12 iter 26399 step
2022-06-12 16:40:12,551   Num examples = 872
2022-06-12 16:40:12,551   Batch size = 32
2022-06-12 16:40:12,553 ***** Eval results *****
2022-06-12 16:40:12,553   att_loss = 0.5462386933593104
2022-06-12 16:40:12,553   global_step = 26399
2022-06-12 16:40:12,553   loss = 1.2242051863753205
2022-06-12 16:40:12,553   rep_loss = 0.6779664925758369
2022-06-12 16:40:12,553 ***** Save model *****
2022-06-12 16:40:39,041 ***** Running evaluation *****
2022-06-12 16:40:39,041   Epoch = 12 iter 26499 step
2022-06-12 16:40:39,041   Num examples = 872
2022-06-12 16:40:39,041   Batch size = 32
2022-06-12 16:40:39,042 ***** Eval results *****
2022-06-12 16:40:39,042   att_loss = 0.546409693362711
2022-06-12 16:40:39,043   global_step = 26499
2022-06-12 16:40:39,043   loss = 1.2245275927580994
2022-06-12 16:40:39,043   rep_loss = 0.678117899443034
2022-06-12 16:40:39,043 ***** Save model *****
2022-06-12 16:41:05,747 ***** Running evaluation *****
2022-06-12 16:41:05,747   Epoch = 12 iter 26599 step
2022-06-12 16:41:05,748   Num examples = 872
2022-06-12 16:41:05,748   Batch size = 32
2022-06-12 16:41:05,749 ***** Eval results *****
2022-06-12 16:41:05,749   att_loss = 0.5456302148558139
2022-06-12 16:41:05,749   global_step = 26599
2022-06-12 16:41:05,749   loss = 1.2236922710229519
2022-06-12 16:41:05,749   rep_loss = 0.6780620562333164
2022-06-12 16:41:05,750 ***** Save model *****
2022-06-12 16:41:32,290 ***** Running evaluation *****
2022-06-12 16:41:32,290   Epoch = 12 iter 26699 step
2022-06-12 16:41:32,290   Num examples = 872
2022-06-12 16:41:32,290   Batch size = 32
2022-06-12 16:41:32,291 ***** Eval results *****
2022-06-12 16:41:32,291   att_loss = 0.5459523397548539
2022-06-12 16:41:32,291   global_step = 26699
2022-06-12 16:41:32,291   loss = 1.2244659907895068
2022-06-12 16:41:32,291   rep_loss = 0.678513651301662
2022-06-12 16:41:32,292 ***** Save model *****
2022-06-12 16:41:59,004 ***** Running evaluation *****
2022-06-12 16:41:59,004   Epoch = 12 iter 26799 step
2022-06-12 16:41:59,004   Num examples = 872
2022-06-12 16:41:59,004   Batch size = 32
2022-06-12 16:41:59,006 ***** Eval results *****
2022-06-12 16:41:59,006   att_loss = 0.5454256917913831
2022-06-12 16:41:59,006   global_step = 26799
2022-06-12 16:41:59,006   loss = 1.2238191701303676
2022-06-12 16:41:59,006   rep_loss = 0.6783934783197695
2022-06-12 16:41:59,007 ***** Save model *****
2022-06-12 16:42:25,624 ***** Running evaluation *****
2022-06-12 16:42:25,624   Epoch = 12 iter 26899 step
2022-06-12 16:42:25,624   Num examples = 872
2022-06-12 16:42:25,625   Batch size = 32
2022-06-12 16:42:25,626 ***** Eval results *****
2022-06-12 16:42:25,626   att_loss = 0.5446985835815328
2022-06-12 16:42:25,626   global_step = 26899
2022-06-12 16:42:25,626   loss = 1.222685407899063
2022-06-12 16:42:25,626   rep_loss = 0.6779868241370194
2022-06-12 16:42:25,626 ***** Save model *****
2022-06-12 16:42:52,370 ***** Running evaluation *****
2022-06-12 16:42:52,371   Epoch = 12 iter 26999 step
2022-06-12 16:42:52,371   Num examples = 872
2022-06-12 16:42:52,371   Batch size = 32
2022-06-12 16:42:52,373 ***** Eval results *****
2022-06-12 16:42:52,373   att_loss = 0.5450428306273499
2022-06-12 16:42:52,373   global_step = 26999
2022-06-12 16:42:52,373   loss = 1.223175939187399
2022-06-12 16:42:52,373   rep_loss = 0.6781331084919684
2022-06-12 16:42:52,374 ***** Save model *****
2022-06-12 16:43:19,384 ***** Running evaluation *****
2022-06-12 16:43:19,386   Epoch = 12 iter 27099 step
2022-06-12 16:43:19,386   Num examples = 872
2022-06-12 16:43:19,386   Batch size = 32
2022-06-12 16:43:19,388 ***** Eval results *****
2022-06-12 16:43:19,388   att_loss = 0.5451756736061368
2022-06-12 16:43:19,389   global_step = 27099
2022-06-12 16:43:19,389   loss = 1.2234360605623193
2022-06-12 16:43:19,389   rep_loss = 0.6782603868273772
2022-06-12 16:43:19,389 ***** Save model *****
2022-06-12 16:43:46,192 ***** Running evaluation *****
2022-06-12 16:43:46,192   Epoch = 12 iter 27199 step
2022-06-12 16:43:46,193   Num examples = 872
2022-06-12 16:43:46,193   Batch size = 32
2022-06-12 16:43:46,194 ***** Eval results *****
2022-06-12 16:43:46,195   att_loss = 0.5446878642011825
2022-06-12 16:43:46,195   global_step = 27199
2022-06-12 16:43:46,195   loss = 1.2226992965905499
2022-06-12 16:43:46,195   rep_loss = 0.678011431778351
2022-06-12 16:43:46,195 ***** Save model *****
2022-06-12 16:44:13,090 ***** Running evaluation *****
2022-06-12 16:44:13,090   Epoch = 12 iter 27299 step
2022-06-12 16:44:13,090   Num examples = 872
2022-06-12 16:44:13,091   Batch size = 32
2022-06-12 16:44:13,093 ***** Eval results *****
2022-06-12 16:44:13,093   att_loss = 0.5455955215950002
2022-06-12 16:44:13,093   global_step = 27299
2022-06-12 16:44:13,093   loss = 1.223751908799371
2022-06-12 16:44:13,093   rep_loss = 0.6781563871317174
2022-06-12 16:44:13,093 ***** Save model *****
2022-06-12 16:44:39,862 ***** Running evaluation *****
2022-06-12 16:44:39,862   Epoch = 13 iter 27399 step
2022-06-12 16:44:39,862   Num examples = 872
2022-06-12 16:44:39,862   Batch size = 32
2022-06-12 16:44:39,863 ***** Eval results *****
2022-06-12 16:44:39,863   att_loss = 0.5239799510925374
2022-06-12 16:44:39,863   global_step = 27399
2022-06-12 16:44:39,863   loss = 1.191401703560606
2022-06-12 16:44:39,864   rep_loss = 0.6674217505657927
2022-06-12 16:44:39,864 ***** Save model *****
2022-06-12 16:45:06,481 ***** Running evaluation *****
2022-06-12 16:45:06,481   Epoch = 13 iter 27499 step
2022-06-12 16:45:06,481   Num examples = 872
2022-06-12 16:45:06,481   Batch size = 32
2022-06-12 16:45:06,483 ***** Eval results *****
2022-06-12 16:45:06,483   att_loss = 0.5407169258513418
2022-06-12 16:45:06,483   global_step = 27499
2022-06-12 16:45:06,483   loss = 1.211595347543963
2022-06-12 16:45:06,483   rep_loss = 0.6708784212871474
2022-06-12 16:45:06,483 ***** Save model *****
2022-06-12 16:45:33,217 ***** Running evaluation *****
2022-06-12 16:45:33,218   Epoch = 13 iter 27599 step
2022-06-12 16:45:33,218   Num examples = 872
2022-06-12 16:45:33,218   Batch size = 32
2022-06-12 16:45:33,220 ***** Eval results *****
2022-06-12 16:45:33,220   att_loss = 0.5422013241511124
2022-06-12 16:45:33,220   global_step = 27599
2022-06-12 16:45:33,220   loss = 1.2145542635126152
2022-06-12 16:45:33,221   rep_loss = 0.6723529399647886
2022-06-12 16:45:33,221 ***** Save model *****
2022-06-12 16:46:00,130 ***** Running evaluation *****
2022-06-12 16:46:00,131   Epoch = 13 iter 27699 step
2022-06-12 16:46:00,132   Num examples = 872
2022-06-12 16:46:00,132   Batch size = 32
2022-06-12 16:46:00,134 ***** Eval results *****
2022-06-12 16:46:00,134   att_loss = 0.5404904955402231
2022-06-12 16:46:00,134   global_step = 27699
2022-06-12 16:46:00,134   loss = 1.2119821803026996
2022-06-12 16:46:00,134   rep_loss = 0.6714916868237323
2022-06-12 16:46:00,135 ***** Save model *****
2022-06-12 16:46:26,905 ***** Running evaluation *****
2022-06-12 16:46:26,905   Epoch = 13 iter 27799 step
2022-06-12 16:46:26,906   Num examples = 872
2022-06-12 16:46:26,906   Batch size = 32
2022-06-12 16:46:26,907 ***** Eval results *****
2022-06-12 16:46:26,907   att_loss = 0.5401240387218911
2022-06-12 16:46:26,907   global_step = 27799
2022-06-12 16:46:26,907   loss = 1.2129478643937932
2022-06-12 16:46:26,907   rep_loss = 0.6728238267386519
2022-06-12 16:46:26,907 ***** Save model *****
2022-06-12 16:46:53,601 ***** Running evaluation *****
2022-06-12 16:46:53,602   Epoch = 13 iter 27899 step
2022-06-12 16:46:53,602   Num examples = 872
2022-06-12 16:46:53,602   Batch size = 32
2022-06-12 16:46:53,604 ***** Eval results *****
2022-06-12 16:46:53,604   att_loss = 0.5398592582786323
2022-06-12 16:46:53,605   global_step = 27899
2022-06-12 16:46:53,605   loss = 1.2123084452949886
2022-06-12 16:46:53,605   rep_loss = 0.6724491879970543
2022-06-12 16:46:53,605 ***** Save model *****
2022-06-12 16:47:20,306 ***** Running evaluation *****
2022-06-12 16:47:20,307   Epoch = 13 iter 27999 step
2022-06-12 16:47:20,307   Num examples = 872
2022-06-12 16:47:20,307   Batch size = 32
2022-06-12 16:47:20,308 ***** Eval results *****
2022-06-12 16:47:20,308   att_loss = 0.5392162114888476
2022-06-12 16:47:20,308   global_step = 27999
2022-06-12 16:47:20,308   loss = 1.2113799631503488
2022-06-12 16:47:20,308   rep_loss = 0.672163752076062
2022-06-12 16:47:20,308 ***** Save model *****
2022-06-12 16:47:46,852 ***** Running evaluation *****
2022-06-12 16:47:46,853   Epoch = 13 iter 28099 step
2022-06-12 16:47:46,853   Num examples = 872
2022-06-12 16:47:46,853   Batch size = 32
2022-06-12 16:47:46,854 ***** Eval results *****
2022-06-12 16:47:46,854   att_loss = 0.539469401879483
2022-06-12 16:47:46,854   global_step = 28099
2022-06-12 16:47:46,855   loss = 1.2114572867332214
2022-06-12 16:47:46,855   rep_loss = 0.6719878844946743
2022-06-12 16:47:46,855 ***** Save model *****
2022-06-12 16:48:13,348 ***** Running evaluation *****
2022-06-12 16:48:13,348   Epoch = 13 iter 28199 step
2022-06-12 16:48:13,348   Num examples = 872
2022-06-12 16:48:13,348   Batch size = 32
2022-06-12 16:48:13,349 ***** Eval results *****
2022-06-12 16:48:13,349   att_loss = 0.538251974084442
2022-06-12 16:48:13,350   global_step = 28199
2022-06-12 16:48:13,350   loss = 1.210777476588275
2022-06-12 16:48:13,350   rep_loss = 0.6725255022927187
2022-06-12 16:48:13,350 ***** Save model *****
2022-06-12 16:48:39,976 ***** Running evaluation *****
2022-06-12 16:48:39,976   Epoch = 13 iter 28299 step
2022-06-12 16:48:39,976   Num examples = 872
2022-06-12 16:48:39,976   Batch size = 32
2022-06-12 16:48:39,977 ***** Eval results *****
2022-06-12 16:48:39,977   att_loss = 0.5381344091992947
2022-06-12 16:48:39,978   global_step = 28299
2022-06-12 16:48:39,978   loss = 1.2111407964005516
2022-06-12 16:48:39,978   rep_loss = 0.6730063865403817
2022-06-12 16:48:39,978 ***** Save model *****
2022-06-12 16:49:06,701 ***** Running evaluation *****
2022-06-12 16:49:06,701   Epoch = 13 iter 28399 step
2022-06-12 16:49:06,701   Num examples = 872
2022-06-12 16:49:06,701   Batch size = 32
2022-06-12 16:49:06,702 ***** Eval results *****
2022-06-12 16:49:06,702   att_loss = 0.5392694362539731
2022-06-12 16:49:06,702   global_step = 28399
2022-06-12 16:49:06,702   loss = 1.2121081675021217
2022-06-12 16:49:06,702   rep_loss = 0.6728387312196843
2022-06-12 16:49:06,703 ***** Save model *****
2022-06-12 16:49:33,389 ***** Running evaluation *****
2022-06-12 16:49:33,389   Epoch = 13 iter 28499 step
2022-06-12 16:49:33,389   Num examples = 872
2022-06-12 16:49:33,389   Batch size = 32
2022-06-12 16:49:33,390 ***** Eval results *****
2022-06-12 16:49:33,391   att_loss = 0.5403443874227761
2022-06-12 16:49:33,391   global_step = 28499
2022-06-12 16:49:33,391   loss = 1.2139610595669867
2022-06-12 16:49:33,391   rep_loss = 0.6736166721442105
2022-06-12 16:49:33,391 ***** Save model *****
2022-06-12 16:50:00,137 ***** Running evaluation *****
2022-06-12 16:50:00,138   Epoch = 13 iter 28599 step
2022-06-12 16:50:00,138   Num examples = 872
2022-06-12 16:50:00,138   Batch size = 32
2022-06-12 16:50:00,139 ***** Eval results *****
2022-06-12 16:50:00,139   att_loss = 0.5398202768639746
2022-06-12 16:50:00,139   global_step = 28599
2022-06-12 16:50:00,140   loss = 1.2136246915712487
2022-06-12 16:50:00,140   rep_loss = 0.6738044147789717
2022-06-12 16:50:00,140 ***** Save model *****
2022-06-12 16:50:26,889 ***** Running evaluation *****
2022-06-12 16:50:26,889   Epoch = 13 iter 28699 step
2022-06-12 16:50:26,890   Num examples = 872
2022-06-12 16:50:26,890   Batch size = 32
2022-06-12 16:50:26,891 ***** Eval results *****
2022-06-12 16:50:26,891   att_loss = 0.5388569952650432
2022-06-12 16:50:26,891   global_step = 28699
2022-06-12 16:50:26,891   loss = 1.212600779117437
2022-06-12 16:50:26,891   rep_loss = 0.6737437841621433
2022-06-12 16:50:26,892 ***** Save model *****
2022-06-12 16:50:53,540 ***** Running evaluation *****
2022-06-12 16:50:53,541   Epoch = 13 iter 28799 step
2022-06-12 16:50:53,541   Num examples = 872
2022-06-12 16:50:53,541   Batch size = 32
2022-06-12 16:50:53,542 ***** Eval results *****
2022-06-12 16:50:53,542   att_loss = 0.5380646923779617
2022-06-12 16:50:53,542   global_step = 28799
2022-06-12 16:50:53,542   loss = 1.2114779455709557
2022-06-12 16:50:53,542   rep_loss = 0.6734132533165695
2022-06-12 16:50:53,543 ***** Save model *****
2022-06-12 16:51:20,104 ***** Running evaluation *****
2022-06-12 16:51:20,105   Epoch = 13 iter 28899 step
2022-06-12 16:51:20,105   Num examples = 872
2022-06-12 16:51:20,105   Batch size = 32
2022-06-12 16:51:20,106 ***** Eval results *****
2022-06-12 16:51:20,106   att_loss = 0.538270933865422
2022-06-12 16:51:20,106   global_step = 28899
2022-06-12 16:51:20,107   loss = 1.2117143409746882
2022-06-12 16:51:20,107   rep_loss = 0.6734434075330871
2022-06-12 16:51:20,107 ***** Save model *****
2022-06-12 16:51:46,570 ***** Running evaluation *****
2022-06-12 16:51:46,571   Epoch = 13 iter 28999 step
2022-06-12 16:51:46,571   Num examples = 872
2022-06-12 16:51:46,571   Batch size = 32
2022-06-12 16:51:46,572 ***** Eval results *****
2022-06-12 16:51:46,572   att_loss = 0.5375804750573367
2022-06-12 16:51:46,572   global_step = 28999
2022-06-12 16:51:46,572   loss = 1.211046692054056
2022-06-12 16:51:46,572   rep_loss = 0.6734662173948074
2022-06-12 16:51:46,573 ***** Save model *****
2022-06-12 16:52:13,123 ***** Running evaluation *****
2022-06-12 16:52:13,124   Epoch = 13 iter 29099 step
2022-06-12 16:52:13,124   Num examples = 872
2022-06-12 16:52:13,124   Batch size = 32
2022-06-12 16:52:13,125 ***** Eval results *****
2022-06-12 16:52:13,125   att_loss = 0.5376824965017075
2022-06-12 16:52:13,125   global_step = 29099
2022-06-12 16:52:13,125   loss = 1.211150659618203
2022-06-12 16:52:13,125   rep_loss = 0.6734681634406192
2022-06-12 16:52:13,125 ***** Save model *****
2022-06-12 16:52:39,705 ***** Running evaluation *****
2022-06-12 16:52:39,706   Epoch = 13 iter 29199 step
2022-06-12 16:52:39,706   Num examples = 872
2022-06-12 16:52:39,706   Batch size = 32
2022-06-12 16:52:39,707 ***** Eval results *****
2022-06-12 16:52:39,707   att_loss = 0.5388542758689162
2022-06-12 16:52:39,707   global_step = 29199
2022-06-12 16:52:39,708   loss = 1.2124482574305409
2022-06-12 16:52:39,708   rep_loss = 0.6735939815938957
2022-06-12 16:52:39,708 ***** Save model *****
2022-06-12 16:53:06,168 ***** Running evaluation *****
2022-06-12 16:53:06,169   Epoch = 13 iter 29299 step
2022-06-12 16:53:06,169   Num examples = 872
2022-06-12 16:53:06,169   Batch size = 32
2022-06-12 16:53:06,170 ***** Eval results *****
2022-06-12 16:53:06,170   att_loss = 0.5389778102797365
2022-06-12 16:53:06,170   global_step = 29299
2022-06-12 16:53:06,170   loss = 1.2126049127221168
2022-06-12 16:53:06,170   rep_loss = 0.6736271022893124
2022-06-12 16:53:06,170 ***** Save model *****
2022-06-12 16:53:32,708 ***** Running evaluation *****
2022-06-12 16:53:32,709   Epoch = 13 iter 29399 step
2022-06-12 16:53:32,709   Num examples = 872
2022-06-12 16:53:32,709   Batch size = 32
2022-06-12 16:53:32,710 ***** Eval results *****
2022-06-12 16:53:32,710   att_loss = 0.5389014771203617
2022-06-12 16:53:32,710   global_step = 29399
2022-06-12 16:53:32,710   loss = 1.212488593418073
2022-06-12 16:53:32,711   rep_loss = 0.6735871162249161
2022-06-12 16:53:32,711 ***** Save model *****
2022-06-12 16:53:59,279 ***** Running evaluation *****
2022-06-12 16:53:59,280   Epoch = 14 iter 29499 step
2022-06-12 16:53:59,280   Num examples = 872
2022-06-12 16:53:59,280   Batch size = 32
2022-06-12 16:53:59,281 ***** Eval results *****
2022-06-12 16:53:59,281   att_loss = 0.5233728747035182
2022-06-12 16:53:59,281   global_step = 29499
2022-06-12 16:53:59,281   loss = 1.1938058900278667
2022-06-12 16:53:59,281   rep_loss = 0.6704330250274303
2022-06-12 16:53:59,282 ***** Save model *****
2022-06-12 16:54:25,921 ***** Running evaluation *****
2022-06-12 16:54:25,921   Epoch = 14 iter 29599 step
2022-06-12 16:54:25,921   Num examples = 872
2022-06-12 16:54:25,921   Batch size = 32
2022-06-12 16:54:25,922 ***** Eval results *****
2022-06-12 16:54:25,922   att_loss = 0.5161064300503764
2022-06-12 16:54:25,923   global_step = 29599
2022-06-12 16:54:25,923   loss = 1.1819270920086573
2022-06-12 16:54:25,923   rep_loss = 0.6658206686273321
2022-06-12 16:54:25,923 ***** Save model *****
2022-06-12 16:54:52,611 ***** Running evaluation *****
2022-06-12 16:54:52,612   Epoch = 14 iter 29699 step
2022-06-12 16:54:52,612   Num examples = 872
2022-06-12 16:54:52,612   Batch size = 32
2022-06-12 16:54:52,613 ***** Eval results *****
2022-06-12 16:54:52,613   att_loss = 0.5298664041262104
2022-06-12 16:54:52,613   global_step = 29699
2022-06-12 16:54:52,613   loss = 1.1991505409464425
2022-06-12 16:54:52,613   rep_loss = 0.6692841418486073
2022-06-12 16:54:52,613 ***** Save model *****
2022-06-12 16:55:19,165 ***** Running evaluation *****
2022-06-12 16:55:19,165   Epoch = 14 iter 29799 step
2022-06-12 16:55:19,165   Num examples = 872
2022-06-12 16:55:19,166   Batch size = 32
2022-06-12 16:55:19,167 ***** Eval results *****
2022-06-12 16:55:19,167   att_loss = 0.5349596436447722
2022-06-12 16:55:19,167   global_step = 29799
2022-06-12 16:55:19,167   loss = 1.2048231941965508
2022-06-12 16:55:19,167   rep_loss = 0.669863555069915
2022-06-12 16:55:19,167 ***** Save model *****
2022-06-12 16:55:45,842 ***** Running evaluation *****
2022-06-12 16:55:45,843   Epoch = 14 iter 29899 step
2022-06-12 16:55:45,843   Num examples = 872
2022-06-12 16:55:45,843   Batch size = 32
2022-06-12 16:55:45,844 ***** Eval results *****
2022-06-12 16:55:45,844   att_loss = 0.5341434021297481
2022-06-12 16:55:45,844   global_step = 29899
2022-06-12 16:55:45,844   loss = 1.2038117702335591
2022-06-12 16:55:45,844   rep_loss = 0.6696683713329565
2022-06-12 16:55:45,844 ***** Save model *****
2022-06-12 16:56:12,390 ***** Running evaluation *****
2022-06-12 16:56:12,390   Epoch = 14 iter 29999 step
2022-06-12 16:56:12,390   Num examples = 872
2022-06-12 16:56:12,391   Batch size = 32
2022-06-12 16:56:12,392 ***** Eval results *****
2022-06-12 16:56:12,392   att_loss = 0.5329886817032022
2022-06-12 16:56:12,392   global_step = 29999
2022-06-12 16:56:12,392   loss = 1.2021166613088787
2022-06-12 16:56:12,392   rep_loss = 0.669127983502481
2022-06-12 16:56:12,392 ***** Save model *****
2022-06-12 16:56:38,990 ***** Running evaluation *****
2022-06-12 16:56:38,991   Epoch = 14 iter 30099 step
2022-06-12 16:56:38,991   Num examples = 872
2022-06-12 16:56:38,991   Batch size = 32
2022-06-12 16:56:38,992 ***** Eval results *****
2022-06-12 16:56:38,992   att_loss = 0.5308155782207916
2022-06-12 16:56:38,992   global_step = 30099
2022-06-12 16:56:38,993   loss = 1.1996653642661834
2022-06-12 16:56:38,993   rep_loss = 0.6688497895215569
2022-06-12 16:56:38,993 ***** Save model *****
2022-06-12 16:57:05,541 ***** Running evaluation *****
2022-06-12 16:57:05,542   Epoch = 14 iter 30199 step
2022-06-12 16:57:05,542   Num examples = 872
2022-06-12 16:57:05,542   Batch size = 32
2022-06-12 16:57:05,543 ***** Eval results *****
2022-06-12 16:57:05,543   att_loss = 0.5331839878979474
2022-06-12 16:57:05,543   global_step = 30199
2022-06-12 16:57:05,543   loss = 1.2034429875710129
2022-06-12 16:57:05,543   rep_loss = 0.6702590018390486
2022-06-12 16:57:05,544 ***** Save model *****
2022-06-12 16:57:32,172 ***** Running evaluation *****
2022-06-12 16:57:32,172   Epoch = 14 iter 30299 step
2022-06-12 16:57:32,172   Num examples = 872
2022-06-12 16:57:32,172   Batch size = 32
2022-06-12 16:57:32,173 ***** Eval results *****
2022-06-12 16:57:32,174   att_loss = 0.533016367350326
2022-06-12 16:57:32,174   global_step = 30299
2022-06-12 16:57:32,174   loss = 1.2029277827957487
2022-06-12 16:57:32,174   rep_loss = 0.6699114180261696
2022-06-12 16:57:32,174 ***** Save model *****
2022-06-12 16:57:58,765 ***** Running evaluation *****
2022-06-12 16:57:58,765   Epoch = 14 iter 30399 step
2022-06-12 16:57:58,766   Num examples = 872
2022-06-12 16:57:58,766   Batch size = 32
2022-06-12 16:57:58,767 ***** Eval results *****
2022-06-12 16:57:58,767   att_loss = 0.5348138446709257
2022-06-12 16:57:58,767   global_step = 30399
2022-06-12 16:57:58,767   loss = 1.2049812691588417
2022-06-12 16:57:58,767   rep_loss = 0.6701674269214035
2022-06-12 16:57:58,767 ***** Save model *****
2022-06-12 16:58:25,367 ***** Running evaluation *****
2022-06-12 16:58:25,367   Epoch = 14 iter 30499 step
2022-06-12 16:58:25,367   Num examples = 872
2022-06-12 16:58:25,367   Batch size = 32
2022-06-12 16:58:25,369 ***** Eval results *****
2022-06-12 16:58:25,369   att_loss = 0.533270936141421
2022-06-12 16:58:25,369   global_step = 30499
2022-06-12 16:58:25,369   loss = 1.2033249575819744
2022-06-12 16:58:25,369   rep_loss = 0.670054023640725
2022-06-12 16:58:25,369 ***** Save model *****
2022-06-12 16:58:51,988 ***** Running evaluation *****
2022-06-12 16:58:51,989   Epoch = 14 iter 30599 step
2022-06-12 16:58:51,989   Num examples = 872
2022-06-12 16:58:51,989   Batch size = 32
2022-06-12 16:58:51,990 ***** Eval results *****
2022-06-12 16:58:51,990   att_loss = 0.5334255526645707
2022-06-12 16:58:51,990   global_step = 30599
2022-06-12 16:58:51,990   loss = 1.2034146417052936
2022-06-12 16:58:51,990   rep_loss = 0.6699890907876657
2022-06-12 16:58:51,990 ***** Save model *****
2022-06-12 16:59:18,589 ***** Running evaluation *****
2022-06-12 16:59:18,589   Epoch = 14 iter 30699 step
2022-06-12 16:59:18,589   Num examples = 872
2022-06-12 16:59:18,589   Batch size = 32
2022-06-12 16:59:18,590 ***** Eval results *****
2022-06-12 16:59:18,590   att_loss = 0.535105521220501
2022-06-12 16:59:18,590   global_step = 30699
2022-06-12 16:59:18,591   loss = 1.2054189410980727
2022-06-12 16:59:18,591   rep_loss = 0.6703134213161391
2022-06-12 16:59:18,591 ***** Save model *****
2022-06-12 16:59:45,311 ***** Running evaluation *****
2022-06-12 16:59:45,311   Epoch = 14 iter 30799 step
2022-06-12 16:59:45,311   Num examples = 872
2022-06-12 16:59:45,312   Batch size = 32
2022-06-12 16:59:45,313 ***** Eval results *****
2022-06-12 16:59:45,313   att_loss = 0.5346764666503344
2022-06-12 16:59:45,313   global_step = 30799
2022-06-12 16:59:45,313   loss = 1.2050760050185652
2022-06-12 16:59:45,313   rep_loss = 0.6703995390783383
2022-06-12 16:59:45,313 ***** Save model *****
2022-06-12 17:00:11,875 ***** Running evaluation *****
2022-06-12 17:00:11,876   Epoch = 14 iter 30899 step
2022-06-12 17:00:11,876   Num examples = 872
2022-06-12 17:00:11,876   Batch size = 32
2022-06-12 17:00:11,877 ***** Eval results *****
2022-06-12 17:00:11,877   att_loss = 0.5346620231763571
2022-06-12 17:00:11,877   global_step = 30899
2022-06-12 17:00:11,877   loss = 1.2051045228356052
2022-06-12 17:00:11,878   rep_loss = 0.6704425002168799
2022-06-12 17:00:11,878 ***** Save model *****
2022-06-12 17:00:38,489 ***** Running evaluation *****
2022-06-12 17:00:38,490   Epoch = 14 iter 30999 step
2022-06-12 17:00:38,490   Num examples = 872
2022-06-12 17:00:38,490   Batch size = 32
2022-06-12 17:00:38,491 ***** Eval results *****
2022-06-12 17:00:38,491   att_loss = 0.5343694091615782
2022-06-12 17:00:38,491   global_step = 30999
2022-06-12 17:00:38,491   loss = 1.2045978719896104
2022-06-12 17:00:38,491   rep_loss = 0.6702284631756938
2022-06-12 17:00:38,491 ***** Save model *****
2022-06-12 17:01:05,144 ***** Running evaluation *****
2022-06-12 17:01:05,144   Epoch = 14 iter 31099 step
2022-06-12 17:01:05,144   Num examples = 872
2022-06-12 17:01:05,144   Batch size = 32
2022-06-12 17:01:05,145 ***** Eval results *****
2022-06-12 17:01:05,146   att_loss = 0.5349898809024058
2022-06-12 17:01:05,146   global_step = 31099
2022-06-12 17:01:05,146   loss = 1.205054909625724
2022-06-12 17:01:05,146   rep_loss = 0.6700650288140128
2022-06-12 17:01:05,146 ***** Save model *****
2022-06-12 17:01:31,779 ***** Running evaluation *****
2022-06-12 17:01:31,779   Epoch = 14 iter 31199 step
2022-06-12 17:01:31,779   Num examples = 872
2022-06-12 17:01:31,779   Batch size = 32
2022-06-12 17:01:31,780 ***** Eval results *****
2022-06-12 17:01:31,781   att_loss = 0.5345789377229759
2022-06-12 17:01:31,781   global_step = 31199
2022-06-12 17:01:31,781   loss = 1.2045878119600004
2022-06-12 17:01:31,781   rep_loss = 0.6700088738437637
2022-06-12 17:01:31,781 ***** Save model *****
2022-06-12 17:01:58,384 ***** Running evaluation *****
2022-06-12 17:01:58,384   Epoch = 14 iter 31299 step
2022-06-12 17:01:58,384   Num examples = 872
2022-06-12 17:01:58,384   Batch size = 32
2022-06-12 17:01:58,385 ***** Eval results *****
2022-06-12 17:01:58,385   att_loss = 0.5348755930890483
2022-06-12 17:01:58,385   global_step = 31299
2022-06-12 17:01:58,386   loss = 1.2047565148307522
2022-06-12 17:01:58,386   rep_loss = 0.669880921563828
2022-06-12 17:01:58,386 ***** Save model *****
2022-06-12 17:02:25,041 ***** Running evaluation *****
2022-06-12 17:02:25,042   Epoch = 14 iter 31399 step
2022-06-12 17:02:25,042   Num examples = 872
2022-06-12 17:02:25,042   Batch size = 32
2022-06-12 17:02:25,043 ***** Eval results *****
2022-06-12 17:02:25,043   att_loss = 0.5349770801096801
2022-06-12 17:02:25,043   global_step = 31399
2022-06-12 17:02:25,043   loss = 1.205020977244571
2022-06-12 17:02:25,043   rep_loss = 0.6700438969968461
2022-06-12 17:02:25,044 ***** Save model *****
2022-06-12 17:02:51,684 ***** Running evaluation *****
2022-06-12 17:02:51,684   Epoch = 14 iter 31499 step
2022-06-12 17:02:51,684   Num examples = 872
2022-06-12 17:02:51,684   Batch size = 32
2022-06-12 17:02:51,685 ***** Eval results *****
2022-06-12 17:02:51,686   att_loss = 0.5344997654927459
2022-06-12 17:02:51,686   global_step = 31499
2022-06-12 17:02:51,686   loss = 1.2043918401542621
2022-06-12 17:02:51,686   rep_loss = 0.6698920747490414
2022-06-12 17:02:51,686 ***** Save model *****
2022-06-12 17:03:08,254 Task finish! 
2022-06-12 17:03:08,254 Task cost 140.48242991666666 minutes, i.e. 2.3413738383333333 hours. 
2022-06-12 17:03:10,726 Task start! 
2022-06-12 17:03:10,749 device: cuda n_gpu: 1
2022-06-12 17:03:10,750 The args: Namespace(aug_train=False, cache_dir='', data_dir='../tiny_bert/data/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, do_predict=False, eval_batch_size=32, eval_step=100, gpu_id=2, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_len=64, max_seq_length=128, no_cuda=False, num_train_epochs=10, output_dir='../tiny_bert/model/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/sst-2/on_original_data', pred_distill=True, seed=42, student_model='../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/sst-2/on_original_data', task_name='sst-2', teacher_model='../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/sst-2/on_original_data', temperature=1.0, tensorboard_log_save_dir='../tiny_bert/tensorboard_log/tiny_bert/distilled_prediction_model/tiny_bert_bert_large_6_768/sst-2/on_original_data', train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001, write_predict_dir='model/knowledge_review/distilled_intermediate_model/tmp')
2022-06-12 17:03:11,036 Writing example 0 of 67349
2022-06-12 17:03:11,037 *** Example ***
2022-06-12 17:03:11,037 guid: train-1
2022-06-12 17:03:11,037 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2022-06-12 17:03:11,037 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 17:03:11,037 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 17:03:11,037 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 17:03:11,037 label: 0
2022-06-12 17:03:11,037 label_id: 0
2022-06-12 17:03:13,017 Writing example 10000 of 67349
2022-06-12 17:03:14,926 Writing example 20000 of 67349
2022-06-12 17:03:16,937 Writing example 30000 of 67349
2022-06-12 17:03:18,849 Writing example 40000 of 67349
2022-06-12 17:03:20,935 Writing example 50000 of 67349
2022-06-12 17:03:22,832 Writing example 60000 of 67349
2022-06-12 17:03:24,988 Writing example 0 of 872
2022-06-12 17:03:24,989 *** Example ***
2022-06-12 17:03:24,989 guid: dev-1
2022-06-12 17:03:24,989 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2022-06-12 17:03:24,989 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 17:03:24,989 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 17:03:24,989 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-06-12 17:03:24,989 label: 1
2022-06-12 17:03:24,989 label_id: 1
2022-06-12 17:03:25,310 Model config {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "sst-2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "min_length": 0,
  "model_type": "bert",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-06-12 17:03:30,570 Loading model ../tiny_bert/model/fine-tuned_pretrained_model/bert-large-uncased/sst-2/on_original_data/pytorch_model.bin
2022-06-12 17:03:31,252 loading model...
2022-06-12 17:03:31,585 done!
2022-06-12 17:03:34,909 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2022-06-12 17:03:36,013 Loading model ../tiny_bert/model/tiny_bert/distilled_intermediate_model/tiny_bert_bert_large_6_768/sst-2/on_original_data/pytorch_model.bin
2022-06-12 17:03:36,141 loading model...
2022-06-12 17:03:36,176 done!
2022-06-12 17:03:37,725 ***** Running training *****
2022-06-12 17:03:37,741   Num examples = 67349
2022-06-12 17:03:37,741   Batch size = 32
2022-06-12 17:03:37,747   Num steps = 21040
2022-06-12 17:03:37,747 n: bert.embeddings.word_embeddings.weight
2022-06-12 17:03:37,747 n: bert.embeddings.position_embeddings.weight
2022-06-12 17:03:37,747 n: bert.embeddings.token_type_embeddings.weight
2022-06-12 17:03:37,747 n: bert.embeddings.LayerNorm.weight
2022-06-12 17:03:37,752 n: bert.embeddings.LayerNorm.bias
2022-06-12 17:03:37,752 n: bert.encoder.layer.0.attention.self.query.weight
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.self.query.bias
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.self.key.weight
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.self.key.bias
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.self.value.weight
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.self.value.bias
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.output.dense.weight
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.output.dense.bias
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.intermediate.dense.weight
2022-06-12 17:03:37,753 n: bert.encoder.layer.0.intermediate.dense.bias
2022-06-12 17:03:37,758 n: bert.encoder.layer.0.output.dense.weight
2022-06-12 17:03:37,769 n: bert.encoder.layer.0.output.dense.bias
2022-06-12 17:03:37,779 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-06-12 17:03:37,794 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.self.query.weight
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.self.query.bias
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.self.key.weight
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.self.key.bias
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.self.value.weight
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.self.value.bias
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.output.dense.weight
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.output.dense.bias
2022-06-12 17:03:37,794 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.1.intermediate.dense.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.1.intermediate.dense.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.1.output.dense.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.1.output.dense.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.self.query.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.self.query.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.self.key.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.self.key.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.self.value.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.self.value.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.output.dense.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.output.dense.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.intermediate.dense.weight
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.intermediate.dense.bias
2022-06-12 17:03:37,795 n: bert.encoder.layer.2.output.dense.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.2.output.dense.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.self.query.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.self.query.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.self.key.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.self.key.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.self.value.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.self.value.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.output.dense.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.output.dense.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.intermediate.dense.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.intermediate.dense.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.output.dense.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.output.dense.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.4.attention.self.query.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.4.attention.self.query.bias
2022-06-12 17:03:37,796 n: bert.encoder.layer.4.attention.self.key.weight
2022-06-12 17:03:37,796 n: bert.encoder.layer.4.attention.self.key.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.attention.self.value.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.attention.self.value.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.attention.output.dense.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.attention.output.dense.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.intermediate.dense.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.intermediate.dense.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.output.dense.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.output.dense.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.self.query.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.self.query.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.self.key.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.self.key.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.self.value.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.self.value.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.output.dense.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.output.dense.bias
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-06-12 17:03:37,797 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-06-12 17:03:37,798 n: bert.encoder.layer.5.intermediate.dense.weight
2022-06-12 17:03:37,798 n: bert.encoder.layer.5.intermediate.dense.bias
2022-06-12 17:03:37,798 n: bert.encoder.layer.5.output.dense.weight
2022-06-12 17:03:37,798 n: bert.encoder.layer.5.output.dense.bias
2022-06-12 17:03:37,798 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-06-12 17:03:37,798 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-06-12 17:03:37,798 n: bert.pooler.dense.weight
2022-06-12 17:03:37,798 n: bert.pooler.dense.bias
2022-06-12 17:03:37,798 n: classifier.weight
2022-06-12 17:03:37,798 n: classifier.bias
2022-06-12 17:03:37,798 n: fit_denses.0.weight
2022-06-12 17:03:37,798 n: fit_denses.0.bias
2022-06-12 17:03:37,798 n: fit_denses.1.weight
2022-06-12 17:03:37,798 n: fit_denses.1.bias
2022-06-12 17:03:37,798 n: fit_denses.2.weight
2022-06-12 17:03:37,798 n: fit_denses.2.bias
2022-06-12 17:03:37,798 n: fit_denses.3.weight
2022-06-12 17:03:37,798 n: fit_denses.3.bias
2022-06-12 17:03:37,798 n: fit_denses.4.weight
2022-06-12 17:03:37,798 n: fit_denses.4.bias
2022-06-12 17:03:37,798 n: fit_denses.5.weight
2022-06-12 17:03:37,798 n: fit_denses.5.bias
2022-06-12 17:03:37,799 n: fit_denses.6.weight
2022-06-12 17:03:37,799 n: fit_denses.6.bias
2022-06-12 17:03:37,799 Total parameters: 72468738
2022-06-12 17:04:01,887 ***** Running evaluation *****
2022-06-12 17:04:01,887   Epoch = 0 iter 99 step
2022-06-12 17:04:01,887   Num examples = 872
2022-06-12 17:04:01,887   Batch size = 32
2022-06-12 17:04:02,641 ***** Eval results *****
2022-06-12 17:04:02,642   acc = 0.8979357798165137
2022-06-12 17:04:02,642   cls_loss = 0.30768074697316294
2022-06-12 17:04:02,642   eval_loss = 0.510918198951653
2022-06-12 17:04:02,642   global_step = 99
2022-06-12 17:04:02,642   loss = 0.30768074697316294
2022-06-12 17:04:02,642 ***** Save model *****
2022-06-12 17:04:27,420 ***** Running evaluation *****
2022-06-12 17:04:27,420   Epoch = 0 iter 199 step
2022-06-12 17:04:27,421   Num examples = 872
2022-06-12 17:04:27,421   Batch size = 32
2022-06-12 17:04:28,172 ***** Eval results *****
2022-06-12 17:04:28,172   acc = 0.8944954128440367
2022-06-12 17:04:28,172   cls_loss = 0.2256268047208163
2022-06-12 17:04:28,172   eval_loss = 0.3132015418793474
2022-06-12 17:04:28,172   global_step = 199
2022-06-12 17:04:28,172   loss = 0.2256268047208163
2022-06-12 17:04:52,574 ***** Running evaluation *****
2022-06-12 17:04:52,575   Epoch = 0 iter 299 step
2022-06-12 17:04:52,575   Num examples = 872
2022-06-12 17:04:52,575   Batch size = 32
2022-06-12 17:04:53,326 ***** Eval results *****
2022-06-12 17:04:53,326   acc = 0.893348623853211
2022-06-12 17:04:53,326   cls_loss = 0.17600658543432834
2022-06-12 17:04:53,326   eval_loss = 0.31014225019940306
2022-06-12 17:04:53,326   global_step = 299
2022-06-12 17:04:53,326   loss = 0.17600658543432834
2022-06-12 17:05:17,691 ***** Running evaluation *****
2022-06-12 17:05:17,692   Epoch = 0 iter 399 step
2022-06-12 17:05:17,692   Num examples = 872
2022-06-12 17:05:17,692   Batch size = 32
2022-06-12 17:05:18,442 ***** Eval results *****
2022-06-12 17:05:18,443   acc = 0.8979357798165137
2022-06-12 17:05:18,443   cls_loss = 0.1498488606348223
2022-06-12 17:05:18,443   eval_loss = 0.31167686358094215
2022-06-12 17:05:18,443   global_step = 399
2022-06-12 17:05:18,443   loss = 0.1498488606348223
2022-06-12 17:05:42,909 ***** Running evaluation *****
2022-06-12 17:05:42,910   Epoch = 0 iter 499 step
2022-06-12 17:05:42,910   Num examples = 872
2022-06-12 17:05:42,910   Batch size = 32
2022-06-12 17:05:43,661 ***** Eval results *****
2022-06-12 17:05:43,662   acc = 0.893348623853211
2022-06-12 17:05:43,662   cls_loss = 0.13378704289246895
2022-06-12 17:05:43,662   eval_loss = 0.33969495525317533
2022-06-12 17:05:43,662   global_step = 499
2022-06-12 17:05:43,662   loss = 0.13378704289246895
2022-06-12 17:06:08,052 ***** Running evaluation *****
2022-06-12 17:06:08,053   Epoch = 0 iter 599 step
2022-06-12 17:06:08,053   Num examples = 872
2022-06-12 17:06:08,053   Batch size = 32
2022-06-12 17:06:08,809 ***** Eval results *****
2022-06-12 17:06:08,810   acc = 0.8944954128440367
2022-06-12 17:06:08,810   cls_loss = 0.12322625211239459
2022-06-12 17:06:08,810   eval_loss = 0.3237962017634085
2022-06-12 17:06:08,810   global_step = 599
2022-06-12 17:06:08,810   loss = 0.12322625211239459
2022-06-12 17:06:33,206 ***** Running evaluation *****
2022-06-12 17:06:33,206   Epoch = 0 iter 699 step
2022-06-12 17:06:33,206   Num examples = 872
2022-06-12 17:06:33,206   Batch size = 32
2022-06-12 17:06:33,957 ***** Eval results *****
2022-06-12 17:06:33,958   acc = 0.893348623853211
2022-06-12 17:06:33,958   cls_loss = 0.11583540509208419
2022-06-12 17:06:33,958   eval_loss = 0.3171122079449041
2022-06-12 17:06:33,958   global_step = 699
2022-06-12 17:06:33,958   loss = 0.11583540509208419
2022-06-12 17:06:58,334 ***** Running evaluation *****
2022-06-12 17:06:58,335   Epoch = 0 iter 799 step
2022-06-12 17:06:58,335   Num examples = 872
2022-06-12 17:06:58,335   Batch size = 32
2022-06-12 17:06:59,085 ***** Eval results *****
2022-06-12 17:06:59,085   acc = 0.8876146788990825
2022-06-12 17:06:59,085   cls_loss = 0.11030563235152349
2022-06-12 17:06:59,085   eval_loss = 0.3246814079050507
2022-06-12 17:06:59,085   global_step = 799
2022-06-12 17:06:59,086   loss = 0.11030563235152349
2022-06-12 17:07:23,432 ***** Running evaluation *****
2022-06-12 17:07:23,432   Epoch = 0 iter 899 step
2022-06-12 17:07:23,432   Num examples = 872
2022-06-12 17:07:23,432   Batch size = 32
2022-06-12 17:07:24,184 ***** Eval results *****
2022-06-12 17:07:24,184   acc = 0.8887614678899083
2022-06-12 17:07:24,184   cls_loss = 0.10609613147639857
2022-06-12 17:07:24,184   eval_loss = 0.35083349062395947
2022-06-12 17:07:24,185   global_step = 899
2022-06-12 17:07:24,185   loss = 0.10609613147639857
2022-06-12 17:07:48,509 ***** Running evaluation *****
2022-06-12 17:07:48,509   Epoch = 0 iter 999 step
2022-06-12 17:07:48,509   Num examples = 872
2022-06-12 17:07:48,509   Batch size = 32
2022-06-12 17:07:49,260 ***** Eval results *****
2022-06-12 17:07:49,261   acc = 0.8876146788990825
2022-06-12 17:07:49,261   cls_loss = 0.10294070052685919
2022-06-12 17:07:49,261   eval_loss = 0.2818898303168161
2022-06-12 17:07:49,261   global_step = 999
2022-06-12 17:07:49,261   loss = 0.10294070052685919
2022-06-12 17:08:13,647 ***** Running evaluation *****
2022-06-12 17:08:13,647   Epoch = 0 iter 1099 step
2022-06-12 17:08:13,647   Num examples = 872
2022-06-12 17:08:13,647   Batch size = 32
2022-06-12 17:08:14,398 ***** Eval results *****
2022-06-12 17:08:14,398   acc = 0.893348623853211
2022-06-12 17:08:14,398   cls_loss = 0.10007425181032095
2022-06-12 17:08:14,398   eval_loss = 0.32900080563766615
2022-06-12 17:08:14,398   global_step = 1099
2022-06-12 17:08:14,398   loss = 0.10007425181032095
2022-06-12 17:08:38,725 ***** Running evaluation *****
2022-06-12 17:08:38,725   Epoch = 0 iter 1199 step
2022-06-12 17:08:38,725   Num examples = 872
2022-06-12 17:08:38,726   Batch size = 32
2022-06-12 17:08:39,479 ***** Eval results *****
2022-06-12 17:08:39,479   acc = 0.8830275229357798
2022-06-12 17:08:39,480   cls_loss = 0.0977444616709529
2022-06-12 17:08:39,480   eval_loss = 0.3490140102803707
2022-06-12 17:08:39,480   global_step = 1199
2022-06-12 17:08:39,480   loss = 0.0977444616709529
2022-06-12 17:09:03,910 ***** Running evaluation *****
2022-06-12 17:09:03,910   Epoch = 0 iter 1299 step
2022-06-12 17:09:03,910   Num examples = 872
2022-06-12 17:09:03,910   Batch size = 32
2022-06-12 17:09:04,661 ***** Eval results *****
2022-06-12 17:09:04,661   acc = 0.8887614678899083
2022-06-12 17:09:04,661   cls_loss = 0.09584591799832198
2022-06-12 17:09:04,661   eval_loss = 0.3151049241423607
2022-06-12 17:09:04,661   global_step = 1299
2022-06-12 17:09:04,661   loss = 0.09584591799832198
2022-06-12 17:09:29,057 ***** Running evaluation *****
2022-06-12 17:09:29,057   Epoch = 0 iter 1399 step
2022-06-12 17:09:29,057   Num examples = 872
2022-06-12 17:09:29,057   Batch size = 32
2022-06-12 17:09:29,811 ***** Eval results *****
2022-06-12 17:09:29,812   acc = 0.8887614678899083
2022-06-12 17:09:29,812   cls_loss = 0.09413849801528104
2022-06-12 17:09:29,812   eval_loss = 0.3267831429839134
2022-06-12 17:09:29,812   global_step = 1399
2022-06-12 17:09:29,812   loss = 0.09413849801528104
2022-06-12 17:09:54,214 ***** Running evaluation *****
2022-06-12 17:09:54,214   Epoch = 0 iter 1499 step
2022-06-12 17:09:54,214   Num examples = 872
2022-06-12 17:09:54,214   Batch size = 32
2022-06-12 17:09:54,964 ***** Eval results *****
2022-06-12 17:09:54,964   acc = 0.8784403669724771
2022-06-12 17:09:54,964   cls_loss = 0.09255727799019073
2022-06-12 17:09:54,965   eval_loss = 0.3738997339137963
2022-06-12 17:09:54,965   global_step = 1499
2022-06-12 17:09:54,965   loss = 0.09255727799019073
2022-06-12 17:10:19,399 ***** Running evaluation *****
2022-06-12 17:10:19,400   Epoch = 0 iter 1599 step
2022-06-12 17:10:19,400   Num examples = 872
2022-06-12 17:10:19,400   Batch size = 32
2022-06-12 17:10:20,150 ***** Eval results *****
2022-06-12 17:10:20,150   acc = 0.8784403669724771
2022-06-12 17:10:20,150   cls_loss = 0.0914487947964385
2022-06-12 17:10:20,151   eval_loss = 0.3555349313787052
2022-06-12 17:10:20,151   global_step = 1599
2022-06-12 17:10:20,151   loss = 0.0914487947964385
2022-06-12 17:10:44,541 ***** Running evaluation *****
2022-06-12 17:10:44,541   Epoch = 0 iter 1699 step
2022-06-12 17:10:44,541   Num examples = 872
2022-06-12 17:10:44,541   Batch size = 32
2022-06-12 17:10:45,293 ***** Eval results *****
2022-06-12 17:10:45,293   acc = 0.8795871559633027
2022-06-12 17:10:45,293   cls_loss = 0.09048558914131666
2022-06-12 17:10:45,293   eval_loss = 0.32458438511405674
2022-06-12 17:10:45,293   global_step = 1699
2022-06-12 17:10:45,293   loss = 0.09048558914131666
2022-06-12 17:11:09,685 ***** Running evaluation *****
2022-06-12 17:11:09,685   Epoch = 0 iter 1799 step
2022-06-12 17:11:09,686   Num examples = 872
2022-06-12 17:11:09,686   Batch size = 32
2022-06-12 17:11:10,437 ***** Eval results *****
2022-06-12 17:11:10,438   acc = 0.8830275229357798
2022-06-12 17:11:10,438   cls_loss = 0.08957703129542145
2022-06-12 17:11:10,438   eval_loss = 0.33590012875252534
2022-06-12 17:11:10,438   global_step = 1799
2022-06-12 17:11:10,438   loss = 0.08957703129542145
2022-06-12 17:11:34,830 ***** Running evaluation *****
2022-06-12 17:11:34,830   Epoch = 0 iter 1899 step
2022-06-12 17:11:34,830   Num examples = 872
2022-06-12 17:11:34,830   Batch size = 32
2022-06-12 17:11:35,581 ***** Eval results *****
2022-06-12 17:11:35,582   acc = 0.8761467889908257
2022-06-12 17:11:35,582   cls_loss = 0.08890404688560254
2022-06-12 17:11:35,582   eval_loss = 0.33901227531688555
2022-06-12 17:11:35,582   global_step = 1899
2022-06-12 17:11:35,582   loss = 0.08890404688560254
2022-06-12 17:11:59,976 ***** Running evaluation *****
2022-06-12 17:11:59,977   Epoch = 0 iter 1999 step
2022-06-12 17:11:59,977   Num examples = 872
2022-06-12 17:11:59,977   Batch size = 32
2022-06-12 17:12:00,728 ***** Eval results *****
2022-06-12 17:12:00,728   acc = 0.8807339449541285
2022-06-12 17:12:00,728   cls_loss = 0.08817403299501982
2022-06-12 17:12:00,728   eval_loss = 0.3957233860024384
2022-06-12 17:12:00,728   global_step = 1999
2022-06-12 17:12:00,728   loss = 0.08817403299501982
2022-06-12 17:12:25,038 ***** Running evaluation *****
2022-06-12 17:12:25,038   Epoch = 0 iter 2099 step
2022-06-12 17:12:25,038   Num examples = 872
2022-06-12 17:12:25,038   Batch size = 32
2022-06-12 17:12:25,789 ***** Eval results *****
2022-06-12 17:12:25,789   acc = 0.8795871559633027
2022-06-12 17:12:25,790   cls_loss = 0.08759396374687063
2022-06-12 17:12:25,790   eval_loss = 0.3648810192410435
2022-06-12 17:12:25,790   global_step = 2099
2022-06-12 17:12:25,790   loss = 0.08759396374687063
2022-06-12 17:12:50,144 ***** Running evaluation *****
2022-06-12 17:12:50,144   Epoch = 1 iter 2199 step
2022-06-12 17:12:50,144   Num examples = 872
2022-06-12 17:12:50,144   Batch size = 32
2022-06-12 17:12:50,898 ***** Eval results *****
2022-06-12 17:12:50,898   acc = 0.8681192660550459
2022-06-12 17:12:50,899   cls_loss = 0.07385548423779638
2022-06-12 17:12:50,899   eval_loss = 0.4345097935625485
2022-06-12 17:12:50,899   global_step = 2199
2022-06-12 17:12:50,899   loss = 0.07385548423779638
2022-06-12 17:13:15,207 ***** Running evaluation *****
2022-06-12 17:13:15,207   Epoch = 1 iter 2299 step
2022-06-12 17:13:15,207   Num examples = 872
2022-06-12 17:13:15,208   Batch size = 32
2022-06-12 17:13:15,958 ***** Eval results *****
2022-06-12 17:13:15,958   acc = 0.8715596330275229
2022-06-12 17:13:15,958   cls_loss = 0.07529246386809227
2022-06-12 17:13:15,958   eval_loss = 0.3613159864076546
2022-06-12 17:13:15,958   global_step = 2299
2022-06-12 17:13:15,958   loss = 0.07529246386809227
2022-06-12 17:13:40,334 ***** Running evaluation *****
2022-06-12 17:13:40,334   Epoch = 1 iter 2399 step
2022-06-12 17:13:40,334   Num examples = 872
2022-06-12 17:13:40,334   Batch size = 32
2022-06-12 17:13:41,085 ***** Eval results *****
2022-06-12 17:13:41,085   acc = 0.875
2022-06-12 17:13:41,085   cls_loss = 0.07494915400521229
2022-06-12 17:13:41,085   eval_loss = 0.36889484005847145
2022-06-12 17:13:41,085   global_step = 2399
2022-06-12 17:13:41,086   loss = 0.07494915400521229
2022-06-12 17:14:05,396 ***** Running evaluation *****
2022-06-12 17:14:05,396   Epoch = 1 iter 2499 step
2022-06-12 17:14:05,397   Num examples = 872
2022-06-12 17:14:05,397   Batch size = 32
2022-06-12 17:14:06,147 ***** Eval results *****
2022-06-12 17:14:06,147   acc = 0.8830275229357798
2022-06-12 17:14:06,147   cls_loss = 0.07521760691004463
2022-06-12 17:14:06,147   eval_loss = 0.3334541057369539
2022-06-12 17:14:06,148   global_step = 2499
2022-06-12 17:14:06,148   loss = 0.07521760691004463
2022-06-12 17:14:30,491 ***** Running evaluation *****
2022-06-12 17:14:30,491   Epoch = 1 iter 2599 step
2022-06-12 17:14:30,491   Num examples = 872
2022-06-12 17:14:30,491   Batch size = 32
2022-06-12 17:14:31,242 ***** Eval results *****
2022-06-12 17:14:31,242   acc = 0.8910550458715596
2022-06-12 17:14:31,242   cls_loss = 0.07456838286133728
2022-06-12 17:14:31,242   eval_loss = 0.295801596183862
2022-06-12 17:14:31,242   global_step = 2599
2022-06-12 17:14:31,242   loss = 0.07456838286133728
2022-06-12 17:14:55,574 ***** Running evaluation *****
2022-06-12 17:14:55,575   Epoch = 1 iter 2699 step
2022-06-12 17:14:55,575   Num examples = 872
2022-06-12 17:14:55,575   Batch size = 32
2022-06-12 17:14:56,327 ***** Eval results *****
2022-06-12 17:14:56,327   acc = 0.8899082568807339
2022-06-12 17:14:56,327   cls_loss = 0.07417285376611878
2022-06-12 17:14:56,327   eval_loss = 0.34112116215484484
2022-06-12 17:14:56,327   global_step = 2699
2022-06-12 17:14:56,327   loss = 0.07417285376611878
2022-06-12 17:15:20,876 ***** Running evaluation *****
2022-06-12 17:15:20,876   Epoch = 1 iter 2799 step
2022-06-12 17:15:20,876   Num examples = 872
2022-06-12 17:15:20,876   Batch size = 32
2022-06-12 17:15:21,628 ***** Eval results *****
2022-06-12 17:15:21,629   acc = 0.8795871559633027
2022-06-12 17:15:21,629   cls_loss = 0.07448361495094334
2022-06-12 17:15:21,629   eval_loss = 0.33535173987703665
2022-06-12 17:15:21,629   global_step = 2799
2022-06-12 17:15:21,629   loss = 0.07448361495094334
2022-06-12 17:15:46,052 ***** Running evaluation *****
2022-06-12 17:15:46,053   Epoch = 1 iter 2899 step
2022-06-12 17:15:46,053   Num examples = 872
2022-06-12 17:15:46,053   Batch size = 32
2022-06-12 17:15:46,803 ***** Eval results *****
2022-06-12 17:15:46,803   acc = 0.8772935779816514
2022-06-12 17:15:46,803   cls_loss = 0.07458073979466216
2022-06-12 17:15:46,803   eval_loss = 0.3481260607285159
2022-06-12 17:15:46,803   global_step = 2899
2022-06-12 17:15:46,804   loss = 0.07458073979466216
2022-06-12 17:16:11,180 ***** Running evaluation *****
2022-06-12 17:16:11,180   Epoch = 1 iter 2999 step
2022-06-12 17:16:11,180   Num examples = 872
2022-06-12 17:16:11,180   Batch size = 32
2022-06-12 17:16:11,932 ***** Eval results *****
2022-06-12 17:16:11,932   acc = 0.8853211009174312
2022-06-12 17:16:11,932   cls_loss = 0.07443825878494278
2022-06-12 17:16:11,932   eval_loss = 0.33327948089156834
2022-06-12 17:16:11,933   global_step = 2999
2022-06-12 17:16:11,933   loss = 0.07443825878494278
2022-06-12 17:16:36,309 ***** Running evaluation *****
2022-06-12 17:16:36,310   Epoch = 1 iter 3099 step
2022-06-12 17:16:36,310   Num examples = 872
2022-06-12 17:16:36,310   Batch size = 32
2022-06-12 17:16:37,062 ***** Eval results *****
2022-06-12 17:16:37,063   acc = 0.8704128440366973
2022-06-12 17:16:37,063   cls_loss = 0.07442721338877127
2022-06-12 17:16:37,063   eval_loss = 0.3591586553624698
2022-06-12 17:16:37,063   global_step = 3099
2022-06-12 17:16:37,063   loss = 0.07442721338877127
2022-06-12 17:17:01,408 ***** Running evaluation *****
2022-06-12 17:17:01,408   Epoch = 1 iter 3199 step
2022-06-12 17:17:01,408   Num examples = 872
2022-06-12 17:17:01,408   Batch size = 32
2022-06-12 17:17:02,159 ***** Eval results *****
2022-06-12 17:17:02,160   acc = 0.8704128440366973
2022-06-12 17:17:02,160   cls_loss = 0.07443701274795075
2022-06-12 17:17:02,160   eval_loss = 0.4127464004393135
2022-06-12 17:17:02,160   global_step = 3199
2022-06-12 17:17:02,160   loss = 0.07443701274795075
2022-06-12 17:17:26,450 ***** Running evaluation *****
2022-06-12 17:17:26,451   Epoch = 1 iter 3299 step
2022-06-12 17:17:26,451   Num examples = 872
2022-06-12 17:17:26,451   Batch size = 32
2022-06-12 17:17:27,202 ***** Eval results *****
2022-06-12 17:17:27,203   acc = 0.8772935779816514
2022-06-12 17:17:27,203   cls_loss = 0.07436611837547694
2022-06-12 17:17:27,203   eval_loss = 0.3415608751986708
2022-06-12 17:17:27,203   global_step = 3299
2022-06-12 17:17:27,203   loss = 0.07436611837547694
2022-06-12 17:17:51,517 ***** Running evaluation *****
2022-06-12 17:17:51,517   Epoch = 1 iter 3399 step
2022-06-12 17:17:51,517   Num examples = 872
2022-06-12 17:17:51,517   Batch size = 32
2022-06-12 17:17:52,267 ***** Eval results *****
2022-06-12 17:17:52,267   acc = 0.8818807339449541
2022-06-12 17:17:52,267   cls_loss = 0.07435718630386595
2022-06-12 17:17:52,267   eval_loss = 0.3430722794894661
2022-06-12 17:17:52,267   global_step = 3399
2022-06-12 17:17:52,268   loss = 0.07435718630386595
2022-06-12 17:18:16,637 ***** Running evaluation *****
2022-06-12 17:18:16,637   Epoch = 1 iter 3499 step
2022-06-12 17:18:16,638   Num examples = 872
2022-06-12 17:18:16,638   Batch size = 32
2022-06-12 17:18:17,389 ***** Eval results *****
2022-06-12 17:18:17,390   acc = 0.8864678899082569
2022-06-12 17:18:17,390   cls_loss = 0.07410802042185193
2022-06-12 17:18:17,390   eval_loss = 0.32485563507569687
2022-06-12 17:18:17,390   global_step = 3499
2022-06-12 17:18:17,390   loss = 0.07410802042185193
2022-06-12 17:18:41,712 ***** Running evaluation *****
2022-06-12 17:18:41,712   Epoch = 1 iter 3599 step
2022-06-12 17:18:41,713   Num examples = 872
2022-06-12 17:18:41,713   Batch size = 32
2022-06-12 17:18:42,470 ***** Eval results *****
2022-06-12 17:18:42,470   acc = 0.8738532110091743
2022-06-12 17:18:42,470   cls_loss = 0.07405103818080497
2022-06-12 17:18:42,470   eval_loss = 0.39178847621328067
2022-06-12 17:18:42,470   global_step = 3599
2022-06-12 17:18:42,470   loss = 0.07405103818080497
2022-06-12 17:19:06,771 ***** Running evaluation *****
2022-06-12 17:19:06,772   Epoch = 1 iter 3699 step
2022-06-12 17:19:06,772   Num examples = 872
2022-06-12 17:19:06,772   Batch size = 32
2022-06-12 17:19:07,523 ***** Eval results *****
2022-06-12 17:19:07,523   acc = 0.8715596330275229
2022-06-12 17:19:07,523   cls_loss = 0.07407243332145357
2022-06-12 17:19:07,524   eval_loss = 0.4295178058424166
2022-06-12 17:19:07,524   global_step = 3699
2022-06-12 17:19:07,524   loss = 0.07407243332145357
2022-06-12 17:19:31,797 ***** Running evaluation *****
2022-06-12 17:19:31,797   Epoch = 1 iter 3799 step
2022-06-12 17:19:31,797   Num examples = 872
2022-06-12 17:19:31,797   Batch size = 32
2022-06-12 17:19:32,546 ***** Eval results *****
2022-06-12 17:19:32,547   acc = 0.8681192660550459
2022-06-12 17:19:32,547   cls_loss = 0.07403661447596409
2022-06-12 17:19:32,547   eval_loss = 0.3678389135748148
2022-06-12 17:19:32,547   global_step = 3799
2022-06-12 17:19:32,547   loss = 0.07403661447596409
2022-06-12 17:19:56,775 ***** Running evaluation *****
2022-06-12 17:19:56,776   Epoch = 1 iter 3899 step
2022-06-12 17:19:56,776   Num examples = 872
2022-06-12 17:19:56,776   Batch size = 32
2022-06-12 17:19:57,528 ***** Eval results *****
2022-06-12 17:19:57,528   acc = 0.8704128440366973
2022-06-12 17:19:57,528   cls_loss = 0.07421421691733815
2022-06-12 17:19:57,528   eval_loss = 0.3858618177473545
2022-06-12 17:19:57,528   global_step = 3899
2022-06-12 17:19:57,528   loss = 0.07421421691733815
2022-06-12 17:20:21,825 ***** Running evaluation *****
2022-06-12 17:20:21,825   Epoch = 1 iter 3999 step
2022-06-12 17:20:21,825   Num examples = 872
2022-06-12 17:20:21,825   Batch size = 32
2022-06-12 17:20:22,575 ***** Eval results *****
2022-06-12 17:20:22,575   acc = 0.8761467889908257
2022-06-12 17:20:22,575   cls_loss = 0.07435525871400468
2022-06-12 17:20:22,575   eval_loss = 0.3685869462788105
2022-06-12 17:20:22,575   global_step = 3999
2022-06-12 17:20:22,575   loss = 0.07435525871400468
2022-06-12 17:20:46,817 ***** Running evaluation *****
2022-06-12 17:20:46,817   Epoch = 1 iter 4099 step
2022-06-12 17:20:46,817   Num examples = 872
2022-06-12 17:20:46,817   Batch size = 32
2022-06-12 17:20:47,568 ***** Eval results *****
2022-06-12 17:20:47,568   acc = 0.8864678899082569
2022-06-12 17:20:47,568   cls_loss = 0.07430390501791671
2022-06-12 17:20:47,568   eval_loss = 0.3425847307911941
2022-06-12 17:20:47,568   global_step = 4099
2022-06-12 17:20:47,569   loss = 0.07430390501791671
2022-06-12 17:21:11,799 ***** Running evaluation *****
2022-06-12 17:21:11,800   Epoch = 1 iter 4199 step
2022-06-12 17:21:11,800   Num examples = 872
2022-06-12 17:21:11,800   Batch size = 32
2022-06-12 17:21:12,551 ***** Eval results *****
2022-06-12 17:21:12,552   acc = 0.8807339449541285
2022-06-12 17:21:12,552   cls_loss = 0.07427140200301685
2022-06-12 17:21:12,552   eval_loss = 0.3284057494519012
2022-06-12 17:21:12,552   global_step = 4199
2022-06-12 17:21:12,552   loss = 0.07427140200301685
2022-06-12 17:21:37,026 ***** Running evaluation *****
2022-06-12 17:21:37,026   Epoch = 2 iter 4299 step
2022-06-12 17:21:37,026   Num examples = 872
2022-06-12 17:21:37,026   Batch size = 32
2022-06-12 17:21:37,776 ***** Eval results *****
2022-06-12 17:21:37,777   acc = 0.8761467889908257
2022-06-12 17:21:37,777   cls_loss = 0.07484457162874085
2022-06-12 17:21:37,777   eval_loss = 0.3255369375858988
2022-06-12 17:21:37,777   global_step = 4299
2022-06-12 17:21:37,777   loss = 0.07484457162874085
2022-06-12 17:22:02,326 ***** Running evaluation *****
2022-06-12 17:22:02,326   Epoch = 2 iter 4399 step
2022-06-12 17:22:02,326   Num examples = 872
2022-06-12 17:22:02,327   Batch size = 32
2022-06-12 17:22:03,082 ***** Eval results *****
2022-06-12 17:22:03,082   acc = 0.8818807339449541
2022-06-12 17:22:03,082   cls_loss = 0.07335546928706593
2022-06-12 17:22:03,082   eval_loss = 0.3449865084673677
2022-06-12 17:22:03,082   global_step = 4399
2022-06-12 17:22:03,082   loss = 0.07335546928706593
2022-06-12 17:22:27,545 ***** Running evaluation *****
2022-06-12 17:22:27,545   Epoch = 2 iter 4499 step
2022-06-12 17:22:27,545   Num examples = 872
2022-06-12 17:22:27,545   Batch size = 32
2022-06-12 17:22:28,298 ***** Eval results *****
2022-06-12 17:22:28,298   acc = 0.8692660550458715
2022-06-12 17:22:28,298   cls_loss = 0.07300715613150105
2022-06-12 17:22:28,298   eval_loss = 0.3605861413691725
2022-06-12 17:22:28,298   global_step = 4499
2022-06-12 17:22:28,299   loss = 0.07300715613150105
2022-06-12 17:22:52,515 ***** Running evaluation *****
2022-06-12 17:22:52,516   Epoch = 2 iter 4599 step
2022-06-12 17:22:52,516   Num examples = 872
2022-06-12 17:22:52,516   Batch size = 32
2022-06-12 17:22:53,267 ***** Eval results *****
2022-06-12 17:22:53,267   acc = 0.8876146788990825
2022-06-12 17:22:53,267   cls_loss = 0.07279790866443568
2022-06-12 17:22:53,267   eval_loss = 0.331817683630756
2022-06-12 17:22:53,267   global_step = 4599
2022-06-12 17:22:53,267   loss = 0.07279790866443568
2022-06-12 17:23:17,751 ***** Running evaluation *****
2022-06-12 17:23:17,751   Epoch = 2 iter 4699 step
2022-06-12 17:23:17,751   Num examples = 872
2022-06-12 17:23:17,751   Batch size = 32
2022-06-12 17:23:18,502 ***** Eval results *****
2022-06-12 17:23:18,502   acc = 0.8807339449541285
2022-06-12 17:23:18,502   cls_loss = 0.07209008511026863
2022-06-12 17:23:18,502   eval_loss = 0.35457861463406254
2022-06-12 17:23:18,502   global_step = 4699
2022-06-12 17:23:18,502   loss = 0.07209008511026863
2022-06-12 17:23:42,981 ***** Running evaluation *****
2022-06-12 17:23:42,981   Epoch = 2 iter 4799 step
2022-06-12 17:23:42,982   Num examples = 872
2022-06-12 17:23:42,982   Batch size = 32
2022-06-12 17:23:43,733 ***** Eval results *****
2022-06-12 17:23:43,734   acc = 0.8899082568807339
2022-06-12 17:23:43,734   cls_loss = 0.0720187398761057
2022-06-12 17:23:43,734   eval_loss = 0.3683864057862333
2022-06-12 17:23:43,734   global_step = 4799
2022-06-12 17:23:43,734   loss = 0.0720187398761057
2022-06-12 17:24:08,206 ***** Running evaluation *****
2022-06-12 17:24:08,206   Epoch = 2 iter 4899 step
2022-06-12 17:24:08,206   Num examples = 872
2022-06-12 17:24:08,206   Batch size = 32
2022-06-12 17:24:08,958 ***** Eval results *****
2022-06-12 17:24:08,958   acc = 0.8876146788990825
2022-06-12 17:24:08,958   cls_loss = 0.07202114774030127
2022-06-12 17:24:08,958   eval_loss = 0.34953536479068653
2022-06-12 17:24:08,958   global_step = 4899
2022-06-12 17:24:08,958   loss = 0.07202114774030127
2022-06-12 17:24:33,297 ***** Running evaluation *****
2022-06-12 17:24:33,298   Epoch = 2 iter 4999 step
2022-06-12 17:24:33,298   Num examples = 872
2022-06-12 17:24:33,298   Batch size = 32
2022-06-12 17:24:34,049 ***** Eval results *****
2022-06-12 17:24:34,049   acc = 0.8910550458715596
2022-06-12 17:24:34,049   cls_loss = 0.07200555935709274
2022-06-12 17:24:34,049   eval_loss = 0.33377877064049244
2022-06-12 17:24:34,049   global_step = 4999
2022-06-12 17:24:34,049   loss = 0.07200555935709274
2022-06-12 17:24:58,295 ***** Running evaluation *****
2022-06-12 17:24:58,296   Epoch = 2 iter 5099 step
2022-06-12 17:24:58,296   Num examples = 872
2022-06-12 17:24:58,296   Batch size = 32
2022-06-12 17:24:59,046 ***** Eval results *****
2022-06-12 17:24:59,046   acc = 0.8899082568807339
2022-06-12 17:24:59,046   cls_loss = 0.07206285078868721
2022-06-12 17:24:59,046   eval_loss = 0.3632223763103996
2022-06-12 17:24:59,046   global_step = 5099
2022-06-12 17:24:59,046   loss = 0.07206285078868721
2022-06-12 17:25:23,342 ***** Running evaluation *****
2022-06-12 17:25:23,342   Epoch = 2 iter 5199 step
2022-06-12 17:25:23,342   Num examples = 872
2022-06-12 17:25:23,342   Batch size = 32
2022-06-12 17:25:24,094 ***** Eval results *****
2022-06-12 17:25:24,095   acc = 0.8910550458715596
2022-06-12 17:25:24,095   cls_loss = 0.07200235622351393
2022-06-12 17:25:24,095   eval_loss = 0.3591474060501371
2022-06-12 17:25:24,095   global_step = 5199
2022-06-12 17:25:24,095   loss = 0.07200235622351393
2022-06-12 17:25:48,426 ***** Running evaluation *****
2022-06-12 17:25:48,426   Epoch = 2 iter 5299 step
2022-06-12 17:25:48,426   Num examples = 872
2022-06-12 17:25:48,426   Batch size = 32
2022-06-12 17:25:49,179 ***** Eval results *****
2022-06-12 17:25:49,179   acc = 0.8922018348623854
2022-06-12 17:25:49,180   cls_loss = 0.07211370911964793
2022-06-12 17:25:49,180   eval_loss = 0.3364815544337034
2022-06-12 17:25:49,180   global_step = 5299
2022-06-12 17:25:49,180   loss = 0.07211370911964793
2022-06-12 17:26:13,594 ***** Running evaluation *****
2022-06-12 17:26:13,594   Epoch = 2 iter 5399 step
2022-06-12 17:26:13,594   Num examples = 872
2022-06-12 17:26:13,594   Batch size = 32
2022-06-12 17:26:14,345 ***** Eval results *****
2022-06-12 17:26:14,345   acc = 0.8876146788990825
2022-06-12 17:26:14,345   cls_loss = 0.07197274099707003
2022-06-12 17:26:14,345   eval_loss = 0.3331026267260313
2022-06-12 17:26:14,345   global_step = 5399
2022-06-12 17:26:14,349   loss = 0.07197274099707003
2022-06-12 17:26:38,835 ***** Running evaluation *****
2022-06-12 17:26:38,835   Epoch = 2 iter 5499 step
2022-06-12 17:26:38,835   Num examples = 872
2022-06-12 17:26:38,835   Batch size = 32
2022-06-12 17:26:39,586 ***** Eval results *****
2022-06-12 17:26:39,586   acc = 0.8899082568807339
2022-06-12 17:26:39,586   cls_loss = 0.07212188615702456
2022-06-12 17:26:39,586   eval_loss = 0.3245285057595798
2022-06-12 17:26:39,586   global_step = 5499
2022-06-12 17:26:39,586   loss = 0.07212188615702456
2022-06-12 17:27:03,885 ***** Running evaluation *****
2022-06-12 17:27:03,886   Epoch = 2 iter 5599 step
2022-06-12 17:27:03,886   Num examples = 872
2022-06-12 17:27:03,886   Batch size = 32
2022-06-12 17:27:04,638 ***** Eval results *****
2022-06-12 17:27:04,638   acc = 0.8876146788990825
2022-06-12 17:27:04,638   cls_loss = 0.07222551085713697
2022-06-12 17:27:04,638   eval_loss = 0.35730881882565363
2022-06-12 17:27:04,638   global_step = 5599
2022-06-12 17:27:04,638   loss = 0.07222551085713697
2022-06-12 17:27:29,163 ***** Running evaluation *****
2022-06-12 17:27:29,163   Epoch = 2 iter 5699 step
2022-06-12 17:27:29,163   Num examples = 872
2022-06-12 17:27:29,163   Batch size = 32
2022-06-12 17:27:29,915 ***** Eval results *****
2022-06-12 17:27:29,915   acc = 0.8899082568807339
2022-06-12 17:27:29,915   cls_loss = 0.07232001558505873
2022-06-12 17:27:29,915   eval_loss = 0.3411393921290125
2022-06-12 17:27:29,915   global_step = 5699
2022-06-12 17:27:29,915   loss = 0.07232001558505873
2022-06-12 17:27:54,256 ***** Running evaluation *****
2022-06-12 17:27:54,257   Epoch = 2 iter 5799 step
2022-06-12 17:27:54,257   Num examples = 872
2022-06-12 17:27:54,257   Batch size = 32
2022-06-12 17:27:55,007 ***** Eval results *****
2022-06-12 17:27:55,007   acc = 0.8876146788990825
2022-06-12 17:27:55,007   cls_loss = 0.07253584949420279
2022-06-12 17:27:55,007   eval_loss = 0.30843449517020155
2022-06-12 17:27:55,007   global_step = 5799
2022-06-12 17:27:55,008   loss = 0.07253584949420279
2022-06-12 17:28:19,344 ***** Running evaluation *****
2022-06-12 17:28:19,344   Epoch = 2 iter 5899 step
2022-06-12 17:28:19,344   Num examples = 872
2022-06-12 17:28:19,344   Batch size = 32
2022-06-12 17:28:20,095 ***** Eval results *****
2022-06-12 17:28:20,095   acc = 0.8944954128440367
2022-06-12 17:28:20,095   cls_loss = 0.07239559201588651
2022-06-12 17:28:20,095   eval_loss = 0.3134482795638697
2022-06-12 17:28:20,095   global_step = 5899
2022-06-12 17:28:20,095   loss = 0.07239559201588651
2022-06-12 17:28:44,581 ***** Running evaluation *****
2022-06-12 17:28:44,581   Epoch = 2 iter 5999 step
2022-06-12 17:28:44,582   Num examples = 872
2022-06-12 17:28:44,582   Batch size = 32
2022-06-12 17:28:45,332 ***** Eval results *****
2022-06-12 17:28:45,332   acc = 0.8944954128440367
2022-06-12 17:28:45,333   cls_loss = 0.07230483355975098
2022-06-12 17:28:45,333   eval_loss = 0.3457836571282574
2022-06-12 17:28:45,333   global_step = 5999
2022-06-12 17:28:45,333   loss = 0.07230483355975098
2022-06-12 17:29:09,675 ***** Running evaluation *****
2022-06-12 17:29:09,676   Epoch = 2 iter 6099 step
2022-06-12 17:29:09,676   Num examples = 872
2022-06-12 17:29:09,676   Batch size = 32
2022-06-12 17:29:10,427 ***** Eval results *****
2022-06-12 17:29:10,427   acc = 0.8795871559633027
2022-06-12 17:29:10,428   cls_loss = 0.0724810075816624
2022-06-12 17:29:10,428   eval_loss = 0.3473404025925057
2022-06-12 17:29:10,428   global_step = 6099
2022-06-12 17:29:10,428   loss = 0.0724810075816624
2022-06-12 17:29:34,783 ***** Running evaluation *****
2022-06-12 17:29:34,783   Epoch = 2 iter 6199 step
2022-06-12 17:29:34,783   Num examples = 872
2022-06-12 17:29:34,783   Batch size = 32
2022-06-12 17:29:35,535 ***** Eval results *****
2022-06-12 17:29:35,535   acc = 0.8899082568807339
2022-06-12 17:29:35,535   cls_loss = 0.07250779215667429
2022-06-12 17:29:35,535   eval_loss = 0.3015893797523209
2022-06-12 17:29:35,535   global_step = 6199
2022-06-12 17:29:35,535   loss = 0.07250779215667429
2022-06-12 17:29:59,891 ***** Running evaluation *****
2022-06-12 17:29:59,891   Epoch = 2 iter 6299 step
2022-06-12 17:29:59,891   Num examples = 872
2022-06-12 17:29:59,891   Batch size = 32
2022-06-12 17:30:00,642 ***** Eval results *****
2022-06-12 17:30:00,643   acc = 0.8899082568807339
2022-06-12 17:30:00,643   cls_loss = 0.07260840614729382
2022-06-12 17:30:00,643   eval_loss = 0.28970535152724813
2022-06-12 17:30:00,643   global_step = 6299
2022-06-12 17:30:00,643   loss = 0.07260840614729382
2022-06-12 17:30:24,995 ***** Running evaluation *****
2022-06-12 17:30:24,996   Epoch = 3 iter 6399 step
2022-06-12 17:30:24,996   Num examples = 872
2022-06-12 17:30:24,996   Batch size = 32
2022-06-12 17:30:25,746 ***** Eval results *****
2022-06-12 17:30:25,746   acc = 0.893348623853211
2022-06-12 17:30:25,746   cls_loss = 0.06829247705038936
2022-06-12 17:30:25,746   eval_loss = 0.3174001720866987
2022-06-12 17:30:25,746   global_step = 6399
2022-06-12 17:30:25,746   loss = 0.06829247705038936
2022-06-12 17:30:50,020 ***** Running evaluation *****
2022-06-12 17:30:50,020   Epoch = 3 iter 6499 step
2022-06-12 17:30:50,020   Num examples = 872
2022-06-12 17:30:50,020   Batch size = 32
2022-06-12 17:30:50,770 ***** Eval results *****
2022-06-12 17:30:50,770   acc = 0.893348623853211
2022-06-12 17:30:50,770   cls_loss = 0.07036007054269633
2022-06-12 17:30:50,770   eval_loss = 0.3350770702319486
2022-06-12 17:30:50,770   global_step = 6499
2022-06-12 17:30:50,770   loss = 0.07036007054269633
2022-06-12 17:31:15,128 ***** Running evaluation *****
2022-06-12 17:31:15,128   Epoch = 3 iter 6599 step
2022-06-12 17:31:15,128   Num examples = 872
2022-06-12 17:31:15,129   Batch size = 32
2022-06-12 17:31:15,879 ***** Eval results *****
2022-06-12 17:31:15,879   acc = 0.8864678899082569
2022-06-12 17:31:15,879   cls_loss = 0.07085352126210409
2022-06-12 17:31:15,879   eval_loss = 0.32518844372992006
2022-06-12 17:31:15,879   global_step = 6599
2022-06-12 17:31:15,879   loss = 0.07085352126210409
2022-06-12 17:31:40,161 ***** Running evaluation *****
2022-06-12 17:31:40,161   Epoch = 3 iter 6699 step
2022-06-12 17:31:40,161   Num examples = 872
2022-06-12 17:31:40,161   Batch size = 32
2022-06-12 17:31:40,911 ***** Eval results *****
2022-06-12 17:31:40,911   acc = 0.8841743119266054
2022-06-12 17:31:40,911   cls_loss = 0.07128041427226338
2022-06-12 17:31:40,912   eval_loss = 0.345098767030452
2022-06-12 17:31:40,912   global_step = 6699
2022-06-12 17:31:40,912   loss = 0.07128041427226338
2022-06-12 17:32:05,230 ***** Running evaluation *****
2022-06-12 17:32:05,230   Epoch = 3 iter 6799 step
2022-06-12 17:32:05,230   Num examples = 872
2022-06-12 17:32:05,230   Batch size = 32
2022-06-12 17:32:05,981 ***** Eval results *****
2022-06-12 17:32:05,981   acc = 0.8830275229357798
2022-06-12 17:32:05,981   cls_loss = 0.07108026392589605
2022-06-12 17:32:05,981   eval_loss = 0.35362529195845127
2022-06-12 17:32:05,982   global_step = 6799
2022-06-12 17:32:05,982   loss = 0.07108026392589605
2022-06-12 17:32:30,314 ***** Running evaluation *****
2022-06-12 17:32:30,314   Epoch = 3 iter 6899 step
2022-06-12 17:32:30,314   Num examples = 872
2022-06-12 17:32:30,314   Batch size = 32
2022-06-12 17:32:31,065 ***** Eval results *****
2022-06-12 17:32:31,066   acc = 0.8818807339449541
2022-06-12 17:32:31,066   cls_loss = 0.07083318194015875
2022-06-12 17:32:31,066   eval_loss = 0.34863624455673353
2022-06-12 17:32:31,066   global_step = 6899
2022-06-12 17:32:31,066   loss = 0.07083318194015875
2022-06-12 17:32:55,416 ***** Running evaluation *****
2022-06-12 17:32:55,417   Epoch = 3 iter 6999 step
2022-06-12 17:32:55,417   Num examples = 872
2022-06-12 17:32:55,417   Batch size = 32
2022-06-12 17:32:56,168 ***** Eval results *****
2022-06-12 17:32:56,168   acc = 0.8841743119266054
2022-06-12 17:32:56,168   cls_loss = 0.07116716639226875
2022-06-12 17:32:56,168   eval_loss = 0.33813082666269373
2022-06-12 17:32:56,168   global_step = 6999
2022-06-12 17:32:56,168   loss = 0.07116716639226875
2022-06-12 17:33:20,502 ***** Running evaluation *****
2022-06-12 17:33:20,502   Epoch = 3 iter 7099 step
2022-06-12 17:33:20,502   Num examples = 872
2022-06-12 17:33:20,503   Batch size = 32
2022-06-12 17:33:21,252 ***** Eval results *****
2022-06-12 17:33:21,253   acc = 0.8876146788990825
2022-06-12 17:33:21,253   cls_loss = 0.07103673332244195
2022-06-12 17:33:21,253   eval_loss = 0.3429496393405965
2022-06-12 17:33:21,253   global_step = 7099
2022-06-12 17:33:21,253   loss = 0.07103673332244195
2022-06-12 17:33:45,547 ***** Running evaluation *****
2022-06-12 17:33:45,548   Epoch = 3 iter 7199 step
2022-06-12 17:33:45,548   Num examples = 872
2022-06-12 17:33:45,548   Batch size = 32
2022-06-12 17:33:46,295 ***** Eval results *****
2022-06-12 17:33:46,295   acc = 0.8795871559633027
2022-06-12 17:33:46,296   cls_loss = 0.07105898758419606
2022-06-12 17:33:46,296   eval_loss = 0.3466557620891503
2022-06-12 17:33:46,296   global_step = 7199
2022-06-12 17:33:46,296   loss = 0.07105898758419606
2022-06-12 17:34:10,527 ***** Running evaluation *****
2022-06-12 17:34:10,527   Epoch = 3 iter 7299 step
2022-06-12 17:34:10,527   Num examples = 872
2022-06-12 17:34:10,527   Batch size = 32
2022-06-12 17:34:11,278 ***** Eval results *****
2022-06-12 17:34:11,278   acc = 0.8864678899082569
2022-06-12 17:34:11,278   cls_loss = 0.07106867418251743
2022-06-12 17:34:11,278   eval_loss = 0.344104961092983
2022-06-12 17:34:11,278   global_step = 7299
2022-06-12 17:34:11,278   loss = 0.07106867418251743
2022-06-12 17:34:35,587 ***** Running evaluation *****
2022-06-12 17:34:35,588   Epoch = 3 iter 7399 step
2022-06-12 17:34:35,588   Num examples = 872
2022-06-12 17:34:35,588   Batch size = 32
2022-06-12 17:34:36,339 ***** Eval results *****
2022-06-12 17:34:36,339   acc = 0.8807339449541285
2022-06-12 17:34:36,339   cls_loss = 0.07112401855463486
2022-06-12 17:34:36,339   eval_loss = 0.35461497945444925
2022-06-12 17:34:36,339   global_step = 7399
2022-06-12 17:34:36,339   loss = 0.07112401855463486
2022-06-12 17:35:00,705 ***** Running evaluation *****
2022-06-12 17:35:00,706   Epoch = 3 iter 7499 step
2022-06-12 17:35:00,706   Num examples = 872
2022-06-12 17:35:00,706   Batch size = 32
2022-06-12 17:35:01,458 ***** Eval results *****
2022-06-12 17:35:01,459   acc = 0.8738532110091743
2022-06-12 17:35:01,459   cls_loss = 0.07120445015406347
2022-06-12 17:35:01,459   eval_loss = 0.31019200863582747
2022-06-12 17:35:01,459   global_step = 7499
2022-06-12 17:35:01,459   loss = 0.07120445015406347
2022-06-12 17:35:25,908 ***** Running evaluation *****
2022-06-12 17:35:25,908   Epoch = 3 iter 7599 step
2022-06-12 17:35:25,908   Num examples = 872
2022-06-12 17:35:25,908   Batch size = 32
2022-06-12 17:35:26,659 ***** Eval results *****
2022-06-12 17:35:26,659   acc = 0.8830275229357798
2022-06-12 17:35:26,659   cls_loss = 0.07124842503039339
2022-06-12 17:35:26,659   eval_loss = 0.32777207331465824
2022-06-12 17:35:26,659   global_step = 7599
2022-06-12 17:35:26,659   loss = 0.07124842503039339
2022-06-12 17:35:51,011 ***** Running evaluation *****
2022-06-12 17:35:51,012   Epoch = 3 iter 7699 step
2022-06-12 17:35:51,012   Num examples = 872
2022-06-12 17:35:51,012   Batch size = 32
2022-06-12 17:35:51,761 ***** Eval results *****
2022-06-12 17:35:51,762   acc = 0.8910550458715596
2022-06-12 17:35:51,762   cls_loss = 0.07126893028455754
2022-06-12 17:35:51,762   eval_loss = 0.3208318739863379
2022-06-12 17:35:51,762   global_step = 7699
2022-06-12 17:35:51,762   loss = 0.07126893028455754
2022-06-12 17:36:16,175 ***** Running evaluation *****
2022-06-12 17:36:16,175   Epoch = 3 iter 7799 step
2022-06-12 17:36:16,176   Num examples = 872
2022-06-12 17:36:16,176   Batch size = 32
2022-06-12 17:36:16,926 ***** Eval results *****
2022-06-12 17:36:16,927   acc = 0.8910550458715596
2022-06-12 17:36:16,927   cls_loss = 0.07139326209389049
2022-06-12 17:36:16,927   eval_loss = 0.3281527478247881
2022-06-12 17:36:16,927   global_step = 7799
2022-06-12 17:36:16,927   loss = 0.07139326209389049
2022-06-12 17:36:41,354 ***** Running evaluation *****
2022-06-12 17:36:41,354   Epoch = 3 iter 7899 step
2022-06-12 17:36:41,354   Num examples = 872
2022-06-12 17:36:41,355   Batch size = 32
2022-06-12 17:36:42,104 ***** Eval results *****
2022-06-12 17:36:42,105   acc = 0.8956422018348624
2022-06-12 17:36:42,105   cls_loss = 0.07138806463189297
2022-06-12 17:36:42,105   eval_loss = 0.30724140043769566
2022-06-12 17:36:42,105   global_step = 7899
2022-06-12 17:36:42,105   loss = 0.07138806463189297
2022-06-12 17:37:06,514 ***** Running evaluation *****
2022-06-12 17:37:06,514   Epoch = 3 iter 7999 step
2022-06-12 17:37:06,514   Num examples = 872
2022-06-12 17:37:06,515   Batch size = 32
2022-06-12 17:37:07,264 ***** Eval results *****
2022-06-12 17:37:07,264   acc = 0.8841743119266054
2022-06-12 17:37:07,265   cls_loss = 0.07144269694766846
2022-06-12 17:37:07,265   eval_loss = 0.3431127036788634
2022-06-12 17:37:07,265   global_step = 7999
2022-06-12 17:37:07,265   loss = 0.07144269694766846
2022-06-12 17:37:31,621 ***** Running evaluation *****
2022-06-12 17:37:31,621   Epoch = 3 iter 8099 step
2022-06-12 17:37:31,622   Num examples = 872
2022-06-12 17:37:31,622   Batch size = 32
2022-06-12 17:37:32,373 ***** Eval results *****
2022-06-12 17:37:32,373   acc = 0.8818807339449541
2022-06-12 17:37:32,373   cls_loss = 0.07148088211695042
2022-06-12 17:37:32,373   eval_loss = 0.3837767301925591
2022-06-12 17:37:32,373   global_step = 8099
2022-06-12 17:37:32,373   loss = 0.07148088211695042
2022-06-12 17:37:56,665 ***** Running evaluation *****
2022-06-12 17:37:56,665   Epoch = 3 iter 8199 step
2022-06-12 17:37:56,665   Num examples = 872
2022-06-12 17:37:56,665   Batch size = 32
2022-06-12 17:37:57,417 ***** Eval results *****
2022-06-12 17:37:57,417   acc = 0.8841743119266054
2022-06-12 17:37:57,417   cls_loss = 0.07162482918461732
2022-06-12 17:37:57,417   eval_loss = 0.342043615345444
2022-06-12 17:37:57,417   global_step = 8199
2022-06-12 17:37:57,417   loss = 0.07162482918461732
2022-06-12 17:38:21,924 ***** Running evaluation *****
2022-06-12 17:38:21,924   Epoch = 3 iter 8299 step
2022-06-12 17:38:21,924   Num examples = 872
2022-06-12 17:38:21,924   Batch size = 32
2022-06-12 17:38:22,679 ***** Eval results *****
2022-06-12 17:38:22,679   acc = 0.8956422018348624
2022-06-12 17:38:22,679   cls_loss = 0.07169100795378022
2022-06-12 17:38:22,680   eval_loss = 0.3313983878386872
2022-06-12 17:38:22,680   global_step = 8299
2022-06-12 17:38:22,680   loss = 0.07169100795378022
2022-06-12 17:38:46,987 ***** Running evaluation *****
2022-06-12 17:38:46,987   Epoch = 3 iter 8399 step
2022-06-12 17:38:46,987   Num examples = 872
2022-06-12 17:38:46,987   Batch size = 32
2022-06-12 17:38:47,739 ***** Eval results *****
2022-06-12 17:38:47,739   acc = 0.8910550458715596
2022-06-12 17:38:47,739   cls_loss = 0.07167549288345686
2022-06-12 17:38:47,739   eval_loss = 0.32926920987665653
2022-06-12 17:38:47,739   global_step = 8399
2022-06-12 17:38:47,739   loss = 0.07167549288345686
2022-06-12 17:39:12,162 ***** Running evaluation *****
2022-06-12 17:39:12,162   Epoch = 4 iter 8499 step
2022-06-12 17:39:12,162   Num examples = 872
2022-06-12 17:39:12,162   Batch size = 32
2022-06-12 17:39:12,913 ***** Eval results *****
2022-06-12 17:39:12,914   acc = 0.8887614678899083
2022-06-12 17:39:12,914   cls_loss = 0.0688461170318615
2022-06-12 17:39:12,914   eval_loss = 0.337784770477031
2022-06-12 17:39:12,914   global_step = 8499
2022-06-12 17:39:12,914   loss = 0.0688461170318615
2022-06-12 17:39:37,213 ***** Running evaluation *****
2022-06-12 17:39:37,213   Epoch = 4 iter 8599 step
2022-06-12 17:39:37,213   Num examples = 872
2022-06-12 17:39:37,213   Batch size = 32
2022-06-12 17:39:37,963 ***** Eval results *****
2022-06-12 17:39:37,963   acc = 0.893348623853211
2022-06-12 17:39:37,964   cls_loss = 0.06978420434835178
2022-06-12 17:39:37,964   eval_loss = 0.32127887276666506
2022-06-12 17:39:37,964   global_step = 8599
2022-06-12 17:39:37,964   loss = 0.06978420434835178
2022-06-12 17:40:02,320 ***** Running evaluation *****
2022-06-12 17:40:02,321   Epoch = 4 iter 8699 step
2022-06-12 17:40:02,321   Num examples = 872
2022-06-12 17:40:02,321   Batch size = 32
2022-06-12 17:40:03,071 ***** Eval results *****
2022-06-12 17:40:03,071   acc = 0.8922018348623854
2022-06-12 17:40:03,071   cls_loss = 0.07055396713026842
2022-06-12 17:40:03,071   eval_loss = 0.31456794483321054
2022-06-12 17:40:03,071   global_step = 8699
2022-06-12 17:40:03,071   loss = 0.07055396713026842
2022-06-12 17:40:27,427 ***** Running evaluation *****
2022-06-12 17:40:27,427   Epoch = 4 iter 8799 step
2022-06-12 17:40:27,427   Num examples = 872
2022-06-12 17:40:27,427   Batch size = 32
2022-06-12 17:40:28,178 ***** Eval results *****
2022-06-12 17:40:28,178   acc = 0.8967889908256881
2022-06-12 17:40:28,179   cls_loss = 0.07084518108837909
2022-06-12 17:40:28,179   eval_loss = 0.32032306119799614
2022-06-12 17:40:28,179   global_step = 8799
2022-06-12 17:40:28,179   loss = 0.07084518108837909
2022-06-12 17:40:52,529 ***** Running evaluation *****
2022-06-12 17:40:52,529   Epoch = 4 iter 8899 step
2022-06-12 17:40:52,530   Num examples = 872
2022-06-12 17:40:52,530   Batch size = 32
2022-06-12 17:40:53,279 ***** Eval results *****
2022-06-12 17:40:53,279   acc = 0.8876146788990825
2022-06-12 17:40:53,279   cls_loss = 0.07083766905910974
2022-06-12 17:40:53,279   eval_loss = 0.3455792266343321
2022-06-12 17:40:53,279   global_step = 8899
2022-06-12 17:40:53,279   loss = 0.07083766905910974
2022-06-12 17:41:17,594 ***** Running evaluation *****
2022-06-12 17:41:17,594   Epoch = 4 iter 8999 step
2022-06-12 17:41:17,594   Num examples = 872
2022-06-12 17:41:17,594   Batch size = 32
2022-06-12 17:41:18,345 ***** Eval results *****
2022-06-12 17:41:18,345   acc = 0.8899082568807339
2022-06-12 17:41:18,345   cls_loss = 0.07109314597516894
2022-06-12 17:41:18,345   eval_loss = 0.34485723823308945
2022-06-12 17:41:18,345   global_step = 8999
2022-06-12 17:41:18,345   loss = 0.07109314597516894
2022-06-12 17:41:42,788 ***** Running evaluation *****
2022-06-12 17:41:42,788   Epoch = 4 iter 9099 step
2022-06-12 17:41:42,788   Num examples = 872
2022-06-12 17:41:42,789   Batch size = 32
2022-06-12 17:41:43,538 ***** Eval results *****
2022-06-12 17:41:43,538   acc = 0.8876146788990825
2022-06-12 17:41:43,539   cls_loss = 0.07088615705489588
2022-06-12 17:41:43,539   eval_loss = 0.37394469124930246
2022-06-12 17:41:43,539   global_step = 9099
2022-06-12 17:41:43,539   loss = 0.07088615705489588
2022-06-12 17:42:07,848 ***** Running evaluation *****
2022-06-12 17:42:07,848   Epoch = 4 iter 9199 step
2022-06-12 17:42:07,848   Num examples = 872
2022-06-12 17:42:07,848   Batch size = 32
2022-06-12 17:42:08,600 ***** Eval results *****
2022-06-12 17:42:08,600   acc = 0.8910550458715596
2022-06-12 17:42:08,600   cls_loss = 0.07082217810425721
2022-06-12 17:42:08,600   eval_loss = 0.3367442935705185
2022-06-12 17:42:08,600   global_step = 9199
2022-06-12 17:42:08,600   loss = 0.07082217810425721
2022-06-12 17:42:32,914 ***** Running evaluation *****
2022-06-12 17:42:32,914   Epoch = 4 iter 9299 step
2022-06-12 17:42:32,914   Num examples = 872
2022-06-12 17:42:32,914   Batch size = 32
2022-06-12 17:42:33,664 ***** Eval results *****
2022-06-12 17:42:33,664   acc = 0.8876146788990825
2022-06-12 17:42:33,664   cls_loss = 0.07105139547184802
2022-06-12 17:42:33,664   eval_loss = 0.33364121722323553
2022-06-12 17:42:33,664   global_step = 9299
2022-06-12 17:42:33,664   loss = 0.07105139547184802
2022-06-12 17:42:57,941 ***** Running evaluation *****
2022-06-12 17:42:57,941   Epoch = 4 iter 9399 step
2022-06-12 17:42:57,941   Num examples = 872
2022-06-12 17:42:57,941   Batch size = 32
2022-06-12 17:42:58,693 ***** Eval results *****
2022-06-12 17:42:58,693   acc = 0.893348623853211
2022-06-12 17:42:58,693   cls_loss = 0.07098995898741307
2022-06-12 17:42:58,693   eval_loss = 0.3372343491230692
2022-06-12 17:42:58,693   global_step = 9399
2022-06-12 17:42:58,693   loss = 0.07098995898741307
2022-06-12 17:43:23,068 ***** Running evaluation *****
2022-06-12 17:43:23,068   Epoch = 4 iter 9499 step
2022-06-12 17:43:23,068   Num examples = 872
2022-06-12 17:43:23,068   Batch size = 32
2022-06-12 17:43:23,820 ***** Eval results *****
2022-06-12 17:43:23,820   acc = 0.8853211009174312
2022-06-12 17:43:23,821   cls_loss = 0.07095346661045501
2022-06-12 17:43:23,821   eval_loss = 0.3464379044515746
2022-06-12 17:43:23,821   global_step = 9499
2022-06-12 17:43:23,821   loss = 0.07095346661045501
2022-06-12 17:43:48,083 ***** Running evaluation *****
2022-06-12 17:43:48,083   Epoch = 4 iter 9599 step
2022-06-12 17:43:48,083   Num examples = 872
2022-06-12 17:43:48,083   Batch size = 32
2022-06-12 17:43:48,834 ***** Eval results *****
2022-06-12 17:43:48,835   acc = 0.8795871559633027
2022-06-12 17:43:48,835   cls_loss = 0.07073409274686522
2022-06-12 17:43:48,835   eval_loss = 0.35485402920416426
2022-06-12 17:43:48,835   global_step = 9599
2022-06-12 17:43:48,835   loss = 0.07073409274686522
2022-06-12 17:44:13,095 ***** Running evaluation *****
2022-06-12 17:44:13,096   Epoch = 4 iter 9699 step
2022-06-12 17:44:13,096   Num examples = 872
2022-06-12 17:44:13,096   Batch size = 32
2022-06-12 17:44:13,849 ***** Eval results *****
2022-06-12 17:44:13,849   acc = 0.8887614678899083
2022-06-12 17:44:13,849   cls_loss = 0.07083072337385089
2022-06-12 17:44:13,849   eval_loss = 0.3353655529873712
2022-06-12 17:44:13,850   global_step = 9699
2022-06-12 17:44:13,850   loss = 0.07083072337385089
2022-06-12 17:44:38,139 ***** Running evaluation *****
2022-06-12 17:44:38,139   Epoch = 4 iter 9799 step
2022-06-12 17:44:38,140   Num examples = 872
2022-06-12 17:44:38,140   Batch size = 32
2022-06-12 17:44:38,889 ***** Eval results *****
2022-06-12 17:44:38,889   acc = 0.8990825688073395
2022-06-12 17:44:38,889   cls_loss = 0.07079317983689726
2022-06-12 17:44:38,890   eval_loss = 0.31502568029931616
2022-06-12 17:44:38,890   global_step = 9799
2022-06-12 17:44:38,890   loss = 0.07079317983689726
2022-06-12 17:44:38,890 ***** Save model *****
2022-06-12 17:45:03,644 ***** Running evaluation *****
2022-06-12 17:45:03,644   Epoch = 4 iter 9899 step
2022-06-12 17:45:03,644   Num examples = 872
2022-06-12 17:45:03,644   Batch size = 32
2022-06-12 17:45:04,395 ***** Eval results *****
2022-06-12 17:45:04,395   acc = 0.8853211009174312
2022-06-12 17:45:04,395   cls_loss = 0.07066902657688187
2022-06-12 17:45:04,395   eval_loss = 0.37658628076314926
2022-06-12 17:45:04,395   global_step = 9899
2022-06-12 17:45:04,395   loss = 0.07066902657688187
2022-06-12 17:45:28,744 ***** Running evaluation *****
2022-06-12 17:45:28,744   Epoch = 4 iter 9999 step
2022-06-12 17:45:28,744   Num examples = 872
2022-06-12 17:45:28,744   Batch size = 32
2022-06-12 17:45:29,494 ***** Eval results *****
2022-06-12 17:45:29,494   acc = 0.8841743119266054
2022-06-12 17:45:29,494   cls_loss = 0.07063169079636819
2022-06-12 17:45:29,495   eval_loss = 0.3436680313731943
2022-06-12 17:45:29,495   global_step = 9999
2022-06-12 17:45:29,495   loss = 0.07063169079636819
2022-06-12 17:45:53,784 ***** Running evaluation *****
2022-06-12 17:45:53,784   Epoch = 4 iter 10099 step
2022-06-12 17:45:53,784   Num examples = 872
2022-06-12 17:45:53,784   Batch size = 32
2022-06-12 17:45:54,536 ***** Eval results *****
2022-06-12 17:45:54,536   acc = 0.8944954128440367
2022-06-12 17:45:54,536   cls_loss = 0.07062944842682874
2022-06-12 17:45:54,536   eval_loss = 0.3361909106107695
2022-06-12 17:45:54,536   global_step = 10099
2022-06-12 17:45:54,536   loss = 0.07062944842682874
2022-06-12 17:46:18,892 ***** Running evaluation *****
2022-06-12 17:46:18,892   Epoch = 4 iter 10199 step
2022-06-12 17:46:18,892   Num examples = 872
2022-06-12 17:46:18,892   Batch size = 32
2022-06-12 17:46:19,642 ***** Eval results *****
2022-06-12 17:46:19,642   acc = 0.8910550458715596
2022-06-12 17:46:19,642   cls_loss = 0.07052476968213005
2022-06-12 17:46:19,642   eval_loss = 0.34298716352454256
2022-06-12 17:46:19,642   global_step = 10199
2022-06-12 17:46:19,642   loss = 0.07052476968213005
2022-06-12 17:46:43,939 ***** Running evaluation *****
2022-06-12 17:46:43,939   Epoch = 4 iter 10299 step
2022-06-12 17:46:43,939   Num examples = 872
2022-06-12 17:46:43,939   Batch size = 32
2022-06-12 17:46:44,690 ***** Eval results *****
2022-06-12 17:46:44,690   acc = 0.8910550458715596
2022-06-12 17:46:44,690   cls_loss = 0.07041107593336703
2022-06-12 17:46:44,690   eval_loss = 0.34571662412158083
2022-06-12 17:46:44,690   global_step = 10299
2022-06-12 17:46:44,691   loss = 0.07041107593336703
2022-06-12 17:47:08,974 ***** Running evaluation *****
2022-06-12 17:47:08,974   Epoch = 4 iter 10399 step
2022-06-12 17:47:08,974   Num examples = 872
2022-06-12 17:47:08,974   Batch size = 32
2022-06-12 17:47:09,726 ***** Eval results *****
2022-06-12 17:47:09,726   acc = 0.8887614678899083
2022-06-12 17:47:09,726   cls_loss = 0.07029844252406739
2022-06-12 17:47:09,726   eval_loss = 0.3550968846040113
2022-06-12 17:47:09,726   global_step = 10399
2022-06-12 17:47:09,726   loss = 0.07029844252406739
2022-06-12 17:47:34,039 ***** Running evaluation *****
2022-06-12 17:47:34,040   Epoch = 4 iter 10499 step
2022-06-12 17:47:34,040   Num examples = 872
2022-06-12 17:47:34,040   Batch size = 32
2022-06-12 17:47:34,789 ***** Eval results *****
2022-06-12 17:47:34,789   acc = 0.8795871559633027
2022-06-12 17:47:34,790   cls_loss = 0.07032403850456699
2022-06-12 17:47:34,790   eval_loss = 0.3854642397324954
2022-06-12 17:47:34,790   global_step = 10499
2022-06-12 17:47:34,790   loss = 0.07032403850456699
2022-06-12 17:47:59,090 ***** Running evaluation *****
2022-06-12 17:47:59,091   Epoch = 5 iter 10599 step
2022-06-12 17:47:59,091   Num examples = 872
2022-06-12 17:47:59,091   Batch size = 32
2022-06-12 17:47:59,842 ***** Eval results *****
2022-06-12 17:47:59,842   acc = 0.8841743119266054
2022-06-12 17:47:59,842   cls_loss = 0.07235633954405785
2022-06-12 17:47:59,842   eval_loss = 0.34218457261366503
2022-06-12 17:47:59,842   global_step = 10599
2022-06-12 17:47:59,842   loss = 0.07235633954405785
2022-06-12 17:48:24,170 ***** Running evaluation *****
2022-06-12 17:48:24,170   Epoch = 5 iter 10699 step
2022-06-12 17:48:24,171   Num examples = 872
2022-06-12 17:48:24,171   Batch size = 32
2022-06-12 17:48:24,921 ***** Eval results *****
2022-06-12 17:48:24,921   acc = 0.8899082568807339
2022-06-12 17:48:24,921   cls_loss = 0.07083610655458945
2022-06-12 17:48:24,922   eval_loss = 0.35255965324384825
2022-06-12 17:48:24,922   global_step = 10699
2022-06-12 17:48:24,922   loss = 0.07083610655458945
2022-06-12 17:48:49,248 ***** Running evaluation *****
2022-06-12 17:48:49,249   Epoch = 5 iter 10799 step
2022-06-12 17:48:49,249   Num examples = 872
2022-06-12 17:48:49,249   Batch size = 32
2022-06-12 17:48:50,000 ***** Eval results *****
2022-06-12 17:48:50,000   acc = 0.893348623853211
2022-06-12 17:48:50,000   cls_loss = 0.07050267749843205
2022-06-12 17:48:50,001   eval_loss = 0.3525773901492357
2022-06-12 17:48:50,001   global_step = 10799
2022-06-12 17:48:50,001   loss = 0.07050267749843205
2022-06-12 17:49:14,435 ***** Running evaluation *****
2022-06-12 17:49:14,436   Epoch = 5 iter 10899 step
2022-06-12 17:49:14,436   Num examples = 872
2022-06-12 17:49:14,436   Batch size = 32
2022-06-12 17:49:15,187 ***** Eval results *****
2022-06-12 17:49:15,187   acc = 0.8841743119266054
2022-06-12 17:49:15,187   cls_loss = 0.07017665391782972
2022-06-12 17:49:15,187   eval_loss = 0.37973125864352497
2022-06-12 17:49:15,187   global_step = 10899
2022-06-12 17:49:15,187   loss = 0.07017665391782972
2022-06-12 17:49:39,531 ***** Running evaluation *****
2022-06-12 17:49:39,531   Epoch = 5 iter 10999 step
2022-06-12 17:49:39,531   Num examples = 872
2022-06-12 17:49:39,531   Batch size = 32
2022-06-12 17:49:40,281 ***** Eval results *****
2022-06-12 17:49:40,281   acc = 0.8853211009174312
2022-06-12 17:49:40,281   cls_loss = 0.0700494184047917
2022-06-12 17:49:40,281   eval_loss = 0.34541780980569975
2022-06-12 17:49:40,282   global_step = 10999
2022-06-12 17:49:40,282   loss = 0.0700494184047917
2022-06-12 17:50:04,586 ***** Running evaluation *****
2022-06-12 17:50:04,586   Epoch = 5 iter 11099 step
2022-06-12 17:50:04,586   Num examples = 872
2022-06-12 17:50:04,586   Batch size = 32
2022-06-12 17:50:05,338 ***** Eval results *****
2022-06-12 17:50:05,339   acc = 0.8956422018348624
2022-06-12 17:50:05,339   cls_loss = 0.07012753817856003
2022-06-12 17:50:05,339   eval_loss = 0.35186161899140905
2022-06-12 17:50:05,339   global_step = 11099
2022-06-12 17:50:05,339   loss = 0.07012753817856003
2022-06-12 17:50:29,760 ***** Running evaluation *****
2022-06-12 17:50:29,760   Epoch = 5 iter 11199 step
2022-06-12 17:50:29,760   Num examples = 872
2022-06-12 17:50:29,760   Batch size = 32
2022-06-12 17:50:30,512 ***** Eval results *****
2022-06-12 17:50:30,512   acc = 0.8876146788990825
2022-06-12 17:50:30,512   cls_loss = 0.07019933091479302
2022-06-12 17:50:30,512   eval_loss = 0.32610790990293026
2022-06-12 17:50:30,512   global_step = 11199
2022-06-12 17:50:30,512   loss = 0.07019933091479302
2022-06-12 17:50:54,891 ***** Running evaluation *****
2022-06-12 17:50:54,892   Epoch = 5 iter 11299 step
2022-06-12 17:50:54,892   Num examples = 872
2022-06-12 17:50:54,892   Batch size = 32
2022-06-12 17:50:55,651 ***** Eval results *****
2022-06-12 17:50:55,651   acc = 0.8910550458715596
2022-06-12 17:50:55,652   cls_loss = 0.07033379910100103
2022-06-12 17:50:55,652   eval_loss = 0.3510656463248389
2022-06-12 17:50:55,652   global_step = 11299
2022-06-12 17:50:55,652   loss = 0.07033379910100103
2022-06-12 17:51:20,040 ***** Running evaluation *****
2022-06-12 17:51:20,041   Epoch = 5 iter 11399 step
2022-06-12 17:51:20,041   Num examples = 872
2022-06-12 17:51:20,041   Batch size = 32
2022-06-12 17:51:20,791 ***** Eval results *****
2022-06-12 17:51:20,791   acc = 0.893348623853211
2022-06-12 17:51:20,792   cls_loss = 0.0702352833269806
2022-06-12 17:51:20,792   eval_loss = 0.35344546288251877
2022-06-12 17:51:20,792   global_step = 11399
2022-06-12 17:51:20,792   loss = 0.0702352833269806
2022-06-12 17:51:45,170 ***** Running evaluation *****
2022-06-12 17:51:45,170   Epoch = 5 iter 11499 step
2022-06-12 17:51:45,170   Num examples = 872
2022-06-12 17:51:45,171   Batch size = 32
2022-06-12 17:51:45,921 ***** Eval results *****
2022-06-12 17:51:45,922   acc = 0.893348623853211
2022-06-12 17:51:45,922   cls_loss = 0.07035309290161415
2022-06-12 17:51:45,922   eval_loss = 0.35666879798684803
2022-06-12 17:51:45,922   global_step = 11499
2022-06-12 17:51:45,922   loss = 0.07035309290161415
2022-06-12 17:52:10,189 ***** Running evaluation *****
2022-06-12 17:52:10,190   Epoch = 5 iter 11599 step
2022-06-12 17:52:10,190   Num examples = 872
2022-06-12 17:52:10,190   Batch size = 32
2022-06-12 17:52:10,939 ***** Eval results *****
2022-06-12 17:52:10,940   acc = 0.8899082568807339
2022-06-12 17:52:10,940   cls_loss = 0.07029586850878704
2022-06-12 17:52:10,940   eval_loss = 0.3509587539093835
2022-06-12 17:52:10,940   global_step = 11599
2022-06-12 17:52:10,940   loss = 0.07029586850878704
2022-06-12 17:52:35,266 ***** Running evaluation *****
2022-06-12 17:52:35,267   Epoch = 5 iter 11699 step
2022-06-12 17:52:35,267   Num examples = 872
2022-06-12 17:52:35,267   Batch size = 32
2022-06-12 17:52:36,016 ***** Eval results *****
2022-06-12 17:52:36,016   acc = 0.8899082568807339
2022-06-12 17:52:36,016   cls_loss = 0.07018306602402397
2022-06-12 17:52:36,016   eval_loss = 0.35081950734768597
2022-06-12 17:52:36,016   global_step = 11699
2022-06-12 17:52:36,017   loss = 0.07018306602402397
2022-06-12 17:53:00,361 ***** Running evaluation *****
2022-06-12 17:53:00,362   Epoch = 5 iter 11799 step
2022-06-12 17:53:00,362   Num examples = 872
2022-06-12 17:53:00,362   Batch size = 32
2022-06-12 17:53:01,115 ***** Eval results *****
2022-06-12 17:53:01,115   acc = 0.8853211009174312
2022-06-12 17:53:01,115   cls_loss = 0.07009578115533253
2022-06-12 17:53:01,115   eval_loss = 0.3688596801034042
2022-06-12 17:53:01,116   global_step = 11799
2022-06-12 17:53:01,116   loss = 0.07009578115533253
2022-06-12 17:53:25,428 ***** Running evaluation *****
2022-06-12 17:53:25,428   Epoch = 5 iter 11899 step
2022-06-12 17:53:25,428   Num examples = 872
2022-06-12 17:53:25,428   Batch size = 32
2022-06-12 17:53:26,178 ***** Eval results *****
2022-06-12 17:53:26,178   acc = 0.8876146788990825
2022-06-12 17:53:26,178   cls_loss = 0.06999521012309239
2022-06-12 17:53:26,178   eval_loss = 0.3403246322912829
2022-06-12 17:53:26,178   global_step = 11899
2022-06-12 17:53:26,178   loss = 0.06999521012309239
2022-06-12 17:53:50,525 ***** Running evaluation *****
2022-06-12 17:53:50,525   Epoch = 5 iter 11999 step
2022-06-12 17:53:50,526   Num examples = 872
2022-06-12 17:53:50,526   Batch size = 32
2022-06-12 17:53:51,276 ***** Eval results *****
2022-06-12 17:53:51,277   acc = 0.8864678899082569
2022-06-12 17:53:51,277   cls_loss = 0.06994795680116608
2022-06-12 17:53:51,277   eval_loss = 0.3509011074368443
2022-06-12 17:53:51,278   global_step = 11999
2022-06-12 17:53:51,278   loss = 0.06994795680116608
2022-06-12 17:54:15,550 ***** Running evaluation *****
2022-06-12 17:54:15,550   Epoch = 5 iter 12099 step
2022-06-12 17:54:15,550   Num examples = 872
2022-06-12 17:54:15,550   Batch size = 32
2022-06-12 17:54:16,300 ***** Eval results *****
2022-06-12 17:54:16,300   acc = 0.8922018348623854
2022-06-12 17:54:16,301   cls_loss = 0.07002279086674294
2022-06-12 17:54:16,301   eval_loss = 0.33415451605937313
2022-06-12 17:54:16,301   global_step = 12099
2022-06-12 17:54:16,301   loss = 0.07002279086674294
2022-06-12 17:54:40,574 ***** Running evaluation *****
2022-06-12 17:54:40,574   Epoch = 5 iter 12199 step
2022-06-12 17:54:40,574   Num examples = 872
2022-06-12 17:54:40,574   Batch size = 32
2022-06-12 17:54:41,324 ***** Eval results *****
2022-06-12 17:54:41,324   acc = 0.8899082568807339
2022-06-12 17:54:41,324   cls_loss = 0.06988095947635038
2022-06-12 17:54:41,325   eval_loss = 0.3319643336747374
2022-06-12 17:54:41,325   global_step = 12199
2022-06-12 17:54:41,325   loss = 0.06988095947635038
2022-06-12 17:55:05,622 ***** Running evaluation *****
2022-06-12 17:55:05,623   Epoch = 5 iter 12299 step
2022-06-12 17:55:05,623   Num examples = 872
2022-06-12 17:55:05,623   Batch size = 32
2022-06-12 17:55:06,375 ***** Eval results *****
2022-06-12 17:55:06,375   acc = 0.8876146788990825
2022-06-12 17:55:06,375   cls_loss = 0.06997524695434283
2022-06-12 17:55:06,375   eval_loss = 0.34087866118976046
2022-06-12 17:55:06,375   global_step = 12299
2022-06-12 17:55:06,375   loss = 0.06997524695434283
2022-06-12 17:55:30,605 ***** Running evaluation *****
2022-06-12 17:55:30,605   Epoch = 5 iter 12399 step
2022-06-12 17:55:30,605   Num examples = 872
2022-06-12 17:55:30,605   Batch size = 32
2022-06-12 17:55:31,354 ***** Eval results *****
2022-06-12 17:55:31,354   acc = 0.8899082568807339
2022-06-12 17:55:31,354   cls_loss = 0.07007293787969751
2022-06-12 17:55:31,354   eval_loss = 0.3702927625605038
2022-06-12 17:55:31,354   global_step = 12399
2022-06-12 17:55:31,354   loss = 0.07007293787969751
2022-06-12 17:55:55,619 ***** Running evaluation *****
2022-06-12 17:55:55,619   Epoch = 5 iter 12499 step
2022-06-12 17:55:55,619   Num examples = 872
2022-06-12 17:55:55,619   Batch size = 32
2022-06-12 17:55:56,372 ***** Eval results *****
2022-06-12 17:55:56,372   acc = 0.8830275229357798
2022-06-12 17:55:56,372   cls_loss = 0.07002034530367376
2022-06-12 17:55:56,372   eval_loss = 0.37394035660794805
2022-06-12 17:55:56,372   global_step = 12499
2022-06-12 17:55:56,372   loss = 0.07002034530367376
2022-06-12 17:56:20,639 ***** Running evaluation *****
2022-06-12 17:56:20,640   Epoch = 5 iter 12599 step
2022-06-12 17:56:20,640   Num examples = 872
2022-06-12 17:56:20,640   Batch size = 32
2022-06-12 17:56:21,390 ***** Eval results *****
2022-06-12 17:56:21,390   acc = 0.8818807339449541
2022-06-12 17:56:21,390   cls_loss = 0.07000577639198866
2022-06-12 17:56:21,390   eval_loss = 0.36830428069723503
2022-06-12 17:56:21,390   global_step = 12599
2022-06-12 17:56:21,390   loss = 0.07000577639198866
2022-06-12 17:56:45,721 ***** Running evaluation *****
2022-06-12 17:56:45,721   Epoch = 6 iter 12699 step
2022-06-12 17:56:45,721   Num examples = 872
2022-06-12 17:56:45,721   Batch size = 32
2022-06-12 17:56:46,471 ***** Eval results *****
2022-06-12 17:56:46,471   acc = 0.8876146788990825
2022-06-12 17:56:46,471   cls_loss = 0.06827953259150187
2022-06-12 17:56:46,471   eval_loss = 0.35495536854224546
2022-06-12 17:56:46,471   global_step = 12699
2022-06-12 17:56:46,471   loss = 0.06827953259150187
2022-06-12 17:57:10,818 ***** Running evaluation *****
2022-06-12 17:57:10,818   Epoch = 6 iter 12799 step
2022-06-12 17:57:10,818   Num examples = 872
2022-06-12 17:57:10,818   Batch size = 32
2022-06-12 17:57:11,568 ***** Eval results *****
2022-06-12 17:57:11,568   acc = 0.8887614678899083
2022-06-12 17:57:11,568   cls_loss = 0.06864867465836662
2022-06-12 17:57:11,568   eval_loss = 0.34471734559961725
2022-06-12 17:57:11,568   global_step = 12799
2022-06-12 17:57:11,568   loss = 0.06864867465836662
2022-06-12 17:57:35,898 ***** Running evaluation *****
2022-06-12 17:57:35,898   Epoch = 6 iter 12899 step
2022-06-12 17:57:35,898   Num examples = 872
2022-06-12 17:57:35,898   Batch size = 32
2022-06-12 17:57:36,647 ***** Eval results *****
2022-06-12 17:57:36,647   acc = 0.8864678899082569
2022-06-12 17:57:36,647   cls_loss = 0.0688353988934647
2022-06-12 17:57:36,647   eval_loss = 0.35544935427606106
2022-06-12 17:57:36,647   global_step = 12899
2022-06-12 17:57:36,647   loss = 0.0688353988934647
2022-06-12 17:58:00,946 ***** Running evaluation *****
2022-06-12 17:58:00,946   Epoch = 6 iter 12999 step
2022-06-12 17:58:00,946   Num examples = 872
2022-06-12 17:58:00,946   Batch size = 32
2022-06-12 17:58:01,699 ***** Eval results *****
2022-06-12 17:58:01,699   acc = 0.8853211009174312
2022-06-12 17:58:01,699   cls_loss = 0.06902613300085068
2022-06-12 17:58:01,699   eval_loss = 0.3369780566011156
2022-06-12 17:58:01,699   global_step = 12999
2022-06-12 17:58:01,699   loss = 0.06902613300085068
2022-06-12 17:58:25,998 ***** Running evaluation *****
2022-06-12 17:58:25,999   Epoch = 6 iter 13099 step
2022-06-12 17:58:25,999   Num examples = 872
2022-06-12 17:58:25,999   Batch size = 32
2022-06-12 17:58:26,748 ***** Eval results *****
2022-06-12 17:58:26,749   acc = 0.8922018348623854
2022-06-12 17:58:26,749   cls_loss = 0.06914764123527627
2022-06-12 17:58:26,749   eval_loss = 0.35472210629710127
2022-06-12 17:58:26,749   global_step = 13099
2022-06-12 17:58:26,749   loss = 0.06914764123527627
2022-06-12 17:58:51,070 ***** Running evaluation *****
2022-06-12 17:58:51,070   Epoch = 6 iter 13199 step
2022-06-12 17:58:51,070   Num examples = 872
2022-06-12 17:58:51,070   Batch size = 32
2022-06-12 17:58:51,823 ***** Eval results *****
2022-06-12 17:58:51,824   acc = 0.8899082568807339
2022-06-12 17:58:51,824   cls_loss = 0.06911517341499743
2022-06-12 17:58:51,824   eval_loss = 0.34499800271753756
2022-06-12 17:58:51,824   global_step = 13199
2022-06-12 17:58:51,824   loss = 0.06911517341499743
2022-06-12 17:59:16,051 ***** Running evaluation *****
2022-06-12 17:59:16,051   Epoch = 6 iter 13299 step
2022-06-12 17:59:16,051   Num examples = 872
2022-06-12 17:59:16,051   Batch size = 32
2022-06-12 17:59:16,800 ***** Eval results *****
2022-06-12 17:59:16,801   acc = 0.8899082568807339
2022-06-12 17:59:16,801   cls_loss = 0.06902199796504445
2022-06-12 17:59:16,801   eval_loss = 0.34147600430463043
2022-06-12 17:59:16,801   global_step = 13299
2022-06-12 17:59:16,801   loss = 0.06902199796504445
2022-06-12 17:59:41,109 ***** Running evaluation *****
2022-06-12 17:59:41,109   Epoch = 6 iter 13399 step
2022-06-12 17:59:41,109   Num examples = 872
2022-06-12 17:59:41,109   Batch size = 32
2022-06-12 17:59:41,859 ***** Eval results *****
2022-06-12 17:59:41,859   acc = 0.8864678899082569
2022-06-12 17:59:41,860   cls_loss = 0.06882530091751006
2022-06-12 17:59:41,860   eval_loss = 0.3594453175153051
2022-06-12 17:59:41,860   global_step = 13399
2022-06-12 17:59:41,860   loss = 0.06882530091751006
2022-06-12 18:00:06,110 ***** Running evaluation *****
2022-06-12 18:00:06,110   Epoch = 6 iter 13499 step
2022-06-12 18:00:06,110   Num examples = 872
2022-06-12 18:00:06,110   Batch size = 32
2022-06-12 18:00:06,860 ***** Eval results *****
2022-06-12 18:00:06,860   acc = 0.8830275229357798
2022-06-12 18:00:06,860   cls_loss = 0.06905950738702502
2022-06-12 18:00:06,860   eval_loss = 0.34458848355071886
2022-06-12 18:00:06,860   global_step = 13499
2022-06-12 18:00:06,860   loss = 0.06905950738702502
2022-06-12 18:00:31,098 ***** Running evaluation *****
2022-06-12 18:00:31,099   Epoch = 6 iter 13599 step
2022-06-12 18:00:31,099   Num examples = 872
2022-06-12 18:00:31,099   Batch size = 32
2022-06-12 18:00:31,850 ***** Eval results *****
2022-06-12 18:00:31,850   acc = 0.8841743119266054
2022-06-12 18:00:31,850   cls_loss = 0.06923348435224631
2022-06-12 18:00:31,850   eval_loss = 0.36483702515917166
2022-06-12 18:00:31,850   global_step = 13599
2022-06-12 18:00:31,850   loss = 0.06923348435224631
2022-06-12 18:00:56,077 ***** Running evaluation *****
2022-06-12 18:00:56,077   Epoch = 6 iter 13699 step
2022-06-12 18:00:56,077   Num examples = 872
2022-06-12 18:00:56,077   Batch size = 32
2022-06-12 18:00:56,828 ***** Eval results *****
2022-06-12 18:00:56,828   acc = 0.8910550458715596
2022-06-12 18:00:56,828   cls_loss = 0.0690607121482838
2022-06-12 18:00:56,829   eval_loss = 0.3506201517635158
2022-06-12 18:00:56,829   global_step = 13699
2022-06-12 18:00:56,829   loss = 0.0690607121482838
2022-06-12 18:01:21,124 ***** Running evaluation *****
2022-06-12 18:01:21,125   Epoch = 6 iter 13799 step
2022-06-12 18:01:21,125   Num examples = 872
2022-06-12 18:01:21,125   Batch size = 32
2022-06-12 18:01:21,877 ***** Eval results *****
2022-06-12 18:01:21,877   acc = 0.8876146788990825
2022-06-12 18:01:21,877   cls_loss = 0.06918426237524825
2022-06-12 18:01:21,877   eval_loss = 0.3542888584945883
2022-06-12 18:01:21,877   global_step = 13799
2022-06-12 18:01:21,877   loss = 0.06918426237524825
2022-06-12 18:01:46,235 ***** Running evaluation *****
2022-06-12 18:01:46,236   Epoch = 6 iter 13899 step
2022-06-12 18:01:46,236   Num examples = 872
2022-06-12 18:01:46,236   Batch size = 32
2022-06-12 18:01:46,987 ***** Eval results *****
2022-06-12 18:01:46,988   acc = 0.8922018348623854
2022-06-12 18:01:46,988   cls_loss = 0.06922132924491284
2022-06-12 18:01:46,988   eval_loss = 0.3614792986107724
2022-06-12 18:01:46,988   global_step = 13899
2022-06-12 18:01:46,988   loss = 0.06922132924491284
2022-06-12 18:02:11,225 ***** Running evaluation *****
2022-06-12 18:02:11,225   Epoch = 6 iter 13999 step
2022-06-12 18:02:11,225   Num examples = 872
2022-06-12 18:02:11,225   Batch size = 32
2022-06-12 18:02:11,976 ***** Eval results *****
2022-06-12 18:02:11,976   acc = 0.8899082568807339
2022-06-12 18:02:11,976   cls_loss = 0.0691869876303456
2022-06-12 18:02:11,976   eval_loss = 0.3512994953032051
2022-06-12 18:02:11,977   global_step = 13999
2022-06-12 18:02:11,977   loss = 0.0691869876303456
2022-06-12 18:02:36,207 ***** Running evaluation *****
2022-06-12 18:02:36,207   Epoch = 6 iter 14099 step
2022-06-12 18:02:36,208   Num examples = 872
2022-06-12 18:02:36,208   Batch size = 32
2022-06-12 18:02:36,959 ***** Eval results *****
2022-06-12 18:02:36,959   acc = 0.893348623853211
2022-06-12 18:02:36,959   cls_loss = 0.06917787486718872
2022-06-12 18:02:36,959   eval_loss = 0.35619610973766874
2022-06-12 18:02:36,959   global_step = 14099
2022-06-12 18:02:36,959   loss = 0.06917787486718872
2022-06-12 18:03:01,275 ***** Running evaluation *****
2022-06-12 18:03:01,276   Epoch = 6 iter 14199 step
2022-06-12 18:03:01,276   Num examples = 872
2022-06-12 18:03:01,276   Batch size = 32
2022-06-12 18:03:02,027 ***** Eval results *****
2022-06-12 18:03:02,027   acc = 0.8922018348623854
2022-06-12 18:03:02,027   cls_loss = 0.06913062259318337
2022-06-12 18:03:02,027   eval_loss = 0.3458199073959674
2022-06-12 18:03:02,027   global_step = 14199
2022-06-12 18:03:02,027   loss = 0.06913062259318337
2022-06-12 18:03:26,503 ***** Running evaluation *****
2022-06-12 18:03:26,503   Epoch = 6 iter 14299 step
2022-06-12 18:03:26,503   Num examples = 872
2022-06-12 18:03:26,503   Batch size = 32
2022-06-12 18:03:27,254 ***** Eval results *****
2022-06-12 18:03:27,254   acc = 0.8841743119266054
2022-06-12 18:03:27,254   cls_loss = 0.06913043266356882
2022-06-12 18:03:27,254   eval_loss = 0.3656947795035584
2022-06-12 18:03:27,254   global_step = 14299
2022-06-12 18:03:27,254   loss = 0.06913043266356882
2022-06-12 18:03:51,577 ***** Running evaluation *****
2022-06-12 18:03:51,578   Epoch = 6 iter 14399 step
2022-06-12 18:03:51,578   Num examples = 872
2022-06-12 18:03:51,578   Batch size = 32
2022-06-12 18:03:52,328 ***** Eval results *****
2022-06-12 18:03:52,328   acc = 0.8876146788990825
2022-06-12 18:03:52,328   cls_loss = 0.06911401195635258
2022-06-12 18:03:52,328   eval_loss = 0.3540388114218201
2022-06-12 18:03:52,329   global_step = 14399
2022-06-12 18:03:52,329   loss = 0.06911401195635258
2022-06-12 18:04:16,595 ***** Running evaluation *****
2022-06-12 18:04:16,595   Epoch = 6 iter 14499 step
2022-06-12 18:04:16,595   Num examples = 872
2022-06-12 18:04:16,595   Batch size = 32
2022-06-12 18:04:17,346 ***** Eval results *****
2022-06-12 18:04:17,346   acc = 0.8887614678899083
2022-06-12 18:04:17,346   cls_loss = 0.06903647983471553
2022-06-12 18:04:17,346   eval_loss = 0.35478807014546226
2022-06-12 18:04:17,346   global_step = 14499
2022-06-12 18:04:17,346   loss = 0.06903647983471553
2022-06-12 18:04:41,717 ***** Running evaluation *****
2022-06-12 18:04:41,717   Epoch = 6 iter 14599 step
2022-06-12 18:04:41,717   Num examples = 872
2022-06-12 18:04:41,717   Batch size = 32
2022-06-12 18:04:42,468 ***** Eval results *****
2022-06-12 18:04:42,468   acc = 0.8807339449541285
2022-06-12 18:04:42,468   cls_loss = 0.06908027133609675
2022-06-12 18:04:42,468   eval_loss = 0.35671569194112507
2022-06-12 18:04:42,468   global_step = 14599
2022-06-12 18:04:42,468   loss = 0.06908027133609675
2022-06-12 18:05:06,827 ***** Running evaluation *****
2022-06-12 18:05:06,827   Epoch = 6 iter 14699 step
2022-06-12 18:05:06,827   Num examples = 872
2022-06-12 18:05:06,827   Batch size = 32
2022-06-12 18:05:07,577 ***** Eval results *****
2022-06-12 18:05:07,577   acc = 0.8887614678899083
2022-06-12 18:05:07,577   cls_loss = 0.06900520350021053
2022-06-12 18:05:07,577   eval_loss = 0.3594330074265599
2022-06-12 18:05:07,577   global_step = 14699
2022-06-12 18:05:07,577   loss = 0.06900520350021053
2022-06-12 18:05:31,921 ***** Running evaluation *****
2022-06-12 18:05:31,921   Epoch = 7 iter 14799 step
2022-06-12 18:05:31,921   Num examples = 872
2022-06-12 18:05:31,921   Batch size = 32
2022-06-12 18:05:32,672 ***** Eval results *****
2022-06-12 18:05:32,672   acc = 0.8853211009174312
2022-06-12 18:05:32,672   cls_loss = 0.06987418213360747
2022-06-12 18:05:32,672   eval_loss = 0.3540566861629486
2022-06-12 18:05:32,672   global_step = 14799
2022-06-12 18:05:32,672   loss = 0.06987418213360747
2022-06-12 18:05:56,951 ***** Running evaluation *****
2022-06-12 18:05:56,951   Epoch = 7 iter 14899 step
2022-06-12 18:05:56,951   Num examples = 872
2022-06-12 18:05:56,951   Batch size = 32
2022-06-12 18:05:57,704 ***** Eval results *****
2022-06-12 18:05:57,704   acc = 0.8899082568807339
2022-06-12 18:05:57,704   cls_loss = 0.06794282661108246
2022-06-12 18:05:57,705   eval_loss = 0.3561356442847422
2022-06-12 18:05:57,705   global_step = 14899
2022-06-12 18:05:57,705   loss = 0.06794282661108246
2022-06-12 18:06:21,983 ***** Running evaluation *****
2022-06-12 18:06:21,984   Epoch = 7 iter 14999 step
2022-06-12 18:06:21,984   Num examples = 872
2022-06-12 18:06:21,984   Batch size = 32
2022-06-12 18:06:22,733 ***** Eval results *****
2022-06-12 18:06:22,733   acc = 0.8841743119266054
2022-06-12 18:06:22,733   cls_loss = 0.06808188444903855
2022-06-12 18:06:22,733   eval_loss = 0.3526483743584582
2022-06-12 18:06:22,733   global_step = 14999
2022-06-12 18:06:22,733   loss = 0.06808188444903855
2022-06-12 18:06:47,194 ***** Running evaluation *****
2022-06-12 18:06:47,194   Epoch = 7 iter 15099 step
2022-06-12 18:06:47,195   Num examples = 872
2022-06-12 18:06:47,195   Batch size = 32
2022-06-12 18:06:47,944 ***** Eval results *****
2022-06-12 18:06:47,944   acc = 0.8864678899082569
2022-06-12 18:06:47,944   cls_loss = 0.0678104600916815
2022-06-12 18:06:47,944   eval_loss = 0.36913604542080847
2022-06-12 18:06:47,944   global_step = 15099
2022-06-12 18:06:47,944   loss = 0.0678104600916815
2022-06-12 18:07:12,337 ***** Running evaluation *****
2022-06-12 18:07:12,337   Epoch = 7 iter 15199 step
2022-06-12 18:07:12,337   Num examples = 872
2022-06-12 18:07:12,337   Batch size = 32
2022-06-12 18:07:13,086 ***** Eval results *****
2022-06-12 18:07:13,087   acc = 0.8864678899082569
2022-06-12 18:07:13,087   cls_loss = 0.06811080583531386
2022-06-12 18:07:13,087   eval_loss = 0.3612608263002975
2022-06-12 18:07:13,087   global_step = 15199
2022-06-12 18:07:13,087   loss = 0.06811080583531386
2022-06-12 18:07:37,433 ***** Running evaluation *****
2022-06-12 18:07:37,433   Epoch = 7 iter 15299 step
2022-06-12 18:07:37,433   Num examples = 872
2022-06-12 18:07:37,433   Batch size = 32
2022-06-12 18:07:38,184 ***** Eval results *****
2022-06-12 18:07:38,184   acc = 0.8876146788990825
2022-06-12 18:07:38,184   cls_loss = 0.06823022228432411
2022-06-12 18:07:38,184   eval_loss = 0.35709016277853933
2022-06-12 18:07:38,184   global_step = 15299
2022-06-12 18:07:38,184   loss = 0.06823022228432411
2022-06-12 18:08:02,468 ***** Running evaluation *****
2022-06-12 18:08:02,468   Epoch = 7 iter 15399 step
2022-06-12 18:08:02,468   Num examples = 872
2022-06-12 18:08:02,468   Batch size = 32
2022-06-12 18:08:03,219 ***** Eval results *****
2022-06-12 18:08:03,219   acc = 0.8922018348623854
2022-06-12 18:08:03,219   cls_loss = 0.06838533764180413
2022-06-12 18:08:03,219   eval_loss = 0.34906465746462345
2022-06-12 18:08:03,219   global_step = 15399
2022-06-12 18:08:03,219   loss = 0.06838533764180413
2022-06-12 18:08:27,655 ***** Running evaluation *****
2022-06-12 18:08:27,655   Epoch = 7 iter 15499 step
2022-06-12 18:08:27,655   Num examples = 872
2022-06-12 18:08:27,655   Batch size = 32
2022-06-12 18:08:28,405 ***** Eval results *****
2022-06-12 18:08:28,405   acc = 0.8887614678899083
2022-06-12 18:08:28,405   cls_loss = 0.06850435889184707
2022-06-12 18:08:28,405   eval_loss = 0.3434193296624081
2022-06-12 18:08:28,405   global_step = 15499
2022-06-12 18:08:28,405   loss = 0.06850435889184707
2022-06-12 18:08:52,699 ***** Running evaluation *****
2022-06-12 18:08:52,700   Epoch = 7 iter 15599 step
2022-06-12 18:08:52,700   Num examples = 872
2022-06-12 18:08:52,700   Batch size = 32
2022-06-12 18:08:53,451 ***** Eval results *****
2022-06-12 18:08:53,451   acc = 0.8853211009174312
2022-06-12 18:08:53,451   cls_loss = 0.0684741912142349
2022-06-12 18:08:53,451   eval_loss = 0.34350600679005894
2022-06-12 18:08:53,451   global_step = 15599
2022-06-12 18:08:53,451   loss = 0.0684741912142349
2022-06-12 18:09:17,771 ***** Running evaluation *****
2022-06-12 18:09:17,772   Epoch = 7 iter 15699 step
2022-06-12 18:09:17,772   Num examples = 872
2022-06-12 18:09:17,772   Batch size = 32
2022-06-12 18:09:18,522 ***** Eval results *****
2022-06-12 18:09:18,522   acc = 0.8864678899082569
2022-06-12 18:09:18,522   cls_loss = 0.0684748645315578
2022-06-12 18:09:18,522   eval_loss = 0.35258970356413294
2022-06-12 18:09:18,522   global_step = 15699
2022-06-12 18:09:18,522   loss = 0.0684748645315578
2022-06-12 18:09:42,881 ***** Running evaluation *****
2022-06-12 18:09:42,882   Epoch = 7 iter 15799 step
2022-06-12 18:09:42,883   Num examples = 872
2022-06-12 18:09:42,883   Batch size = 32
2022-06-12 18:09:43,636 ***** Eval results *****
2022-06-12 18:09:43,637   acc = 0.8830275229357798
2022-06-12 18:09:43,637   cls_loss = 0.06856464633212037
2022-06-12 18:09:43,637   eval_loss = 0.3504797812285168
2022-06-12 18:09:43,637   global_step = 15799
2022-06-12 18:09:43,637   loss = 0.06856464633212037
2022-06-12 18:10:07,926 ***** Running evaluation *****
2022-06-12 18:10:07,926   Epoch = 7 iter 15899 step
2022-06-12 18:10:07,926   Num examples = 872
2022-06-12 18:10:07,926   Batch size = 32
2022-06-12 18:10:08,677 ***** Eval results *****
2022-06-12 18:10:08,677   acc = 0.8830275229357798
2022-06-12 18:10:08,677   cls_loss = 0.06871402769784557
2022-06-12 18:10:08,677   eval_loss = 0.3458945600848113
2022-06-12 18:10:08,677   global_step = 15899
2022-06-12 18:10:08,677   loss = 0.06871402769784557
2022-06-12 18:10:32,922 ***** Running evaluation *****
2022-06-12 18:10:32,923   Epoch = 7 iter 15999 step
2022-06-12 18:10:32,923   Num examples = 872
2022-06-12 18:10:32,923   Batch size = 32
2022-06-12 18:10:33,674 ***** Eval results *****
2022-06-12 18:10:33,674   acc = 0.8841743119266054
2022-06-12 18:10:33,674   cls_loss = 0.06870060308267055
2022-06-12 18:10:33,674   eval_loss = 0.3423737094604543
2022-06-12 18:10:33,674   global_step = 15999
2022-06-12 18:10:33,674   loss = 0.06870060308267055
2022-06-12 18:10:57,921 ***** Running evaluation *****
2022-06-12 18:10:57,922   Epoch = 7 iter 16099 step
2022-06-12 18:10:57,922   Num examples = 872
2022-06-12 18:10:57,922   Batch size = 32
2022-06-12 18:10:58,677 ***** Eval results *****
2022-06-12 18:10:58,677   acc = 0.8853211009174312
2022-06-12 18:10:58,677   cls_loss = 0.06858948105980148
2022-06-12 18:10:58,677   eval_loss = 0.3552931807935238
2022-06-12 18:10:58,678   global_step = 16099
2022-06-12 18:10:58,678   loss = 0.06858948105980148
2022-06-12 18:11:22,938 ***** Running evaluation *****
2022-06-12 18:11:22,939   Epoch = 7 iter 16199 step
2022-06-12 18:11:22,939   Num examples = 872
2022-06-12 18:11:22,939   Batch size = 32
2022-06-12 18:11:23,688 ***** Eval results *****
2022-06-12 18:11:23,689   acc = 0.8864678899082569
2022-06-12 18:11:23,689   cls_loss = 0.06858310842143282
2022-06-12 18:11:23,689   eval_loss = 0.3424127564898559
2022-06-12 18:11:23,689   global_step = 16199
2022-06-12 18:11:23,689   loss = 0.06858310842143282
2022-06-12 18:11:47,961 ***** Running evaluation *****
2022-06-12 18:11:47,961   Epoch = 7 iter 16299 step
2022-06-12 18:11:47,961   Num examples = 872
2022-06-12 18:11:47,961   Batch size = 32
2022-06-12 18:11:48,712 ***** Eval results *****
2022-06-12 18:11:48,712   acc = 0.8864678899082569
2022-06-12 18:11:48,712   cls_loss = 0.06854952036632542
2022-06-12 18:11:48,712   eval_loss = 0.3532449761405587
2022-06-12 18:11:48,712   global_step = 16299
2022-06-12 18:11:48,712   loss = 0.06854952036632542
2022-06-12 18:12:13,010 ***** Running evaluation *****
2022-06-12 18:12:13,011   Epoch = 7 iter 16399 step
2022-06-12 18:12:13,011   Num examples = 872
2022-06-12 18:12:13,011   Batch size = 32
2022-06-12 18:12:13,761 ***** Eval results *****
2022-06-12 18:12:13,761   acc = 0.8830275229357798
2022-06-12 18:12:13,761   cls_loss = 0.06849319145377705
2022-06-12 18:12:13,761   eval_loss = 0.36017972736486364
2022-06-12 18:12:13,762   global_step = 16399
2022-06-12 18:12:13,762   loss = 0.06849319145377705
2022-06-12 18:12:38,106 ***** Running evaluation *****
2022-06-12 18:12:38,107   Epoch = 7 iter 16499 step
2022-06-12 18:12:38,107   Num examples = 872
2022-06-12 18:12:38,107   Batch size = 32
2022-06-12 18:12:38,858 ***** Eval results *****
2022-06-12 18:12:38,858   acc = 0.8807339449541285
2022-06-12 18:12:38,858   cls_loss = 0.06852561346282063
2022-06-12 18:12:38,858   eval_loss = 0.35217445104249884
2022-06-12 18:12:38,858   global_step = 16499
2022-06-12 18:12:38,858   loss = 0.06852561346282063
2022-06-12 18:13:03,204 ***** Running evaluation *****
2022-06-12 18:13:03,205   Epoch = 7 iter 16599 step
2022-06-12 18:13:03,205   Num examples = 872
2022-06-12 18:13:03,205   Batch size = 32
2022-06-12 18:13:03,956 ***** Eval results *****
2022-06-12 18:13:03,956   acc = 0.8818807339449541
2022-06-12 18:13:03,956   cls_loss = 0.06865801599288866
2022-06-12 18:13:03,956   eval_loss = 0.35577355631228
2022-06-12 18:13:03,956   global_step = 16599
2022-06-12 18:13:03,957   loss = 0.06865801599288866
2022-06-12 18:13:28,327 ***** Running evaluation *****
2022-06-12 18:13:28,328   Epoch = 7 iter 16699 step
2022-06-12 18:13:28,328   Num examples = 872
2022-06-12 18:13:28,328   Batch size = 32
2022-06-12 18:13:29,078 ***** Eval results *****
2022-06-12 18:13:29,078   acc = 0.8807339449541285
2022-06-12 18:13:29,078   cls_loss = 0.06859465550304306
2022-06-12 18:13:29,078   eval_loss = 0.35290331340261866
2022-06-12 18:13:29,078   global_step = 16699
2022-06-12 18:13:29,078   loss = 0.06859465550304306
2022-06-12 18:13:53,373 ***** Running evaluation *****
2022-06-12 18:13:53,374   Epoch = 7 iter 16799 step
2022-06-12 18:13:53,374   Num examples = 872
2022-06-12 18:13:53,374   Batch size = 32
2022-06-12 18:13:54,123 ***** Eval results *****
2022-06-12 18:13:54,123   acc = 0.8853211009174312
2022-06-12 18:13:54,123   cls_loss = 0.06862649769094588
2022-06-12 18:13:54,123   eval_loss = 0.33467924248959335
2022-06-12 18:13:54,123   global_step = 16799
2022-06-12 18:13:54,124   loss = 0.06862649769094588
2022-06-12 18:14:18,435 ***** Running evaluation *****
2022-06-12 18:14:18,436   Epoch = 8 iter 16899 step
2022-06-12 18:14:18,436   Num examples = 872
2022-06-12 18:14:18,436   Batch size = 32
2022-06-12 18:14:19,187 ***** Eval results *****
2022-06-12 18:14:19,187   acc = 0.8807339449541285
2022-06-12 18:14:19,187   cls_loss = 0.06933202060745723
2022-06-12 18:14:19,187   eval_loss = 0.34288894744323833
2022-06-12 18:14:19,187   global_step = 16899
2022-06-12 18:14:19,187   loss = 0.06933202060745723
2022-06-12 18:14:43,475 ***** Running evaluation *****
2022-06-12 18:14:43,476   Epoch = 8 iter 16999 step
2022-06-12 18:14:43,476   Num examples = 872
2022-06-12 18:14:43,476   Batch size = 32
2022-06-12 18:14:44,227 ***** Eval results *****
2022-06-12 18:14:44,227   acc = 0.8807339449541285
2022-06-12 18:14:44,227   cls_loss = 0.06860312808327332
2022-06-12 18:14:44,227   eval_loss = 0.3429960190717663
2022-06-12 18:14:44,227   global_step = 16999
2022-06-12 18:14:44,227   loss = 0.06860312808327332
2022-06-12 18:15:08,500 ***** Running evaluation *****
2022-06-12 18:15:08,500   Epoch = 8 iter 17099 step
2022-06-12 18:15:08,500   Num examples = 872
2022-06-12 18:15:08,500   Batch size = 32
2022-06-12 18:15:09,249 ***** Eval results *****
2022-06-12 18:15:09,249   acc = 0.8830275229357798
2022-06-12 18:15:09,249   cls_loss = 0.06800871230801393
2022-06-12 18:15:09,249   eval_loss = 0.35230790903525694
2022-06-12 18:15:09,249   global_step = 17099
2022-06-12 18:15:09,250   loss = 0.06800871230801393
2022-06-12 18:15:33,507 ***** Running evaluation *****
2022-06-12 18:15:33,508   Epoch = 8 iter 17199 step
2022-06-12 18:15:33,508   Num examples = 872
2022-06-12 18:15:33,508   Batch size = 32
2022-06-12 18:15:34,258 ***** Eval results *****
2022-06-12 18:15:34,258   acc = 0.8807339449541285
2022-06-12 18:15:34,258   cls_loss = 0.06815990766038361
2022-06-12 18:15:34,258   eval_loss = 0.35764666912811144
2022-06-12 18:15:34,259   global_step = 17199
2022-06-12 18:15:34,259   loss = 0.06815990766038361
2022-06-12 18:15:58,489 ***** Running evaluation *****
2022-06-12 18:15:58,489   Epoch = 8 iter 17299 step
2022-06-12 18:15:58,489   Num examples = 872
2022-06-12 18:15:58,489   Batch size = 32
2022-06-12 18:15:59,238 ***** Eval results *****
2022-06-12 18:15:59,238   acc = 0.8795871559633027
2022-06-12 18:15:59,238   cls_loss = 0.06807150305657622
2022-06-12 18:15:59,238   eval_loss = 0.35700013847755535
2022-06-12 18:15:59,238   global_step = 17299
2022-06-12 18:15:59,238   loss = 0.06807150305657622
2022-06-12 18:16:23,537 ***** Running evaluation *****
2022-06-12 18:16:23,537   Epoch = 8 iter 17399 step
2022-06-12 18:16:23,538   Num examples = 872
2022-06-12 18:16:23,538   Batch size = 32
2022-06-12 18:16:24,292 ***** Eval results *****
2022-06-12 18:16:24,292   acc = 0.8841743119266054
2022-06-12 18:16:24,292   cls_loss = 0.06815480956613912
2022-06-12 18:16:24,292   eval_loss = 0.3583281881042889
2022-06-12 18:16:24,292   global_step = 17399
2022-06-12 18:16:24,292   loss = 0.06815480956613912
2022-06-12 18:16:48,645 ***** Running evaluation *****
2022-06-12 18:16:48,645   Epoch = 8 iter 17499 step
2022-06-12 18:16:48,645   Num examples = 872
2022-06-12 18:16:48,645   Batch size = 32
2022-06-12 18:16:49,400 ***** Eval results *****
2022-06-12 18:16:49,400   acc = 0.8853211009174312
2022-06-12 18:16:49,400   cls_loss = 0.06833333664502518
2022-06-12 18:16:49,400   eval_loss = 0.3476617844509227
2022-06-12 18:16:49,400   global_step = 17499
2022-06-12 18:16:49,400   loss = 0.06833333664502518
2022-06-12 18:17:13,756 ***** Running evaluation *****
2022-06-12 18:17:13,756   Epoch = 8 iter 17599 step
2022-06-12 18:17:13,756   Num examples = 872
2022-06-12 18:17:13,756   Batch size = 32
2022-06-12 18:17:14,507 ***** Eval results *****
2022-06-12 18:17:14,507   acc = 0.8876146788990825
2022-06-12 18:17:14,507   cls_loss = 0.0683372274400591
2022-06-12 18:17:14,507   eval_loss = 0.3506924259875502
2022-06-12 18:17:14,507   global_step = 17599
2022-06-12 18:17:14,507   loss = 0.0683372274400591
2022-06-12 18:17:38,756 ***** Running evaluation *****
2022-06-12 18:17:38,757   Epoch = 8 iter 17699 step
2022-06-12 18:17:38,757   Num examples = 872
2022-06-12 18:17:38,757   Batch size = 32
2022-06-12 18:17:39,505 ***** Eval results *****
2022-06-12 18:17:39,505   acc = 0.8876146788990825
2022-06-12 18:17:39,505   cls_loss = 0.0685701862819742
2022-06-12 18:17:39,505   eval_loss = 0.3392478552247797
2022-06-12 18:17:39,505   global_step = 17699
2022-06-12 18:17:39,506   loss = 0.0685701862819742
2022-06-12 18:18:03,766 ***** Running evaluation *****
2022-06-12 18:18:03,767   Epoch = 8 iter 17799 step
2022-06-12 18:18:03,767   Num examples = 872
2022-06-12 18:18:03,767   Batch size = 32
2022-06-12 18:18:04,517 ***** Eval results *****
2022-06-12 18:18:04,517   acc = 0.8876146788990825
2022-06-12 18:18:04,518   cls_loss = 0.06863596932933644
2022-06-12 18:18:04,518   eval_loss = 0.3498398746762957
2022-06-12 18:18:04,518   global_step = 17799
2022-06-12 18:18:04,518   loss = 0.06863596932933644
2022-06-12 18:18:28,845 ***** Running evaluation *****
2022-06-12 18:18:28,845   Epoch = 8 iter 17899 step
2022-06-12 18:18:28,845   Num examples = 872
2022-06-12 18:18:28,845   Batch size = 32
2022-06-12 18:18:29,597 ***** Eval results *****
2022-06-12 18:18:29,597   acc = 0.8899082568807339
2022-06-12 18:18:29,597   cls_loss = 0.06859201876814944
2022-06-12 18:18:29,597   eval_loss = 0.35625596397689413
2022-06-12 18:18:29,598   global_step = 17899
2022-06-12 18:18:29,598   loss = 0.06859201876814944
2022-06-12 18:18:53,890 ***** Running evaluation *****
2022-06-12 18:18:53,890   Epoch = 8 iter 17999 step
2022-06-12 18:18:53,890   Num examples = 872
2022-06-12 18:18:53,890   Batch size = 32
2022-06-12 18:18:54,642 ***** Eval results *****
2022-06-12 18:18:54,643   acc = 0.8887614678899083
2022-06-12 18:18:54,643   cls_loss = 0.06856800578758057
2022-06-12 18:18:54,643   eval_loss = 0.3389723210462502
2022-06-12 18:18:54,643   global_step = 17999
2022-06-12 18:18:54,643   loss = 0.06856800578758057
2022-06-12 18:19:18,953 ***** Running evaluation *****
2022-06-12 18:19:18,953   Epoch = 8 iter 18099 step
2022-06-12 18:19:18,953   Num examples = 872
2022-06-12 18:19:18,953   Batch size = 32
2022-06-12 18:19:19,704 ***** Eval results *****
2022-06-12 18:19:19,704   acc = 0.8887614678899083
2022-06-12 18:19:19,704   cls_loss = 0.06855717965570038
2022-06-12 18:19:19,704   eval_loss = 0.35294633944119724
2022-06-12 18:19:19,704   global_step = 18099
2022-06-12 18:19:19,704   loss = 0.06855717965570038
2022-06-12 18:19:44,011 ***** Running evaluation *****
2022-06-12 18:19:44,011   Epoch = 8 iter 18199 step
2022-06-12 18:19:44,011   Num examples = 872
2022-06-12 18:19:44,011   Batch size = 32
2022-06-12 18:19:44,761 ***** Eval results *****
2022-06-12 18:19:44,761   acc = 0.8876146788990825
2022-06-12 18:19:44,761   cls_loss = 0.06864961044319197
2022-06-12 18:19:44,762   eval_loss = 0.3517233290310417
2022-06-12 18:19:44,762   global_step = 18199
2022-06-12 18:19:44,762   loss = 0.06864961044319197
2022-06-12 18:20:09,113 ***** Running evaluation *****
2022-06-12 18:20:09,113   Epoch = 8 iter 18299 step
2022-06-12 18:20:09,113   Num examples = 872
2022-06-12 18:20:09,113   Batch size = 32
2022-06-12 18:20:09,864 ***** Eval results *****
2022-06-12 18:20:09,864   acc = 0.8887614678899083
2022-06-12 18:20:09,864   cls_loss = 0.0685411855685077
2022-06-12 18:20:09,864   eval_loss = 0.356745374788131
2022-06-12 18:20:09,864   global_step = 18299
2022-06-12 18:20:09,864   loss = 0.0685411855685077
2022-06-12 18:20:34,225 ***** Running evaluation *****
2022-06-12 18:20:34,226   Epoch = 8 iter 18399 step
2022-06-12 18:20:34,226   Num examples = 872
2022-06-12 18:20:34,226   Batch size = 32
2022-06-12 18:20:34,976 ***** Eval results *****
2022-06-12 18:20:34,976   acc = 0.8830275229357798
2022-06-12 18:20:34,976   cls_loss = 0.06855843761780114
2022-06-12 18:20:34,977   eval_loss = 0.3563445947532143
2022-06-12 18:20:34,977   global_step = 18399
2022-06-12 18:20:34,977   loss = 0.06855843761780114
2022-06-12 18:20:59,318 ***** Running evaluation *****
2022-06-12 18:20:59,319   Epoch = 8 iter 18499 step
2022-06-12 18:20:59,319   Num examples = 872
2022-06-12 18:20:59,319   Batch size = 32
2022-06-12 18:21:00,070 ***** Eval results *****
2022-06-12 18:21:00,070   acc = 0.8876146788990825
2022-06-12 18:21:00,070   cls_loss = 0.06857970315598769
2022-06-12 18:21:00,070   eval_loss = 0.3451199696532318
2022-06-12 18:21:00,070   global_step = 18499
2022-06-12 18:21:00,070   loss = 0.06857970315598769
2022-06-12 18:21:24,408 ***** Running evaluation *****
2022-06-12 18:21:24,408   Epoch = 8 iter 18599 step
2022-06-12 18:21:24,408   Num examples = 872
2022-06-12 18:21:24,408   Batch size = 32
2022-06-12 18:21:25,158 ***** Eval results *****
2022-06-12 18:21:25,158   acc = 0.8864678899082569
2022-06-12 18:21:25,158   cls_loss = 0.06852759412438707
2022-06-12 18:21:25,159   eval_loss = 0.356405895203352
2022-06-12 18:21:25,159   global_step = 18599
2022-06-12 18:21:25,159   loss = 0.06852759412438707
2022-06-12 18:21:49,416 ***** Running evaluation *****
2022-06-12 18:21:49,416   Epoch = 8 iter 18699 step
2022-06-12 18:21:49,416   Num examples = 872
2022-06-12 18:21:49,416   Batch size = 32
2022-06-12 18:21:50,167 ***** Eval results *****
2022-06-12 18:21:50,167   acc = 0.8864678899082569
2022-06-12 18:21:50,167   cls_loss = 0.06850526456127017
2022-06-12 18:21:50,167   eval_loss = 0.34948146236794336
2022-06-12 18:21:50,167   global_step = 18699
2022-06-12 18:21:50,167   loss = 0.06850526456127017
2022-06-12 18:22:14,461 ***** Running evaluation *****
2022-06-12 18:22:14,461   Epoch = 8 iter 18799 step
2022-06-12 18:22:14,461   Num examples = 872
2022-06-12 18:22:14,461   Batch size = 32
2022-06-12 18:22:15,211 ***** Eval results *****
2022-06-12 18:22:15,212   acc = 0.8853211009174312
2022-06-12 18:22:15,212   cls_loss = 0.06847424148551147
2022-06-12 18:22:15,212   eval_loss = 0.34427989593573977
2022-06-12 18:22:15,212   global_step = 18799
2022-06-12 18:22:15,212   loss = 0.06847424148551147
2022-06-12 18:22:39,463 ***** Running evaluation *****
2022-06-12 18:22:39,463   Epoch = 8 iter 18899 step
2022-06-12 18:22:39,463   Num examples = 872
2022-06-12 18:22:39,463   Batch size = 32
2022-06-12 18:22:40,213 ***** Eval results *****
2022-06-12 18:22:40,213   acc = 0.8864678899082569
2022-06-12 18:22:40,213   cls_loss = 0.06846307242225205
2022-06-12 18:22:40,213   eval_loss = 0.34524258731731344
2022-06-12 18:22:40,213   global_step = 18899
2022-06-12 18:22:40,213   loss = 0.06846307242225205
2022-06-12 18:23:04,578 ***** Running evaluation *****
2022-06-12 18:23:04,578   Epoch = 9 iter 18999 step
2022-06-12 18:23:04,578   Num examples = 872
2022-06-12 18:23:04,578   Batch size = 32
2022-06-12 18:23:05,329 ***** Eval results *****
2022-06-12 18:23:05,329   acc = 0.8887614678899083
2022-06-12 18:23:05,329   cls_loss = 0.06894669309258461
2022-06-12 18:23:05,329   eval_loss = 0.3595464735158852
2022-06-12 18:23:05,329   global_step = 18999
2022-06-12 18:23:05,329   loss = 0.06894669309258461
2022-06-12 18:23:29,576 ***** Running evaluation *****
2022-06-12 18:23:29,577   Epoch = 9 iter 19099 step
2022-06-12 18:23:29,577   Num examples = 872
2022-06-12 18:23:29,577   Batch size = 32
2022-06-12 18:23:30,327 ***** Eval results *****
2022-06-12 18:23:30,327   acc = 0.8876146788990825
2022-06-12 18:23:30,327   cls_loss = 0.06872317119518671
2022-06-12 18:23:30,327   eval_loss = 0.3555748002337558
2022-06-12 18:23:30,328   global_step = 19099
2022-06-12 18:23:30,328   loss = 0.06872317119518671
2022-06-12 18:23:54,668 ***** Running evaluation *****
2022-06-12 18:23:54,668   Epoch = 9 iter 19199 step
2022-06-12 18:23:54,668   Num examples = 872
2022-06-12 18:23:54,668   Batch size = 32
2022-06-12 18:23:55,418 ***** Eval results *****
2022-06-12 18:23:55,419   acc = 0.8876146788990825
2022-06-12 18:23:55,419   cls_loss = 0.06862444825782069
2022-06-12 18:23:55,419   eval_loss = 0.35249101503619124
2022-06-12 18:23:55,419   global_step = 19199
2022-06-12 18:23:55,419   loss = 0.06862444825782069
2022-06-12 18:24:19,651 ***** Running evaluation *****
2022-06-12 18:24:19,652   Epoch = 9 iter 19299 step
2022-06-12 18:24:19,652   Num examples = 872
2022-06-12 18:24:19,652   Batch size = 32
2022-06-12 18:24:20,400 ***** Eval results *****
2022-06-12 18:24:20,401   acc = 0.8853211009174312
2022-06-12 18:24:20,401   cls_loss = 0.06876054731487571
2022-06-12 18:24:20,401   eval_loss = 0.34294265348996433
2022-06-12 18:24:20,401   global_step = 19299
2022-06-12 18:24:20,401   loss = 0.06876054731487571
2022-06-12 18:24:44,646 ***** Running evaluation *****
2022-06-12 18:24:44,646   Epoch = 9 iter 19399 step
2022-06-12 18:24:44,646   Num examples = 872
2022-06-12 18:24:44,646   Batch size = 32
2022-06-12 18:24:45,397 ***** Eval results *****
2022-06-12 18:24:45,397   acc = 0.8887614678899083
2022-06-12 18:24:45,397   cls_loss = 0.06906924564109765
2022-06-12 18:24:45,398   eval_loss = 0.35136791824230124
2022-06-12 18:24:45,398   global_step = 19399
2022-06-12 18:24:45,398   loss = 0.06906924564109765
2022-06-12 18:25:09,622 ***** Running evaluation *****
2022-06-12 18:25:09,622   Epoch = 9 iter 19499 step
2022-06-12 18:25:09,622   Num examples = 872
2022-06-12 18:25:09,622   Batch size = 32
2022-06-12 18:25:10,372 ***** Eval results *****
2022-06-12 18:25:10,373   acc = 0.8887614678899083
2022-06-12 18:25:10,373   cls_loss = 0.06879584673878143
2022-06-12 18:25:10,373   eval_loss = 0.3522861067737852
2022-06-12 18:25:10,373   global_step = 19499
2022-06-12 18:25:10,373   loss = 0.06879584673878143
2022-06-12 18:25:34,640 ***** Running evaluation *****
2022-06-12 18:25:34,641   Epoch = 9 iter 19599 step
2022-06-12 18:25:34,641   Num examples = 872
2022-06-12 18:25:34,641   Batch size = 32
2022-06-12 18:25:35,394 ***** Eval results *****
2022-06-12 18:25:35,394   acc = 0.8899082568807339
2022-06-12 18:25:35,394   cls_loss = 0.06877730279419217
2022-06-12 18:25:35,394   eval_loss = 0.35410909274859087
2022-06-12 18:25:35,394   global_step = 19599
2022-06-12 18:25:35,394   loss = 0.06877730279419217
2022-06-12 18:25:59,657 ***** Running evaluation *****
2022-06-12 18:25:59,657   Epoch = 9 iter 19699 step
2022-06-12 18:25:59,657   Num examples = 872
2022-06-12 18:25:59,658   Batch size = 32
2022-06-12 18:26:00,409 ***** Eval results *****
2022-06-12 18:26:00,409   acc = 0.8899082568807339
2022-06-12 18:26:00,409   cls_loss = 0.06861978558381607
2022-06-12 18:26:00,409   eval_loss = 0.35699911415576935
2022-06-12 18:26:00,409   global_step = 19699
2022-06-12 18:26:00,409   loss = 0.06861978558381607
2022-06-12 18:26:24,685 ***** Running evaluation *****
2022-06-12 18:26:24,685   Epoch = 9 iter 19799 step
2022-06-12 18:26:24,685   Num examples = 872
2022-06-12 18:26:24,685   Batch size = 32
2022-06-12 18:26:25,434 ***** Eval results *****
2022-06-12 18:26:25,434   acc = 0.8899082568807339
2022-06-12 18:26:25,435   cls_loss = 0.06850220069185607
2022-06-12 18:26:25,435   eval_loss = 0.3539947279329811
2022-06-12 18:26:25,435   global_step = 19799
2022-06-12 18:26:25,435   loss = 0.06850220069185607
2022-06-12 18:26:49,681 ***** Running evaluation *****
2022-06-12 18:26:49,681   Epoch = 9 iter 19899 step
2022-06-12 18:26:49,681   Num examples = 872
2022-06-12 18:26:49,681   Batch size = 32
2022-06-12 18:26:50,432 ***** Eval results *****
2022-06-12 18:26:50,432   acc = 0.8887614678899083
2022-06-12 18:26:50,432   cls_loss = 0.06846415454198762
2022-06-12 18:26:50,433   eval_loss = 0.34923397935926914
2022-06-12 18:26:50,433   global_step = 19899
2022-06-12 18:26:50,433   loss = 0.06846415454198762
2022-06-12 18:27:14,676 ***** Running evaluation *****
2022-06-12 18:27:14,676   Epoch = 9 iter 19999 step
2022-06-12 18:27:14,676   Num examples = 872
2022-06-12 18:27:14,677   Batch size = 32
2022-06-12 18:27:15,427 ***** Eval results *****
2022-06-12 18:27:15,428   acc = 0.8887614678899083
2022-06-12 18:27:15,428   cls_loss = 0.06843665464030384
2022-06-12 18:27:15,428   eval_loss = 0.348676844100867
2022-06-12 18:27:15,428   global_step = 19999
2022-06-12 18:27:15,428   loss = 0.06843665464030384
2022-06-12 18:27:39,710 ***** Running evaluation *****
2022-06-12 18:27:39,710   Epoch = 9 iter 20099 step
2022-06-12 18:27:39,710   Num examples = 872
2022-06-12 18:27:39,710   Batch size = 32
2022-06-12 18:27:40,461 ***** Eval results *****
2022-06-12 18:27:40,462   acc = 0.8876146788990825
2022-06-12 18:27:40,462   cls_loss = 0.06841778813485738
2022-06-12 18:27:40,462   eval_loss = 0.350421185738274
2022-06-12 18:27:40,462   global_step = 20099
2022-06-12 18:27:40,462   loss = 0.06841778813485738
2022-06-12 18:28:04,687 ***** Running evaluation *****
2022-06-12 18:28:04,687   Epoch = 9 iter 20199 step
2022-06-12 18:28:04,688   Num examples = 872
2022-06-12 18:28:04,688   Batch size = 32
2022-06-12 18:28:05,438 ***** Eval results *****
2022-06-12 18:28:05,438   acc = 0.8899082568807339
2022-06-12 18:28:05,438   cls_loss = 0.06836804454395531
2022-06-12 18:28:05,438   eval_loss = 0.3552048924778189
2022-06-12 18:28:05,438   global_step = 20199
2022-06-12 18:28:05,438   loss = 0.06836804454395531
2022-06-12 18:28:29,657 ***** Running evaluation *****
2022-06-12 18:28:29,657   Epoch = 9 iter 20299 step
2022-06-12 18:28:29,657   Num examples = 872
2022-06-12 18:28:29,657   Batch size = 32
2022-06-12 18:28:30,408 ***** Eval results *****
2022-06-12 18:28:30,408   acc = 0.8887614678899083
2022-06-12 18:28:30,408   cls_loss = 0.06827365718262648
2022-06-12 18:28:30,408   eval_loss = 0.3519060018339327
2022-06-12 18:28:30,408   global_step = 20299
2022-06-12 18:28:30,408   loss = 0.06827365718262648
2022-06-12 18:28:54,737 ***** Running evaluation *****
2022-06-12 18:28:54,738   Epoch = 9 iter 20399 step
2022-06-12 18:28:54,738   Num examples = 872
2022-06-12 18:28:54,738   Batch size = 32
2022-06-12 18:28:55,489 ***** Eval results *****
2022-06-12 18:28:55,489   acc = 0.8899082568807339
2022-06-12 18:28:55,489   cls_loss = 0.06821471971656391
2022-06-12 18:28:55,489   eval_loss = 0.35016554221510887
2022-06-12 18:28:55,489   global_step = 20399
2022-06-12 18:28:55,489   loss = 0.06821471971656391
2022-06-12 18:29:19,742 ***** Running evaluation *****
2022-06-12 18:29:19,742   Epoch = 9 iter 20499 step
2022-06-12 18:29:19,742   Num examples = 872
2022-06-12 18:29:19,742   Batch size = 32
2022-06-12 18:29:20,493 ***** Eval results *****
2022-06-12 18:29:20,493   acc = 0.8876146788990825
2022-06-12 18:29:20,493   cls_loss = 0.06819257341558858
2022-06-12 18:29:20,493   eval_loss = 0.35078347633991924
2022-06-12 18:29:20,493   global_step = 20499
2022-06-12 18:29:20,494   loss = 0.06819257341558858
2022-06-12 18:29:44,810 ***** Running evaluation *****
2022-06-12 18:29:44,810   Epoch = 9 iter 20599 step
2022-06-12 18:29:44,810   Num examples = 872
2022-06-12 18:29:44,811   Batch size = 32
2022-06-12 18:29:45,561 ***** Eval results *****
2022-06-12 18:29:45,561   acc = 0.8887614678899083
2022-06-12 18:29:45,561   cls_loss = 0.06829265469905847
2022-06-12 18:29:45,562   eval_loss = 0.3479081996317421
2022-06-12 18:29:45,562   global_step = 20599
2022-06-12 18:29:45,562   loss = 0.06829265469905847
2022-06-12 18:30:09,921 ***** Running evaluation *****
2022-06-12 18:30:09,921   Epoch = 9 iter 20699 step
2022-06-12 18:30:09,921   Num examples = 872
2022-06-12 18:30:09,922   Batch size = 32
2022-06-12 18:30:10,672 ***** Eval results *****
2022-06-12 18:30:10,672   acc = 0.8887614678899083
2022-06-12 18:30:10,672   cls_loss = 0.06824680133360865
2022-06-12 18:30:10,672   eval_loss = 0.34974062176687376
2022-06-12 18:30:10,672   global_step = 20699
2022-06-12 18:30:10,672   loss = 0.06824680133360865
2022-06-12 18:30:34,901 ***** Running evaluation *****
2022-06-12 18:30:34,901   Epoch = 9 iter 20799 step
2022-06-12 18:30:34,901   Num examples = 872
2022-06-12 18:30:34,901   Batch size = 32
2022-06-12 18:30:35,654 ***** Eval results *****
2022-06-12 18:30:35,655   acc = 0.8887614678899083
2022-06-12 18:30:35,655   cls_loss = 0.0681665008942638
2022-06-12 18:30:35,655   eval_loss = 0.35277521131294115
2022-06-12 18:30:35,655   global_step = 20799
2022-06-12 18:30:35,655   loss = 0.0681665008942638
2022-06-12 18:30:59,866 ***** Running evaluation *****
2022-06-12 18:30:59,867   Epoch = 9 iter 20899 step
2022-06-12 18:30:59,867   Num examples = 872
2022-06-12 18:30:59,867   Batch size = 32
2022-06-12 18:31:00,617 ***** Eval results *****
2022-06-12 18:31:00,617   acc = 0.8887614678899083
2022-06-12 18:31:00,617   cls_loss = 0.06811050528312179
2022-06-12 18:31:00,618   eval_loss = 0.3520737285060542
2022-06-12 18:31:00,618   global_step = 20899
2022-06-12 18:31:00,618   loss = 0.06811050528312179
2022-06-12 18:31:24,855 ***** Running evaluation *****
2022-06-12 18:31:24,855   Epoch = 9 iter 20999 step
2022-06-12 18:31:24,855   Num examples = 872
2022-06-12 18:31:24,855   Batch size = 32
2022-06-12 18:31:25,604 ***** Eval results *****
2022-06-12 18:31:25,604   acc = 0.8887614678899083
2022-06-12 18:31:25,604   cls_loss = 0.06813765637002347
2022-06-12 18:31:25,604   eval_loss = 0.3518938339714493
2022-06-12 18:31:25,604   global_step = 20999
2022-06-12 18:31:25,605   loss = 0.06813765637002347
2022-06-12 18:31:35,535 **************S*************
task_name = sst-2
best_metirc = 0.8990825688073395
**************E*************

2022-06-12 18:31:35,714 Task finish! 
2022-06-12 18:31:35,714 Task cost 88.41646659999999 minutes, i.e. 1.4736077833333332 hours. 
